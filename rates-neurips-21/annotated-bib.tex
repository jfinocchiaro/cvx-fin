\documentclass{article}

%\usepackage[preprint]{neurips_2021}  % for arxiv
%\usepackage[final]{neurips_2021}     % camera ready
\usepackage[nonatbib]{neurips_2021}             % USE THIS for paper submission

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage[numbers, sort, compress]{natbib}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}

\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
\usepackage{bbm}
\usepackage{amsmath,amsthm,amsfonts,amssymb}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\newtheorem*{theorem*}{Theorem}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{proposition*}{Proposition}

\theoremstyle{definition}\newtheorem{definition}{Definition}
\theoremstyle{definition}\newtheorem{assumption}{Assumption}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{BTW: #1}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
%\ifnum\Comments=1               % fix margins for todonotes
%  \setlength{\marginparwidth}{1in}
%\fi


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\defeq}{\doteq}%\vcentcolon=} % define equals

\newcommand{\prop}[1]{\mathrm{prop}[#1]}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\DeclareMathOperator{\E}{\mathbb{E}}  % expectation
%\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
% \DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\regret}[3]{R_{#1}(#2,#3)}
\newcommand{\Reg}{\mathrm{Regret}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Annotated Bibliography: Surrogate Regret Bounds for Polyhedral Losses}
%Rate Transfers with Polyhedral Surrogate Losses
% \And separates authors; \AND separates with forced line break
\author{%
  Rafael Frongillo \\
  U. Colorado, Boulder \\
  \texttt{raf@colorado.edu} \\
  \And
  Bo Waggoner \\
  U. Colorado, Boulder \\
  \texttt{bwag@colorado.edu}
}

\begin{document}

\maketitle


% Yet that several commonly-used surrogates are actually not consistent for 0-1 loss.
% For example, the common practice of one-versus-all hinge loss is actually consistent for multiclass classification with an abstain option;  Related: hierarchical classification, which also has a polyhedral surrogate and linear regret bound.  Our results would have greatly simplified the development of these regret bounds; we describe how to recover the main BEP bound in \S.

% Finally, structured prediction be pumpin out surrogates.... general results... but usually quadratic losses.  Show lower bounds like our Thm 2 in those two papers.

% Surrogate regret bounds have been well studied in some settings.
% It is known that $\zeta$ \raf{examples: logistic, exponential, and hinge}.
% Some general bounds are known for classes of surrogates, e.g. \raf{refs}.
% Lacking, however, is a general understanding of what $\zeta$ one can expect from a surrogates, especially for arbitrary target losses.

% 1. super general
% 2. lower bound could explain why you get sqrt or linear
%   - take exp: not strong convex or strong smooth, so why sqrt??
% 3. polyhedral pipeline


Some references by setting.  P = polyhedral.  Surrogate regret bound: L = linear, Q = quadratic (sqrt).  $\leq$ = gives a ``lower bound'' for the surrogate regret too.
\begin{itemize}
\item Binary classification.
  \begin{itemize}
  \item \citet{zhang2004statistical} -- early analysis, didn't use ``surrogate'' but had some quantitative bounds that implied consistency
  \item \citet{bartlett2006convexity} -- tight bounds for \textbf{all margin losses}
  \item \citet{reid2009surrogate} -- tight bounds for \textbf{all proper composite losses}
  \item \citet{mahdavi2014binary} -- consider the generalization rate of the surrogate together with the ``excess risk bound'' (surrogate regret bound) (\textbf{and} the optimization actually), both as a function of the smoothness parameter of the surrogate.  They don't seem to give lower bounds on the excess risk bound though, just a degrading upper bound on it.  Still, very close to our work (and more generally the ``full stack'' idea!) in spirit.
  \end{itemize}
  
\item Weighted / cost-sensitive variants
  \begin{itemize}
  \item \citet{scott2011surrogate} -- weird example-dependent problem
  \item \citet{scott2012calibrated} -- general cost-sensitive binary classification
  \end{itemize}
  
\item Bipartite ranking (which is basically a set of pairwise binary classifications)
  \begin{itemize}
  \item \citet{agarwal2014surrogate} -- builds on binary classification losses (Mark + Bob), introduces strongly proper
  \item \citet{menon2016bipartite} -- different sort of regret bound, same problem
  \end{itemize}
  
\item Abstain and hierarchical
  \begin{itemize}
  \item[P,L] \citet{ramaswamy2018consistent} -- Abstain paper; shows bounds for 3 different surrogates, all \textbf{linear}, each ``from scratch''.
  \item[Q] \citet{yuan2010classification} -- Smooth? losses and characterizations for abstain; \textbf{binary classification only}
  \item[P,L] \citet{bartlett2008classification} -- The OG abstain paper, with a polyhedral hinge-like loss.  \textbf{Linear} regret bound.
  \item[P,L] \citet{ramaswamy2015hierarchical} -- Nested OvA surrogate; linear bound
  \end{itemize}

\item Ordinal regression
  \begin{itemize}
  \item \citet{pedregosa2017consistency} -- Pretty fascinating paper actually, about ordinal regression, i.e., when the label is the ordinal rank of the outcome, and how to construct surrogates.  For a class of surrogates based on binary classification, which is also cool, they get similar excess risk bounds to \citet{bartlett2006convexity}.
  \end{itemize}

\item List ranking
  \begin{itemize}
  \item[Q] \citet[Theorem 10]{ravikumar2011ndcg}
  \end{itemize}
  
\item Structured prediction (covering lots of target problems simultaneously)
  \begin{itemize}
  \item[Q,$\leq$] \citet[Theorem 7]{osokin2017structured} -- Prove excess risk bounds for a broad family of structured prediction problems, using a specific quadratic surrogate.  Also give an upper bound on the calibration function, i.e. a lower bound in the sense that we prove (Theorem 8).
  \item[Q] \citet{ciliberto2016consistent} -- Calibration function for a similar quadratic surrogate, for a similarly general class of structed pred problems.
  \item[Q,$\leq$] \citet[Theorem 4.4]{nowak2019general} -- Even more general than the previous Bach paper \cite{osokin2017structured} (e.g. includes ordinal regression).  Quadratic risk bound for a proper loss for the mean from a strongly-convex potential/entropy.
    Also give a matching lower bound in our sense (Theorem 4.5).
  \item[Q] \citet[Proposition 5]{blondel2019structured} -- Tighter losses, still quadratic, still quadratic excess risk.
  \end{itemize}

\item Polyhedral surrogates without risk/regret bounds
  \begin{itemize}
  \item \citet{wang2020weston} -- Weston--Watkins: calibrated for ordered partition
  \item \citet{yu2018lovasz} -- Lov\'asz hinge: calibrated for an abstain-like loss (from our paper)
  \item Top-$k$ losses, like \citet{lapin2016loss} and any others from \citet{yang2018consistency}
  \end{itemize}
  
\item Fast rates
  \begin{itemize}
  \item \citet{koren2015fast} -- fast rates for exp-concave surrogates ($O(d/n)$ where $d$ is the dimension) .  Note here exp-concavex is in the \emph{parameter}.  So $F(\theta) = L(h_\theta(x),y)$ should be exp-concave in $\theta$.  For $\alpha>0$ we say $f$ is $\alpha$-exp-concave if $x \mapsto \exp(-\alpha f(x))$ is concave.
  \item \citet{grunwald2015fast} -- Tim et al paper than Anish referenced!  
  \end{itemize}
  
\item Misc / didn't read
  \begin{itemize}
  \item Excess Risk Bounds for Exponentially Concave Losses \url{https://arxiv.org/pdf/1401.4566.pdf}
  \end{itemize}
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
