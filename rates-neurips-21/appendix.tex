
\appendix

\section{Omitted Proofs - Upper Bound} \label{app:upper}
This section contains omitted proofs from Section \ref{sec:upper}.

\begin{lemma*}[Lemma \ref{lemma:fixed-p}]
  Let $\ell: \R \to \reals_+^{\Y}$ be a discrete target loss and suppose the surrogate $L: \reals^d \to \reals_+^{\Y}$ and link $\psi: \reals^d \to \R$ are calibrated for $\ell$.
  Then for any $p \in \simplex$, there exists $\alpha_p \geq 0$ such that, for all $u \in \reals^d$,
    \[ R_{\ell}(\psi(u),p) \leq \alpha_p R_L(u,p) . \]
\end{lemma*}
\begin{proof}
  Fix $p \in \simplex$.
  Let $C_p = \max_{r \in \R} R_{\ell}(r,p)$.
  The maximum exists because $\ell$ is discrete, i.e. $\R$ is finite.
  Meanwhile, recall that, when defining calibration, we let $B_{L,\psi,\ell}(p) = \{R_L(u,p) ~:~ \psi(u) \not\in \gamma(p)\}$.
  Let $B_p = \inf B_{L,\psi,\ell}(p)$.
  By definition of calibration, we have $B_p > 0$.

  To combine these bounds, let $\alpha_p = \frac{C_p}{B_p}$.
  Let $u \in \reals^d$.
  There are two cases.
  If $\psi(u) \in \gamma(p)$, then $R_{\ell}(\psi(u),p) = 0 \leq R_L(u,p)$ immediately.
  If $\psi(u) \not\in \gamma(p)$, then
  \begin{align*}
    R_{\ell}(\psi(u),p)
    &\leq C_p \\
    &=    \alpha_p \cdot B_p  \\
    &\leq \alpha_p R_L(u,p) .
  \end{align*}
\end{proof}

\begin{lemma*}[Lemma \ref{lemma:refines}]
  If $(L,\psi)$ indirectly elicits $\ell$, then $\Gamma = \prop{L}$ \emph{refines} $\gamma = \prop{\ell}$ in the sense that, for all $u \in \reals^d$, there exists $r \in \R$ such that $\Gamma_u \subseteq \gamma_r$.
\end{lemma*}
\begin{proof}
  For any $u$, let $r = \psi(u)$.
  By indirect elicitation, $u \in \Gamma(p) \implies r \in \gamma(p)$.
  So $\Gamma_u = \{p ~:~ u \in \Gamma(p)\} \subseteq \{p ~:~ r \in \gamma(p)\} = \gamma_r$.
\end{proof}

\begin{lemma*}[Lemma \ref{lemma:linear-on-levelset}]
  Suppose $(L,\psi)$ indirectly elicits $\ell$ and let $\Gamma = \prop{L}$.
  Then for any fixed $u,u^* \in \reals^d$ and $r \in \R$, the functions $R_L(u,\cdot)$ and $R_{\ell}(r,\cdot)$ are linear in their second arguments on $\Gamma_{u^*}$.
\end{lemma*}
\begin{proof}
  Let $u^* \in \reals^d$ and $p \in \Gamma_{u^*}$.
  By definition, for all $p \in \Gamma_{u^*}$, $\risk{L}(p) = \inprod{p}{L(u^*)}$.
  So for fixed $u$,
    \[ R_L(u,p) = \inprod{p}{L(u)} - \inprod{p}{L(u^*)} = \inprod{p}{L(u) - L(u^*)} , \]
  a linear function of $p$ on $\Gamma_{u^*}$.
  Next, by Lemma \ref{lemma:refines}, there exists $r^*$ such that $\Gamma_{u^*} \subseteq \gamma_{r^*}$.
  By the same argument, for fixed $r$, $R_{\ell}(r,p) = \inprod{p}{\ell(r) - \ell(r^*)}$, a linear function of $p$ on $\gamma_{r^*}$ and thus on $\Gamma_{u^*}$.
\end{proof}

\begin{lemma*}[Lemma \ref{lemma:polyhedral-finite}]
  If $L: \reals^d \to \reals_+^{\Y}$ is polyhedral, then $\Gamma = \prop{L}$ has a finite set of level sets that union to $\simplex$. Also, these level sets are polytopes.
\end{lemma*}
\begin{proof}
  This follows from results in \cite{finocchiaro2019embedding}.
  \botodo{I'll go ahead and prove this, because it's the main upper bound. TODO}
%  We rely on concepts in \cite{finocchiaro2019embedding}.
%  If $L$ is polyhedral, then define $G: \simplex \to \reals$ to be the negative of its Bayes risk, i.e. $G(p) = -\risk{L}(p) = - \min_{u \in \reals^d} \inprod{p}{L(u)}$.
%  We claim $G$ is a polyhedral function.\bo{why?}
%  \bo{TODO}
\end{proof}

\begin{theorem*}[Theorem \ref{thm:main-upper-details}]
  Suppose the surrogate loss $L: \reals^d \to \reals_+^{\Y}$ and link $\psi: \reals^d \to \R$ are consistent for the target loss $\ell: \R \to \reals_+^{\Y}$.
  If $L$ is polyhedral, then $(L,\psi)$ guarantee a linear regret transfer for $\ell$, i.e. there exists $\alpha \geq 0$ such that, for all $\D$ and all measurable $h: \X \to \reals^d$,
    \[ R_{\ell}(\psi \circ h ; \D) \leq \alpha R_L(h ; \D) . \]
\end{theorem*}
\begin{proof}
  We first recall that by Fact \ref{fact:consistent-calibrated-elicits}, consistency implies that $(L,\psi)$ are calibrated for $\ell$ and that $(L,\psi)$ indirectly elicit $\ell$.
  Next, by Observation \ref{obs:transfer}, it suffices to show a linear \emph{conditional} regret transfer, i.e. for all $p \in \simplex$ and $u \in \reals^d$, we show $R_{\ell}(\psi(u),p) \leq \alpha R_L(u,p)$.
  
  By Lemma \ref{lemma:polyhedral-finite}, the polyhedral loss $L$ has a finite set $U \subset \reals^d$ of predictions such that (a) for each $u \in U$, the level set $\Gamma_u$ is a polytope, and (b) $\cup_{u \in U} \Gamma_u = \simplex$.
  Let $\mathcal{Q}_u \subset \simplex$ be the finite set of vertices of the polytope $\Gamma_u$, and define the finite set $\mathcal{Q} = \cup_{u \in U} \mathcal{Q}_u$.
  
  By Lemma \ref{lemma:fixed-p}, for each $q \in \mathcal{Q}$, there exists $\alpha_q \geq 0$ such that $R_{\ell}(\psi(u),q) \leq \alpha_q R_L(u,q)$ for all $u$.
  We choose
    \[ \alpha = \max_{q \in \mathcal{Q}} \alpha_q . \]
  To prove the conditional regret transfer, consider any $p \in \simplex$ and any $u \in \reals^d$.
  There exists $u \in U$ such that $p \in \Gamma_u$, a polytope.
  So we can write $p$ as a convex combination of its vertices, i.e.
    \[ p = \sum_{q \in \mathcal{Q}_u} \beta(q) q \]
  for some probability distribution $\beta$.
  Recall that $\mathcal{Q}_u \subseteq \Gamma_u$ and $R_L$ and $R_{\ell}$ are linear in $p$ on $\Gamma_u$ by Lemma \ref{lemma:linear-on-levelset}.
  So, for any $u'$:
  \begin{align*}
    R_{\ell}(\psi(u'),p)
    &=    R_{\ell}\left(\psi(u') ~,~ \sum_{q \in \mathcal{Q}_u} \beta(q) q\right)  \\
    &=    \sum_{q \in \mathcal{Q}_u} \beta(q) R_{\ell}(\psi(u'),q)  \\
    &\leq \sum_{q \in \mathcal{Q}_u} \beta(q) \alpha_{q} R_L(u',q)  \\
    &\leq \alpha \sum_{q \in \mathcal{Q}_u} \beta(q) R_L(u',q)  \\
    &=    \alpha R_L(u', p) .
  \end{align*}
\end{proof}




\section{Omitted Proofs - Lower Bound} \label{app:lower}
This section contains omitted proofs from Section \ref{sec:lower}.

\begin{theorem*}[Theorem \ref{thm:main-lower-details}]
  Suppose the surrogate loss $L$ and link $\psi$ satisfy a regret transfer of $\zeta$ for a target loss $\ell$.
  If $L$, $\psi$, and $\ell$ satisfy Assumption \ref{assumption:lower}, then there exists $c > 0$ such that, for arbitrarily small $\epsilon$, $\zeta(\epsilon) \geq c \sqrt{\epsilon}$.
\end{theorem*}
By ``arbitrarily small'', we mean that for any $\epsilon_0 > 0$, we can find $\epsilon$ with $0 < \epsilon < \epsilon_0$ such that $\zeta(\epsilon) \geq c \sqrt{\epsilon}$.
Thus, there is no neighborhood of zero where $\zeta(\epsilon)$ shrinks faster than on the order of $\sqrt{\epsilon}$.

\emph{Proof outline:} By assumption we have a boundary point $u_0$ which is $L$-optimal for a distribution $p_0$.
We have some $r,r'$ which are both optimal for $p_0$, and $\psi(u_0) = r'$.
First, we will choose a $p_1$ where $r$ is uniquely optimal, hence $u_0$ is a strictly suboptimal choice.
We then consider a sequence of distributions $p_{\lambda} = (1-\lambda) p_0 + \lambda p_1$, approaching $p_0$ as $\lambda \to 0$.
For all such $p_{\lambda}$, it will happen that $r$ is optimal while $u_0$ and $r' = \psi(u_0)$ are strictly suboptimal.
We show that $R_{\ell}(r', p_{\lambda}) = c_{\ell} \lambda$ for some constant $c_{\ell}$ and all small enough $\lambda$.
Meanwhile, we will show that $R_L(u_0, p_{\lambda}) \leq O(\lambda^2)$, proving the result.
The last fact will use the properties of strong smoothness and strong convexity in a neighborhood of $u_0$.

\begin{proof}
  Obtain $\alpha, u_0, p_0, r, r'$, and an open neighborhood of $u_0$ from Assumption \ref{assumption:lower} and the definition of boundary point.
  Assume without loss of generality that $\psi(u_0) = r'$; otherwise, swap the roles of $r$ and $r'$.

  \paragraph{Linearity of $R_{\ell}(r',p_{\lambda})$.}
  As $\ell$ is non-redundant by assumption, there exists some $p_1 \in \inter{\gamma_r}$, the relative interior of the full-dimensional level set $\gamma_r$.
  We therefore have $R_\ell(r',p_1) = \inprod{p_1}{\ell(r')-\ell(r)} =: c_\ell > 0$, and $R_\ell(r',p_0) = 0$.
  Let $p_\lambda := (1-\lambda) p_0 + \lambda p_1$.
  By convexity of $\gamma_r$, we have $p_\lambda \in \gamma_r$ for all $\lambda \in [0,1]$, which gives $R_\ell(r',p_\lambda) = \lambda c_\ell$.

  \paragraph{Obtaining the global minimizer $u_{\lambda}$ of $L_{\lambda}$.}
  Let $L_\lambda:\reals^d\to\reals_+$ be given by $L_\lambda(u) = \inprod{p_\lambda}{L(u)} = (1-\lambda) \inprod{p_0}{L(u)} + \lambda \inprod{p_1}{L(u)}$.
  Let $\delta >0$ such that the above open neighborhood of $u_0$ contains the Euclidean ball $B_\delta(u_0)$ of radius $\delta$ around $u_0$.
  Let $u_1 \in \Gamma(p_1)$.
  We argue that for all small enough $\lambda$, $L_{\lambda}(u)$ is uniquely minimized by some $u_{\lambda} \in B_{\delta}(u_0)$.
  For any $u\notin B_\delta(u_0)$, we have, using local strong convexity and the optimality of $u_1$,
  \begin{align*}
    L_\lambda(u) - L_\lambda(u_0)
    &=
      (1-\lambda) \left( L_0(u) - L_0(u_0) \right)
      + \lambda \left( L_1(u) - L_1(u_0) \right)
    \\
    &\geq
      (1-\lambda) \left( \frac \alpha 2 \delta^2 \right)
      + \lambda \left( L_1(u_1) - L_1(u_0) \right)~  \\
    &> 0
  \end{align*}
  if $\lambda < \lambda^* := \alpha \delta^2 / (2 \alpha \delta^2 + 4 L_1(u_0) - 4 L_1(u_1))$.
  For the remainder of the proof, let $\lambda < \lambda^*$.
  Then any $u\notin B_{\delta}(u_0)$ has $L_{\lambda}(u) > L_{\lambda}(u_0)$, hence is suboptimal.
  By $\alpha$-strong convexity of $L_0$ on $B_\delta(u_0)$, $L_\lambda$ is strictly convex on $B_\delta(u_0)$.
  So it has a unique minimizer $u_{\lambda}$, and by the above argument this is the global minimizer of $L_{\lambda}$.
%  Extending our definition of $u_0$ and $u_1$, let $u_\lambda \in \Gamma(p_\lambda)$, which, as $\lambda \leq \lambda^*$ and thus $u_\lambda \in B_\delta(u_0)$, is the unique minimizer of $L_\lambda$ on $\reals^d$.
  % Moreover, by the previous paragraph, we have $u_\lambda \in B_\delta(u_0)$.
  Then $\risk{L}(p_\lambda) = L_\lambda(u_\lambda)$, and thus $R_L(u_0,p_\lambda) = L_\lambda(u_0) - L_\lambda(u_\lambda)$.

  \paragraph{Showing $R_L$ is quadratic in $\lambda$.}
  By assumption, the gradient of $L_y$ is locally Lipschitz for all $y\in\Y$.
  We will apply this fact to the compact set $\mathcal C = \{u \in \reals^d : \|u - u_1\| \leq \|u_0 - u_1\| + \delta\}$.
  By compactness, we have a finite subcover of open neighborhoods; let $\beta$ be the minimum Lipschitz constant over this finite set of neighborhoods.
  We thus have that $L_y$ is $\beta$-strongly smooth on $\mathcal C$, and hence so is $L_\lambda$ for any $\lambda \in [0,1]$.
%  \bo{I thought maybe strong smoothness is equivalent to locally lipschitz gradient? If so this gets easier and it lets us assume the parameter $\beta$ from the beginning, which is nice because we get a somewhat constructive $c$ at the end.}
  
  We now upper bound $\|u_\lambda - u_0\|_2$, and then apply strong smoothness to upper bound $L_\lambda(u_0) - L_\lambda(u_\lambda)$.
  Consider the first-order optimality condition of $L_\lambda$:
  \begin{align*}
    \label{eq:first-order-opt-smooth}
    & 0 = \nabla L_\lambda(u_\lambda) = (1-\lambda) \nabla L_0(u_\lambda) + \lambda \nabla L_1(u_\lambda)
    \\
    & \implies (1-\lambda) \|\nabla L_0(u_\lambda)\|_2 = \lambda \|\nabla L_1(u_\lambda)\|_2~.
  \end{align*}
  By optimality of $u_0$ and $u_1$, strong convexity of $L_0$ and strong smoothness of $L_1$, and the triangle inequality, we have
  \begin{align*}
    \|\nabla L_0(u_\lambda)\|_2 &= \|\nabla L_0(u_\lambda) - \nabla L_0(u_0)\|_2 \geq \alpha \|u_\lambda - u_0\|_2~,
    \\
    \|\nabla L_1(u_\lambda)\|_2 &= \|\nabla L_1(u_\lambda) - \nabla L_1(u_1)\|_2 \leq \beta \|u_\lambda - u_1\|_2
    \\
    &\leq \beta \left( \|u_\lambda - u_0\|_2 + \|u_0 - u_1\|_2 \right)~.
  \end{align*}
  Combining,
  \begin{align*}
    (1-\lambda) \alpha \|u_\lambda - u_0\|_2
    &\leq
      (1-\lambda) \|\nabla L_0(u_\lambda)\|_2
    \\
    &= \lambda \|\nabla L_1(u_\lambda)\|_2
    \\
    &\leq
      \lambda \beta \left( \|u_\lambda - u_0\|_2 + \|u_0 - u_1\|_2 \right)~.
  \end{align*}
  Now rearranging and taking $\lambda \leq \tfrac 1 2 \tfrac {\alpha}{\alpha+\beta}$, we have
  \begin{align*}
    \|u_\lambda - u_0\|_2 \leq \frac{\lambda\beta}{(1-\lambda)\alpha-\lambda\beta} \|u_0 - u_1\|_2  \leq \lambda \frac{2\beta}{\alpha} \|u_0 - u_1\|_2 ~.
  \end{align*}
  Finally, from strong smoothness of $L_\lambda$ and optimality of $u_\lambda$,
  \begin{align*}
    L_\lambda(u_0) - L_\lambda(u_\lambda) \leq \frac{\beta}{2} \|u_0 - u_\lambda\|_2^2 \leq \frac{\beta}{2} \left(\lambda \frac{2\beta}{\alpha} \|u_0 - u_1\|_2\right)^2 = c_L \lambda^2~,
  \end{align*}
  where $c_L = \frac{2\beta^3}{\alpha^2} \|u_0 - u_1\|_2^2$.
  Take $c = \frac{c_\ell}{\sqrt{c_L}} = \frac{c_\ell \alpha}{\|u_0 - u_1\|_2 \sqrt{2\beta^3}}$.
  Recall $R_\ell(r',p_\lambda) = c_\ell \lambda$ and $R_L(u_0,p_\lambda) \leq c_L \lambda^2$.
  Then letting $\epsilon = R_L(u_0,p_\lambda)$, we have $\zeta(\epsilon) \geq R_\ell(r',p_\lambda) \geq c \sqrt{\epsilon}$.
\end{proof}


\section{Omitted Proofs - Constant Derivation} \label{app:constant}
This section contains omitted proofs from Section \ref{sec:constant}.

\paragraph{Hoffman constants.}
First we appeal to a known fact, the existence of Hoffman constants for systems of linear inequalities.
\begin{theorem}[Hoffman constant, \bo{cite}]
  \label{thm:hoffman}
  Given a matrix $A\in\reals^{m\times n}$, there exists some smallest $H(A)\geq 0$, called the \emph{Hoffman constant} (with respect to $\|\cdot\|_\infty$), such that for all $b\in\reals^m$ and all $x\in\reals^n$,
  \begin{equation}
    \label{eq:hoffman}
    d_\infty(x,S(A,b)) \leq H(A) \|(A x - b)_+\|_\infty~,
  \end{equation}
  where $S(A,b) = \{x\in\reals^n \mid A x \leq b\}$ and $(u)_+ \defeq \max(u,0)$ component-wise.
\end{theorem}

\begin{lemma*}[Lemma \ref{lemma:hoffman-polyhedral}]
  Let $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral loss with $\Gamma = \prop{L}$.
  Then for any fixed $p$, there exists some smallest constant $H_{L,p} \geq 0$ such that $d_{\infty}(u,\Gamma(p)) \leq H_{L,p} R_L(u,p)$ for all $u \in \reals^d$.
\end{lemma*}
\begin{proof}
  Since $L$ is polyhedral, there exist $a_1,\ldots,a_m \in \reals^d$ and $c\in\reals^m$ such that we may write $\inprod{p}{L(u)} = \max_{1\leq j\leq m} a_j \cdot u + c_j$.
  Let $A \in \reals^{m\times d}$ be the matrix with rows $a_j$, and let $b = \risk{L}(p)\ones - c$, where $\ones\in\reals^m$ is the all-ones vector.
  Then we have
  \begin{align*}
    S(A,b)
    &\defeq \{u\in\reals^d \mid A u \leq b\}
    \\
    &= \{u\in\reals^d \mid A u + c \leq \risk{L}(p)\ones\}
    \\
    &= \{u\in\reals^d \mid \forall i\, (A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \max_i \;(A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \inprod{p}{L(u)} \leq \risk{L}(p)\}
    \\
    & = \Gamma(p)~.
  \end{align*}
  Similarly, we have $\max_i\; (A u - b)_i = \inprod{p}{L(u)} - \risk{L}(p) = \regret{L}{u}{p} \geq 0$.
  Thus,
  \begin{align*}
    \|(Au - b)_+\|_\infty
    &= \max_i\; ((Au - b)_+)_i
    \\
    &= \max((Au - b)_1,\ldots,(Au - b)_m, 0)
    \\
    &= \max(\max_i\; (Au - b)_i, \, 0)
    \\
    &= \max_i\; (Au - b)_i
    % \\
    % &= \inprod{p}{L(u)} - \risk{L}(p)
    \\
    &= \regret{L}{u}{p}~.
  \end{align*}
  Now applying Theorem~\ref{thm:hoffman}, we have
  \begin{align*}
    d_\infty(u,\Gamma(p))
    &=    d_\infty(u,S(A,b))
    \\
    &\leq H(A) \|(Au-b)_+\|_\infty
    \\
    &= H(A) \regret{L}{u}{p}~.
  \end{align*}
\end{proof}

\paragraph{Separated links.}

\begin{lemma*}[Lemma \ref{lemma:calibrated-eps-sep}]
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is calibrated with respect to $\ell$.
  Then there exists $\epsilon>0$ such that $\psi$ is $\epsilon$-separated with respect to   $\Gamma \defeq \prop{L}$ and $\gamma \defeq \prop{\ell}$.
\end{lemma*}
\begin{proof}
  Suppose that $\psi$ is not $\epsilon$-separated for any $\epsilon>0$.
  Then letting $\epsilon_i \defeq 1/i$ we have sequences $\{p_i\}_i \subset \simplex$ and  $\{u_i\}_i \subset \reals^d$ such that for all $i\in\mathbb N$ we have both $\psi(u_i) \notin \gamma(p_i)$ and $d_\infty(u_i,\Gamma(p_i)) \leq \epsilon_i$.
  First, observe that there are only finitely many values for $\gamma(p_i)$ and $\Gamma(p_i)$, as $\R$ is finite and $L$ is polyhedral.
  \raft{Using a result from the polyhedral paper here, that $\Gamma$ only takes on finitely many values}
  Thus, there must be some $p\in\simplex$ and some infinite subsequence indexed by $j\in J \subseteq \mathbb N$ where
  for all $j\in J$, we have $\psi(u_j) \notin \gamma(p)$ and $\Gamma(p_j) = \Gamma(p)$.

  Next, observe that, as $L$ is polyhedral, the expected loss $\inprod{p}{L(u)}$ is $\beta$-Lipschitz in $\|\cdot\|_\infty$ for some $\beta>0$.
  \raft{Could write $\beta$ explicitly; it's in my notes}
  Thus, for all $j\in J$, we have
  \begin{align*}
    d_\infty(u_i,\Gamma(p)) \leq \epsilon_j
    &\implies \exists u^*\in\Gamma(p) \|u_j-u^*\|_\infty \leq \epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \inprod{p}{L(u^*)} \right| \leq \beta\epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \risk{L}(p) \right| \leq \beta\epsilon_j~.
  \end{align*}
  Finally, for this $p$, we have
  \begin{align*}
    \inf_{u:\psi(u)\notin\gamma(p)} \inprod{p}{L(u)}
    \leq
    \inf_{j\in J} \inprod{p}{L(u_j)}
    =
    \risk{L}(p)~,
  \end{align*}
  contradicting the calibration of $\psi$.
\end{proof}

\paragraph{Combining the loss and link.}
\begin{lemma*}[Lemma \ref{lemma:separated-constant-p}]
  Let $\ell: \R \to \reals_+^{\Y}$ be a discrete target loss, $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral surrogate loss, and $\psi: \reals^d \to \R$ a link function.
  If $(L,\psi)$ indirectly elicit $\ell$ and $\psi$ is $\epsilon$-separated, then for all $u$ and $p$,
    \[ R_{\ell}(\psi(u),p) \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p) . \]
\end{lemma*}
\begin{proof}
  If $\psi(u) \in \gamma(p)$, then $R_{\ell}(u,p) = 0$ and we are done.
  Otherwise, applying the definition of $\epsilon$-separated and Lemma \ref{lemma:hoffman-polyhedral},
  \begin{align*}
    \epsilon &<    d_{\infty}(u,\Gamma(p))  \\
             &\leq H_{L,p} R_L(u,p) .
  \end{align*}
  So $R_{\ell}(\psi(u),p) \leq C_{\ell} \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p)$.
\end{proof}

\begin{theorem*}[Constructive linear transfer, Theorem \ref{thm:separated-constant}]
  Let $\ell: \R \to \reals_+^{\Y}$ be a discrete target loss, $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral surrogate loss, and $\psi: \reals^d \to \R$ a link function.
  If $(L,\psi)$ are consistent for $\ell$, then
    \[ (\forall h,\D) \quad R_{\ell}(\psi \circ h ; \D) \leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(h ; \D) ~. \]
\end{theorem*}
The proof closely mirrors the proof of the nonconstructive upper bound, Theorem \ref{thm:main-upper}.
\begin{proof}
  By Lemma \ref{lemma:calibrated-eps-sep}, $\psi$ is separated and $\epsilon_{\psi}$ well-defined.
  By Lemma \ref{lemma:separated-constant-p}, for each $p \in \mathcal{Q}$, $R_{\ell}(\psi(u),p) \leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(u,p)$ for all $u$.
  Now consider a general $p$, which is in some full-dimensional polytope level set $\Gamma_u$.
  Write $p = \sum_{q \in \mathcal{Q}_u} \beta(q) q$ for some probability distribution $\beta$, where $\mathcal{Q}_u$ is the set of vertices of $\Gamma_u$.
  By Lemma \ref{lemma:linear-on-levelset}, $R_L$ and $R_{\ell}$ are linear in $p$ on $\Gamma_u$, so for any $u'$,
  \begin{align*}
    R_{\ell}(\psi(u'),p)
    &=    \sum_{q \in \mathcal{Q}_u} \beta(q) R_{\ell}(\psi(u'), q)  \\
    &\leq \sum_{q \in \mathcal{Q}_u} \beta(q) \frac{C_{\ell} H_{L,p}}{\epsilon_{\psi}} R_L(u', q)  \\
    &\leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} \sum_{q \in \mathcal{Q}_u} \beta(q) R_L(u', q)  \\
    &\leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(u', p) .
  \end{align*}
  This conditional regret transfer implies a full regret transfer with the same constant by Observation \ref{obs:transfer}.
\end{proof}





%\section{Omitted Jokes}
%
%Q: why is a surrogate loss function like a popup ad?
%
%A: minimizing either is inevitably futile.
%
%A': if designed correctly it takes you to Target.
%
%\vskip1em
%Q: Why is walking into a closed door more painful than one that is slightly ajar?
%
%A: hinge loss.
%
%\vskip1em
%Q: what do you call a loss function whose penalties are drawn uniformly at random?
%
%A: peer review.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "rates"
%%% End:
