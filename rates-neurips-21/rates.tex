\documentclass{article}

%\usepackage[preprint]{neurips_2021}  % for arxiv
%\usepackage[final]{neurips_2021}     % camera ready
\usepackage{neurips_2021}             % USE THIS for paper submission

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
\usepackage{bbm}
\usepackage{amsmath,amsthm,amsfonts,amssymb}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}\newtheorem{definition}{Definition}
\theoremstyle{definition}\newtheorem{assumption}{Assumption}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{BTW: #1}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
%\ifnum\Comments=1               % fix margins for todonotes
%  \setlength{\marginparwidth}{1in}
%\fi


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\defeq}{\doteq}%\vcentcolon=} % define equals

\newcommand{\prop}[1]{\mathrm{prop}[#1]}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\DeclareMathOperator{\E}{\mathbb{E}}  % expectation
%\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
% \DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\regret}[3]{R_{#1}(#2,#3)}
\newcommand{\Reg}{\mathrm{Regret}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Surrogate Regret Bounds for Polyhedral Losses}
%Rate Transfers with Polyhedral Surrogate Losses
% \And separates authors; \AND separates with forced line break
\author{%
  Rafael Frongillo \\
  U. Colorado, Boulder \\
  \texttt{raf@colorado.edu} \\
  \And
  Bo Waggoner \\
  U. Colorado, Boulder \\
  \texttt{bwag@colorado.edu}
}

\begin{document}

\maketitle

\begin{abstract}
  Surrogate risk minimization is an ubiquitous paradigm in supervised machine learning, wherein a target problem is solved by minimizing a surrogate loss on a dataset.
  Surrogate regret bounds are a common tool to prove generalization rates for surrogate risk minimization.
  While surrogate regret bounds have been developed for certain classes of loss functions, such as proper losses, general results are relatively sparse.
  We provide two general results.
  The first gives a linear surrogate regret bound for any polyhedral (piecewise-linear convex) surrogate, meaning that surrogate generalization rates translate directly to target rates.
  The second shows that for sufficiently non-polyhedral surrogates, the regret bound is a square root, meaning fast surrogate generalization rates translate to slow rates for the target.
  Together, these results suggest polyhedral surrogates are weakly optimal in many cases.
\end{abstract}

% \begin{abstract}
%   In supervised machine learning, often a target problem is solved by minimizing a surrogate loss on a dataset.
%   Study of convergence rates, which bound generalization error as a function of dataset size, typically neglects the relationship between the target and surrogate problems.
%   We consider the transfer of rate guarantees from the surrogate loss to the target problem.
%   We show that any consistent piecewise-linear convex (i.e. polyhedral) surrogate achieves a linear transfer rate, i.e. fast convergence of excess risk in the surrogate implies fast convergence in the target.
%   Simple counterexamples show that transfer rates can be much slower, e.g. quadratic, when the surrogate is smooth or strongly convex.
% \end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In supervised learning, our goal is to learn a hypothesis that solves some target problem given labeled data.
This target problem is given by a target loss $\ell$, which is often discrete in the sense that the prediction lies in a finite set, such as with classification, ranking, and structured prediction.
As minimizing the target $\ell$ over a data set is typically NP-hard, we instead turn to the general paradigm of \emph{surrogate risk minimization}.
Here a surrogate loss function $L$ is chosen and minimized instead of the target $\ell$.
A link function $\psi$ then maps the surrogate predictions to target predictions.

Naturally, a central question in surrogate risk minimization is how to choose the surrogate $L$.
A minimal requirement is statistical \emph{consistency}, which roughly says that approaching the best possible surrogate loss will eventually lead to the best possible target loss.
A more precise goal is to understand the rate at which this minimization occurs, called the \emph{generalization rate}: in terms of the number of data points, how quickly the target loss approaches the best possible.
In other words, if we define the \emph{regret} of a hypothesis to be its performance relative to the Bayes optimal hypothesis, we wish to know how quickly the $\ell$-regret approaches zero.

Surrogate regret bounds are a common tool to prove generalization rates for surrogate risk minimization.
The idea is that we have ample results to quantify the rate at which surrogate regret approaches zero, and if we could bound the target regret in terms of the surrogate regret, we would arrive at an overall generalization rate.
Roughly speaking, a surrogate regret bound takes the form
\begin{equation}
  \label{eq:surrogate-regret-bound-informal}
  \forall \text{ surrogate hypotheses } h, \quad \Reg_\ell(\psi\circ h) \leq \zeta(\, \Reg_L(h) \,)~,\qquad
\end{equation}
where the \emph{regret transfer function} $\zeta : \reals_+ \to \reals_+$ controls how surrogate regret translates to target regret.
For example, if we have a sequence of surrogate hypothesis $h_n$ achieving a fast rate $\Reg_L(h_n) = O(1/n)$, and $\zeta(\epsilon) = \sqrt{\epsilon}$, then we can guarante an overall slow rate of $\Reg_\ell(\psi \circ h_n) = O(1/\sqrt{n})$.

As the above example illustrates, understanding the behavior of $\zeta$ near $0$ is crucial to analyzing the overall generalization rate; were $\zeta$ linear near $0$, the overall rate would have been a fast $O(1/n)$.
\raft{Maybe a good place to cite the IEEE rates paper}
While much is known about surrogate regret bounds for important classes of losses, like proper losses\raf{Bob, others} and margin losses\raf{refs}, general results spanning very broad classes of loss functions are sparse.
Lacking such results, authors instead often prove bounds for specific losses from scratch, e.g. \raf{abstain}.

\botodo{I don't love the below phrasing. I think I prefer the approach I took of saying, wouldn't it be cool if we could get linear regret bounds? Actually, we can for this special class of surrogates. I'm not sold that this result is a step toward more general bounds. And it's interesting for its own reasons, not just as a step.}
\raft{Yeah, I think I agree.  This phrasing omits the coolness that polyhedral losses are basically optimal when it comes to the regret transfer.  Yet I do think our results are impressive for their generality---previous work assumed a lot of structure on the target problem, and on the form of the loss (I mean, arguably polyhedral is a lot of structure, but I'd still argue less than differentiable 1d margin losse).  Let's maybe try for a compromise of the two?}
We take a large step toward general surrogate regret bounds, with two broad results.
The first gives a linear surrogate regret bound for any polyhedral (piecewise-linear convex) surrogate, meaning that surrogate generalization rates translate directly to target rates.
  % \begin{theorem}[Linear bounds for polyhedral surrogates]
\begin{theorem}[Linear bound for polyhedral]
  \label{thm:main-upper}
  Let $L$ be any polyhedral surrogate and $\psi$ any link function such that $(L,\psi)$ is consistent for the target loss $\ell$.
  Then there is a linear regret transfer function $\zeta$.
  % Then $(L,\psi)$ guarantee a linear regret transfer function $\zeta$ to $\ell$.
  %Then there exists a surrogate regret bound with a linear transfer function $\zeta$.
\end{theorem}

The second shows that $\zeta$ cannot be smaller than a square root for sufficiently ``non-polyhedral'' surrogates, which are smooth and locally strongly convex.
For these losses, therefore, fast surrogate generalization rates will likely still translate to slow rates for the target.
This result could be viewed as a partial converse to the first, providing evidence that some element of piecewise-linearity is necessary for optimal surrogate regret bounds.
\begin{theorem}[Lower bound for non-polyhedral]
  \label{thm:main-lower}
  Let $L$ be a locally strongly convex surrogate, with locally Lipschitz gradient.
  If $L$ satisfies a regret transfer rate of $\zeta$ for a target loss $\ell$, then there exists $c>0$ such that, for arbitrarily small $\epsilon>0$, we have $\zeta(\epsilon) \geq c\cdot\sqrt{\epsilon}$.
%  If $\zeta$ is a transfer function in a surrogate regret bound for $L$ to a target loss $\ell$, then there exists $c>0$ such that, for arbitrarily small $\epsilon>0$, we have $\zeta(\epsilon) \geq c\cdot\sqrt{\epsilon}$.
\end{theorem}
\raf{Comments on general implications, e.g. ``optimality'' of polyhedral?}

\raf{Some comments about our proof techniques, with forward refs to sections / outline sprinkled in?}


\paragraph{Other related work.}
Surrogate regret bounds have been well studied in some settings.
It is known that $\zeta$ \raf{examples: logistic, exponential, and hinge}.
Some general bounds are known for classes of surrogates, e.g. \raf{refs}.
Lacking, however, is a general understanding of what $\zeta$ one can expect from a surrogates, especially for arbitrary target losses.


%\section{Introduction}
%\paragraph{The surrogate risk framework.}
%In supervised learning, data is of the form $(x,y)$ and the goal is to learn a hypothesis $g(x)$.
%This paper considers the common setting where the space of labels $y$ is finite, as is the space of predictions $g(x)$, and accuracy is measured with a \emph{target loss function} $\ell(g(x),y)$.
%
%Because these discrete settings are generally computationally challenging, a common paradigm is to learn $g$ via \emph{surrogate risk minimization}:
%\begin{enumerate}
%  \item Define a \emph{surrogate loss function} $L(h(x),y)$ over a more-tractable continuous, convex prediction space.
%  \item Learn a hypothesis $h(x)$ mapping $x$ to this new prediction space.
%  \item Produce a hypothesis $g(x)$ for the original, target problem using some link function $\psi$ via $g(x) = \psi(h(x))$.
%\end{enumerate}
%For example, in binary classification, the target problem is given by $0$-$1$ loss, $\ell(r)_y = \ones\{r\neq y\}$, with surrogate losses typically defined over $\reals$.
%Common surrogate losses include hinge loss and logistic loss.
%These surrogates are typically associated with the link $\psi(u) = \mathrm{sign}(u)$, which maps negative predictions to $-1$ and nonnegative predictions to $+1$.
%
%\paragraph{Rates of convergence.}
%Given $n$ data points and a surrogate loss $L$, a learning algorithm should produce a hypothesis $h_n$ whose loss converges to the optimal possible as $n$ increases.
%Specifically, we can define $\Reg_{L}(h)$ as the expected loss of $h$ on the data distribution, minus the expected loss of the Bayes optimal predictor, i.e. the measurable function with least loss.
%Then a primary goal is to bound the \emph{convergence rate} of this algorithm, i.e. provide a bound of the form $\Reg_{L}(h_n) \leq O\left(\tfrac{1}{n^{\alpha}}\right)$.
%Such bounds for $\alpha = \tfrac{1}{2}$ are considered ``slow'' rates and are guaranteed by e.g. VC-dimension bounds.\bo{cite}
%Faster rates such as $\alpha = 1$ are obtainable is some settings.\bo{cite}
%
%\paragraph{Problem: relating surrogate and target regret.}
%However, results of the above form do not guarantee the success of the learning framework for the original, target problem.
%Even if the surrogate regret $\Reg_L(h_n)$ is small, we actually care about the induced target hypothesis $g_n = \psi \circ h_n$.
%Will the target regret necessarily be small?
%Motivated by this problem, this paper will study \emph{surrogate regret bounds}.\bo{cite}
%%A surrogate regret bound answers the following question: when we minimize the \emph{surrogate} regret at a given rate, how fast does the \emph{target} regret diminish?
%Informally, a surrogate regret bound takes the form
%\begin{equation}
%  \label{eq:surrogate-regret-bound-informal}
%  \forall \text{ hypotheses } h:\X\to\reals^d, \quad \Reg_\ell(\psi\circ h) \leq \zeta(\, \Reg_L(h) \,)~,
%\end{equation}
%where the regret transfer function $\zeta : \reals_+ \to \reals_+$ controls how surrogate regret translates to target regret.
%For example, \bo{cite} studies the \bo{??} surrogate loss for the \bo{??} problem, achieving a guarantee of $\zeta(\epsilon) \leq O\left(\sqrt{\epsilon}\right)$.
%
%While such a guarantee is certainly nice, it is not ideal: even if one achieves a fast rate of $O(\tfrac{1}{n})$ for the surrogate problem, this only translates to the slow rate guarantee of $O(\tfrac{1}{n^{1/2}})$ in the target problem that we actually want to solve.
%Preferable would be a transfer function such as $\zeta(\epsilon) = O(\epsilon^{3/4})$ or even $\zeta(\epsilon) = O(\epsilon)$, a linear transfer rate.
%
%\raf{Of particular interest is the concavity of $\zeta$} \bo{explain why?}
%
%By considering a specific class of surrogate losses -- \emph{polyhedral}, generalizations of piecwise-linear convex -- we will show that this utopia is in fact achieved for every surrogate in this class.
%
%%\begin{theorem}[Linear bounds for polyhedral surrogates]
%\begin{theorem}[Main surrogate regret bound]
%  \label{thm:main-upper}
%  Let $L$ be any polyhedral surrogate and $\psi$ any link function such that $(L,\psi)$ is consistent for the target loss $\ell$.
%  Then $(L,\psi)$ guarantee a linear regret transfer rate for $\ell$.
%  %Then there exists a surrogate regret bound with a linear transfer function $\zeta$.
%\end{theorem}
%
%We will also show a class of counterexamples for surrogate losses that are far from polyhedral, i.e. those that are smooth and strongly convex.
%This could be viewed as a partial converse, providing evidence that some element of ``piecewise-linearity'' is \emph{necessary} for optimal surrogate regret bounds.
%\begin{theorem}[Main lower bound]
%  \label{thm:main-lower}
%  Let $L$ be a differentiable convex surrogate, with locally Lipschitz gradient, which is locally strongly convex.
%  If $L$ satisfies a regret transfer rate of $\zeta$ for a target loss $\ell$, then there exists $c>0$ such that, for arbitrarily small $\epsilon>0$, we have $\zeta(\epsilon) \geq c\cdot\sqrt{\epsilon}$.
%%  If $\zeta$ is a transfer function in a surrogate regret bound for $L$ to a target loss $\ell$, then there exists $c>0$ such that, for arbitrarily small $\epsilon>0$, we have $\zeta(\epsilon) \geq c\cdot\sqrt{\epsilon}$.
%\end{theorem}
%
%\bo{example/discussion?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}

In what follows, we introduce our notation while recalling the motivation behind surrogate risk minimization and surrogate regret bounds.

\paragraph{Target problems.}
We consider supervised machine learning problems where data points have features in $\X$ and labels in $\Y$, a finite set.
The \emph{target problem} is to learn a hypothesis mapping $\X$ to a \emph{report} (or \emph{prediction}) \emph{space} $\R$, possibly different from $\Y$.
In this paper, we assume $\R$ is a finite set.
%For example, standard binary classification has $\Y = \R = \{-1,+1\}$, whereas in a ranking problem, $\R$ is the set of all permutations of $\Y$.
We suppose the target problem is specified by the \emph{target loss} $\ell:\R\to\reals^\Y_+$, which maps a report (prediction) $r \in \R$ to the vector $\ell(r)$ of nonnegative loss values for each label, i.e. $\ell(r) = (\ell(r)_y)_{y\in\Y}$.
Given a hypothesis $g: \X \to \R$ and a data point $(x,y) \in \X \times \Y$, the loss is $\ell(g(x))_y$.
\bo{We will make a minimal non-redundancy assumption on $\ell$ in [Ref].}
We may refer to $\ell$ as a \emph{discrete} target loss to emphasize our assumption that $\R$ and $\Y$ are finite.

\bo{given the intro, perhaps we can omit examples? but we do need to emphasize that $\R \neq \Y$ generally.}



\paragraph{Surrogate problems.}
\bo{Given the intro, this paragraph can probably be shortened.}
Because optimizing a discrete target loss on a dataset is typically intractable, the usual approach is \emph{surrogate risk minimization}.
First, one constructs a \emph{surrogate loss function} $L:\reals^d\to\reals^\Y_+$.
Then, one minimizes it to learn a hypothesis of the form $h: \X \to \reals^d$.
Finally, one applies a \emph{link function} $\psi: \reals^d \to \R$ that is associated with the surrogate loss.
If the learned hypothesis predicts $h(x) \in \reals^d$, then the implied prediction for the target problem is $\psi(h(x)) \in \R$.

For example, in binary classification, the target problem is given by $0$-$1$ loss, $\ell(r)_y = \ones\{r\neq y\}$, with surrogate losses typically defined over $\reals$.
Common surrogate losses include hinge loss and logistic loss.
These surrogates are typically associated with the link $\psi(u) = \mathrm{sign}(u)$, which maps negative predictions to $-1$ and nonnegative predictions to $+1$.

\paragraph{Surrogate regret bounds.}
Given a target problem, we would like to design surrogates which efficiently solve it.
A standard tool to establish this efficiency is a \emph{surrogate regret bound}.
We define the \emph{regret} of a hypothesis $h: \X \to \reals^d$ as the expected loss relative to the best possible.
Formally, the \emph{surrogate regret} of $h$ with respect to data distribution $\D$ is given by $R_L(h;\D) = \E_{(X,Y)\sim\D} L(h(X))_Y - \inf_{h':\X\to\reals^d} \E_{(X,Y)\sim\D} L(h'(X))_Y$, where the minimum is taken over all measurable functions.
(One may equivalently assume the Bayes optimal hypothesis is in the function class.)
Given $h$, the \emph{target regret} of the implied hypothesis $\psi \circ h$ is $R_\ell(\psi\circ h;\D) = \E_{(X,Y)\sim\D} \ell(\psi(h(X)))_Y - \inf_{h':\X\to\R} \E_{(X,Y)\sim\D} \ell(h'(X))_Y$, where we recall that $\psi$ is the link function.

%% Addressed in intro instead
%A central concern for learning algorithms is the \emph{rate of convergence} of regret.
%If $h_n$ denotes an algorithm's output on $n$ data points, then a typical bound is of the form $R_L(h_n;\D) \leq O\left(\frac{1}{n^{\alpha}}\right)$, usually for some $\alpha$ between $\tfrac{1}{2}$ (``slow'') and $1$ (``fast'').
%However, such bounds for the surrogate leave open the question of the implied rate of convergence for the target problem.
%This is the question we address by studying \emph{surrogate regret bounds}.

A surrogate regret bound answers the following question: when we minimize the surrogate regret at a rate $f(n)$, given $n$ data points, how fast does the target regret diminish?
Formally, we say that $\zeta : \reals_+ \to \reals_+$ is a \emph{rate transfer function} if $\zeta$ is continuous at $0$ and satisfies $\zeta(0) = 0$.
Given a rate transfer function $\zeta$, we say that $(L,\psi)$ guarantees a \emph{regret transfer rate of $\zeta$} for $\ell$ if
\begin{equation}
  \label{eq:surrogate-regret-bound}
  \forall \D, \forall h:\X\to\reals^d, \quad R_\ell(\psi\circ h;\D) \leq \zeta(\, R_L(h;\D) \,)~.
\end{equation}
We note that a classic minimal requirement for $(L,\psi)$ is that they be \emph{consistent} for $\ell$, which is the case if there exists any regret transfer rate $\zeta$ guaranteed by $(L,\psi)$ for $\ell$.\bo{cite}
In other words, consistency implies that $R_L(h;\D) \to 0 \implies R_{\ell}(\psi \circ h;\D) \to 0$, but says nothing about the relative rates.

%% Addressed in intro
%However, consistency alone could be compatible with slow transfer functions such as $\zeta(\epsilon) = \epsilon^{1/4}$.
%For such transfers, a ``fast'' learning rate of e.g. $O\left(\tfrac{1}{n}\right)$ in the surrogate problem would only transfer to a very slow rate of $O\left(\tfrac{1}{n^{1/4}}\right)$ in the target.
%On the other hand, we could hope to choose surrogates with as fast as a linear transfer rate, $\zeta(\epsilon) \leq C \cdot \epsilon$, implying $R_{\ell}(\psi \circ h;\D) \leq C \cdot R_L(h;\D)$.

%% Moved to intro
%Surrogate regret bounds have been well studied in some settings.
%It is known that $\zeta$ \raf{examples: logistic, exponential, and hinge}.
%Some general bounds are known for classes of surrogates, e.g. \raf{refs}.
%Lacking, however, is a general understanding of what $\zeta$ one can expect from a surrogates, especially for arbitrary target losses.

%% Addressed in intro
%We give two main results to deepen this understanding.
%The first is that polyhedral (piecewise-linear and convex) surrogates always yield \emph{linear} regret bounds.
%The second is that sufficiently convex and smooth losses (i.e., ``non-polyhedral'') require $\zeta(\epsilon) = \Omega(\sqrt{\epsilon})$.
%
%\begin{theorem}[Main surrogate regret bound]
%  Let $L$ be any polyhedral surrogate and $\psi$ any link function such that $(L,\psi)$ is consistent for the target loss $\ell$.
%  Then $(L,\psi)$ guarantee a linear regret transfer rate for $\ell$.
%  %Then there exists a surrogate regret bound with a linear transfer function $\zeta$.
%\end{theorem}
%
%\begin{theorem}[Main counterexample]
%  Let $L$ be a differentiable convex surrogate, with locally Lipschitz gradient, which is locally strongly convex.
%  If $L$ satisfies a regret transfer rate of $\zeta$ for a target loss $\ell$, then there exists $c>0$ such that, for arbitrarily small $\epsilon>0$, we have $\zeta(\epsilon) \geq c\cdot\sqrt{\epsilon}$.
%%  If $\zeta$ is a transfer function in a surrogate regret bound for $L$ to a target loss $\ell$, then there exists $c>0$ such that, for arbitrarily small $\epsilon>0$, we have $\zeta(\epsilon) \geq c\cdot\sqrt{\epsilon}$.
%\end{theorem}
%
%\raf{Paragraph on implication for rates here.  Maybe Huber example?}


\paragraph{Polyhedral surrogates.}
In this paper, we consider the class of polyhedral surrogates, which includes hinge loss.\bo{And?}
Formally, a function $f: \reals^d \to \reals$ is called \emph{polyhedral} if it can be written as a pointwise maximum of a finite set of affine functions~\cite{rockafellar1997convex}.\bo{theorem/chapter?}
The epigraph of a polyhedral function is a polyhedral set, i.e. the intersection of a finite set of closed halfspaces; thus polyhedral functions are always convex.
We say a surrogate loss $L: \reals^d \to \reals_+^{\Y}$ is \emph{polyhedral} if, for each fixed $y \in \Y$, the loss as a function of prediction, i.e. $r \mapsto L(r)_y$, is polyhedral.

\subsection{Property elicitation and the conditional approach}
Our technical approach is based on property elicitation, which studies the structure of $\ell$ and $L$ as they relate to $\simplex$.
This involves a \emph{conditional} perspective.
Given a data distribution $\D$ on $\X \times \Y$, for $(X,Y) \sim \D$, we consider the format of possible target predictions $g(x) = r \in \R$; surrogate predictions $h(x) = u \in \reals^d$; and conditional distributions $\Pr[ \cdot \mid X] = p \in \simplex$.
By dropping the role of the hypotheses and the feature space $\X$, we can focus on the relationships of the functions $\ell(r)_y$ and $L(u)_y$ and the conditional distribution $p$.

In particular, the \emph{Bayes risk} of $L$ is $\risk{L}(p) = \inf_{u \in \reals^d} \inprod{p}{L(u)}$, and similarly the Bayes risk of $\ell$ is $\risk{\ell}(p) = \inf_{r \in \R} \inprod{p}{\ell(r)}$.
We abuse notation to define the \emph{conditional surrogate regret} $R_L(u,p) = \inprod{p}{L(u)} - \risk{L}(p)$, and similarly the \emph{conditional target regret} $R_{\ell}(r,p) = \inprod{p}{\ell(r)} - \risk{\ell}(p)$.
Given a rate transfer function $\zeta$, we say $(L,\psi)$ guarantees a \emph{conditional regret transfer rate} of $\zeta$ for $\ell$ if, for all $p \in \simplex$ and all $u \in \reals^d$, $R_{\ell}(\psi(u),p) \leq \zeta(R_L(u,p))$.

The following is a direct result of Jensen's inequality, using that $R_L(h;\D) = \E_X R_L(h(x),p_X)$ where $p_X$ is the conditional distribution on $Y$ given $X$.
\begin{observation} \label{obs:transfer}
  If $(L,\psi)$ guarantee a conditional regret transfer rate of $\zeta$ for $\ell$, and $\zeta$ is concave, then $(L,\psi)$ guarantee a regret transfer rate of $\zeta$ for $\ell$.
\end{observation}
 
\paragraph{Property elicitation.}
To begin, we define a \emph{property}, which is essentially a statistic or summary of a distribution, such as the mode.
We then define what it means for a loss function to \emph{elicit} a property.
We write $\Gamma: \simplex \toto \R$ as shorthand for $\Gamma: \simplex \to 2^{\R} \setminus \{\emptyset\}$.
We write $\inprod{p}{L(u)}$ for the expected surrogate loss of prediction $u$ on distribution $p \in \simplex$, and similarly $\inprod{p}{\ell(r)}$ is the expected target loss of $r$.




\bo{TO revisit after looking through proofs -- not sure yet how to best present this!}
\begin{definition}[Property, level set]\label{def:property}
  A \emph{property} is a function $\Gamma:\simplex\toto\R$.
  The \emph{level set} of $\Gamma$ for report $r$ is the set $\Gamma_r = \{p \in \simplex : r \in \Gamma(p)\}$.
\end{definition}
In general, a loss $f: R \to \reals^{\Y}$, for some prediction (or ``report'') space $R$, is said to \emph{elicit} a property $\Gamma$ if
\begin{equation}
  \forall p\in\simplex,\;\;\;\Gamma(p) = \argmin_{r \in \R} \inprod{p}{f(r)}~.
\end{equation}

The following definition applies both to target losses and surrogate losses.
\begin{definition}[Elicits, report space]
  \label{def:elicits}
  A loss function $f:R\to\reals^\Y_+$, where the domain $R$ is called the \emph{report space}, \emph{elicits} a property $\Gamma:\simplex \toto \R$ if
  As each $f$ elicits a unique property, we refer to it as $\prop{f}$.
\end{definition}
In particular, we will consistently use the notation $\Gamma = \prop{L}$ where $L$ is a surrogate loss with report space $\reals^d$, and we will use $\gamma = \prop{\ell}$ where $\ell$ is a target loss with finite report space $\R$.

We will assume throughout that the target loss is \emph{non-redundant}, meaning that all $r \in \R$ are possibly uniquely optimal.
Formally, we suppose that for all $r \in \R$, there exists $p \in \simplex$ with $\gamma(p) = \{r\}$.

\paragraph{Calibration and consistency.}
It is known that consistency implies the weaker condition of \emph{calibration}, which implies the still-weaker, but very useful, condition of \emph{indirect elicitation}.
To define these conditions, consider $L,\psi,\ell$ and $\gamma = \prop{\ell}$.
Recall that $\gamma(p)$ of target reports that are optimal for $p$.
If $\psi(u) \not\in \gamma(p)$, then $u$ is a ``bad'' (suboptimal) surrogate prediction.
Let $B_{L,\psi,\ell}(p) = \{R_L(u,p) ~:~ \psi(u) \not\in \gamma(p)\}$, the set of expected surrogate losses resulting from bad predictions.
\begin{definition}[Indirectly elicits, Calibrated]
  A surrogate-link pair $(L,\psi)$ \emph{indirectly elicits} a target $\ell$ if $0 \not\in B_{L,\psi,\ell}(p)$ for all $p \in \simplex$.
  Meanwhile, $(L,\psi)$ is \emph{calibrated} for $\ell$ if $0 < \inf B_{L,\psi,\ell}(p)$ for all $p \in \simplex$.
%  \begin{equation}
%    \inf_{u : \psi(u) \not\in \gamma(p)} \inprod{p}{L(u)} > \risk{L}(p)$ .
%  \end{equation}
%  Throughout, $\Gamma = \prop{L}$ and $\gamma = \prop{\ell}$.
\end{definition}
Equivalently, $(L,\psi)$ indirectly elicits $\ell$ if $u \in \Gamma(p) \implies \psi(u) \in \gamma(p)$.
Calibration is the slightly more subtle condition that there is a nonzero gap between the expected loss of a ``good'' report and that of any bad one.
%Observe that indirect elicitation is the condition that $0 \not\in \{\inprod{p}{L(u)} ~:~ \psi(u) \not\in \gamma(p)\}$.
%Calibration is the slightly stronger condition that $0$ is not the infimum of the set.
\begin{fact}[\bo{cite who?}\raf{see fig 1 (left-hand double arrow) from flats paper}]
  \label{fact:consistent-calibrated-elicits}
  Let $\ell: \R \to \reals_+^{\Y}$ be a target loss with finite $\R$ and $\Y$.
  If a surrogate and link $(L,\psi)$ are consistent for $\ell$, then $(L,\psi)$ are calibrated for $L$.
  If $(L,\psi)$ are calibrated for $\ell$, then they indirectly elicit $\ell$.
\end{fact}

%% move to inline where used
%\begin{definition}[Embedding a loss]\label{def:loss-embed}
%  A loss $L:\reals^d\to\reals^\Y$ \emph{embeds} a loss $\ell:\R\to\reals^\Y$ if there exists some injective embedding $\varphi:\R\to\reals^d$ such that
%  (i) for all $r\in\R$ we have $L(\varphi(r)) = \ell(r)$, and (ii) for all $p\in\simplex,r\in\R$ we have
%  \begin{equation}\label{eq:embed-loss}
%    r \in \prop{\ell}(p) \iff \varphi(r) \in \prop{L}(p)~.
%  \end{equation}
%\end{definition}


\section{Upper Bound: Polyhedral Surrogates}
In this section, we develop the proof of Theorem \ref{thm:main-upper}.
At a high level, the proof has two parts:
\begin{enumerate}
  \item Fix $p \in \simplex$.
        Prove that there is some constant $\alpha_p$ such that $R_{\ell}(\psi(u),p) \leq \alpha_p R_L(u,p)$ for all $u\in\reals^d$.
      \item Apply this result to each $p$ in a carefully chosen finite subset of $\simplex$.
        Argue that the maximum $\alpha_p$ from this finite set suffices for the slope of the linear transfer function.
\end{enumerate}
\raft{On ``We view both parts as relatively surprising'' I'm not sure the first part is.  I mean, it is to \textbf{us}, but as you point out, in retrospect it follows immediately from the definition...}
The first part will actually hold for any calibrated surrogate.
The second part relies on a crucial fact about polyhedral losses $L$: their properties $\Gamma=\prop{L}$ cover the whole simplex with only a finite number of polyhedral level sets.
In other words, although the prediction space $\reals^d$ is infinite, one can restrict to just a finite set of prediction points and always have a global minimizer of expected loss.
This observation is related to embeddings \bo{cite}.

\subsection{Part 1: Linear transfer for fixed $p$}
First, we establish a linear transfer rate for any fixed conditional distribution $p$.
This holds for any calibrated surrogate, not necessarily polyhedral.
\begin{lemma} \label{lemma:fixed-p}
  Let $\ell: \R \to \reals_+^{\Y}$ be a discrete target loss and suppose the surrogate $L: \reals^d \to \reals_+^{\Y}$ and link $\psi: \reals^d \to \R$ are calibrated for $\ell$.
  Then for any $p \in \simplex$, there exists $\alpha_p \geq 0$ such that, for all $u \in \reals^d$,
    \[ R_{\ell}(\psi(u),p) \leq \alpha_p R_L(u,p) . \]
\end{lemma}
\begin{proof}
  Fix $p \in \simplex$.
  Let $C_p = \max_{r \in \R} R_{\ell}(r,p)$.
  The maximum exists because $\ell$ is discrete, i.e. $\R$ is finite.
  Meanwhile, recall that, when defining calibration, we let $B_{L,\psi,\ell}(p) = \{R_L(u,p) ~:~ \psi(u) \not\in \gamma(p)\}$.
  Let $B_p = \inf B_{L,\psi,\ell}(p)$.
  By definition of calibration, we have $B_p > 0$.

  To combine these bounds, let $\alpha_p = \frac{C_p}{B_p}$.
  Let $u \in \reals^d$.
  There are two cases.
  If $\psi(u) \in \gamma(p)$, then $R_{\ell}(\psi(u),p) = 0 \leq R_L(u,p)$ immediately.
  If $\psi(u) \not\in \gamma(p)$, then
  \begin{align*}
    R_{\ell}(\psi(u),p)
    &\leq C_p \\
    &=    \alpha_p \cdot B_p  \\
    &\leq \alpha_p R_L(u,p) .
  \end{align*}
\end{proof}
While the above proof has the advantage of being quite short, it is also somewhat nonconstructive and potentially loose.
In \bo{Section TODO}, we investigate the constant $\alpha_p$ for polyhedral losses in particular.

\subsection{Part 2: Linear overall transfer function}
Given Lemma \ref{lemma:fixed-p}, we might hope to obtain that, for any calibrated surrogate, there exists $\alpha = \sup_p \alpha_p$ such that $R_{\ell}(\psi(u),p') \leq \alpha R_L(u,p')$ for all $p'$.
However, we know this is false in general: not all surrogates yield linear transfer rates!
Indeed, for many surrogates, the supremum will be $+\infty$.

However, polyhedral surrogates have a special structure that allows us to show $\alpha$ is finite.
We first present several general lemmas, then the key implication from $L$ being polyhedral.
The general results are known \bo{cite}\raft{Not sure what this sentence means}.

\bo{For these lemmas, I tried to state no more than we need.}
\begin{lemma} \label{lemma:refines}
  If $(L,\psi)$ indirectly elicits $\ell$, then $\Gamma = \prop{L}$ \emph{refines} $\gamma = \prop{\ell}$ in the sense that, for all $u \in \reals^d$, there exists $r \in \R$ such that $\Gamma_u \subseteq \gamma_r$.
\end{lemma}
\begin{proof}
  For any $u$, let $r = \psi(u)$.
  By indirect elicitation, $u \in \Gamma(p) \implies r \in \gamma(p)$.
  Phrased differently, $\Gamma_u = \{p ~:~ u \in \Gamma(p)\} \subseteq \{p ~:~ r \in \gamma(p)\} = \gamma_r$.
\end{proof}
The next lemma states that surrogate regret is linear in $p$ on any fixed level set $\Gamma_{u^*}$ of $\Gamma = \prop{L}$.
By combining this fact with Lemma \ref{lemma:refines}, we obtain that target regret is linear on these level sets as well.
\begin{lemma} \label{lemma:linear-on-levelset}
  Suppose $(L,\psi)$ indirectly elicits $\ell$ and let $\Gamma = \prop{L}$.
  Then for any fixed $u,u^* \in \reals^d$ and $r \in \R$, the functions $R_L(u,\cdot)$ and $R_{\ell}(u,\cdot)$ are linear in their second arguments on $\Gamma_{u^*}$.
\end{lemma}
\begin{proof}
  Let $u^* \in \reals^d$ and $p \in \Gamma_{u^*}$.
  By definition, for all $p \in \Gamma_{u^*}$, $\risk{L}(p) = \inprod{p}{L(u^*)}$.
  So for fixed $u$,
    \[ R_L(u,p) = \inprod{p}{L(u)} - \inprod{p}{L(u^*)} = \inprod{p}{L(u) - L(u^*)} , \]
  a linear function of $p$.
  Next, by Lemma \ref{lemma:refines}, there exists $r^*$ such that $\Gamma_{u^*} \subseteq \gamma_{r^*}$.
  By the same argument, for fixed $r$, $R_{\ell}(r,p) = \inprod{p}{\ell(r) - \ell(r^*)}$, a linear function.
\end{proof}

We now arrive at the key property of polyhedral losses.
\begin{lemma} \label{lemma:polyhedral-finite}
  If $L: \reals^d \to \reals_+^{\Y}$ is polyhedral, then $\Gamma = \prop{L}$ has a finite set of level sets that union to $\simplex$. Also, these level sets are polytopes.
\end{lemma}
\begin{proof}
  \bo{prove, cite, etc}
\end{proof}

We are now ready to restate and prove the main upper bound.
\begin{theorem}[Theorem \ref{thm:main-upper} restated]
  Suppose the surrogate loss $L: \reals^d \to \reals_+^{\Y}$ and link $\psi: \reals^d \to \R$ are consistent for the target loss $\ell: \R \to \reals_+^{\Y}$.
  If $L$ is polyhedral, then $(L,\psi)$ guarantee a linear regret transfer rate for $\ell$, i.e. there exists $\alpha > 0$ such that, for all $\D$ and all measurable $h: \X \to \reals^d$,
    \[ R_{\ell}(\psi \circ h ; \D) \leq \alpha R_L(h ; \D) . \]
\end{theorem}
\begin{proof}
  We first recall that by Fact \ref{fact:consistent-calibrated-elicits}, consistency implies that $(L,\psi)$ are calibrated for $\ell$ and that $(L,\psi)$ indirectly elicit $\ell$.
  Next, by Observation \ref{obs:transfer}, it suffices to show a linear \emph{conditional} regret transfer rate, i.e. for all $p \in \simplex$ and $u \in \reals^d$, we show $R_{\ell}(\psi(u),p) \leq \alpha R_L(u,p)$.
  
  By Lemma \ref{lemma:polyhedral-finite}, the polyhedral loss $L$ has a finite set $U \subset \reals^d$ of predictions such that (a) for each $u \in U$, the level set $\Gamma_u$ is a polytope, and (b) $\cup_{u \in U} \Gamma_u = \simplex$.
  Let $\mathcal{Q}_u \subset \simplex$ be the finite set of vertices of the polytope $\Gamma_u$, and define the finite set $\mathcal{Q} = \cup_{u \in U} \mathcal{Q}_u$.
  
  By Lemma \ref{lemma:fixed-p}, for each $q \in \mathcal{Q}$, there exists $\alpha_q \geq 0$ such that $R_{\ell}(\psi(u),q) \leq \alpha_p R_L(u,q)$ for all $u$.
  We choose
    \[ \alpha = \max_{q \in \mathcal{Q}} \alpha_q . \]
  To prove the conditional transfer rate, consider any $p \in \simplex$ and any $u \in \reals^d$.
  There exists $u \in U$ such that $p \in \Gamma_u$, a polytope.
  So we can write $p$ as a convex combination of its vertices, i.e.
    \[ p = \sum_{q \in \mathcal{Q}_u} \beta(q) q \]
  for some probability distribution $\beta$.
  Because $\mathcal{Q}_u \subseteq \Gamma_u$, $R_L$ and $R_{\ell}$ are linear on it by Lemma \ref{lemma:linear-on-levelset}.
  So:
  \begin{align*}
    R_{\ell}(u,p)
    &=    R_{\ell}\left(\psi(u) ~,~ \sum_{q \in \mathcal{Q}_u} \beta(q) q\right)  \\
    &=    \sum_{q \in \mathcal{Q}_u} \beta(q) R_{\ell}(\psi(u),q)  \\
    &\leq \sum_{q \in \mathcal{Q}_u} \beta(q) \alpha_{q} R_L(u,q)  \\
    &\leq \alpha \sum_{q \in \mathcal{Q}_u} \beta(q) R_L(u,q)  \\
    &=    \alpha R_L(u, p) .
  \end{align*}
\end{proof}


\section{Lower bound}

In this section, we show that if a surrogate loss is sufficiently ``non-polyhedral'', then the best regret transfer it can achieve is $\sqrt{\epsilon}$.
Specifically, we consider a relaxation of $L$ being both strongly smooth and strongly convex.

We next formalize this relaxation.
It will assume $L$ is strongly smooth everywhere and strongly convex around some \emph{boundary prediction}, which we define to be a $u_0 \in \reals^d$ such that, for some $r,r' \in \R$, there exists a distribution $p_0 \in \Gamma_{u_0} \cap \gamma_r \cap \gamma_{r'}$.
Here $\Gamma = \prop{L}$ and $\gamma = \prop{\ell}$.
The upshot of this definition is that for the conditional distribution $p_0$, $u_0$ minimizes expected $L$-loss, and it may link to either $r$ or $r'$, as either of these minimize expected $\ell$-loss.
We also note that an \emph{open neighborhood} of $u_0$ is an open set in $\reals^d$ containing $u_0$.

\begin{assumption} \label{assumption:lower}
  $L: \reals^d \to \reals^{\Y}_+$ is a surrogate and $\psi: \reals^d \to \R$ a link, consistent with the discrete target $\ell: \R \to \reals^{\Y}_+$, satisfying the following: (i) For all $y \in \Y$, the function $L(\cdot)_y$ is differentiable with a locally Lipschitz gradient.\bo{footnote def of locally lipschitz} (ii) For some $\alpha > 0$ and some $u_0 \in \reals^d$, the expected loss function $u \mapsto \inprod{p}{L(u)}$ is $\alpha$-strongly convex on an open neighborhood \bo{define neighborhood} of one of its boundary prediction $u_0$.
\end{assumption}

\begin{observation} \label{obs:lower-implied}
  If $L$ is strongly smooth and $\alpha$ strongly convex, then it satisfies Assumption \ref{assumption:lower} for any link $\psi$ and discrete target loss $\ell$ such that $(L,\psi)$ are consistent for $\ell$.
\end{observation}
Observation \ref{obs:lower-implied} follows because boundary points are guaranteed to exist for $L,\psi,\ell$.
This holds because $\ell$ has a finite report space $\R$, so for example the fact that its full-dimensional level sets union to the simplex \bo{cite?} implies the existence of $r,r' \in \R$ and $p_0 \in \gamma_r \cap \gamma_{r'}$; and some surrogate prediction $u_0$ minimizes expected surrogate loss on $p_0$.

The following result is our main lower bound.
With Observation \ref{obs:lower-implied}, it immediately implies Theorem \ref{thm:main-lower} from the introduction.
\begin{theorem}
  Suppose the surrogate loss $L$ and link $\psi$ satisfy a regret transfer of $\zeta$ for a target loss $\ell$.
  If $L$, $\psi$, and $\ell$ satisfy Assumption \ref{assumption:lower}, then there exists $c > 0$ such that, for arbitrarily small $\epsilon$, $\zeta(\epsilon) \geq c \sqrt{\epsilon}$.
\end{theorem}
By ``arbitrarily small'', we mean that for any $\epsilon_0 > 0$, we can find $\epsilon$ with $0 < \epsilon < \epsilon_0$ such that $\zeta(\epsilon) \geq c \sqrt{\epsilon}$.
Thus, there is no neighborhood of zero where $\zeta$ shrinks faster than on the order of $\sqrt{\cdot}$.
\begin{proof}
  Obtain $\alpha, u_0, p_0, r, r'$, and an open neighborhood of $u_0$ from Assumption \ref{assumption:lower} and the definition of boundary point.
  Assume without loss of generality that $\psi(u_0) = r'$; otherwise, swap the roles of $r$ and $r'$.

  \paragraph{Linearity of $R_{\ell}(r',p_{\lambda})$.}
  As $\ell$ is non-redundant by assumption, there exists some $p_1 \in \inter{\gamma_r}$, the relative interior of the full-dimensional level set $\gamma_r$.
  We therefore have $R_\ell(r',p_1) = \inprod{p_1}{\ell(r')-\ell(r)} =: c_\ell > 0$, and $R_\ell(r',p_0) = 0$.
  Let $p_\lambda := (1-\lambda) p_0 + \lambda p_1$.
  By convexity of $\gamma_r$, we have $p_\lambda \in \gamma_r$ for all $\lambda \in [0,1]$, which gives $R_\ell(r',p_\lambda) = \lambda c_\ell$.
  \bo{move to proof sketch: We will upper bound $R_L(u_0,p_\lambda)$ by $O(\lambda^2)$, which will imply the result.}
  % Our goal is to find some $c>0$ and some path of reports $u_\lambda \in \reals^d$ such that $\psi(u_\lambda)=r'$ and $R_L(u_\lambda,p_\lambda) \geq c\cdot\sqrt{\lambda \epsilon_1}$, for $\lambda$ sufficiently small.

  \paragraph{Obtaining the global minimizer $u_{\lambda}$ of $L_{\lambda}$.}
  Let $L_\lambda:\reals^d\to\reals_+$ be given by $L_\lambda(u) = \inprod{p_\lambda}{L(u)} = (1-\lambda) \inprod{p_0}{L(u)} + \lambda \inprod{p_1}{L(u)}$.
  Let $\delta >0$ such that the above open neighborhood of $u_0$ contains the Euclidean ball $B_\delta(u_0)$ of radius $\delta$ around $u_0$.
  Let $u_1 \in \Gamma(p_1)$.
  We argue that for all small enough $\lambda$, $L_{\lambda}(u)$ is uniquely minimized by some $u_{\lambda} \in B_{\delta}(u_0)$.
  For any $u\notin B_\delta(u_0)$, we have, using local strong convexity and the optimality of $u_1$,
  \begin{align*}
    L_\lambda(u) - L_\lambda(u_0)
    &=
      (1-\lambda) \left( L_0(u) - L_0(u_0) \right)
      + \lambda \left( L_1(u) - L_1(u_0) \right)
    \\
    &\geq
      (1-\lambda) \left( \frac \alpha 2 \delta^2 \right)
      + \lambda \left( L_1(u_1) - L_1(u_0) \right)~  \\
    &> 0
  \end{align*}
  if $\lambda < \lambda^* := \alpha \delta^2 / (2 \alpha \delta^2 + 4 L_1(u_0) - 4 L_1(u_1))$.
  For the remainder of the proof, let $\lambda < \lambda^*$.
  Then any $u\notin B_{\delta}(u_0)$ has $L_{\lambda}(u) > L_{\lambda}(u_0)$, hence is suboptimal.
  By $\alpha$-strong convexity of $L_0$ on $B_\delta(u_0)$, $L_\lambda$ is stricly convex on $B_\delta(u_0)$.
  So it has a unique minimizer $u_{\lambda}$, and by the above argument this is the global minimizer of $L_{\lambda}$.
%  Extending our definition of $u_0$ and $u_1$, let $u_\lambda \in \Gamma(p_\lambda)$, which, as $\lambda \leq \lambda^*$ and thus $u_\lambda \in B_\delta(u_0)$, is the unique minimizer of $L_\lambda$ on $\reals^d$.
  % Moreover, by the previous paragraph, we have $u_\lambda \in B_\delta(u_0)$.
  Then $\risk{L}(p_\lambda) = L_\lambda(u_\lambda)$, and thus $R_L(u_0,p_\lambda) = L_\lambda(u_0) - L_\lambda(u_\lambda)$.

  By assumption, the gradient of $L_y$ is locally Lipschitz for all $y\in\Y$.
  We will apply this fact to the compact set $\mathcal C = \{u \in \reals^d : \|u - u_1\| \leq \|u_0 - u_1\| + \delta\}$.
  By compactness, we have a finite subcover of open neighborhoods; let $\beta$ be the minimum Lipschitz constant over this finite set of neighborhoods.
  We thus have that $L_y$ is $\beta$-strongly smooth on $\mathcal C$, and hence so is $L_\lambda$ for any $\lambda \in [0,1]$.
  \bo{I thought maybe strong smoothness is equivalent to locally lipschitz gradient? If so this gets easier and it lets us assume the parameter $\beta$ from the beginning, which is nice because we get a somewhat constructive $c$ at the end.}
  
  We now upper bound $\|u_\lambda - u_0\|_2$, and then apply strong smoothness to upper bound $L_\lambda(u_0) - L_\lambda(u_\lambda)$.
  Consider the first-order optimality condition of $L_\lambda$:
  \begin{align*}
    \label{eq:first-order-opt-smooth}
    & 0 = \nabla L_\lambda(u_\lambda) = (1-\lambda) \nabla L_0(u_\lambda) + \lambda \nabla L_1(u_\lambda)
    \\
    & \implies (1-\lambda) \|\nabla L_0(u_\lambda)\|_2 = \lambda \|\nabla L_1(u_\lambda)\|_2~.
  \end{align*}
  By optimality of $u_0$ and $u_1$, strong convexity of $L_0$ and strong smoothness of $L_1$, and the triangle inequality, we have
  \begin{align*}
    \|\nabla L_0(u_\lambda)\|_2 &= \|\nabla L_0(u_\lambda) - \nabla L_0(u_0)\|_2 \geq \alpha \|u_\lambda - u_0\|_2~,
    \\
    \|\nabla L_1(u_\lambda)\|_2 &= \|\nabla L_1(u_\lambda) - \nabla L_1(u_1)\|_2 \leq \beta \|u_\lambda - u_1\|_2
    \\
    &\leq \beta \left( \|u_\lambda - u_0\|_2 + \|u_0 - u_1\|_2 \right)~.
  \end{align*}
  Combining,
  \begin{align*}
    (1-\lambda) \alpha \|u_\lambda - u_0\|_2
    &\leq
      (1-\lambda) \|\nabla L_0(u_\lambda)\|_2
    \\
    &= \lambda \|\nabla L_1(u_\lambda)\|_2
    \\
    &\leq
      \lambda \beta \left( \|u_\lambda - u_0\|_2 + \|u_0 - u_1\|_2 \right)~.
  \end{align*}
  Now rearranging and taking $\lambda \leq \tfrac 1 2 \tfrac {\alpha}{\alpha+\beta}$, we have
  \begin{align*}
    \|u_\lambda - u_0\|_2 \leq \frac{\lambda\beta}{(1-\lambda)\alpha-\lambda\beta} \|u_0 - u_1\|_2  \leq \lambda \frac{2\beta}{\alpha} \|u_0 - u_1\|_2 ~.
  \end{align*}
  Finally, from strong smoothness of $L_\lambda$ and optimality of $u_\lambda$,
  \begin{align*}
    L_\lambda(u_0) - L_\lambda(u_\lambda) \leq \frac{\beta}{2} \|u_0 - u_\lambda\|_2^2 \leq \frac{\beta}{2} \left(\lambda \frac{2\beta}{\alpha} \|u_0 - u_1\|_2\right)^2 = c_L \lambda^2~,
  \end{align*}
  where $c_L = \frac{2\beta^3}{\alpha^2} \|u_0 - u_1\|_2^2$.
  Take $c = \frac{c_\ell}{\sqrt{c_L}} = \frac{c_\ell \alpha}{\|u_0 - u_1\|_2 \sqrt{2\beta^3}}$.
  Recall $R_\ell(r',p_\lambda) = c_\ell \lambda$ and $R_L(u_0,p_\lambda) \leq c_L \lambda^2$.
  Then letting $\epsilon = R_L(u_0,p_\lambda)$, we have $\zeta(\epsilon) \geq R_\ell(r',p_\lambda) \geq c \sqrt{\epsilon}$.

\end{proof}


\section{Unpacking the constant for polyhedral losses}

\bo{Stuff about Hoffman constants, etc}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Acknowledgements (automatically hidden in submission)
\begin{ack}
  \raf{Stephen Becker for help with the strong convexity stuff}
  \raf{Nishant Mehta for many discussions}
  \raf{Jessie Finocchiaro for discussions}
\end{ack}

\section*{References}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerTODO{}
  \item Did you describe the limitations of your work?
    \answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerTODO{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerTODO{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerTODO{}
	\item Did you include complete proofs of all theoretical results?
    \answerTODO{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerTODO{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerTODO{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerTODO{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerTODO{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerTODO{}
  \item Did you mention the license of the assets?
    \answerTODO{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerTODO{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerTODO{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerTODO{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerTODO{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerTODO{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerTODO{}
\end{enumerate}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix}



\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
