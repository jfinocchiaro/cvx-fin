\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

%     to avoid loading the natbib package, add option nonatbib:
\PassOptionsToPackage{numbers, sort, compress}{natbib}
\usepackage[final]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{draft} % weird pdfendlink error
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green!80!blue}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{Bo: #1}}
\newcommand{\btw}[1]{\mytodo{gray!10!white}{\textcolor{gray}{BTW: #1}}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\simplex}{\Delta_\Y}
\newcommand{\relint}[1]{\mathrm{relint}(#1)}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\elic}{\mathrm{elic}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ccdim}{\mathrm{cc\,dim}}
\newcommand{\codim}{\mathrm{codim}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\spn}{\mathrm{span}}
\newcommand{\propdis}{\mu}
\newcommand{\affhull}{\mathrm{affhull}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\ext}{\mathrm{ext}}


\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Sc}{\mathcal{S}}  % jessie, feel free to redef, just not \S :-)
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\ellbar}{\underline{\ell}}
\newcommand{\lbar}{\underline{L}} % couldn't do L* while proofreading...
\newcommand{\iden}{\mathrm{iden}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\exploss}[3]{\E_{#3} #1(#2,Y)}
\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\ones}{\mathbbm{1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\usepackage{thmtools, thm-restate}
%\declaretheorem{corollary}


\title{Indirect elicitation as a necessary condition: followup}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jessie Finocchiaro\\
%  Department of Computer Science\\
  CU Boulder\\
  \texttt{jefi8453@colorado.edu} 
  % examples of more authors
  \And
   Rafael Frongillo\\
%   Department of Computer Science\\
   CU Boulder\\
  % Address \\
   \texttt{raf@colorado.edu} 
   \And
   Bo Waggoner\\
%   Department of Computer Science\\
   CU Boulder \\
  % Address \\
   \texttt{bwag@colorado.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}

\maketitle

\section{The question: can we get rid of the assumption that $\Gamma$ is nonempty?}
Let's address this by studying logistic loss.


\paragraph{Observation 1:} Logistic loss (with $\psi(u) = \sgn(u)$) does not (indirectly) elicit the mode \emph{over the simplex}.
\begin{proof}
	Consider $\Gamma := \prop{\text{Logistic loss}}$.
	We want to show that for some $u \in \reals$, there is a $p \in \mathrm{mode}_{\sgn(u)}$ so that $p \not \in \Gamma_u$.
	In particular, take $p := \delta_y$ for either $y$ (say, $y = 1$ WLOG), and $u = 1$.
	We can verify $\delta_1 \in \mathrm{mode}_{\sgn(1)} = 1$; however, $\delta_1$ is not in $\Gamma_1$, as the minimum is never attained.
	Therefore, logistic loss does not indirectly elicit the mode over the simplex.
\end{proof}

However, it does (indirectly) elicit the mode \emph{over $\relint{\simplex}$}.
Suppose we slightly alter our definition of indirect elicitation.

\begin{definition}[Indirect elicitation: current]
	A loss and link $(L, \psi)$ \emph{indirectly elicit} a property $\gamma:\simplex \toto \R'$ if $L$ elicits a property $\Gamma: \simplex \toto \U$ such that for all $u \in \U$, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$.
	We say $L$ \emph{indirectly elicits} $\gamma$ if such a link $\psi$ exists.
\end{definition}

Instead, we propose the following modification to consider when we directly elicit properties on \emph{subsets} of the simplex to cope with distributions where the minimum of the expected loss is never attained, and the property is therefore empty.


\begin{definition}[Indirect elicitation: proposed modification]
	A loss and link $(L, \psi)$ \emph{indirectly elicit} a property $\Gamma':\P \toto \R'$ if $L$ (with $\P \subseteq \simplex$) elicits a property $\Gamma: \simplex \toto \U$ such that for all $u \in \U$, we have $\Gamma_u \subseteq \Gamma'_{\psi(u)}$.
	We say $L$ \emph{indirectly elicits} $\gamma$ if such a link $\psi$ exists.
\end{definition}

\paragraph{Claim:} By the new definition, (logistic loss, $\sgn$) indirectly elicits the mode over $\P := \relint{\simplex}$.
\begin{proof}
	The min of expected logistic loss is attained for all $p \in \relint{\simplex}$, and the subset inclusion can be verified for all such $p$.
\end{proof}


\begin{proposition}\label{prop:restrict-loss-domain}
	Let $L: \reals^d \times\Y\to\reals$ be a convex loss, and $\Gamma := \prop{L}$.
	For a compact set $R$ such that $\Gamma(\simplex)\subseteq R \subseteq \reals^d$, the loss function $L' := L|_R$ eliciting $\Gamma'$, we have $\Gamma'(p) \subseteq \Gamma(p)$ for all $p$ such that $\Gamma(p)$ is nonempty.
\end{proposition}
\begin{proof}
	This largely follows from~\cite[Lemma 3]{finocchiaro2019embedding}.
	
	For all $p$ such that $\Gamma(p) \neq \emptyset$, we want to show $\Gamma(p) = \argmin_{u \in \reals^d} L(u;p) \supseteq \argmin_{u \in R} L(u;p) = \Gamma'(p)$. 
	
	To start, for any $p$ such that $\Gamma(p)$ is nonempty, suppose we have some $u' \in \Gamma'(p)$.
	By \cite[Lemma 3]{finocchiaro2019embedding}, we know $\Gamma' : p \mapsto \Gamma(p) \cap R$ , so we have $u' \in \Gamma'(p) \implies u' \in \Gamma(p) \wedge u' \in R$.
	We know such a $u'$ exists since we require $\Gamma$ is nonempty, and the statement holds for any $u \in \Gamma(p)$.
\end{proof}

The above result allows us to work with domains that are subsets of the reals to elicit something close to the desired property.
Now, we modify the original link function to move from $R$ back to $\reals^d$ while indirectly eliciting the property of interest. 

\begin{definition}\label{def:extended-prop}
	Suppose we are given an elicitable property $\Gamma: \P \toto \R$.  
	We define the \emph{extended property} $\ext(\Gamma) : \simplex \toto \R$ as the property so that for all $p \in \P$, $\ext(\Gamma)(p) = \Gamma(p)$.
	Moreover, if $\Gamma$ is finite, then for all $p \not \in \P$, we define $\ext(\Gamma)(p) = \Gamma(c_i)$, where $c_i = \argmin_{c \in C} \|p - c\|_2\}$, and $C$ is the set of sites of the power diagram defining $\Gamma$.
	If $\R$ is not finite
\end{definition}
We know that $\Gamma$ is defined by a power diagram thanks to the characterization of~\cite{lambert2009eliciting}.



\begin{proposition}\label{prop:extend-to-reals-d}
	For all $u'$, taking $R$ as above and $u^* \in \argmin_{u \in \reals^d} \|u - u'\|_2$ \jessiet{Check?} and designating the link $\psi'(u) := \psi(u^*)$.
	Suppose $(L, \psi)$ indirectly elicits the property $\Gamma' := \prop{L|_R}$.
	Then $(L,\psi')$ indirectly elicits $\ext(\Gamma')$.
\end{proposition}
\begin{proof}

\end{proof}

Proposition~\ref{prop:extend-to-reals-d} would allow us to extend results to losses whose elicited property is empty on part of the simplex by restricting the loss to a compact subset of the reals so that the minimum is attained, then redefining the link function so that we have the desired link generalizing to the original property.

\newpage
\section{Moving to infinite outcomes}
\begin{theorem}\label{thm:cvx-flats}
	Let a property $\gamma:\simplex\toto\R$ be given, let $p\in\simplex$ such that either (i) $|\gamma(p)|=1$ or (ii) $\gamma$ is elicitable and $p\in\relint\simplex$, and let $r\in\gamma(p)$.
	Then if $L: \reals^d \times \Y \to \reals$ indirectly elicits $\gamma$, there is a flat $F$ with $p\in F \cap \simplex \subseteq \gamma_r$ and $\codim(F) \leq d$.
	% COmmented 06.03.20
	%	If there is a convex surrogate loss $L : \reals^d \times\Y \to \reals$ indirectly eliciting $\gamma$, then for all $p \in \simplex$ and $r \in \R$ such that $r \in \gamma(p)$, there is a flat $F \subseteq \affhull(\simplex)$ of codimension at most $d$ such that $F \cap \simplex \subseteq \gamma_r$ and $p \in F$.
\end{theorem}
\begin{proof}
	
	%	There is some u in Gamma(p). (Gamma nonempty.)
	%	Gamma_u contains a big ol flat F.
	%
	%	Let r' = psi(u).
	%	Hence gamma_{r'} contains F.
	%	If singleton, we know r' = r, so done.
	%	Otherwise, elicitable and p in relint.
	%	If r' = r, done. Otherwise:
	%	We can apply Lemma 3: p is in gamma_r cap gamma_{r'}, F contains p and is in gamma_{r'}.
	%	So F is also in gamma_r.
	
	Let $\Gamma := \prop{L}$ and suppose $(L, \psi)$ indirectly elicits $\gamma$.
	By definition of a property, there is some $u \in \Gamma(p)$.
	We will first show $F \cap \simplex \subseteq \Gamma_u$.
	
	As $L$ is convex and elicits $\Gamma$, we have $u \in \Gamma(p) \iff \vec 0 \in \partial \exploss{L}{u}{p} \iff \vec 0 \in \oplus_y p_y \partial L(u,y)$, where $\oplus$ denotes the Minkowski sum~\citep[Theorem 4.1.1]{hiriart2012fundamentals}.
	% (Observe that in order for $L$ to be convex, $L(\cdot, y)$ must be convex for all $y \in \Y$.) 
	Expanding, we have $\oplus_y p_y \partial L(u,y) = \{ \sum_{y\in\Y} p_y x_y : x_y \in \partial L(u,y) \; \forall y\in\Y\}$,
	and thus $W p = \sum_y p_y x_y = \vec 0$ where $W = [x_1, \ldots, x_n] \in \reals^{d\times n}$;
	cf.~\cite[$\mathbf{A}^m$ in Theorem 16]{ramaswamy2016convex}.
	Now taking $F := \ker(W) \cap \affhull(\simplex)$, we have $\codim(F) = \rank(W) \leq d$ by Definition~\ref{def:flat}, and $p \in F$ by construction.
	%% Bo: the below sentence seemed to bring in gamma_r too early. Saved that for later. June 3 10pm
	To see $F \cap \simplex \subseteq \Gamma_u$, from the chain of equivalences above, we have for any $q\in\simplex$ that $q \in \ker W \implies u \in \Gamma(q) \implies q \in \Gamma_u$.
	
	%now the justification of r vs r'
	Now, we show $\Gamma_u \subseteq \gamma_r$, which will complete the proof.
	Let $r' = \psi(u)$; by definition of indirect elicitation, we have $\gamma_{r'} \supseteq \Gamma_u \supseteq F \cap \simplex$.
	If $|\gamma(p)| = 1$, then $r' = r$, so we are done.
	Otherwise, we have $\gamma$ elictable and $p \in \relint{\simplex}$.
	Apply Lemma~\ref{lem:set-valued-prop-flats}, which states that for elicitable properties with $p \in \gamma_r \cap \gamma_{r'}$, so $p \in F \cap \simplex \subseteq \gamma_{r'} \iff p \in F \cap \simplex \subseteq \gamma_r$.
\end{proof}

Proof with comments on infinite outcomes.
\begin{proof}
We will first show $F \cap \simplex \subseteq \Gamma_u$.

As $L$ is convex and elicits $\Gamma$, we have $u \in \Gamma(p) \iff \vec 0 \in \partial \exploss{L}{u}{p}$ \jessiet{Nope.  Minkowski sum of infinitely many terms not defined} $\iff \vec 0 \in \oplus_y p_y \partial L(u,y)$, where $\oplus$ denotes the Minkowski sum~\citep[Theorem 4.1.1]{hiriart2012fundamentals}.
\end{proof}

\bigskip\hrule \bigskip

Reference: \url{https://sites.math.washington.edu/~rtr/papers/rtr169-VarAnalysis-RockWets.pdf}, Chapters 8 and 10.

Seemingly useful statements and definitions:
\begin{definition}[Locally optimal]
A point $x$ is \emph{locally optimal} in minimizing a function $f$ if $x \in \argmin (f + \delta_V)$ for some $V \in \N(x)$, where $\delta_V$ is a $(0, \infty)$ indicator function for set inclusion, and $\N(x)$ is the collection of all neighborhoods of $x$.
\end{definition}
The \emph{normal cone} of a set $C$ at point $x$ is denoted $N_C(x)$, and similarly for the set of tangent vectors $T_C(x)$ (Def 6.1).

\begin{definition}[Subderivatives, RW Def 8.1]
	For a function $f : \reals^d \to \bar \reals$ and a point $\bar x$ with $f(\bar x)$ finite, the \emph{subderivative function} $df(\bar x) : \reals^d \to \bar \reals$ is defined
	\begin{equation*}
	df(\bar x)(\bar w) := \liminf_{\tau \searrow 0, w \to \bar w} \frac{f(\bar x + \tau w) - f(\bar x)}{\tau}
	\end{equation*}
\end{definition}

\begin{definition}[subgradients, RW Def 8.3]
	Consider a function $f : \reals^d \to \bar \reals$ and a point $\bar x$ with $f(\bar x)$ finite.
	For a vector $v \in \reals^d$, one says that
	\begin{enumerate}
		\item $v$ is a \emph{regular subgradient} of $f$ at $\bar x$, written $v \in \hat \partial f(\bar x)$, if for all $x$, we have $f(x) \geq f(\bar x) + \langle v, x- \bar x \rangle + o(|x - \bar x|).$
		\item $v$ is a \emph{general subgradient} of $f$, written $v \in \partial f(\bar x)$ if there are sequences $x^\nu \to_f \bar x$ and $v^\nu \in \hat \partial f(x^\nu)$ with $v^\nu \to v$.
		\item $v$ is a \emph{horizon subgradient} of $f$ at $\bar x$, written $v \in \partial^\infty f(\bar x)$, if the same holds as general subgradients, except that instead of $v^\nu \to v$, one has $\lambda^\nu v^\nu \to v$ for some sequence $\lambda^\nu \searrow 0$. 
	\end{enumerate}
\end{definition}

\begin{theorem}[Optimality relative to a set, RW Theorem 8.15]
	Consider the problem of minimizing a proper, lower semicontinuous function $f_0 : \reals^d \to \bar \reals$ over a closed set $C \subset \reals^d$.
	Let $\bar x$ be a point of $C$ at which the following constraint is satisfied: the set  $\partial^\infty f_0(\bar x)$ contains no vector $v \neq \vec 0$ so that $-v \in N_C(\bar x)$.
	Then for $\bar x$ to be locally optimal, it is necessary that $\vec 0 \in \partial f_0(\bar x) + N_C(\bar x)$.
	
	If $f_0$ nand $C$ are regular at $\bar x$, this is equivalent to $d f_0(\bar x)(w) \geq 0$ for all $w \in T_C(\bar x)$.
	
	When $f_0$ and $C$ are convex, either condition is sufficient for $\bar x$ to be globally optimal, even if the constraint qualification is not fulfilled.
\end{theorem}

Just noting that we can now break this down to either definition and see if this says anything about dimension of flats?
Or requirements of flat dimension in order for the conditions to be satisfied.
I have a feeling the first statement is more likely to be useful for us.

With $f$ convex and $C = \reals^d$, we have $\psi(\bar x) \in \Gamma(p) \iff \bar x$ globally optimal $\iff \vec 0 \in \partial f(\bar x) + N_C (\bar x)$.

We can also search for $C$ so that the statement holds for the same set of $V$.
The dimension of $C$ gives us information about the flat that might be useful.
However, searching for $C$ seems hard... 
 

\bibliographystyle{natbib}
\bibliography{diss,extra}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
