\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{[BTW: #1]}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\simplex}{\Delta_\Y}
\newcommand{\relint}[1]{\mathrm{relint}(#1)}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\elic}{\mathrm{elic}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ccdim}{\mathrm{ccdim}}
\newcommand{\codim}{\mathrm{codim}}

\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\toto}{\rightrightarrows}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\title{cvx-flats}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jessie Finocchiaro\\
%  Department of Computer Science\\
  CU Boulder\\
  \texttt{jefi8453@colorado.edu} 
  % examples of more authors
  \And
   Rafael Frongillo\\
%   Department of Computer Science\\
   CU Boulder\\
  % Address \\
   \texttt{raf@colorado.edu} 
   \And
   Bo Waggoner\\
%   Department of Computer Science\\
   CU Boulder \\
  % Address \\
   \texttt{bwag@colorado.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}

\maketitle

\begin{abstract}
%	For finite prediction tasks, one can often naturally model their specific problem with a discrete loss.
%	However, these discrete losses are often hard to optimize, so one often seeks a surrogate losses yielding desirable statistical guarantees with respect to the original problem.
	In prediction tasks, one typically seeks to minimize \emph{empirical risk} by learning a hypothesis function making predictions from a given feature vector about a desired future outcome.
	This hypothesis is learned by minimizing a loss function over a set of labeled training data.
	However, as not all loss functions are easily optimized, one often wishes to optimize a \emph{surrogate loss} yielding the same statistical guarantees as the original.
	In this paper, we are specifically interested in the construction of \emph{consistent} surrogate losses that are ``efficient'' in the dimension of their input.
	We observe a connection between consistent surrogates and indirect property elicitation, which allows us to apply property elicitation complexity results to the minimal dimension of a consistent surrogate.
	In particular, we characterize the minimal input dimension needed in order to construct a \emph{convex}, consistent surrogate using tools from property elicitation.
\end{abstract}

\section{Introduction}\label{sec:intro}


\subsection{Notation}

\section{Related work}\label{sec:related-work}

\subsection{Property elicitation}\label{subsec:properties}

\begin{definition}[Property, elicits, level set]
	\jessiet{Don't like the notation here... fix later.}
	A \emph{property} is a (possibly set-valued) function $\Gamma : \simplex \toto \R$ mapping distributions to reports.
	A loss $L : \R \times \Y \to \reals_+$ \emph{elicits} the property $\Gamma$ if,
	\begin{equation}
	\forall p \in \simplex, \;\; \Gamma(p) = \arginf_{u \in \R}\inprod{p}{L(u)}
	\end{equation}
	Moreover, we call a \emph{level set} $\Gamma_r := \{p \in \simplex : r \in \Gamma(p)\}$ to be the set of distributions for which reporting $u$ minimizes the expected loss of the loss eliciting $\Gamma$.
\end{definition}

\begin{definition}[Indirect Elicitation]\label{def:indirectly-elicits}
\jessiet{Explain to reader why this is a good def. Made note in original doc.}
	A loss $L$ \emph{indirectly elicits} a property $\gamma:\simplex \toto \R'$ if it elicits a property $\Gamma: \simplex \toto \R$ such that there is a function $\psi:\R \to \R'$ such that for all $r \in \R$, we have $\Gamma_r \subseteq \gamma_{\psi(r)}$.
\end{definition}


\subsubsection{Elicitation Complexity}\label{subsec:elic-cplx}

\begin{definition}[Convex Elicitation Complexity]
	The \emph{convex elicitation complexity} of a property $\eliccvx(\Gamma)$ is the minimum dimension $d$ such that there is a convex loss $L : \reals^d \times \Y \to \reals$ indirectly eliciting $\Gamma$.
\end{definition}
%\jessie{Just introduce cvx definitions.  Combine two bracketed lines from Raf's notes.  Just define the convex version.  Want to use def 2 with set-valued properties.}


\subsection{Consistency and calibration for convex losses}\label{subsec:convex-surrogates}

\begin{definition}[Consistent: loss]
	Let $\{f_m : \X \to \R\}$ be a sequence of hypothesis functions.
	A loss and link $(L,\psi)$ are consistent with respect to an original loss $\ell$ if, for all distributions $D$ over input and label spaces $\X \times\Y$, 
	\begin{align*}
	\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y) &\implies \E_D \ell((\psi \circ f_m)(X), Y) \to \inf_f \E_D \ell((\psi \circ f)(X), Y)~.~
	\end{align*}
\end{definition}

%\raf{Issues with the definition:
%\begin{enumerate}
%\item Good to clarify $d:\R\times\R\to\reals_+$ -- done
%\item We have two $d$'s so we'll need a new letter... -- changed to $\mu$
%\item $D_X$ is not defined, but I got it -- clarified it was the marginal distribution
%\item You can't take an expectation over $\R$ values (not necessarily a subset of a vector space) and want the expectation over $X$ on the outside
%\end{enumerate}
%At this point I lost the thread. It looks like you're missing the hypothesis function, and $\Gamma(D)$ doesn't seem to type check -- but perhaps $\Gamma(D)$ was meant to depend on $X$ and be the hypothesis? -- and the implication also seems backwards.}
\begin{definition}[Consistent: property]
	A loss $L : \R \times \Y \to \reals$ and link $\psi : \R' \to \R$ pair $(L, \psi)$ is \emph{$\mu$-consistent with respect to a property} $\gamma: \simplex \toto \R'$ for a distance $\mu:\R \times \R \to \reals_+$ if, for all $D$ over $\X \times \Y$ with marginal distributions $D_x$, and sequences of properties $\{\gamma_m: \simplex \toto \R'\} \to \gamma$, we observe
	\begin{equation}
	\E_{(X,Y) \sim D} L( \psi \circ \gamma_m(D_X), Y) \to \E_{(X,Y) \sim D} L(f(X), Y) \implies \E_X \mu(\psi \circ \gamma_m(D_X), f(X)) \to 0~,~
	\end{equation}
	where $f = \arginf_{f^* \mathrm{msbl}} \E_{(X,Y) \sim D} L(f^*(X), Y)$.
\end{definition}


%%I don't think we're going to need this definition...
%\begin{definition}[Excess risk bound]
%	A surrogate loss and link pair $(L,\psi)$ satisfies the \emph{excess risk bound} with respect to a loss $\ell$ if there exists an increasing function $\zeta : \reals \to \reals$ that is continuous at $0$ with $\zeta(0) = 0$ so that for all $f:\X \to \R$ and data distributions $D$ over $\X \times \Y$, we have
%	\begin{multline}
%	\E_{(X,Y) \sim D} \ell(\psi \circ f(X), Y) - \inf_{f^*} \E_{(X,Y) \sim D} \ell(\psi \circ f^*(X), Y) \\ 
%	\leq \zeta \left( \E_{(X,Y) \sim D} L(f(X), Y) - \inf_{f^*} \E_{(X,Y) \sim D} L(f^*(X), Y) \right)
%	\end{multline}
%\end{definition}

%\begin{proposition}
%	A surrogate link pair $(L, \psi)$ is consistent with respect to a loss $\ell$ if and only if it is consistent with respect to the property elicited by $\gamma := \prop{\ell}$.
%\end{proposition}
%\begin{proof}
%	$\implies:$ 
%	If $(L, \psi)$ is consistent with respect to $\ell$, then for all $x \in \X$ with $p = Pr[Y|X]$, we have $L(u; p) \to \inf_{u'} L(u';p) \implies \ell(\psi(u); p) \to \inf_{u'} \ell(\psi(u');p)$.
%	As $\inf_{u'} L(u';p) = L(\Gamma(p); p)$, we then have $L(\Gamma(p); p) - L(u;p) \to 0 \implies \ell(\psi(\Gamma(p)); p) - \ell(\psi(u);p) \to 0$.
%	If $\gamma: p \mapsto \psi(\Gamma(p))$ for all $p \in \simplex$, then we have consistency with respect to $\gamma$, as $\zeta$ being the identity satisfies the requirements on $\zeta$ and yields the bound as the convergence yields $0 \leq 0$ for all $p \in \simplex$.
%	
%	To see $\gamma = \psi \circ \Gamma$, consider that for all $p \in \simplex$, we have $\arginf_{u \in \reals^d} L(u;p) = \arginf_{u \in \reals^d} \ell(\psi(u); p)$ as a corollary of consistency with respect to $\ell$, which implies the statement.
%	
%	$\impliedby:$ 
%	If $(L, \psi)$ is consistent with respect to $\gamma$, then there is a monotonic function $\zeta:\reals \to \reals$ that is continuous at $0$ with $\zeta(0) = 0$ such that, for all $p \in \simplex$, we have $\ell( \psi(u); p) - \ell(\gamma(p); p)  \leq \zeta \left(  L(u;p) - L(\Gamma(p); p) \right)$.
%	
%	As this is true for all $p = Pr[Y|X] \in \simplex$, this also holds in expectation over $X$.
%	This gives the excess risk bound being satisfied, which has been shown implies consistency with respect to $\ell$.\jessiet{In Hari abstain paper, but might be worth proving it here, given the subtleties of consistency definitions...}
%\end{proof}


\begin{definition}[Calibrated]\label{def:calibrated-general}
	A loss $L:\reals^d \times \Y \to \reals$ is \emph{calibrated} with respect to a loss $\ell : \R \times \Y \to \reals$ eliciting the property $\gamma$ if there is a link $\psi : \reals^d \to \R$ such that, for all distributions $p \in \simplex$, there exists a function $\zeta : \reals_+ \to \reals_+$ with $\zeta$ continuous at $0^+$ and $\zeta(0) = 0$ such that for all $u \in \reals^d$, we have
	\begin{equation}\label{eq:calibrated-general}
	\ell( \psi(u); p) - \ell(\gamma(p); p)  \leq \zeta \left(  L(u;p) - L(\Gamma(p); p) \right)~.~
	\end{equation}
\end{definition}
\begin{definition}[Calibrated: Finite predictions]\label{def:calibrated-finite}
	Let $\ell : \R \times \Y \to \reals_+$ be a discrete loss eliciting the property $\gamma$.
	A surrogate loss $L : \reals^d \times \Y \to \reals_+$  and link $\psi:\reals^d \to \R$ pair $(L, \psi)$ is \emph{calibrated} with respect to $\ell$ if 
	\begin{equation}\label{eq:calibration}
	\forall p \in \simplex: \inf_{u \in \reals^d : \psi(u) \not \in \gamma(p)} L(u;p) > \inf_{u \in \reals^d} L(u;p)~.~
	\end{equation}
\end{definition}

\begin{definition}[Convex Calibration Dimension]
The \emph{convex calibration dimension} $\ccdim(\ell)$ of a discrete loss $\ell$ is the minimum dimension $d$ such that there is a convex loss $L: \reals^d \times \Y \to \reals$ and link $\psi$ such that $L$ is calibrated with respect to $\ell$.
\end{definition}


\section{Characteristics of Consistent Convex Surrogates}\label{sec:char-convex}

\jessie{Should this also be a lemma instead of a proposition?}
\begin{proposition}\label{prop:consistent-implies-calibrated}
	If a loss and link $(L, \psi)$ are consistent with respect to a loss $\ell$, then they are calibrated with respect to $\ell$.
	\jessiet{Probably appendix later.}
\end{proposition}
\begin{proof}
	Let $\gamma$ be the property elicited by $\ell$.
	Suppose $(L,\psi)$ is not calibrated with respect to $\ell$; we will show they are not consistent with respect to $\ell$ either.
	$(L, \psi)$ not calibrated with respect to $\ell$ means that there is some distribution $p \in \simplex = D_x$ for some $x \in \X$ such that there is no function $\zeta : \reals_+ \to \reals_+$ with $\zeta(0) = 0$ and continuous at $0^+$ such that, for all $u \in \reals^d$, we have $\ell(\psi(u); p) - \ell(\gamma(p);p) \leq \zeta(L(u;p) - L(\Gamma(p);p))$. 
	In other words, for all $\zeta$ satisfying the assumptions, we have $\ell(\psi(u); p) - \ell(\gamma(p);p) > \zeta(L(u;p) - L(\Gamma(p);p))$.
	
	In particular, there must exist a sequence $\{u_m\} \to \Gamma(p)$ so that $\psi(u_m) \not \in \gamma(p)$ for all $m$.
	Now consider the data distribution $D$ that has a point mass on some $x \in \X$, where $p = D_x$.
	Moreover, let $f_m$ be any sequence of functions with $f_m(x) = u_m$ for all $m$.
	We then have $L(f_m(x); p) \to L(\Gamma(p);p)$, so $\lim_{m \to \infty}\zeta(L(u;p) - L(\Gamma(p);p)) \to 0$.
	This gives $\ell(\psi(u);p) > \ell(\gamma(p);p) + \epsilon$ for some $\epsilon \to 0$, which implies $\ell(\psi(u_m); p) \not \to \ell(\gamma(p);p)$; thus, we do not have consistency.
	Therefore, if $(L, \psi)$ are consistent with respect to $\ell$, they are also calibrated with respect to $\ell$.
	
	
%commented out May 14, 2020 to adapt Hari/Shivani's proof
%Since consistency must hold for all $D$ over $\X \times \Y$, let us focus on distributions $D$ such that the probability of any $x \in \X$ is a point mass.
%If consistency implies calibration for the marginal distributions $D_x$ for all $x \in \X$, then it holds for all $D$.
%Now, for such distributions, we have 
%\begin{align*}
%\E_{D_x} L(f_m(x), Y) \to \inf_f \E_{D_x} L(f(x), Y) &\implies \E_{D_x} \ell((\psi \circ f_m)(x), Y) \to \inf_f \E_{D_x} \ell((\psi \circ f)(x), Y)~,~
%\end{align*}
%for all $x \in \X.$
%This allows us to focus on $p \in \simplex$, abstracting away hypothesis classes to consider general predictions $u$.
%
%Now, fix $p \in \simplex$.
%We proceed in two cases; first, when we approach the optimal loss, then when we do not. 
%If $L(u;p) \to \inf_{u'} L(u';p)$, then $\ell(\psi(u); p) \to \inf_{r}\ell(r; p)$.
%Substituting into~\eqref{eq:calibrated-general}, we see the bound is satisfied by any function $\zeta$ satisfying calibration requirements; namely, $\zeta(0) = 0$ and continuity at $0$.
%%%adapting tewari proof
%%%Suppose $(L, \psi)$ is calibrated with respect to $\ell$ but there exists $\epsilon > 0$ and seequence $\{p_m\}$ such that $\ell(\psi(u_m); p_m) \leq \inf_r\ell(r;p_m) - \epsilon$ and $\left(L(u_m;p_m) - \inf_{u'}L(u'; p_m)\right) \to 0$.
%%%Since $\{p_m\}$ comes from a compact set, there is a convergent subsequence that limits to $p$.
%%%As $\risk{L}$ is continuous over $\simplex$, we get $L(u_m; p_m) \to \inf_{u'} L(u'; p)$.
%
%\jessie{I'm not 100\% sure this is the \emph{only} case... this is what isn't covered by Tewari's proof~\cite[Appendix A; pf of thm 2]{tewari2007consistency}}
%Now if $L(u;p) \not \to \inf_{u'}L(u; p)$, we do not know anything about the convergence of $\ell$.
%However, if there was distribution $p \in \simplex$ such that $\ell(\psi(u); p) - \inf_r \ell(r;p) > \zeta(L(u;p) - \inf_{u'} L(u';p))$ for all strictly increasing functions $\zeta$, it must be because there exists a $u'$ so that $L(u';p) > L(u;p), \ell(\psi(u); p) = \infty$, and $\ell(\psi(u'); p) < \infty$.
%However, we cannot have $\ell(\psi(u); p) = \infty$ unless $\ell$ was allowed to take on infinite values, which we do not allow in the definition of $\ell$.
\end{proof}

\begin{lemma}\label{lem:calib-implies-indir}
	If a surrogate and link $(L, \psi)$ are calibrated with respect to a loss $\ell:\R \times\Y \to \reals$, then $L$ indirectly elicits the property $\gamma := \prop{\ell}$.
\end{lemma}
\begin{proof}
	Let $\Gamma$ be the unique property directly elicited by $L$, and fix $p \in \simplex$ with $u$ such that $p \in \Gamma_u$.
	As $p \in \Gamma_u$, then $L(u;p) - L(\Gamma(p); p) = 0$, and $\zeta(0) = 0$, we observe the bound $\ell(\psi(u); p) \leq \ell(\gamma(p); p)$.
	We also have $\ell(\psi(u); p) \geq \ell(\gamma(p); p)$ by definition of $\gamma$, so we must have $\ell(\psi(u);p) = \ell(\gamma(p); p)$, and therefore, $p \in \gamma_{\psi(u)}$.
	Thus, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, so $L$ indirectly elicits $\gamma$.
\end{proof}

\begin{theorem}\label{thm:consistent-implies-indir-elic}
	If a surrogate and link pair $(L, \psi)$ is consistent with respect to a property $\gamma$ or loss $\ell$ eliciting $\gamma$, then $L$ indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}
If we are given a loss $\ell$, then we can string together Proposition~\ref{prop:consistent-implies-calibrated} and Lemma~\ref{lem:calib-implies-indir} to observe consistency $\implies$ calibration $\implies$ indirect elicitation.

If we are just given the property $\gamma$ without a loss, assume that we have a sequence of properties $\{\gamma_m\} \to \gamma$ such that $\E_{(X,Y) \sim D} L( \psi \circ \gamma_m(D_X), Y) \to \E_{(X,Y) \sim D} L(f(X), Y)$, where $f$ minimizes $E_D L(f^*(X), Y)$ over measurable functions $f^*$.
We must then have $\E_X \mu(\gamma_m(D_X), f(X)) \to 0$ by consistency.
As in the proof of Proposition~\ref{prop:consistent-implies-calibrated}, this must be true for all $D_x$ for $x \in \X$, so we can abstract to distributions over $\simplex$.
Now, by consistency, we have the expected losses converging implies $\{\psi \circ \gamma_m\}$ is converging toward the optimal hypothesis for all possible inputs $x \in \X$.

Let $\Gamma$ be the property directly elicited by $L$.
We have $\Gamma(D_x) = f(x)$ for all $x \in \X$, so we can see that the precedent being true implies $\mu(\psi \circ \gamma_m(p), \Gamma(p)) \to 0$ for all $p\in \simplex$.
As $\mu$ is a distance\jessiet{Maybe need $\mu$ continuous in $\R^2$?}, $\mu(\psi \circ \gamma(p), \Gamma(p)) = 0$ for all $p \in \simplex$, which implies $\Gamma = \psi \circ \gamma$ as $\mu$ is a distance.
Therefore, $L$ must indirectly elicit the property $\gamma = \lim_{m \to \infty} \{\gamma_m\}$ via the link $\psi$.
\end{proof}


\subsection{On dimensionality of consistent convex surrogates}
\begin{definition}[Flat]
	A \emph{flat} $F$ of $\reals^n$ is an affine subspace of $\reals^n$.
	In other words, $F\subseteq \reals^n$ can be written $F=\{x\in\reals^n : Wx + b = 0\}$ for some $W\in\reals^{d\times n}$ and $b\in\reals^d$ where $d\leq n$.
	The dimension of the flat $F$ is given by $f := n - \mathrm{rank}(W)$.
\end{definition}

\begin{theorem}\label{thm:cvx-flats}
	Suppose we are given a property $\gamma$ and distribution $p \in \simplex$.
	For all $r\in\gamma(p)$, if there is no $(n - d-1)$-dimensional flat $F$ containing $p$ so that $F \cap \simplex \subseteq \gamma_r$, then there is no convex surrogate loss $L : \reals^d \times \Y \to \reals$ that indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}[Proof]
	We prove the contrapositive: if there is a surrogate loss $L$ indirectly eliciting $\gamma$, then for all $p \in \simplex$ and $r \in \R$ such that $r \in \gamma(p)$, there is an $(n-d-1)$-dimensional flat $F$ such that $F \cap \simplex \subseteq \gamma_r$ and $p \in F$.
	
	Suppose we have $L:\reals^d \times \Y \to \reals$ eliciting $\Gamma$ and link $\psi : \reals^d \to \R$ such that for all $u \in \reals^d$, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, i.e., $L$ indirectly elicits $\gamma$.
	Fix $p \in \simplex$, and take any $u \in \Gamma(p)$ such that $\psi(u) = r \in \gamma(p)$.
	We know that $u \in \Gamma(p) \iff \vec 0 \in \partial L(u; p) \iff \vec 0 \in \oplus_y p_y \partial L(u,y)$, where $\oplus$ denotes the nonnegative weighted Minkowski sum~\cite[Theorem 4.1.1]{hiriart2012fundamentals}.  (Observe that in order for $L$ to be convex, $L(\cdot, y)$ must be convex for all $y \in \Y$.) 
	
	Observe that the Minkowski sum $\oplus_y p_y \partial L(u,y) = \{p_1 x_1 + \ldots + p_n x_n : x_1 \in \partial L(u,1), \ldots, x_n \in \partial L(u,n) \}$.
	In order for $\vec 0$ to be in this $p$-weighted Minkowski sum, we then have that each choice of $x_i$ is a flat for $\partial L(u, i)$.
	Moreover, with $p$ fixed, we know there is a vector of witnesses $W' = [x_1, \ldots, x_n] \in \reals^{d\times n}$ such that $\sum_y p_y x_y = \vec 0$.

	Observe that $F' := \ker(W')$ is at most $(n-d)$ dimensions in $\reals^n$ and contains $p$ by construction.
	Since we are only concerned with a flat intersected with the simplex being a subset of $\gamma_r$, we can reduce the dimension by adding an affine simplex constraint. 
	That is, take $F := \{x \in \reals^n : W x + b = \vec 0\}$, where we have $W := [W'; \mathbbm{1}^T] \in \reals^{(d+1) \times n}$ and $b \in \reals^{d+1}$.
	The dimension of $F$ is then $n - \mathrm{rank}(W)$, which is $n - (d+1)$ if $W$ is full rank.
	We also claim that $F \cap \simplex \subseteq \gamma_r$.
	To see this, consider $q \in F$, but $q \not \in \gamma_r$.
	That would mean that $Wq + b = \vec 0 \iff W'q + b' = \vec 0$ (since $q \in \simplex$ satisfies the simplex constraints).
	This in turn implies $\vec 0 \in \partial L(u;q)$, implying $q \in \Gamma_u$.
	Therefore, $q \not \in \gamma_r$ would contradict $L$ indirectly eliciting $\gamma$.
\end{proof}


\section{Discrete-valued predictions}\label{sec:finite-calib}

%\begin{lemma}
%	Suppose we are given a discrete loss $\ell : \R \times\Y \to \reals_+$ eliciting the property $\gamma$.
%	If $(L, \psi)$ is calibrated with respect to $\ell$, then $L$ indirectly elicits $\gamma$.
%\end{lemma}
%\begin{proof}
%	Fix $p \in \simplex$ and consider $u \in \Gamma(p)$.
%	Since $(L, \psi)$ is calibrated with respect to $\ell$, we have $p \in \Gamma_u$.
%	We simply need to show $\psi(u) \in \gamma(p)$.
%	If $\psi(u) \not \in \gamma(p)$, then the terms in the condition for calibration (Equation~\eqref{eq:calibration}) are equal; hence $L$ would not be calibrated with respect to $\ell$.
%\end{proof}

\begin{definition}[Subspace of feasible directions]
	Define the \emph{subspace of feasible directions} $\S_\C(p)$ of a convex set $\C \subseteq \reals^n$ at a point $p \in \C$ as the subspace $\S_\C(p) = \{ v \in \reals^n : \exists \epsilon_0 > 0 $ such that $p + \epsilon v \in \C \; \forall \epsilon \in (-\epsilon_0,\epsilon_0) \}$.
%  \raf{Let's simplify to $\epsilon \in (-\epsilon_0,\epsilon_0)$}
\end{definition}

\begin{lemma}\label{lem:feas-sub-is-a-flat}
	Suppose we have the finite elicitable property $\gamma$ and distribution $p \in \relint{\simplex}$ with $r \in \gamma(p)$.
	If $F$ is a flat containing $p$ such that $F \cap \simplex \subseteq \gamma_r$, then $F - p$ is a subspace contained in $\S_{\gamma_r}(p)$.
  \raf{I think you want $p$ in the interior of the simplex for now}
\end{lemma}
\begin{proof}
  To spell it out, observe $F-p$ is a subspace as it is a linear transformation of $F$, which is a subspace by definition of a flat.
  
  Now consider $q \in F - p$.
  First, if $p+q \in \gamma_r$ and $p -q \in \gamma_r$, then we have $q \in \S_{\gamma_r}(p)$ with $\epsilon_0 = 1$ as level sets of elicitable properties are convex by~\cite{lambert2009eliciting}.
  If either $p + q$ or $p - q \not \in \gamma_r$, it must be because the term is out of the simplex by definition of $F$.
  
  However, if both $p + \alpha^+ q$ and $p - \alpha^- q \in \simplex$ for some $\alpha^\pm \in (0,1)$, then $q \in \S_{\gamma_r}(p)$ with $\epsilon_0 = \min(\alpha^+, \alpha^-)$.
  As $p \in \relint{\simplex}$, there is always such an $\epsilon_0$; if there were not, then we would observe $q \not \in F - p$.
  Therefore, we have $q \in F - p \implies q \in \S_{\gamma_r}(p)$, so $(F - p) \subseteq \S_{\gamma_r}(p)$.
\end{proof}


%\begin{lemma}\label{lem:feas-sub-is-a-flat}
%	Suppose we have the finite property $\gamma$ and distribution $p \in \simplex$.
%	For ant $r$ such that $r \in \gamma(p)$, the subspace $F := \mu_{\gamma_r}(p) + p $ is a flat, $F$ contains $p$, and $F \cap \simplex \subseteq \gamma_r$.
%\end{lemma}
%\begin{proof}
%	Write the level set $\gamma_r = \{q \in \reals^n : A_1 q \leq b_1, \; A_2 q \leq b_2, \; A_3 q \leq b_3 \}$, where $A_1 p \leq b_1, A_2 p < b_2,$ and $A_3 p = b_3$.
%	
%	We want to show three things: first, $F$ is a flat as it is $\ker([A_1 ; A_3])$ by~\cite{ramaswamy2016convex}.
%	Second, $p \in F + p$ by construction, since $\vec 0 \in F$.
%	(Consider that $p \in \gamma_r$, so with $epsilon_0 = 1$, we have $p + \epsilon\vec 0 = p \in \gamma_r$ for all $\epsilon \in (0, 1)$; thus, $\vec 0 \in F$.)
%	
%	Third, we want to show $(F + p) \cap \simplex \subseteq \gamma_r$.
%	Take some $q := p + \epsilon v$ for $v \in F$.
%	By construction of $F$, we have $q \in F \implies q \in \simplex$, so $F + p \subseteq \simplex \implies F + p \cap \simplex = F + p$.
%	Thus it just remains to be shown that $F + p \subseteq \gamma_r$.
%	Since $v\in F \implies q \in F + p \implies p + \epsilon v \in \gamma_r$ by construction, so we have $F+p \subseteq \gamma_r$.	
%\end{proof}

\begin{corollary}[\cite{ramaswamy2016convex}]
	Suppose we are given a discrete loss $\ell:\R \to \reals^\Y_+$ eliciting $\gamma$.
	Take $p \in \simplex$ and $r \in \R$ such that $p \in \gamma_r$.
	\begin{equation}
	\ccdim(\ell) \geq \|p\|_0 - \dim(\S_{\gamma_r}(p)) - 1~.~
	\end{equation}
\end{corollary}

\subsection{Elicitation complexity and convex calibration dimension}

\begin{proposition}
	Consider the discrete loss $\ell : \R \times \Y \to \reals_+$ and $\gamma:= \prop{\ell}$.
	Then $\eliccvx(\gamma) \leq \ccdim(\ell)$.
	\jessie{Need to define $\eliccvx$ and $\ccdim$.}
\end{proposition}
\begin{proof}
	Let $d$ be the convex calibration dimension of $\ell$, and $(L, \psi)$ be calibrated with respect to $\ell$ with $L$ convex.
	Then $L$ indirectly elicits $\gamma$.
\end{proof}



\section{Continuous-valued predictions}\label{sec:contin-consis}



\newpage

\section*{Broader Impact}

\begin{ack}
Nishant, Adam
\end{ack}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

\newpage
\appendix
\section{Omitted proofs}
\begin{proposition}
	When $\R$ is finite and $L$ continuous in $\reals^d$, calibration of a property via Definition~\ref{def:calibrated-general} implies calibration via Definition~\ref{def:calibrated-finite}.
\end{proposition}
\begin{proof}
	Let $\Gamma$ be the property elicited by $L$.
	We prove the contrapositive; if $(L, \psi)$ is not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then it is not calibrated via Definition~\ref{def:calibrated-general} either.
	
	Suppose there was a distribution $p \in \simplex$ and sequence $\{u_m\} \to u$ so that $\psi(u_m) \not \to \gamma(p)$ but $L(u_m;p) \to L(\Gamma(p);p)$.
	We would then have $\inf_{u : \psi(u) \not \in \gamma(p)} L(u;p) = \inf_{u} L(u;p)$.
	
	We want to show that there is no $\zeta:\reals_+ \to \reals_+$ continuous at $0^+$ with $\zeta(0) = 0$ so that 
	\begin{align*}
	\ell(\psi(u);p) - \ell(\gamma(p);p) \leq \zeta(L(u;p) - L(\Gamma(p); p))
	\end{align*}
	for all $u \in \reals^d$.
	We show that if this bound is satisfied, one of the two conditions on $\zeta$ (continuity at $0^+$ and $\zeta(0) = 0$) must be violated.
	
	Fix $x \in \X$ and take $u_m = f_m(x)$ \jessie{...}

	
	First, suppose that $\zeta$ is continuous at $0^+$ and the bound is satisfied.
	Then for the sequence $\{u_m\} \to \Gamma(p)$, we have 
	\begin{align*}
	\lim_{m \to \infty}\ell(\psi(u_m); p) - \ell(\gamma(p);p)&\leq \lim_{m \to \infty} \zeta(L(u_m; p) - L(\Gamma(p);p))\\
	\lim_{m \to \infty}\ell(\psi(u_m) ; p) &\leq \ell(\gamma(p);p) + \lim_{m \to \infty} \zeta(L(u_m; p) - L(\Gamma(p);p))\\
	\lim_{m \to \infty}\ell(\psi(u_m) ; p) &\leq \ell(\gamma(p);p) + \lim_{\epsilon \to 0^+} \zeta(\epsilon)
	\end{align*}
	and the last inequality can only happen if $\lim_{\epsilon \to 0^+} \zeta(\epsilon) = c > 0$, and by continuity of $\zeta$, we then have $\lim_{\epsilon \to 0} \zeta(\epsilon) = \zeta(0) = c > 0$. 
	
	Now, suppose $\zeta(0) = 0$, and take the sequence $\{u_m\} \to \Gamma(p)$ where $\psi(u_m) \neq \gamma(p)$ for all $m$.	
	We have 
	\begin{align*}
	\lim_{m \to \infty}\ell(\psi(u_m); p) - \ell(\gamma(p);p)&\leq \lim_{m \to \infty} \zeta(L(u_m; p) - L(\Gamma(p);p))\\
	\lim_{m \to \infty}\ell(\psi(u_m); p) &\leq \ell(\gamma(p);p) +  \lim_{\epsilon \to 0^+} \zeta(\epsilon)\\
%	\ell(\psi(\Gamma(p)); p) - \ell(\gamma(p);p)&\leq \zeta(L(\Gamma(p); p) - L(\Gamma(p);p))\\
%	\ell(\psi(\Gamma(p)); p) - \ell(\gamma(p);p)&\leq \zeta(0)\\
%	\ell(\psi(\Gamma(p)); p) &\leq \zeta(0) + \ell(\gamma(p); p)\\
%	\ell(\psi(\Gamma(p)); p) &\leq \ell(\gamma(p); p)~.~
	\end{align*}
	As $\ell(\psi(u_m); p)> \ell(\gamma(p); p)$ (since $\R$ is finite) for all $m$, we then observe that $\lim_{\epsilon \to 0^+} \zeta(\epsilon) = c > 0$.
	However, since $\zeta(0) = 0$, we violate continuity of $\zeta$ at $0^+$.
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
