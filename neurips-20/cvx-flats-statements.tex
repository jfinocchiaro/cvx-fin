\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{[BTW: #1]}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\simplex}{\Delta_\Y}
\newcommand{\relint}[1]{\mathrm{relint}(#1)}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\elic}{\mathrm{elic}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ccdim}{\mathrm{ccdim}}
\newcommand{\codim}{\mathrm{codim}}

\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{#1^*}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\toto}{\rightrightarrows}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\title{cvx-flats}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jessie Finocchiaro\\
%  Department of Computer Science\\
  CU Boulder\\
  \texttt{jefi8453@colorado.edu} 
  % examples of more authors
  \And
   Rafael Frongillo\\
%   Department of Computer Science\\
   CU Boulder\\
  % Address \\
   \texttt{raf@colorado.edu} 
   \And
   Bo Waggoner\\
%   Department of Computer Science\\
   CU Boulder \\
  % Address \\
   \texttt{bwag@colorado.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}

\maketitle

\begin{abstract}
%	For finite prediction tasks, one can often naturally model their specific problem with a discrete loss.
%	However, these discrete losses are often hard to optimize, so one often seeks a surrogate losses yielding desirable statistical guarantees with respect to the original problem.
	In prediction tasks, one typically seeks to minimize \emph{empirical risk} by learning a hypothesis function making predictions from a given feature vector about a desired future outcome.
	This hypothesis is learned by minimizing a loss function over a set of labeled training data.
	However, as not all loss functions are easily optimized, one often wishes to optimize a \emph{surrogate loss} yielding the same statistical guarantees as the original.
	In this paper, we are specifically interested in the construction of \emph{consistent} surrogate losses that are ``efficient'' in the dimension of their input.
	We observe a connection between consistent surrogates and indirect property elicitation, which allows us to apply property elicitation complexity results to the minimal dimension of a consistent surrogate.
	In particular, we characterize the minimal input dimension needed in order to construct a \emph{convex}, consistent surrogate using tools from property elicitation.
\end{abstract}

\section{Introduction}\label{sec:intro}


\subsection{Notation}

\section{Background and Related work}\label{sec:related-work}

\subsection{Consistency and calibration for convex losses}\label{subsec:convex-surrogates}




%%I don't think we're going to need this definition...
%\begin{definition}[Excess risk bound]
%	A surrogate loss and link pair $(L,\psi)$ satisfies the \emph{excess risk bound} with respect to a loss $\ell$ if there exists an increasing function $\zeta : \reals \to \reals$ that is continuous at $0$ with $\zeta(0) = 0$ so that for all $f:\X \to \R$ and data distributions $D$ over $\X \times \Y$, we have
%	\begin{multline}
%	\E_{(X,Y) \sim D} \ell(\psi \circ f(X), Y) - \inf_{f^*} \E_{(X,Y) \sim D} \ell(\psi \circ f^*(X), Y) \\ 
%	\leq \zeta \left( \E_{(X,Y) \sim D} L(f(X), Y) - \inf_{f^*} \E_{(X,Y) \sim D} L(f^*(X), Y) \right)
%	\end{multline}
%\end{definition}

%\begin{proposition}
%	A surrogate link pair $(L, \psi)$ is consistent with respect to a loss $\ell$ if and only if it is consistent with respect to the property elicited by $\gamma := \prop{\ell}$.
%\end{proposition}
%\begin{proof}
%	$\implies:$ 
%	If $(L, \psi)$ is consistent with respect to $\ell$, then for all $x \in \X$ with $p = Pr[Y|X]$, we have $L(u; p) \to \inf_{u'} L(u';p) \implies \ell(\psi(u); p) \to \inf_{u'} \ell(\psi(u');p)$.
%	As $\inf_{u'} L(u';p) = \risk{L}(p)$, we then have $\risk{L}(p) - L(u;p) \to 0 \implies \ell(\psi(\Gamma(p)); p) - \ell(\psi(u);p) \to 0$.
%	If $\gamma: p \mapsto \psi(\Gamma(p))$ for all $p \in \simplex$, then we have consistency with respect to $\gamma$, as $\zeta$ being the identity satisfies the requirements on $\zeta$ and yields the bound as the convergence yields $0 \leq 0$ for all $p \in \simplex$.
%	
%	To see $\gamma = \psi \circ \Gamma$, consider that for all $p \in \simplex$, we have $\arginf_{u \in \reals^d} L(u;p) = \arginf_{u \in \reals^d} \ell(\psi(u); p)$ as a corollary of consistency with respect to $\ell$, which implies the statement.
%	
%	$\impliedby:$ 
%	If $(L, \psi)$ is consistent with respect to $\gamma$, then there is a monotonic function $\zeta:\reals \to \reals$ that is continuous at $0$ with $\zeta(0) = 0$ such that, for all $p \in \simplex$, we have $\ell( \psi(u); p) - \ell(\gamma(p); p)  \leq \zeta \left(  L(u;p) - L(\Gamma(p); p) \right)$.
%	
%	As this is true for all $p = Pr[Y|X] \in \simplex$, this also holds in expectation over $X$.
%	This gives the excess risk bound being satisfied, which has been shown implies consistency with respect to $\ell$.\jessiet{In Hari abstain paper, but might be worth proving it here, given the subtleties of consistency definitions...}
%\end{proof}



\begin{definition}[Calibrated: Finite predictions]\label{def:calibrated-finite}
	Let $\ell : \R \times \Y \to \reals_+$ be a discrete loss eliciting the property $\gamma$.
	A surrogate loss $L : \reals^d \times \Y \to \reals_+$  and link $\psi:\reals^d \to \R$ pair $(L, \psi)$ is \emph{calibrated} with respect to $\ell$ if 
	\begin{equation}\label{eq:calibration}
	\forall p \in \simplex: \inf_{u \in \reals^d : \psi(u) \not \in \gamma(p)} L(u;p) > \inf_{u \in \reals^d} L(u;p)~.~
	\end{equation}
\end{definition}

\begin{definition}[Convex Calibration Dimension]
The \emph{convex calibration dimension} $\ccdim(\ell)$ of a discrete loss $\ell$ is the minimum dimension $d$ such that there is a convex loss $L: \reals^d \times \Y \to \reals$ and link $\psi$ such that $L$ is calibrated with respect to $\ell$.
\end{definition}

\subsection{Property elicitation}\label{subsec:properties}

\begin{definition}[Property, elicits, level set]
	\jessiet{Don't like the notation here... fix later.}
	A \emph{property} is a (possibly set-valued) function $\Gamma : \simplex \toto \R$ mapping distributions to reports.
	A loss $L : \R \times \Y \to \reals_+$ \emph{elicits} the property $\Gamma$ if,
	\begin{equation}
	\forall p \in \simplex, \;\; \Gamma(p) = \arginf_{u \in \R}\inprod{p}{L(u)}
	\end{equation}
	Moreover, we call a \emph{level set} $\Gamma_r := \{p \in \simplex : r \in \Gamma(p)\}$ to be the set of distributions for which reporting $u$ minimizes the expected loss of the loss eliciting $\Gamma$.
\end{definition}

\begin{definition}[Indirect Elicitation]\label{def:indirectly-elicits}
	\jessiet{Explain to reader why this is a good def. Made note in original doc.}
	A loss $L$ \emph{indirectly elicits} a property $\gamma:\simplex \toto \R'$ if it elicits a property $\Gamma: \simplex \toto \R$ such that there is a function $\psi:\R \to \R'$ such that for all $r \in \R$, we have $\Gamma_r \subseteq \gamma_{\psi(r)}$.
\end{definition}


\subsubsection{Elicitation Complexity}\label{subsec:elic-cplx}

\begin{definition}[Convex Elicitation Complexity]
	The \emph{convex elicitation complexity} of a property $\eliccvx(\Gamma)$ is the minimum dimension $d$ such that there is a convex loss $L : \reals^d \times \Y \to \reals$ indirectly eliciting $\Gamma$.
\end{definition}
%\jessie{Just introduce cvx definitions.  Combine two bracketed lines from Raf's notes.  Just define the convex version.  Want to use def 2 with set-valued properties.}


\section{Consistency implies indirect elicitation}\label{sec:consis-implies-indir}
\begin{definition}[Consistent: loss]
	Let $\{f_m : \X \to \R\}$ be a sequence of hypothesis functions.
	A loss and link $(L,\psi)$ are consistent with respect to an original loss $\ell$ if, for all distributions $D$ over input and label spaces $\X \times\Y$, 
	\begin{align*}
	\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y) &\implies \E_D \ell((\psi \circ f_m)(X), Y) \to \inf_f \E_D \ell((\psi \circ f)(X), Y)~.~
	\end{align*}
\end{definition}

\begin{definition}[Consistent: property]
	Suppose we are given a loss $L : \R \times \Y \to \reals$ and link $\psi : \R \to \R'$.
  \raft{You have $\mu$ and $\mu^\gamma$; I'd vote just $\mu$.  We'll want to change $\mu$ to something else eventually anyway, so maybe a macro unless we're sure $\mu$ won't show up elsewhere.}
  \raft{Maybe clearer this way: Introduce $L,\psi,\gamma$.  Then intro $\mu$ as any such function.  Then the def.}
	$(L, \psi)$ is \emph{$\mu$-consistent with respect to a property} $\gamma: \simplex \toto \R'$ for a function $\mu^\gamma:\R' \times \simplex \to \reals_+$ with $\mu^\gamma(r,p) = 0 \iff r \in \gamma(p)$, if for all $D$ over $\X \times \Y$ with marginal distributions $D_x$, and sequences $\{f_m: \X \to \R\}$, 
	\begin{equation}
	\E_{D} L(f_m(X), Y) \to \inf_f \E_{D} L( f(X), Y) \implies \E_X \mu^\gamma(\psi \circ f_m(X), D_X) \to 0~.~
	\end{equation}
	
	We simply say the pair is consistent with respect to $\gamma$ if there is a $\mu^\gamma$ such that $(L,\psi)$ are $\mu$-consistent with respect to $\gamma$.
\end{definition}


\begin{proposition}\label{prop:consistent-implies-calibrated}
	If a (continuous) loss and link $(L, \psi)$ are consistent with respect to a loss $\ell$, then they are calibrated with respect to $\ell$.
	\jessiet{Probably appendix later; also, should this be a lemma instead of a proposition?}
  \raft{Do we define ``continuous loss''?  Maybe ``(not necessarily discrete)'' is clearer.}
\end{proposition}
\begin{proof}
%	\jessie{Intuition: Contrapositive.
%		Not calibrated means there is a sequence $\{u_m\}$ such that $R_L(u_m; p) \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$ via Lemma~\ref{lem:calib-converging-regrets}.
%		Take $D = D_x = p$ for some $x \in \X$, and any sequence of functions $\{f_m\} \to f$ with $f_m(x) = u_m$ for all $f_m$. 
%		Now we have $\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y)$, but $\E_D \ell(\psi(f(X)), Y) \not \to \inf_f \E_D \ell(\psi(f(X)), Y)$, and therefore $(L, \psi)$ is not calibrated with respect to $\ell$. 
%	}

	We show the contrapositive.
	If $(L, \psi)$ are not calibrated with respect to $\ell$, then there is a sequence $\{u_m\}$ such that $R_L(u_m; p) \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$ via Lemma~\ref{lem:calib-converging-regrets}.
	Take $D = D_x = p$ for some $x \in \X$, and any sequence of functions $\{f_m\} \to f$ with $f_m(x) = u_m$ for all $f_m$.
  \raft{Type error with $D=D_x$; see AA15 for how they phrase}
	Now we have $\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y)$, but $\E_D \ell(\psi(f(X)), Y) \not \to \inf_f \E_D \ell(\psi(f(X)), Y)$, and therefore $(L, \psi)$ is not calibrated with respect to $\ell$. 
	
% Commented 05.19.2020 to simplify with proof of lemma~\ref{lem:calib-converging-regrets}.
%	Let $\gamma$ be the property elicited by $\ell$.
%	Suppose $(L,\psi)$ is not calibrated with respect to $\ell$; we will show they are not consistent with respect to $\ell$ either.
%	$(L, \psi)$ not calibrated with respect to $\ell$ means that there is some distribution $p \in \simplex$ such that there is no function $\zeta : \reals_+ \to \reals_+$ with $\zeta(0) = 0$ and continuous at $0^+$ such that, for all $u \in \reals^d$, we have $\ell(\psi(u); p) - \risk{\ell}(p) \leq \zeta(L(u;p) - \risk{L}(p))$. 
%	In other words, for all $\zeta$ satisfying the assumptions, we have $\ell(\psi(u); p) - \risk{\ell}(p) > \zeta(L(u;p) - \risk{L}(p))$ for some $u \in \reals^d$.
%	In particular, there must exist a sequence $\{u_m\}$ so that $L(u_m;p) \to \risk{L}(p)$.
%	
%	
%	Now consider the data distribution $D$ that has a point mass on some $x \in \X$, where $p = D_x$.
%	Moreover, let $f_m$ be any sequence of functions with $f_m(x) = u_m$ for all $m$.
%	We then have $L(f_m(x); p) = L(u_m; p) \to \risk{L}(p)$, so $\lim_{m \to \infty}\zeta(L(u_m;p) - \risk{L}(p)) \to 0$.
%	This gives $\ell(\psi(u_m);p) > \risk{\ell}(p) + \epsilon_m$ for all $m$, and $\epsilon_m \to 0$.
%	We can now observe that we do not have consistency as $\ell(\psi(u_m); p) \not \to \risk{\ell}(p)$.
%	Therefore, if $(L, \psi)$ are consistent with respect to $\ell$, they are also calibrated with respect to $\ell$.
\end{proof}

\begin{lemma}\label{lem:calib-implies-indir}
	If a surrogate and link $(L, \psi)$ are calibrated with respect to a loss $\ell:\R \times\Y \to \reals$, then $L$ indirectly elicits the property $\gamma := \prop{\ell}$.
\end{lemma}
\begin{proof}
	Let $\Gamma$ be the unique property directly elicited by $L$, and fix $p \in \simplex$ with $u$ such that $p \in \Gamma_u$.
	As $p \in \Gamma_u$, then $\zeta(L(u;p) - \risk{L}(p)) = \zeta(0) = 0$, we observe the bound $\ell(\psi(u); p) \leq \risk{\ell}(p)$.
	We also have $\ell(\psi(u); p) \geq \risk{\ell}(p)$ by definition of $\risk{\ell}$, so we must have $\ell(\psi(u);p) = \risk{\ell}(p) = \ell(\gamma(p); p)$, and therefore, $p \in \gamma_{\psi(u)}$.
	Thus, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, so $L$ indirectly elicits $\gamma$.
\end{proof}

\begin{lemma}\label{lem:consistent-loss-implies-prop}
	If $(L, \psi)$ are consistent with respect to $\ell$, then they are consistent with respect to $\gamma := \prop{\ell}$.
\end{lemma}
\begin{proof}
	Define $\mu^\gamma(r, p) := R_\ell(r;p)$.
	First, observe that $\mu^\gamma(r,p) = 0 \iff r \in \gamma(p)$, since $r \in \gamma(p)$ implies $\ell(r;p) = \risk{\ell}(p)$, and if $r \not \in \gamma(p)$, then $\ell(r;p) > \risk{\ell}(p)$.
	
	Now suppose $(L, \psi)$ are consistent with respect to $\ell$, and take any sequence $\{f_m\}$ so that, for all $D$ over $\X \times \Y$, we have $\E_D L(f_m(X)) \to \inf_f \E_D L(f(X), Y)$.
	This implies 
	\begin{align*}
	&\; \E_D \ell(\psi(f_m(X)), Y)\to \inf_f \E_D \ell(\psi(f(X)), Y) \\
	&\iff \E_D \ell(\psi(f_m(X)), Y) - \inf_f \ell(\psi(f(X)), Y) \to 0 \\
	&\implies \E_X R_\ell(\psi(f_m(X)), D_X) \to 0~.~
	\end{align*}
  \raft{Close; missing expectation inside inf, and I think you can skip the first $\iff$ and give the $\implies$ as an $\iff$ (right?), and then follow with a second $\iff$ with $\mu$.}
  \raft{BTW, I think it's less messy to use $\psi \circ f_m(X)$ as above}
	As regret defines $\mu^\gamma$ and tends to $0$, we observe consistency with respect to $\gamma$.
\end{proof}

\begin{theorem}\label{thm:consistent-implies-indir-elic}
	If a surrogate and link pair $(L, \psi)$ is consistent with respect to a property $\gamma$ or loss $\ell$ eliciting $\gamma$, then $L$ indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}
  \raft{Maybe ref the consistency definition here explicitly, at the top and the end}
	As consistency with respect to $\ell$ implies consistency with respect to $\gamma := \prop{\ell}$ (Lemma~\ref{lem:consistent-loss-implies-prop}), it suffices to show the result for consistency with respect to a property $\gamma$.
  
	We show the contrapositive; suppose $(L, \psi)$ does not indirectly elicit $\gamma$, and take $\Gamma := \prop{L}$.
  \raft{Oh good!  I was going to say we should use this way of stating indirect elicitation (``$(L, \psi)$ indirectly elicits $\gamma$'') when we define it above, and use it.  We can then also define the version without $\psi$ easily.}
	Then there is a distribution $p \in \simplex$ so that $u \in \Gamma(p)$ but $\psi(u) \not \in \gamma(p)$.

	Consider the constant sequence $\{u_m\} = u$\raft{$\{u_m\}$ with $u_m = u$} for all $m$, a distribution $D$ with point mass on $x$ and $D_x = p$, and any sequence $\{f_m\}$ so that $f_m(x) = u_m$ for all $m$.
  \raft{Check $D$ wording here too}
	This yields $\E_D L(f_m(X), Y) \to \inf_f L(f(X),Y)$ since $u \in \Gamma(p)$.
  \raft{Maybe clearer to first say $\E_D L(f_m(X), Y) = \inf_f L(f(X),Y)$ for all $m$, and then the easy implication of the convergence}
	However, we have $\mu(\psi(u_m), \gamma(p)) \not \to 0$ as $\mu(\psi(u), \gamma(p)) \neq 0$ (since $\psi(u) \not \in \gamma(p)$) and the sequence is constant.
  \raft{Almost -- just clarify that this is the expectation over $D$ too by construction}
	Thus, we do not have consistency.
\end{proof}

\section{Characteristics of Consistent Convex Surrogates}\label{sec:char-convex}

\begin{definition}[Flat]
	A \emph{flat} $F$ of $\reals^n$ is an affine subspace of $\reals^n$.
	In other words, $F\subseteq \reals^n$ can be written $F=\{x\in\reals^n : Wx + b = 0\}$ for some $W\in\reals^{d\times n}$ and $b\in\reals^d$ where $d\leq n$.
	The dimension of the flat $F$ is given by $f := n - \mathrm{rank}(W)$.
\end{definition}

\begin{theorem}\label{thm:cvx-flats}
	Suppose we are given a property $\gamma$ and distribution $p \in \simplex$.
	For all $r\in\gamma(p)$, if there is no $(n - d-1)$-dimensional flat $F$ containing $p$ so that $F \cap \simplex \subseteq \gamma_r$, then there is no convex surrogate loss $L : \reals^d \times \Y \to \reals$ that indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}[Proof]
	We prove the contrapositive: if there is a surrogate loss $L$ indirectly eliciting $\gamma$, then for all $p \in \simplex$ and $r \in \R$ such that $r \in \gamma(p)$, there is an $(n-d-1)$-dimensional flat $F$ such that $F \cap \simplex \subseteq \gamma_r$ and $p \in F$.
	
	Suppose we have $L:\reals^d \times \Y \to \reals$ eliciting $\Gamma$ and link $\psi : \reals^d \to \R$ such that for all $u \in \reals^d$, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, i.e., $L$ indirectly elicits $\gamma$.
	Fix $p \in \simplex$, and take any $u \in \Gamma(p)$ such that $\psi(u) = r \in \gamma(p)$.
	We know that $u \in \Gamma(p) \iff \vec 0 \in \partial L(u; p) \iff \vec 0 \in \oplus_y p_y \partial L(u,y)$, where $\oplus$ denotes the nonnegative weighted Minkowski sum~\cite[Theorem 4.1.1]{hiriart2012fundamentals}.  (Observe that in order for $L$ to be convex, $L(\cdot, y)$ must be convex for all $y \in \Y$.) 
	
	Observe that the Minkowski sum $\oplus_y p_y \partial L(u,y) = \{p_1 x_1 + \ldots + p_n x_n : x_1 \in \partial L(u,1), \ldots, x_n \in \partial L(u,n) \}$.
	In order for $\vec 0$ to be in this $p$-weighted Minkowski sum, we then have that each choice of $x_i$ is a flat for $\partial L(u, i)$.
	Moreover, with $p$ fixed, we know there is a vector of witnesses $W' = [x_1, \ldots, x_n] \in \reals^{d\times n}$ such that $\sum_y p_y x_y = \vec 0$.

	Observe that $F' := \ker(W')$ is at most $(n-d)$ dimensions in $\reals^n$ and contains $p$ by construction.
	Since we are only concerned with a flat intersected with the simplex being a subset of $\gamma_r$, we can reduce the dimension by adding an affine simplex constraint. 
	That is, take $F := \{x \in \reals^n : W x + b = \vec 0\}$, where we have $W := [W'; \mathbbm{1}^T] \in \reals^{(d+1) \times n}$ and $b \in \reals^{d+1}$.
	The dimension of $F$ is then $n - \mathrm{rank}(W)$, which is $n - (d+1)$ if $W$ is full rank.
	We also claim that $F \cap \simplex \subseteq \gamma_r$.
	To see this, consider $q \in F$, but $q \not \in \gamma_r$.
	That would mean that $Wq + b = \vec 0 \iff W'q + b' = \vec 0$ (since $q \in \simplex$ satisfies the simplex constraints).
	This in turn implies $\vec 0 \in \partial L(u;q)$, implying $q \in \Gamma_u$.
	Therefore, $q \not \in \gamma_r$ would contradict $L$ indirectly eliciting $\gamma$.
\end{proof}


\section{Discrete-valued predictions}\label{sec:finite-calib}

%\begin{lemma}
%	Suppose we are given a discrete loss $\ell : \R \times\Y \to \reals_+$ eliciting the property $\gamma$.
%	If $(L, \psi)$ is calibrated with respect to $\ell$, then $L$ indirectly elicits $\gamma$.
%\end{lemma}
%\begin{proof}
%	Fix $p \in \simplex$ and consider $u \in \Gamma(p)$.
%	Since $(L, \psi)$ is calibrated with respect to $\ell$, we have $p \in \Gamma_u$.
%	We simply need to show $\psi(u) \in \gamma(p)$.
%	If $\psi(u) \not \in \gamma(p)$, then the terms in the condition for calibration (Equation~\eqref{eq:calibration}) are equal; hence $L$ would not be calibrated with respect to $\ell$.
%\end{proof}

\begin{definition}[Subspace of feasible directions]
	Define the \emph{subspace of feasible directions} $\S_\C(p)$ of a convex set $\C \subseteq \reals^n$ at a point $p \in \C$ as the subspace $\S_\C(p) = \{ v \in \reals^n : \exists \epsilon_0 > 0 $ such that $p + \epsilon v \in \C \; \forall \epsilon \in (-\epsilon_0,\epsilon_0) \}$.
%  \raf{Let's simplify to $\epsilon \in (-\epsilon_0,\epsilon_0)$}
\end{definition}

\begin{lemma}\label{lem:feas-sub-is-a-flat}
	Suppose we have the finite elicitable property $\gamma$ and distribution $p \in \relint{\simplex}$ with $r \in \gamma(p)$.
	If $F$ is a flat containing $p$ such that $F \cap \simplex \subseteq \gamma_r$, then $F - p$ is a subspace contained in $\S_{\gamma_r}(p)$.
  \raf{I think you want $p$ in the interior of the simplex for now}
\end{lemma}
\begin{proof}
  To spell it out, observe $F-p$ is a subspace as it is a linear transformation of $F$, which is a subspace by definition of a flat.
  
  Now consider $q \in F - p$.
  First, if $p+q \in \gamma_r$ and $p -q \in \gamma_r$, then we have $q \in \S_{\gamma_r}(p)$ with $\epsilon_0 = 1$ as level sets of elicitable properties are convex by~\cite{lambert2009eliciting}.
  If either $p + q$ or $p - q \not \in \gamma_r$, it must be because the term is out of the simplex by definition of $F$.
  
  However, if both $p + \alpha^+ q$ and $p - \alpha^- q \in \simplex$ for some $\alpha^\pm \in (0,1)$, then $q \in \S_{\gamma_r}(p)$ with $\epsilon_0 = \min(\alpha^+, \alpha^-)$.
  As $p \in \relint{\simplex}$, there is always such an $\epsilon_0$; if there were not, then we would observe $q \not \in F - p$.
  Therefore, we have $q \in F - p \implies q \in \S_{\gamma_r}(p)$, so $(F - p) \subseteq \S_{\gamma_r}(p)$.
\end{proof}


%\begin{lemma}\label{lem:feas-sub-is-a-flat}
%	Suppose we have the finite property $\gamma$ and distribution $p \in \simplex$.
%	For ant $r$ such that $r \in \gamma(p)$, the subspace $F := \mu_{\gamma_r}(p) + p $ is a flat, $F$ contains $p$, and $F \cap \simplex \subseteq \gamma_r$.
%\end{lemma}
%\begin{proof}
%	Write the level set $\gamma_r = \{q \in \reals^n : A_1 q \leq b_1, \; A_2 q \leq b_2, \; A_3 q \leq b_3 \}$, where $A_1 p \leq b_1, A_2 p < b_2,$ and $A_3 p = b_3$.
%	
%	We want to show three things: first, $F$ is a flat as it is $\ker([A_1 ; A_3])$ by~\cite{ramaswamy2016convex}.
%	Second, $p \in F + p$ by construction, since $\vec 0 \in F$.
%	(Consider that $p \in \gamma_r$, so with $epsilon_0 = 1$, we have $p + \epsilon\vec 0 = p \in \gamma_r$ for all $\epsilon \in (0, 1)$; thus, $\vec 0 \in F$.)
%	
%	Third, we want to show $(F + p) \cap \simplex \subseteq \gamma_r$.
%	Take some $q := p + \epsilon v$ for $v \in F$.
%	By construction of $F$, we have $q \in F \implies q \in \simplex$, so $F + p \subseteq \simplex \implies F + p \cap \simplex = F + p$.
%	Thus it just remains to be shown that $F + p \subseteq \gamma_r$.
%	Since $v\in F \implies q \in F + p \implies p + \epsilon v \in \gamma_r$ by construction, so we have $F+p \subseteq \gamma_r$.	
%\end{proof}

\begin{corollary}[\cite{ramaswamy2016convex}]
	Suppose we are given a discrete loss $\ell:\R \to \reals^\Y_+$ eliciting $\gamma$.
	Take $p \in \simplex$ and $r \in \R$ such that $p \in \gamma_r$.
	\begin{equation}
	\ccdim(\ell) \geq \|p\|_0 - \dim(\S_{\gamma_r}(p)) - 1~.~
	\end{equation}
\end{corollary}

\subsection{Elicitation complexity and convex calibration dimension}

\begin{proposition}
	Consider the discrete loss $\ell : \R \times \Y \to \reals_+$ and $\gamma:= \prop{\ell}$.
	Then $\eliccvx(\gamma) \leq \ccdim(\ell)$.
	\jessie{Need to define $\eliccvx$ and $\ccdim$.}
\end{proposition}
\begin{proof}
	Let $d$ be the convex calibration dimension of $\ell$, and $(L, \psi)$ be calibrated with respect to $\ell$ with $L$ convex.
	Then $L$ indirectly elicits $\gamma$ by Lemma~\ref{lem:calib-implies-indir}.
\end{proof}



\section{Continuous-valued predictions}\label{sec:contin-consis}



\newpage

\section*{Broader Impact}

\begin{ack}
Nishant, Adam
\end{ack}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

\newpage
\appendix
\section{A general notion of calibration}

\begin{definition}[Calibrated]\label{def:calibrated-general}
	A loss $L:\reals^d \times \Y \to \reals$ is \emph{calibrated} with respect to a loss $\ell : \R \times \Y \to \reals$ eliciting the property $\gamma$ if there is a link $\psi : \reals^d \to \R$ such that, for all distributions $p \in \simplex$, there exists a function $\zeta : \reals_+ \to \reals_+$ with $\zeta$ continuous at $0^+$ and $\zeta(0) = 0$ such that for all $u \in \reals^d$, we have
	\begin{equation}\label{eq:calibrated-general}
	\ell( \psi(u); p) - \risk{\ell}(p)  \leq \zeta \left(  L(u;p) - \risk{L}(p) \right)~.~
	\end{equation}
\end{definition}

\jessie{Consider the following four conditions:
\begin{enumerate}
	\item [A] $\zeta:\reals \to \reals_+$ satisfies $\zeta : 0 \mapsto 0$ and is continuous at $0$.
	\item [B] $\epsilon_m \to 0 \implies \zeta(\epsilon_m) \to 0$.
	\item [C] Given $\zeta:\reals \to \reals_+$, for all $u \in \reals^d$, $R_\ell(\psi(u); p) \leq \zeta(R_L(u;p))$.
	\item [D] For all $p \in \simplex$ and sequences $\{u_m\}$ so that $R_L(u_m; p) \to 0$, we have $R_\ell(\psi(u_m); p) \to 0$.
\end{enumerate}
$(A \wedge C)$ define calibration, and we have $A \iff B$ in shown in Lemma~\ref{lem:continuous-iff-limits}.  Lemma~\ref{lem:calib-converging-regrets} shoes calibration iff $D$.}

\begin{proposition}
	When $\R$ is finite and $L$ continuous in $\reals^d$, calibration via Definition~\ref{def:calibrated-general} implies calibration via Definition~\ref{def:calibrated-finite}.
	\jessie{eventually want iff}
\end{proposition}
\begin{proof}
	\jessie{Old proof commented out; simplified using Lemma~\ref{lem:calib-converging-regrets}}

	We prove the contrapositive; if $(L, \psi)$ is not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then it is not calibrated via Definition~\ref{def:calibrated-general} either.
	If $(L, \psi)$ are not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then there is a $p \in \simplex$ so that $\inf_{u : \psi(u) \not \in \gamma(p)} L(u;p) = \inf_u L(u; p)$.
	Thus there is a sequence $\{u_m\}$ so that $\lim_{m \to \infty} \psi(u_m) \not \in \gamma(p)$ and $L(u_m; p) \to \risk{L}(p)$.  
	Now we have $R_L(u_m; p \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$, so by Lemma~\ref{lem:calib-converging-regrets}, we contradict calibration by Def~\ref{def:calibrated-general}.

% Commented out 05.19.2020 for easier proof if Lemma 5 is true.
%	We prove the contrapositive; if $(L, \psi)$ is not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then it is not calibrated via Definition~\ref{def:calibrated-general} either.
%	
%	Suppose there was a distribution $p \in \simplex$ so that $\inf_{u : \psi(u) \not \in \gamma(p)} L(u;p) = \inf_{u} L(u;p)$.
%	There must then be a sequence $\{u_m\} \to u$ so that $\lim_{m \to \infty} \psi(u_m) \not \in \gamma(p)$ and $L(u_m; p) \to \risk{L}(p)$.
%	
%	This consequently implies $R_L(u_m;p) \to 0$ as $L(u_m; p) \to \risk{L}(p)$, but as $\ell(\psi(u_m); p) \not \to \risk{\ell}(p)$ (if it did converge, then we would have $\psi(u_m) \to r \in \gamma(p)$), so $R_\ell(\psi(u_m); p) \not \to 0$.
%	Thus, by Lemma~\ref{lem:calib-converging-regrets}, we have no calibration via Definition~\ref{def:calibrated-general}.  
		
	
\end{proof}

\begin{lemma}\label{lem:continuous-iff-limits}
	A function $\zeta:\reals \to \reals$ is continuous at $0$ and $\zeta(0) = 0$ if and only if the sequence $\{u_m\} \to 0 \implies \zeta(u_m) \to 0$.
	\jessie{$A \iff B$}
\end{lemma}
\begin{proof}
	$\implies$ Suppose we have a sequence $\{u_m\} \to 0$.
	By continuity, we have $\lim_{u_m \to 0}\zeta(u_m) = \zeta(0) = 0$, so $\zeta(u_m) \to 0$.
	
	$\impliedby$ Suppose $\zeta(0) \neq 0$ but $\zeta$ was continuous at $0$.
	The constant sequence $\{u_m\} = 0$ then converges to $0$, but as $\zeta$ is continuous at $0$, we must have $\lim_{m \to \infty}\zeta(u_m) = \zeta(0) \neq 0$, so $\zeta(u_m) \not \to 0$.
	
	Now suppose $\zeta(0) = 0$ but $\zeta$ was not continuous at $0$.
	There must be a sequence $\{u_m\} \to 0$ so that $\lim_{m \to \infty}\zeta(u_m) \neq \zeta(0) = 0$, so $\zeta(u_m) \not \to 0$.
\end{proof}

\begin{lemma}\label{lem:calib-converging-regrets}
	A continuous surrogate and link $(L,\psi)$ are calibrated (via definition~\ref{def:calibrated-general}) with respect to $\ell$ if and only if, for all $p \in \simplex$ and sequences $\{u_m\}$ so that $R_L(u_m; p) \to 0$, we have $R_\ell(\psi(u_m); p) \to 0$.
	\jessie{$(A \wedge C) \iff D$}
\end{lemma}
\begin{proof}
\jessie{$(A \wedge C) \implies D$}
	$\implies$ Take a sequence $\{u_m\}$ so that $R_L(u_m;p) \to 0$.
	Since $\zeta(0) = 0$ and $\zeta$ is continuous at $0$, $\zeta(R_L(u_m;p)) \to 0$ by calibration.
	As regret is nonnegative, we then have $R_\ell(\psi(u_m); p) \to 0$
	
	$\impliedby$ Consider $\zeta(c) := \sup_{u: R_L(u;p) \leq c} R_\ell(\psi(u); p)$.for nonnegative values of $c$, and $\zeta(c) = 0$ for $c < 0$.
	We will show $R_L(u_m; p) \to 0 \implies R_\ell(\psi(u_m); p) \to 0$ gives calibration via the function $\zeta$ constructed above. 
	
\jessie{D $\implies$ A}
	First, we can verify that $\zeta$ as constructed maps $\zeta(0) = 0$ and is continuous at $0$.	
	As $\risk{L}$ and $\risk{\ell}$ are well-defined for all $p \in \simplex$, we note that $\zeta(0) = \sup_{u':R_L(u;p) = 0} (R_\ell(\psi(u'); p)) = \sup_{u' \in \Gamma(p)} (R_\ell(\psi(u'); p))$.
	If there is a $u \in \Gamma(p)$ so that $R_\ell(\psi(u); p) > \risk{\ell}(p)$ (i.e. $\zeta(0) > 0$), then the constant sequence $\{u_m\} = u$ for all $m$ gives $R_L(u_m;p) \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$, yielding a contradiction; therefore, we must have $\zeta(0) = 0$.
	
	To see $\zeta$ is continuous at $0$, we just need to show $\lim_{c \to 0^+} \zeta(c) = 0$. 
	Fix $p \in \simplex$ and take any sequence $\{u_m\}$ so that $R_L(u_m; p) \to 0$, and therefore $R_\ell(\psi(u_m)) \to 0$.
	Since this is true for all sequences, we have the upper bound tending to $0$, and thus $\lim_{c\to 0^+}\zeta(c) = 0$.
		
\jessie{D $\implies$ C}
	Now, we observe that the bound in Equation~\eqref{eq:calibrated-general} is satisfied for all $u \in \reals^d$ by construction of $\zeta$.	
	For all $u \in \reals^d$, calibration via $\zeta$ yields $R_\ell(\psi(u);p) \leq \sup_{u' : R_L(u';p) \leq R_L(u,p)} R_\ell(\psi(u') ; p)$.
	As $u$ is in the space of the supremum ($R_L(u;p) \leq R_L(u;p)$), we then have calibration by definition of the supremum.
\end{proof}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
