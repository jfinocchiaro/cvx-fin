\documentclass[12pt]{article}
\linespread{1.05}
%\usepackage{fancyhdr}
%\usepackage{mathptmx}
%\usepackage{sidecap}
\usepackage{amsfonts, amsmath, amsthm, amssymb, graphicx, verbatim}
\usepackage[margin=1.0in]{geometry}
\usepackage{booktabs} % Top and bottom rules for table
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures
\usepackage{amsfonts, amsmath, amsthm, amssymb, graphicx, verbatim} % For math fonts, symbols and environments
\usepackage{wrapfig} % Allows wrapping text around tables and figures
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage{bbm}
\usepackage{thm-restate}
%\usepackage[backend=bibtex, style=footnote-dw]{biblatex}
%\addbibresource{diss.bib}

\usepackage[colorlinks=true,breaklinks=true,bookmarks=true,urlcolor=blue,
citecolor=blue,linkcolor=blue,bookmarksopen=false,draft=false]{hyperref}
\usepackage{url}
\usepackage{float}
\usepackage{enumitem}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{blue}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{blue!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{green!75!black}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{green!20!white}{JF: #1}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{\textcolor{gray}{#1}}}
\newcommand{\future}[1]{}%\mytodo{blue!20!white}{\textcolor{gray!50!black}{FUTURE: #1}}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

%\pagestyle{fancy}
%\lhead{Moving to infinite outcomes}
%\rhead{ }


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\myderiv}[1]{\tfrac{d}{d#1}} % \partial_{#1}
\newcommand{\myrderiv}[1]{\tfrac{\,d^+\!\!}{d#1}} % \partial_{#1}
\newcommand{\dz}{\myderiv{z}}
\newcommand{\dx}{\myderiv{x}}
\newcommand{\dr}{\myderiv{r}}
\newcommand{\du}{\myderiv{u}}
\newcommand{\rdx}{\myrderiv{x}}
\newcommand{\toto}{\rightrightarrows}

%m upper and lower bounds
\newcommand{\mup}{\overline{m}}
\newcommand{\mlow}{\underline{m}}


% alphabetical order, by convention
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\simplex}{\Delta_{\Y}}
\newcommand{\cl}{\mathbf{cl}}
\newcommand{\dom}{\mathbf{dom}}
\newcommand{\effdom}{\mathbf{effdom}}
\newcommand{\codim}{\mathbf{codim}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}


\newtheorem{definition}{Definition}	 
\newtheorem{proposition}{Proposition}	 
\newtheorem{condition}{Condition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}

\begin{document}
\begin{center}
{\large\textbf{Generalizing Theorem 2 to infinite outcomes}}
\end{center}

\section{Background and notation}

\begin{definition}[Domain]
	The domain of a function $f:X \to \reals$ is the set $X$ so that $f(x)$ is defined for all $x \in X$. 
\end{definition}

\begin{definition}[Effective Domain]
	The \emph{effective domain} of a convex function $f: X \to \bar \reals$ is the set $S \subseteq X = \{x \in X : f(x) < \infty\}$,
\end{definition}

Let $(\Y, \B_\Y, p)$ define a measure space, where $\B_\Y$ is the $\sigma$-algebra of Borel sets on $\Y$.
If we are given a loss $f(u,y)$, we want to understand when $\partial \{\E_p f(\cdot,Y)\} = \E_p \partial f(\cdot,Y)$. 

Note: expectation of a closed-valued multifunction $G:\Y \to 2^\reals$ is defined $\E_p G = \cl(\{\int \nu(y) p(dy) : \nu \in \L^1_d(\B_\Y), \nu(y) \in G(y) a.s.\})$.

\subsection{Definitions needed}
\begin{definition}[Measurable multifunction]
	A multifunction $F:\Y \to \reals^d$ is measurable iff for every open $E \subseteq \reals^d$, we have $F^{-1}(E) \in \B_\Y$. \jessie{Check?? Apparently in \cite{aubin2009set} but I can't find it easily.}
\end{definition}

\begin{definition}[$\A$-normal convex integrand]
	A function $f : \reals^d \times \Y \to \bar \reals$ is an $\A$-normal convex integrand if the map
	\begin{equation}
	y \mapsto \epi(f(\cdot, y))  = \{(u, \alpha) \in \reals^{d+1} : \alpha \geq f(u,y)\}
	\end{equation}
	is a closed-convex-valued measurable multifunction.
\end{definition}

\section{Result}
\begin{corollary}[\cite{rockafellar1982interchange,ioffe1969minimization}]
	Suppose $f$ is an $\B_\Y$-normal convex integrand such that $F(x) < \infty$ whenever $x \in \dom(f(\cdot,y))$ almost surely, where $F(x) = \E_p f(x,Y)$.
	
	Suppose moreover that there exists $x_0 \in \reals^d$ at which $F$ is finite and continuous, and that the multifunction $G: y \mapsto \cl(\dom(f(\cdot,y)))$ is almost surely constant.
	Then for all $x \in \reals^d$, we have $\E_p \partial f(x, \cdot) = \partial F(x)$.
\end{corollary}


\section{Examples}
We verify the satisfaction of the following three assumptions of the corollary:
\begin{enumerate}
	\item $f$ is a $\B_\Y$-normal convex integrand.
	\item $F(u) < \infty$ for $u \in \effdom(f(\cdot,y))$ almost surely (on $y$).
	\item There exists $x_0 \in \reals^d$ so that $\E_p f(x_0, Y)$ is (a) finite (b) continuous at $x_0$ and (c) $y \mapsto \cl \dom (f(\cdot, y))$ is almost surely constant.
\end{enumerate}
\subsection{$f(u,y) = |u|$.}

Since randomness is in $\Y$ and the loss function is not over $y$, so we know that $\E_p f(u,Y) = |u|$ regardless of $p$, therefore we can interchange expectation and subdifferentials no matter the distribution, but let's work through the conditions.

First, $G : y \mapsto \{(u,\alpha) : \alpha \geq |u| \}$ can be verified that the function is closed, convex-valued, and measurable.
Second, since $\E_p f(u,Y) = |u|$, $u \in \effdom(f)$ (almost surely) implies $u \in \reals$, and by substitution, we have $\E_p f(u,Y) = |u| < \infty$.
Third, Take $x_0 = 0$.  For all $p$, we have $\E_p f(x_0, Y) = 0$, and $f$ is continuous at $0$.

Therefore, the three conditions of the corollary are attained, and we confirm we can interchange expectation and subdifferentials.


\subsection{$f(u,y) = (u-y)^2$, $p$ is a Pareto distribution with $\alpha = 3/2$}
First, consider the pareto distrubtion with $t_a = 1$ and $\alpha = 3/2$ whose PDF is given 
\begin{equation*}
p(t) = \frac{3}{2t^{5/2}}, \,\,\, t > 1~,~
\end{equation*}
and is $0$ otherwise.

This distribution has mean $6$ and infinite variance.
Observe that this is Reviewer 6's ``counterexample'' they claimed did not work.
We want to understand which assumptions, if any, do not apply.

In particular, let's look at the second condition.
We want to verify that $\E_p (u-Y)^2 < \infty$ for all $x \in \effdom (f, \cdot, y)$ almost surely.
Take $u = 0$, and observe $f(0,y) = y^2$ is finite almost surely. 
However, $\E_p (u-Y)^2 = \E_p Y^2$, and since $p$ has infinite variance, this value is infinite as well.
Therefore, we do not satisfy the second condition of the corollary.


\subsection{$f(u,y) = (u-y)^2$, $p = \mathcal{N}(0,1)$}
Here's a softball... seemingly.
First, we can verify $y \mapsto \epi((u-y)^2)$ is closed, convex-valued, and measurable, as $f$ is convex in $u$ for constant $y$.

Next, consider $u \in \effdom(f(\cdot, y)$ almost surely iff $u \neq \pm \infty$.
That is, $(u - y)^2 = u^2 + y^2 - 2uy$ is finite for almost all $y$.
In this case, $F(u) = u^2 - [\E_p Y]^2 -2 u \E_p [Y]$.
Plugging in $p$, we have $F(u) = u^2$, which is finite as $u$ must be finite.

Finally, take $x_0 = 0$.
We can verify $F$ is finite and continuous at $x_0$.
Moreover, the function $y \mapsto \cl \dom f(0, y)$ is $\reals$ for all $y$, so the third condition holds.

Therefore, we can interchange subdifferentiation and expectation. 


\section{The bigger picture}
This matters so that we can generalize the proof of Theorem 2 to infinite outcome settings.

Essentially, we want to show that for convex losses, the level sets are unions of $d$-dimensional flats, which are loosely defined as the kernel space of a loss.

\hrule
From Zulip convo:

We can rewrite the statement "Every level set of a d-convex-elicitable property is the union of codim-d flats" as 
``$\Gamma$ is d-convex-elicitable $\implies \forall u, \exists \mathbb X \subseteq  (\cal Y \to\mathbb{R}^d)$  s.t.  $\Gamma_u = \bigcup_{X \in \mathbb{X}} \mathrm{ker} X \cap \,\,\Delta_{\mathcal{Y}}$''

The implication is weaker than showing that for all $u$ and $p \in \Gamma_u$, there is some $\bar X$ so that (a) $\bar X$ is $d$-dimensional and (b) $p \in \ker( \bar X)$.  ($\bar X$ dimension $d$ implies the flat, $Ker(\bar X)$, is codimension $d$.)

So now we assume $\Gamma$ is $d$-convex elicitable (by L), and want to show that such a $\bar X$ exists.

Fix u, and consider that $p \in \Gamma_u \iff \vec 0 \in \partial \mathbb{E}_p L(u,Y) = \{\E_p X(Y) : \forall y \in \mathcal{Y}, \, X(y) \in \partial L(u,y) \}$, where the first $\iff$ follows specifically since $\Gamma$ is convex elicitable.  
This implies $\exists \bar X$ in this set so that $\vec 0 = \E_p \bar X(Y)$.  
(well... if the conditions of the Corollary are satisfied.)  
Moreover, since $\E_p X(Y) \in \partial \mathbb{E}_p L(u,Y)$ and $L$ is $d$-dimensional, we have $\bar X$ is $d$-dimensional.

We can then (as before) construct the flat $ \ker(\bar X) \cap \Delta_{\mathcal{Y}}$.  
By construction, we have $p \in \ker(\bar X)$ and that the flat is at most codimension $d$.

\section{What's next?}
We want to understand the restrictions on (a) the loss and (b) on $\P$ that are made in order to apply this corollary (for all $p\in \P$).

Restrict to convex losses that are $\A$-normal convex integrands... is this the same as identifiable functions?
\jessie{Reminds me of~\cite[Theorem 5]{steinwart2014elicitation}, but they require $\Gamma$ to be real valued and strictly locally non-constant...}
\jessie{Also related~\cite[Theorem 2]{gneiting2011making}}

\jessie{Stronger assumption than necessary, but if we can show $L$ is continuous at $\Gamma(p)$, that covers condition 3}

\begin{conjecture}
	If a convex loss $L:\reals^d \times \Y \to \reals$ elicits a nonempty property $\Gamma$, then there is a function $V:\reals^d \times\Y \to \reals$ such that $L(r,y) = \int V(r,y) dy$.
\end{conjecture}

\begin{theorem}[Theorem 2]
	Let a property $\gamma : \P \toto \R$ be given, and let $p \in \P$, and $r \in \gamma(p)$.
	Then if a convex loss \jessie{$\A$-normal convex integrand? such that $\E_p L(\cdot,Y)$ is finite for all $u \in \reals^d$} $L : \reals^d \times \Y \to \reals$ indirectly elicits $\gamma$, there is a flat $F$ with $p \in F \cap \P \subseteq \gamma_r$ with $\codim(F) \leq d$.
\end{theorem}
\begin{proof}
	Consider the probability space $(\Y, \A, p)$ \jessie{Don't think $\B_\Y$ is necessary... generic $\A$ is fine?}, where $p \in \P$, and let $\Gamma := \prop{L}$.
	Moreover, suppose $(L, \psi)$ indirectly elicits $\gamma$.
	By definition of properties, there is some $u \in \Gamma(p)$.
	
	We first construct the flat $F$ and show $F \cap \P \subseteq \Gamma_u$, which implies $F \cap \P \subseteq \gamma_r$ by indirect elicitation \jessie{though this needs more detail since we don't necessarily have $u = \psi(r)$.}. 
	
	As $L$ is convex and elicits $\Gamma$, we have $u \in \Gamma(p) \iff \vec 0 \in \partial \E_p L(u,Y)$.
	Since, for each $p \in \P$ by Lemma~\ref{lem:conditions-sat}, $L$ satisfies the assumptions of~\cite[Corollary 1]{rockafellar1982interchange}, we also have $\vec 0 \in \partial \E_p L(u,Y) \iff \vec 0 \in \E_p \partial L(u,Y) := \cl(\{\int \nu(y) p(dy) : \nu \in \L^1_d(\B_\Y), \nu(y) \in \partial L(u,Y)\,\, a.s.\})$.
	As $\vec 0$ is in the expectation of the subgradient, such a $\nu$ exists in the closure of this set.
	We now take $F := \ker(\nu) \cap \P$.
	
	By construction we have $\vec 0 = \E_p \nu(Y)$, meaning $p \in \ker(\nu) \cap \P = F$.
	Moreover, we have that $\codim(F) = \dim(\P \setminus F) \leq d$ since \jessie{rationale.}
\end{proof}

\begin{lemma}\label{lem:conditions-sat}
	For convex $L : \reals^d \times \Y \to \reals$ eliciting the nonempty property $\Gamma:\P\toto\R$, if $\P$ is a set such that $\E_p L(u,Y) < \infty$ for all $u \in \reals^d$ and $p \in \P$, then the conditions of \cite[Corollary 1]{rockafellar1982interchange} are satisfied.
\end{lemma}
\begin{proof}
	First, we enumerate the conditions of \cite[Corollary 1]{rockafellar1982interchange}.\begin{enumerate}
		\item $L$ is a $\B_\Y$-normal convex integrand.
		\item $\E_p L(u,Y) < \infty$ for $u \in \effdom(L(\cdot,y))$ almost surely (on $y$).
		\item There exists $x_0 \in \reals^d$ so that $\E_p f(x_0, Y)$ is (a) finite (b) continuous at $x_0$ and (c) $y \mapsto \cl \dom (f(\cdot, y))$ is almost surely constant.
	\end{enumerate}


As we constrain $L$ to be real-valued, the effective domain and domain are equal, and assumed to be $\reals^d$.
The first condition follows as a corollary of \jessie{another result? do we need to prove this?  see conjecture above}
Therefore, the second condition is satisfied when $\E_p L(u,Y)$ is finite for all $u \in \reals^d$, by assumption on $\P$.
Third, for each $p$, take $x_0 \in \Gamma(p)$.
As $L$ is finite, we know that $\E_p L(\Gamma(p), Y)$ is finite, and part (c) follows by assumption on $L$.
Moreover, $\E_p L(\cdot, Y)$ is continuous at $x_0$ by convexity of $L$ in its first argument and the assumption that $L(u,y)$ is finite for all $u,y$.
\end{proof}

\bibliographystyle{ieeetr}
\bibliography{diss,extra}

\end{document}