\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{[BTW: #1]}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\simplex}{\Delta_\Y}
\newcommand{\relint}[1]{\mathrm{relint}(#1)}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\elic}{\mathrm{elic}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ccdim}{\mathrm{ccdim}}
\newcommand{\codim}{\mathrm{codim}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\spn}{\mathrm{span}}
\newcommand{\propdis}{\mu}

\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{#1^*}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\toto}{\rightrightarrows}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\title{Bounds on the dimension of consistent convex surrogates}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jessie Finocchiaro\\
%  Department of Computer Science\\
  CU Boulder\\
  \texttt{jefi8453@colorado.edu} 
  % examples of more authors
  \And
   Rafael Frongillo\\
%   Department of Computer Science\\
   CU Boulder\\
  % Address \\
   \texttt{raf@colorado.edu} 
   \And
   Bo Waggoner\\
%   Department of Computer Science\\
   CU Boulder \\
  % Address \\
   \texttt{bwag@colorado.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}

\maketitle

\begin{abstract}
%	For finite prediction tasks, one can often naturally model their specific problem with a discrete loss.
%	However, these discrete losses are often hard to optimize, so one often seeks a surrogate losses yielding desirable statistical guarantees with respect to the original problem.
	In prediction tasks, one typically seeks to minimize \emph{empirical risk} by learning a hypothesis function making predictions from a given feature vector about a desired future outcome.
	This hypothesis is learned by minimizing a loss function over a set of labeled training data.
	However, as not all loss functions are easily optimized, one often wishes to optimize a \emph{surrogate loss} yielding the same statistical guarantees as the original.
	In this paper, we are specifically interested in the construction of \emph{consistent} surrogate losses; particularly those that are  ``efficient'' in the dimension of their input.
	We extend the general notion of consistency to properties, so that we may understand when one can construct a convex, consistent surrogate without requiring an original loss, which are uncommon for continuous prediction tasks (for example, the median).
	This allows us to observe that indirect property elicitation is necessary to construct a consistent surrogate, which allows us to apply property elicitation complexity results to the minimal dimension of a consistent surrogate.
	Moreover, when surrogate losses are convex, we state a new lower bound on the dimension of a convex surrogate by using tools from property elicitation.
\end{abstract}

\section{Introduction}\label{sec:intro}


\subsection{Notation}

\section{Background and Related work}\label{sec:related-work}

\subsection{Consistency and calibration for convex losses}\label{subsec:convex-surrogates}




%%I don't think we're going to need this definition...
%\begin{definition}[Excess risk bound]
%	A surrogate loss and link pair $(L,\psi)$ satisfies the \emph{excess risk bound} with respect to a loss $\ell$ if there exists an increasing function $\zeta : \reals \to \reals$ that is continuous at $0$ with $\zeta(0) = 0$ so that for all $f:\X \to \R$ and data distributions $D$ over $\X \times \Y$, we have
%	\begin{multline}
%	\E_{(X,Y) \sim D} \ell(\psi \circ f(X), Y) - \inf_{f^*} \E_{(X,Y) \sim D} \ell(\psi \circ f^*(X), Y) \\ 
%	\leq \zeta \left( \E_{(X,Y) \sim D} L(f(X), Y) - \inf_{f^*} \E_{(X,Y) \sim D} L(f^*(X), Y) \right)
%	\end{multline}
%\end{definition}

%\begin{proposition}
%	A surrogate link pair $(L, \psi)$ is consistent with respect to a loss $\ell$ if and only if it is consistent with respect to the property elicited by $\gamma := \prop{\ell}$.
%\end{proposition}
%\begin{proof}
%	$\implies:$ 
%	If $(L, \psi)$ is consistent with respect to $\ell$, then for all $x \in \X$ with $p = Pr[Y|X]$, we have $L(u; p) \to \inf_{u'} L(u';p) \implies \ell(\psi(u); p) \to \inf_{u'} \ell(\psi(u');p)$.
%	As $\inf_{u'} L(u';p) = \risk{L}(p)$, we then have $\risk{L}(p) - L(u;p) \to 0 \implies \ell(\psi(\Gamma(p)); p) - \ell(\psi(u);p) \to 0$.
%	If $\gamma: p \mapsto \psi(\Gamma(p))$ for all $p \in \simplex$, then we have consistency with respect to $\gamma$, as $\zeta$ being the identity satisfies the requirements on $\zeta$ and yields the bound as the convergence yields $0 \leq 0$ for all $p \in \simplex$.
%	
%	To see $\gamma = \psi \circ \Gamma$, consider that for all $p \in \simplex$, we have $\arginf_{u \in \reals^d} L(u;p) = \arginf_{u \in \reals^d} \ell(\psi(u); p)$ as a corollary of consistency with respect to $\ell$, which implies the statement.
%	
%	$\impliedby:$ 
%	If $(L, \psi)$ is consistent with respect to $\gamma$, then there is a monotonic function $\zeta:\reals \to \reals$ that is continuous at $0$ with $\zeta(0) = 0$ such that, for all $p \in \simplex$, we have $\ell( \psi(u); p) - \ell(\gamma(p); p)  \leq \zeta \left(  L(u;p) - L(\Gamma(p); p) \right)$.
%	
%	As this is true for all $p = Pr[Y|X] \in \simplex$, this also holds in expectation over $X$.
%	This gives the excess risk bound being satisfied, which has been shown implies consistency with respect to $\ell$.\jessiet{In Hari abstain paper, but might be worth proving it here, given the subtleties of consistency definitions...}
%\end{proof}



\begin{definition}[Calibrated: Finite predictions]\label{def:calibrated-finite}
	Let $\ell : \R \times \Y \to \reals_+$ be a discrete loss eliciting the property $\gamma$.
	A surrogate loss $L : \reals^d \times \Y \to \reals_+$  and link $\psi:\reals^d \to \R$ pair $(L, \psi)$ is \emph{calibrated} with respect to $\ell$ if 
	\begin{equation}\label{eq:calibration}
	\forall p \in \simplex: \inf_{u \in \reals^d : \psi(u) \not \in \gamma(p)} L(u;p) > \inf_{u \in \reals^d} L(u;p)~.~
	\end{equation}
\end{definition}

\begin{definition}[Convex Calibration Dimension]
The \emph{convex calibration dimension} $\ccdim(\ell)$ of a discrete loss $\ell$ is the minimum dimension $d$ such that there is a convex loss $L: \reals^d \times \Y \to \reals$ and link $\psi$ such that $L$ is calibrated with respect to $\ell$.
\end{definition}

\subsection{Property elicitation}\label{subsec:properties}

\begin{definition}[Property, elicits, level set]
	\jessiet{Don't like the notation here... fix later.}
	A \emph{property} is a (possibly set-valued) function $\Gamma : \simplex \toto \R$ mapping distributions to reports.
	A loss $L : \R \times \Y \to \reals_+$ \emph{elicits} the property $\Gamma$ if,
	\begin{equation}
	\forall p \in \simplex, \;\; \Gamma(p) = \arginf_{u \in \R}\inprod{p}{L(u)}
	\end{equation}
	Moreover, we call a \emph{level set} $\Gamma_r := \{p \in \simplex : r \in \Gamma(p)\}$ to be the set of distributions for which reporting $u$ minimizes the expected loss of the loss eliciting $\Gamma$.
\end{definition}

\begin{definition}[Indirect Elicitation]\label{def:indirectly-elicits}
	\jessiet{Explain to reader why this is a good def. Made note in original doc.}
	A loss and link $(L, \psi)$ \emph{indirectly elicits} a property $\gamma:\simplex \toto \R'$ if $L$ elicits a property $\Gamma: \simplex \toto \R$ such that for all $r \in \R$, we have $\Gamma_r \subseteq \gamma_{\psi(r)}$.
	
	Moreover, $L$ indirectly elicits $\gamma$ if such a link $\psi$ exists.
\end{definition}


\subsubsection{Elicitation Complexity}\label{subsec:elic-cplx}

\begin{definition}[Convex Elicitation Complexity]
	The \emph{convex elicitation complexity} of a property $\eliccvx(\Gamma)$ is the minimum dimension $d$ such that there is a convex loss $L : \reals^d \times \Y \to \reals$ indirectly eliciting $\Gamma$.
\end{definition}
%\jessie{Just introduce cvx definitions.  Combine two bracketed lines from Raf's notes.  Just define the convex version.  Want to use def 2 with set-valued properties.}


\section{Consistency implies indirect elicitation}\label{sec:consis-implies-indir}
\begin{definition}[Consistent: loss]\label{def:consistent-ell}
	Let $\{f_m : \X \to \R\}$ be a sequence of hypothesis functions.
	A loss and link $(L,\psi)$ are consistent with respect to an original loss $\ell$ if, for all distributions $D$ over input and label spaces $\X \times\Y$, 
	\begin{align*}
	\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y) &\implies \E_D \ell((\psi \circ f_m)(X), Y) \to \inf_f \E_D \ell((\psi \circ f)(X), Y)~.~
	\end{align*}
\end{definition}

\begin{definition}[Consistent: property]\label{def:consistent-prop}
%  \raft{You have $\mu$ and $\mu^\gamma$; I'd vote just $\mu$.  We'll want to change $\mu$ to something else eventually anyway, so maybe a macro unless we're sure $\mu$ won't show up elsewhere. \jessie{Changed}}
%  \raft{Maybe clearer this way: Introduce $L,\psi,\gamma$.  Then intro $\mu$ as any such function.  Then the def. \jessie{Done}}
	Suppose we are given a loss $L : \R \times \Y \to \reals$, link function $\psi: \R \to \R'$, and property $\gamma:\simplex \toto \R'$.
	Moreover, let $\propdis : \R' \times \simplex \to \reals_+$ any function where $\propdis(r,p) = 0 \iff r \in \gamma(p)$.
	
	We say $(L, \psi)$ is \emph{$\mu$-consistent with respect to the property} $\gamma$ if, for all $D$ over $\X \times \Y$ with marginal distributions $D_x$, and sequences $\{f_m: \X \to \R\}$, 
	\begin{equation}
	\E_{D} L(f_m(X), Y) \to \inf_f \E_{D} L( f(X), Y) \implies \E_X \propdis(\psi \circ f_m(X), D_X) \to 0~.~
	\end{equation}
	
	We simply say the pair is consistent with respect to $\gamma$ if there is a $\propdis$ such that $(L,\psi)$ are $\propdis$-consistent with respect to $\gamma$.
\end{definition}


\begin{proposition}\label{prop:consistent-implies-calibrated}
	If a loss and link $(L, \psi)$ are consistent with respect to a loss $\ell$, then they are calibrated with respect to $\ell$.
	\jessiet{Probably appendix later; also, should this be a lemma instead of a proposition?}
  \raft{Do we define ``continuous loss''?  Maybe ``(not necessarily discrete)'' is clearer.}
\end{proposition}
\begin{proof}
%	\jessie{Intuition: Contrapositive.
%		Not calibrated means there is a sequence $\{u_m\}$ such that $R_L(u_m; p) \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$ via Lemma~\ref{lem:calib-converging-regrets}.
%		Take $D = D_x = p$ for some $x \in \X$, and any sequence of functions $\{f_m\} \to f$ with $f_m(x) = u_m$ for all $f_m$. 
%		Now we have $\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y)$, but $\E_D \ell(\psi(f(X)), Y) \not \to \inf_f \E_D \ell(\psi(f(X)), Y)$, and therefore $(L, \psi)$ is not calibrated with respect to $\ell$. 
%	}

	We show the contrapositive.
	If $(L, \psi)$ are not calibrated with respect to $\ell$, then there is a sequence $\{u_m\}$ such that $R_L(u_m; p) \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$ via Lemma~\ref{lem:calib-converging-regrets}.
	Suppose $D \sim \X \times\Y$ has only one $x \in \X$ with $Pr_D(X = x) > 0$ so that $p := D_x$ and $\E_D f(X,Y) = \E_p f(x, Y)$.
	Consider any sequence of functions $\{f_m\} \to f$ with $f_m(x) = u_m$ for all $f_m$.
  \raft{Type error with $D=D_x$; see AA15 for how they phrase \jessie{better?}}
	Now we have $\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y)$, but $\E_D \ell(\psi \circ f(X), Y) \not \to \inf_f \E_D \ell(\psi \circ f(X), Y)$, and therefore $(L, \psi)$ is not calibrated with respect to $\ell$. 
\end{proof}

\begin{lemma}\label{lem:calib-implies-indir}
	If a surrogate and link $(L, \psi)$ are calibrated with respect to a loss $\ell:\R \times\Y \to \reals$, then $L$ indirectly elicits the property $\gamma := \prop{\ell}$.
\end{lemma}
\begin{proof}
	Let $\Gamma$ be the unique property directly elicited by $L$, and fix $p \in \simplex$ with $u$ such that $p \in \Gamma_u$.
	As $p \in \Gamma_u$, then $\zeta(L(u;p) - \risk{L}(p)) = \zeta(0) = 0$, we observe the bound $\ell(\psi(u); p) \leq \risk{\ell}(p)$.
	We also have $\ell(\psi(u); p) \geq \risk{\ell}(p)$ by definition of $\risk{\ell}$, so we must have $\ell(\psi(u);p) = \risk{\ell}(p) = \ell(\gamma(p); p)$, and therefore, $p \in \gamma_{\psi(u)}$.
	Thus, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, so $L$ indirectly elicits $\gamma$.
\end{proof}

\begin{lemma}\label{lem:consistent-loss-implies-prop}
	If $(L, \psi)$ are consistent with respect to $\ell$, then they are consistent with respect to $\gamma := \prop{\ell}$.
\end{lemma}
\begin{proof}
	Define $\propdis(r, p) := R_\ell(r;p)$.
	First, observe that $\propdis(r,p) = 0 \iff r \in \gamma(p)$, since $r \in \gamma(p)$ implies $\ell(r;p) = \risk{\ell}(p)$, and if $r \not \in \gamma(p)$, then $\ell(r;p) > \risk{\ell}(p)$.
	
	Now suppose $(L, \psi)$ are consistent with respect to $\ell$, and take any sequence $\{f_m\}$ so that, for all $D$ over $\X \times \Y$, we have $\E_D L(f_m(X)) \to \inf_f \E_D L(f(X), Y)$.
	This implies 
	\begin{align*}
	&\; \E_D \ell(\psi \circ f_m(X), Y)\to \inf_f \E_D \ell(\psi \circ f(X), Y) \\
	&\iff \E_X R_\ell(\psi \circ f_m(X), D_X) \to 0\\
	&\iff \E_X \propdis(\psi \circ f_m(X), D_X) \to 0~.~
	\end{align*}
%  \raft{Close; missing expectation inside inf, and I think you can skip the first $\iff$ and give the $\implies$ as an $\iff$ (right?), and then follow with a second $\iff$ with $\mu$. \jessie{I had the same doubtful thought, hence the implication, but the iff is true.}}
%  \raft{BTW, I think it's less messy to use $\psi \circ f_m(X)$ as above \jessie{Changed}}
%	As regret defines $\propdis$ and tends to $0$, we observe consistency with respect to $\gamma$.
\end{proof}

\begin{theorem}\label{thm:consistent-implies-indir-elic}
	If a surrogate and link pair $(L, \psi)$ is consistent with respect to a property $\gamma$ or loss $\ell$ eliciting $\gamma$, then $(L, \psi)$ indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}
%  \raft{Maybe ref the consistency definition here explicitly, at the top and the end}
	As consistency with respect to $\ell$ implies consistency with respect to $\gamma := \prop{\ell}$ (Lemma~\ref{lem:consistent-loss-implies-prop}), it suffices to show the result for consistency with respect to a property $\gamma$ (Definition~\ref{def:consistent-prop}).
  
	We show the contrapositive; suppose $(L, \psi)$ does not indirectly elicit $\gamma$, and take $\Gamma := \prop{L}$.
%  \raft{Oh good!  I was going to say we should use this way of stating indirect elicitation (``$(L, \psi)$ indirectly elicits $\gamma$'') when we define it above, and use it.  We can then also define the version without $\psi$ easily. \jessie{Changed the definition of indirect elicitation and this Theorem statement.}}
	Then there is a distribution $p \in \simplex$ so that $u \in \Gamma(p)$ but $\psi(u) \not \in \gamma(p)$.

	Consider the constant sequence $\{u_m\}$ with $u_m = u$ for all $m$, and take $D \sim \X \times\Y$ with only one $x \in \X$ with $Pr_D(X = x) > 0$ so that $p := D_x$ and $\E_D g(X,Y) = \E_p g(x, Y)$ for all functions $g : \X \times \Y \to \reals$, and any sequence $\{f_m\}$ so that $f_m(x) = u_m$ for all $m$.
  \raft{Check $D$ wording here too \jessie{better?}}
  	Since $\{u_m\}$ is a constant sequence, we observe $\E_D L(f_m(X), Y) = \inf_f L(f(X),Y)$ for all $m$ since $u \in \Gamma(p)$.
	This yields $\E_D L(f_m(X), Y) \to \inf_f L(f(X),Y)$.
%  \raft{Maybe clearer to first say $\E_D L(f_m(X), Y) = \inf_f L(f(X),Y)$ for all $m$, and then the easy implication of the convergence \jessie{Done}}
	However, we have $\E_X \propdis(\psi \circ f_m(X), D_X) = \propdis(\psi(u_m), p) \not \to 0$ as $\propdis(\psi(u), p) \neq 0$ (since $\psi(u) \not \in \gamma(p)$) and the sequence is constant.
%  \raft{Almost -- just clarify that this is the expectation over $D$ too by construction \jessie{Kinda done?}}
	Thus, we observe $(L, \psi)$ is not consistent with respect to $\gamma$ (Definition~\ref{def:consistent-prop}).
\end{proof}

\section{Characteristics of Consistent Convex Surrogates}\label{sec:char-convex}

\begin{definition}[Flat]
	A \emph{flat} $F$ of $\reals^n$ is an affine subspace of $\reals^n$.
	In other words, $F\subseteq \reals^n$ can be written $F=\{x\in\reals^n : Wx + b = 0\}$ for some $W\in\reals^{d\times n}$ and $b\in\reals^d$ where $d\leq n$.
	The dimension of the flat $F$ is given by $f := n - \mathrm{rank}(W)$.
\end{definition}

\begin{theorem}\label{thm:cvx-flats}
	Suppose we are given a property $\gamma$ and distribution $p \in \simplex$.
	For all $r\in\gamma(p)$, if there is no $(n - d-1)$-dimensional flat $F$ containing $p$ so that $F \cap \simplex \subseteq \gamma_r$, then there is no convex surrogate loss $L : \reals^d \times \Y \to \reals$ that indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}[Proof]
	We prove the contrapositive: if there is a surrogate loss $L$ indirectly eliciting $\gamma$, then for all $p \in \simplex$ and $r \in \R$ such that $r \in \gamma(p)$, there is an $(n-d-1)$-dimensional flat $F$ such that $F \cap \simplex \subseteq \gamma_r$ and $p \in F$.
	
	Suppose we have $L:\reals^d \times \Y \to \reals$ eliciting $\Gamma$ and link $\psi : \reals^d \to \R$ such that for all $u \in \reals^d$, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, i.e., $L$ indirectly elicits $\gamma$.
	Fix $p \in \simplex$, and take any $u \in \Gamma(p)$ such that $\psi(u) = r \in \gamma(p)$.
	We know that $u \in \Gamma(p) \iff \vec 0 \in \partial L(u; p) \iff \vec 0 \in \oplus_y p_y \partial L(u,y)$, where $\oplus$ denotes the nonnegative weighted Minkowski sum~\cite[Theorem 4.1.1]{hiriart2012fundamentals}.  (Observe that in order for $L$ to be convex, $L(\cdot, y)$ must be convex for all $y \in \Y$.) 
	
	Observe that the Minkowski sum $\oplus_y p_y \partial L(u,y) = \{p_1 x_1 + \ldots + p_n x_n : x_1 \in \partial L(u,1), \ldots, x_n \in \partial L(u,n) \}$.
	In order for $\vec 0$ to be in this $p$-weighted Minkowski sum, we then have that each choice of $x_i$ is a flat for $\partial L(u, i)$.
	Moreover, with $p$ fixed, we know there is a vector of witnesses $W' = [x_1, \ldots, x_n] \in \reals^{d\times n}$ such that $\sum_y p_y x_y = \vec 0$.

	Observe that $F' := \ker(W')$ is at most $(n-d)$ dimensions in $\reals^n$ and contains $p$ by construction.
	Since we are only concerned with a flat intersected with the simplex being a subset of $\gamma_r$, we can reduce the dimension by adding an affine simplex constraint. 
	That is, take $F := \{x \in \reals^n : W x + b = \vec 0\}$, where we have $W := [W'; \mathbbm{1}^T] \in \reals^{(d+1) \times n}$ and $b \in \reals^{d+1}$.
	The dimension of $F$ is then $n - \mathrm{rank}(W)$, which is $n - (d+1)$ if $W$ is full rank.
	We also claim that $F \cap \simplex \subseteq \gamma_r$.
	To see this, consider $q \in F$, but $q \not \in \gamma_r$.
	That would mean that $Wq + b = \vec 0 \iff W'q + b' = \vec 0$ (since $q \in \simplex$ satisfies the simplex constraints).
	This in turn implies $\vec 0 \in \partial L(u;q)$, implying $q \in \Gamma_u$.
	Therefore, $q \not \in \gamma_r$ would contradict $L$ indirectly eliciting $\gamma$.
\end{proof}


\section{Discrete-valued predictions}\label{sec:finite-calib}

%\begin{lemma}
%	Suppose we are given a discrete loss $\ell : \R \times\Y \to \reals_+$ eliciting the property $\gamma$.
%	If $(L, \psi)$ is calibrated with respect to $\ell$, then $L$ indirectly elicits $\gamma$.
%\end{lemma}
%\begin{proof}
%	Fix $p \in \simplex$ and consider $u \in \Gamma(p)$.
%	Since $(L, \psi)$ is calibrated with respect to $\ell$, we have $p \in \Gamma_u$.
%	We simply need to show $\psi(u) \in \gamma(p)$.
%	If $\psi(u) \not \in \gamma(p)$, then the terms in the condition for calibration (Equation~\eqref{eq:calibration}) are equal; hence $L$ would not be calibrated with respect to $\ell$.
%\end{proof}

\begin{definition}[Subspace of feasible directions]
	Define the \emph{subspace of feasible directions} $\S_\C(p)$ of a convex set $\C \subseteq \reals^n$ at a point $p \in \C$ as the subspace $\S_\C(p) = \{ v \in \reals^n : \exists \epsilon_0 > 0 $ such that $p + \epsilon v \in \C \; \forall \epsilon \in (-\epsilon_0,\epsilon_0) \}$.
%  \raf{Let's simplify to $\epsilon \in (-\epsilon_0,\epsilon_0)$}
\end{definition}

\begin{lemma}\label{lem:feas-sub-is-a-flat}
	Suppose we have the finite elicitable property $\gamma$ and distribution $p \in \relint{\simplex}$ with $r \in \gamma(p)$.
	If $F$ is a flat containing $p$ such that $F \cap \simplex \subseteq \gamma_r$, then $F - p$ is a subspace contained in $\S_{\gamma_r}(p)$.
%  \raf{I think you want $p$ in the interior of the simplex for now}
\end{lemma}
\begin{proof}
  To spell it out, observe $F-p$ is a subspace as it is a linear shift of $F$, which is a subspace by definition of a flat.
  Now consider $v \in F - p$.
  Since $p \in \relint{\simplex}$, there is an open ball of radius $\epsilon$ in the same affine space as $\simplex$ so that for all $q \in B(p, \epsilon)$, we have $q \in \simplex$.
  In particular, take $\alpha = \epsilon / 2$, and we observe $p \pm \alpha v \in B(p, \epsilon)$, and therefore $p \pm \alpha v \in \simplex$.
  Moreover, by the assumption $v \in F - p$, we also have $p \pm \alpha v \in \gamma_r$. 
  Since level sets of elicitable properties are convex (\cite{lambert2009eliciting}) this is true for all $\alpha' \leq \alpha$.
  Therefore, we observe $v \in S_{\gamma_r}(p)$, so $F-p \subseteq S_{\gamma_r}(p)$.
%  First, if $p+v \in \gamma_r$ and $p -v \in \gamma_r$, then we have $v \in \S_{\gamma_r}(p)$ with $\epsilon_0 = 1$ as level sets of elicitable properties are convex by~\cite{lambert2009eliciting}.
%  If either $p + v$ or $p - v \not \in \gamma_r$, it must be because the term is out of the simplex by definition of $F$.
%  
%  However, if both $p + \alpha^+ v$ and $p - \alpha^- v \in \simplex$ for some $\alpha^\pm \in (0,1)$, then $v \in \S_{\gamma_r}(p)$ with $\epsilon_0 = \min(\alpha^+, \alpha^-)$.
%  As $p \in \relint{\simplex}$, there is always such an $\epsilon_0$; if there were not, then we would observe $v \not \in F - p$.
%  Therefore, we have $v \in F - p \implies v \in \S_{\gamma_r}(p)$, so $(F - p) \subseteq \S_{\gamma_r}(p)$.
\end{proof}


%\begin{lemma}\label{lem:feas-sub-is-a-flat}
%	Suppose we have the finite property $\gamma$ and distribution $p \in \simplex$.
%	For ant $r$ such that $r \in \gamma(p)$, the subspace $F := \mu_{\gamma_r}(p) + p $ is a flat, $F$ contains $p$, and $F \cap \simplex \subseteq \gamma_r$.
%\end{lemma}
%\begin{proof}
%	Write the level set $\gamma_r = \{q \in \reals^n : A_1 q \leq b_1, \; A_2 q \leq b_2, \; A_3 q \leq b_3 \}$, where $A_1 p \leq b_1, A_2 p < b_2,$ and $A_3 p = b_3$.
%	
%	We want to show three things: first, $F$ is a flat as it is $\ker([A_1 ; A_3])$ by~\cite{ramaswamy2016convex}.
%	Second, $p \in F + p$ by construction, since $\vec 0 \in F$.
%	(Consider that $p \in \gamma_r$, so with $epsilon_0 = 1$, we have $p + \epsilon\vec 0 = p \in \gamma_r$ for all $\epsilon \in (0, 1)$; thus, $\vec 0 \in F$.)
%	
%	Third, we want to show $(F + p) \cap \simplex \subseteq \gamma_r$.
%	Take some $q := p + \epsilon v$ for $v \in F$.
%	By construction of $F$, we have $q \in F \implies q \in \simplex$, so $F + p \subseteq \simplex \implies F + p \cap \simplex = F + p$.
%	Thus it just remains to be shown that $F + p \subseteq \gamma_r$.
%	Since $v\in F \implies q \in F + p \implies p + \epsilon v \in \gamma_r$ by construction, so we have $F+p \subseteq \gamma_r$.	
%\end{proof}

\begin{lemma}\label{lem:p-boundary-fstar}
	For a given property $\gamma$, fix $p \in \simplex$ and $r \in\R$ so that $r \in \gamma(p)$.
	Consider the flat $F'$ containing $p$ of dimension at least $n-d-1$.
	The flat $F^* = F' \cap \spn(\{e_y : y \in \supp(p)\})$ has dimension $\dim(F') - (n - \|p\|_0)$.
\end{lemma}
\begin{proof}
	Consider $P := \spn(\{e_y : y \in \supp(p) \})$.
	\begin{align*}
	\dim(F^*) &= \dim(F') + \|p\|_0 - \dim(F' + P)
	\end{align*}
	The claim then holds if $n = \dim(F' + P)$.
	\jessie{This is where I'm stuck??}
\end{proof}

\begin{lemma}\label{lem:p-boundary-fsd}
	For any $p \in \simplex$ and $r$ such that $p \in \gamma_r$, we have $\dim(\S_{\gamma_r}(p)) = \dim(\S'_{\gamma_r}(p))$, where $\S_{\gamma_r}(p) := \S_{\gamma_r}(p) \cap \spn(\{e_y : y \in \supp(p) \}$.
\end{lemma}
\begin{proof}
	Consider $P := \spn(\{e_y : y \in \supp(p) \})$.
	\begin{align*}
	\dim(\S'_{\gamma_r}(p)) &= \dim(\S_{\gamma_r}(p)) + \dim(P) - \dim(\S_{\gamma_r}(p) + P)
	\end{align*}
	If $\dim(P) = \dim(\S_{\gamma_r}(p) + P)$, the claim holds.
	Be definition of $P$, we know $\dim(P) = \|p\|_0$, so we can show $\dim(\S_{\gamma_r}(p) + P) = \|p\|_0$.		\jessie{This is where I'm stuck??}
	
\end{proof}

\begin{corollary}[\cite{ramaswamy2016convex}]
	Suppose we are given a discrete loss $\ell:\R \to \reals^\Y_+$ eliciting $\gamma$.
	Take $p \in \simplex$ and $r \in \R$ such that $p \in \gamma_r$.
	\begin{equation}
	\ccdim(\ell) \geq \|p\|_0 - \dim(\S_{\gamma_r}(p)) - 1~.~
	\end{equation}
\end{corollary}
\begin{proof}
	Let $L : \reals^d \times \Y \to \reals$ be a calibrated surrogate for $\ell$.
	First, for intuition, consider $p \in \relint{\simplex}$. 
	Theorem~\ref{thm:cvx-flats} implies that there exists a flat $F'$ of affine dimension at least $n-d-1$.
	Lemma~\ref{lem:feas-sub-is-a-flat} then says that $\dim(\S_{\gamma_r}(p)) \geq \dim(F') \geq n-d-1 \implies d \geq n - \dim(\S_{\gamma_r}(p)) - 1$.
	
	Now, if $p \not \in \relint{\simplex}$, we can apply Lemma~\ref{lem:p-boundary-fstar} to observe the existence of an $F^*$ with dimension $\dim(F') - (n - \|p\|_0)$.
	Moreover, since projecting to $(\{e_y : y \in \supp(p)\})$ does not change the feasible subspace dimension, so we observe the result.
\end{proof}

\subsection{Elicitation complexity and convex calibration dimension}

\begin{proposition}
	Consider the discrete loss $\ell : \R \times \Y \to \reals_+$ and $\gamma:= \prop{\ell}$.
	Then $\eliccvx(\gamma) \leq \ccdim(\ell)$.
	\jessie{Need to define $\eliccvx$ and $\ccdim$.}
\end{proposition}
\begin{proof}
	Let $d$ be the convex calibration dimension of $\ell$, and $(L, \psi)$ be calibrated with respect to $\ell$ with $L$ convex.
	Then $L$ indirectly elicits $\gamma$ by Lemma~\ref{lem:calib-implies-indir}.
\end{proof}



\section{Continuous-valued predictions}\label{sec:contin-consis}


\newcommand{\lbar}{\underline{L}} % couldn't do L* while proofreading...
\newcommand{\iden}{\mathrm{iden}}
\newcommand{\Var}{\mathrm{Var}}

\begin{lemma}[\cite{frongillo2018elicitation}]
  \label{lem:elic-complex-bayes-concave}
  Suppose the loss $L$ elicits $\Gamma:\simplex\to\R$.
  Let $\lbar$ be the Bayes risk of $L$.
  Then for any $p,p'\in\simplex$ with $\Gamma(p)\neq\Gamma(p')$, we have $\lbar(\lambda p + (1-\lambda) p') > \lambda \lbar(p) + (1-\lambda) \lbar(p')$ for all $\lambda\in(0,1)$.
\end{lemma}

\begin{definition}
  A property $\Gamma:\simplex\to\R$ is \emph{$d$-identifiable} if its level sets are flats of co-dimension at most $d$ intersected with $\simplex$.
  \raft{Update to be consistent with our definitions / notation / conventions.  I'd vote for using co-dimension since it makes so much of the analysis easier...  Could make more formal too.}
  We write $\iden(\Gamma) = \min\{d \in \mathbb{N} : \Gamma\text{ is $d$-identifiable}\}$.
\end{definition}
\jessiet{Flats or unions of flats?}

\raf{A modification of the argument of \citet[Corollary 7]{frongillo2018elicitation}}

\begin{theorem}
  \label{thm:bayes-risk-lower-bound}
  Let $L$ elicit some identifiable $\Gamma:\simplex\to\reals^d$ with $\iden(\Gamma)=d$.
  If $\lbar$ is non-constant on every non-singleton level set of $\Gamma$, then $\eliccvx(\lbar) \geq \min(d+1,n-1)$.
\end{theorem}
\begin{proof}
  Suppose for a contradiction that we have some convex-elicitable property $\hat\Gamma:\simplex\to\reals^d$ and link $\psi : \reals^d \to \reals$ such that $\lbar = \psi \circ \hat\Gamma$.
  The condition $\elic_\I(\Gamma)=d$ implies that $\Gamma$ is $d$-identifiable but not $(d-1)$-identifiable, and thus there must be some level set $\Gamma_r$ which is a flat of dimension $n-d-1$.
  \raft{What I mean is you can't describe $\Gamma_r$ using a flat of another dimension; revisit once we settle terminology}

  The proof of \citet[Theorem 4]{frongillo2018elicitation} argues that $\hat\Gamma$ must refine $\Gamma$, in the sense that every level set of $\hat\Gamma$ is contained in a level set of $\Gamma$; for completeness we give the argument here.
  Suppose for a contradiction that we have $p,p'$ with $\hat\Gamma(p)=\hat\Gamma(p')$ but $\Gamma(p) \neq \Gamma(p')$.
  As $\lbar = \psi \circ \hat\Gamma$, we also have $\lbar(p) = \lbar(p')$.
  Letting $p'' = \tfrac 1 2 p +  \tfrac 1 2 p'$, Lemma~\ref{lem:elic-complex-bayes-concave} would then give us $\lbar(p'') >  \tfrac 1 2 \lbar(p) +  \tfrac 1 2 \lbar(p') = \lbar(p)$.
  By \citet{osband1985providing}, the level sets $\hat\Gamma_{\hat r}$ are convex, giving $\hat\Gamma(p'') = \hat\Gamma(p)$, which would imply $\lbar(p'')=\lbar(p)$, contradicting $\lbar = \psi \circ \hat\Gamma$.
  We conclude $\hat\Gamma$ must refine $\Gamma$, and in particular, we must have $\hat\Gamma_{\hat r} \subseteq \Gamma_r$ for some $\hat r\in\hat\Gamma(\simplex)$.
  % The proof of \cite[Theorem 4]{frongillo2018elicitation} argues that $\hat\Gamma$ must refine $\Gamma$, in the sense that for all $\hat r \in \hat\Gamma(\simplex)$ we have $\hat\Gamma_{\hat r} \subseteq \Gamma_r$ for some $r\in\reals^k$.
  
  By Theorem~\ref{thm:cvx-flats}, as $\hat\Gamma$ is $d$-convex elicitable $\hat\Gamma_{\hat r}$ is the union of flats of dimension $n-d-1$ interesected with $\simplex$.
  \raft{Should flesh out this next step; this is the key step}
  As $\hat\Gamma_{\hat r} \subseteq \Gamma_r$ and $\Gamma_r$ is a flat of dimension $n-d-1$ interesected with $\simplex$, we conclude $\hat\Gamma_{\hat r} = \Gamma_r$.
  Since by assumption $\lbar$ is non-constant on $\Gamma_r = \hat\Gamma_{\hat r}$, we have distributions $p,p' \in\hat\Gamma_{\hat r}$ with $\lbar(p)\neq\lbar(p')$, which again contradicts $\lbar = \psi \circ \hat\Gamma$.
  \raft{We actually needed $\Gamma_r$ to be a non-singleton for this last step; otherwise we need to conclude the $n-1$ case; revisit after flat/dimension stuff}
\end{proof}


\subsection{Examples \raf{all informal for now}}

Both these results exactly match what Ian and I showed for the class of identifiable properties, which is super interesting...
\begin{itemize}
\item Variance: $\iden(\E Y) = 1$, and over $\simplex$ every non-singleton level set of $\E Y$ has different variances, so the theorem gives $\eliccvx(\Var) \geq 2$.  We then have the matching upper bound since $L(r,y) = (r_1-y)^2 + (r_2-y^2)^2$ is convex in $r$, and we can link to $\Var$ from there.
\item Entropy and norms: In fact, \emph{any} strictly convex/concave function $G$ of $p$ will do here.  You can write it as the Bayes risk (or negative Bayes risk) of a strictly proper scoring rule, which elicits the full distribution $\Gamma(p) = p$.  Since $\iden(\Gamma) = n-1$ since you don't actually need the $n$th coordinate, you have $\eliccvx(G) = n-1$; the upper bound comes from the fact that you can elicit the full distribution with a convex loss and just compute $G$.
\end{itemize}





\newpage

\section*{Broader Impact}

\begin{ack}
Nishant, Adam
\end{ack}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

\newpage
\appendix
\section{A general notion of calibration}

\begin{definition}[Calibrated]\label{def:calibrated-general}
	A loss $L:\reals^d \times \Y \to \reals$ is \emph{calibrated} with respect to a loss $\ell : \R \times \Y \to \reals$ eliciting the property $\gamma$ if there is a link $\psi : \reals^d \to \R$ such that, for all distributions $p \in \simplex$, there exists a function $\zeta : \reals_+ \to \reals_+$ with $\zeta$ continuous at $0^+$ and $\zeta(0) = 0$ such that for all $u \in \reals^d$, we have
	\begin{equation}\label{eq:calibrated-general}
	\ell( \psi(u); p) - \risk{\ell}(p)  \leq \zeta \left(  L(u;p) - \risk{L}(p) \right)~.~
	\end{equation}
\end{definition}

\jessie{Consider the following four conditions: Suppose we are given $\zeta:\reals_+ \to \reals_+$.
\begin{enumerate}
	\item [A] $\zeta$ satisfies $\zeta : 0 \mapsto 0$ and is continuous at $0$.
	\item [B] $\epsilon_m \to 0 \implies \zeta(\epsilon_m) \to 0$.
	\item [C] Given $\zeta:\reals \to \reals_+$, for all $u \in \reals^d$, $R_\ell(\psi(u); p) \leq \zeta(R_L(u;p))$.
	\item [D] For all $p \in \simplex$ and sequences $\{u_m\}$ so that $R_L(u_m; p) \to 0$, we have $R_\ell(\psi(u_m); p) \to 0$.
\end{enumerate}
$\exists \zeta$ so that $(A \wedge C)$ defines calibration, and we have $A \iff B$ shown in Lemma~\ref{lem:continuous-iff-limits}.  Lemma~\ref{lem:calib-converging-regrets} shoes calibration iff $D$.}

\begin{proposition}
	When $\R$ is finite, calibration via Definition~\ref{def:calibrated-general} implies calibration via Definition~\ref{def:calibrated-finite}.
	\jessie{eventually want iff}
\end{proposition}
\begin{proof}
	\jessie{Old proof commented out; simplified using Lemma~\ref{lem:calib-converging-regrets}}

	We prove the contrapositive; if $(L, \psi)$ is not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then it is not calibrated via Definition~\ref{def:calibrated-general} either.
	If $(L, \psi)$ are not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then there is a $p \in \simplex$ so that $\inf_{u : \psi(u) \not \in \gamma(p)} L(u;p) = \inf_u L(u; p)$.
	Thus there is a sequence $\{u_m\}$ so that $\lim_{m \to \infty} \psi(u_m) \not \in \gamma(p)$ and $L(u_m; p) \to \risk{L}(p)$.  
	Now we have $R_L(u_m; p \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$, so by Lemma~\ref{lem:calib-converging-regrets}, we contradict calibration by Def~\ref{def:calibrated-general}.

% Commented out 05.19.2020 for easier proof if Lemma 5 is true.
%	We prove the contrapositive; if $(L, \psi)$ is not calibrated with respect to $\ell$ by Definition~\ref{def:calibrated-finite}, then it is not calibrated via Definition~\ref{def:calibrated-general} either.
%	
%	Suppose there was a distribution $p \in \simplex$ so that $\inf_{u : \psi(u) \not \in \gamma(p)} L(u;p) = \inf_{u} L(u;p)$.
%	There must then be a sequence $\{u_m\} \to u$ so that $\lim_{m \to \infty} \psi(u_m) \not \in \gamma(p)$ and $L(u_m; p) \to \risk{L}(p)$.
%	
%	This consequently implies $R_L(u_m;p) \to 0$ as $L(u_m; p) \to \risk{L}(p)$, but as $\ell(\psi(u_m); p) \not \to \risk{\ell}(p)$ (if it did converge, then we would have $\psi(u_m) \to r \in \gamma(p)$), so $R_\ell(\psi(u_m); p) \not \to 0$.
%	Thus, by Lemma~\ref{lem:calib-converging-regrets}, we have no calibration via Definition~\ref{def:calibrated-general}.  
		
	
\end{proof}

\begin{lemma}\label{lem:continuous-iff-limits}
	A function $\zeta:\reals \to \reals$ is continuous at $0$ and $\zeta(0) = 0$ if and only if the sequence $\{u_m\} \to 0 \implies \zeta(u_m) \to 0$.
	\jessie{$A \iff B$}
\end{lemma}
\begin{proof}
	$\implies$ Suppose we have a sequence $\{u_m\} \to 0$.
	By continuity, we have $\lim_{u_m \to 0}\zeta(u_m) = \zeta(0) = 0$, so $\zeta(u_m) \to 0$.
	
	$\impliedby$ Suppose $\zeta(0) \neq 0$ but $\zeta$ was continuous at $0$.
	The constant sequence $\{u_m\} = 0$ then converges to $0$, but as $\zeta$ is continuous at $0$, we must have $\lim_{m \to \infty}\zeta(u_m) = \zeta(0) \neq 0$, so $\zeta(u_m) \not \to 0$.
	
	Now suppose $\zeta(0) = 0$ but $\zeta$ was not continuous at $0$.
	There must be a sequence $\{u_m\} \to 0$ so that $\lim_{m \to \infty}\zeta(u_m) \neq \zeta(0) = 0$, so $\zeta(u_m) \not \to 0$.
\end{proof}

\begin{lemma}\label{lem:calib-converging-regrets}
	A continuous surrogate and link $(L,\psi)$ are calibrated (via definition~\ref{def:calibrated-general}) with respect to $\ell$ if and only if, for all $p \in \simplex$ and sequences $\{u_m\}$ so that $R_L(u_m; p) \to 0$, we have $R_\ell(\psi(u_m); p) \to 0$.
	\jessie{$(A \wedge C) \iff D$}
\end{lemma}
\begin{proof}
\jessie{$(A \wedge C) \implies D$}
	$\implies$ Take a sequence $\{u_m\}$ so that $R_L(u_m;p) \to 0$.
	Since $\zeta(0) = 0$ and $\zeta$ is continuous at $0$, we have $\zeta(R_L(u_m;p)) \to 0$.
	As the bound from Equation~\eqref{eq:calibrated-general} is satisfied for all $u \in \reals^d$ by assumption, we observe
	\begin{align*}
	\forall m, \; &0 \leq R_\ell(\psi(u_m); p) \leq \zeta(R_L(u_m;p))\\
	\implies &0 \leq \lim_{m \to \infty} R_\ell(\psi(u_m); p) \leq \lim_{m \to \infty} \zeta(R_L(u_m;p)) = 0\\
	\implies &0 = \lim_{m\to\infty} R_\ell(\psi(u_m); p) ~.~
	\end{align*}
	
	
	$\impliedby$ 
\jessie{$D \implies (A \wedge C)$}
	Fix $p \in \simplex$, and consider $\zeta(c) := \sup_{u: R_L(u;p) \leq c} R_\ell(\psi(u); p)$.  
	We will show $R_L(u_m; p) \to 0 \implies R_\ell(\psi(u_m); p) \to 0$ gives calibration via the function $\zeta$ constructed above. 
	With $\zeta$ as constructed, we should that the bound in equation~\eqref{eq:calibrated-general} is satisfied for all $u \in \reals^d$ and apply Lemma~\ref{lem:continuous-iff-limits} to observe that if there is a sequence $\{\epsilon_m\} \to 0$ so that $\zeta(\epsilon_m) \not \to 0$, it is because $R_L(u_m, p) \not \to 0 \not \implies R_\ell(\psi(u_m), p) \to 0$.
	

\jessie{D $\implies$ C}
Now, we observe that the bound in Equation~\eqref{eq:calibrated-general} is satisfied for all $u \in \reals^d$ by construction of $\zeta$.
Let $S(v) := \{u' \in \reals^d : R_L(u';p) \leq R_L(v,p) \}$.
Showing $R_\ell(\psi(u);p) \leq \sup_{u' \in S(u)} R_\ell(\psi(u') ; p)$ for all $u \in \reals^d$ gives the condition $C$.
As $u$ is in the space over which the surpremum is being taken (as $R_L(u;p) \leq R_L(u;p)$), we then have calibration by definition of the supremum.

\jessie{Not $B$ leads to contradiction of $D$.}
Now suppose there exists a sequence $\{\epsilon_m\} \to 0$ so that $\zeta(\epsilon_m) \not \to 0$.
Consider $S(\epsilon) = \{u \in \reals^d : R_L(u,p) \leq \epsilon\}$.

\begin{align*}
\epsilon_1 \leq \epsilon_2 &\implies S(\epsilon_1) \subseteq S(\epsilon_2)\\
&\implies \zeta(\epsilon_1) \leq \zeta(\epsilon_2)~.~
\end{align*}
Now suppose there exists a sequence $\{u_m\}$ so that $R_L(u_m, p) \to 0$.
Then for all $\epsilon > 0$, there exists a $m' \in \mathbb{N}$ so that $R_L(u_m, p) < \epsilon$ for all $m \geq m'$.
Since this is true for all $\epsilon$, we have $S(\epsilon)$ nonempty for all $\epsilon > 0$, and therefore $\zeta(c)$ is finite for all $c > 0$.
Now if $\zeta(\epsilon_m) \not \to 0$, it must be because $R_\ell(\psi(u_m), p) \not \to 0$ for some sequence converging to zero surrogate regret, and therefore we contradict the statement $R_L(u_m, p) \to 0 \implies R_\ell(\psi(u_m), p) \to 0$.

Moreover, we argue that such a sequence of $\{u_m\}$ with converging surrogate regret always exists by continuity and boundedness from below \jessiet{really just need lower semi-continuity and boundedness from below} of the surrogate loss, since we can take the constant sequence at the (attained) infimum.

%\jessie{$D \implies A$, but actually $\lnot B \wedge C \implies \lnot D$}
%Fix $p \in \simplex$.
%Suppose we have a sequence $\{\epsilon_m\}$ so that $\epsilon_m \to 0$, but $\zeta(\epsilon_m) \not \to 0$.
%If $L$ is continuous over the reals \jessiet{Need this, right?}, then for each $m$, we can construct a subsequence $\{u^j_m\}$ so that $R_L(u^j_m, p) \to \epsilon_m$ for all $m$.
%We can now construct the sequence $\{\epsilon'_m\}$ with $\epsilon'_m = \lim_{j \to \infty} R_L(u^j_m, p)$.
%This sequence converges to $0$, but we have $\lim_{m \to \infty} \zeta(\epsilon'_m) = \lim_{m \to \infty} \lim_{j \to \infty} \zeta(R_L(u^j_m, p)) \not \to 0$. 
%As $R_\ell(\psi(u_m); p) \leq R_L(u_m; p)$ for all $m$, we then have this being true in the limit.
%For this sequence, this then gives a loose bound of $0 \leq \lim_{m\to\infty} R_\ell(\psi(u_m); p) \leq c$. 
	
%commented out post meeting 05.20.2020
%\jessie{D $\implies$ B ($\iff$ A)}
%	First, we can verify that $\zeta$ as constructed maps $\zeta(0) = 0$ and is continuous at $0$.	
%	As $\risk{L}$ and $\risk{\ell}$ are well-defined for all $p \in \simplex$, we note that $\zeta(0) = \sup_{u':R_L(u;p) = 0} (R_\ell(\psi(u'); p)) = \sup_{u' \in \Gamma(p)} (R_\ell(\psi(u'); p))$, which is finite if $\Gamma$ is non-degenerate (i.e. there is no $p \in \simplex$ so that $\Gamma(p) = \emptyset$).
%	If there is a $u \in \Gamma(p)$ so that $R_\ell(\psi(u); p) > 0$ (i.e. $\zeta(0) > 0$), then the constant sequence $\{u_m\} = u$ for all $m$ gives $R_L(u_m;p) \to 0$ but $R_\ell(\psi(u_m); p) \not \to 0$, yielding a contradiction; therefore, we must have $\zeta(0) = 0$.
%	
%	To see $\zeta$ is continuous at $0$, we just need to show $\lim_{c \to 0^+} \zeta(c) = 0$. 
%	Take any sequence $\{u_m\}$ so that $R_L(u_m; p) \to 0$, and therefore $R_\ell(\psi(u_m);p) \to 0$.
%	Since this is true for all sequences, we have the upper bound tending to $0$, and thus $\lim_{c\to 0^+}\zeta(c) = 0$.
%		

\end{proof}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
