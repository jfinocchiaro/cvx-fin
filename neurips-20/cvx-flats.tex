\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath, amsfonts, amssymb, amsthm}

\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{[BTW: #1]}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\simplex}{\Delta_\Y}
\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\elic}{\mathrm{elic}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ccdim}{\mathrm{ccdim}}
\newcommand{\codim}{\mathrm{codim}}

\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathcal{R}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
\newcommand{\toto}{\rightrightarrows}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\title{cvx-flats}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Jessie Finocchiaro\\
%  Department of Computer Science\\
  CU Boulder\\
  \texttt{jefi8453@colorado.edu} 
  % examples of more authors
  \And
   Rafael Frongillo\\
%   Department of Computer Science\\
   CU Boulder\\
  % Address \\
   \texttt{raf@colorado.edu} 
   \And
   Bo Waggoner\\
%   Department of Computer Science\\
   CU Boulder \\
  % Address \\
   \texttt{bwag@colorado.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}



\begin{document}

\maketitle

\begin{abstract}
%	For finite prediction tasks, one can often naturally model their specific problem with a discrete loss.
%	However, these discrete losses are often hard to optimize, so one often seeks a surrogate losses yielding desirable statistical guarantees with respect to the original problem.
	In prediction tasks, one typically seeks to minimize \emph{empirical risk} by learning a hypothesis function making predictions from a given feature vector about a desired future outcome.
	This hypothesis is learned by minimizing a loss function over a set of labeled training data.
	However, as not all loss functions are easily optimized, one often wishes to optimize a \emph{surrogate loss} yielding the same statistical guarantees as the original.
	In this paper, we are specifically interested in the construction of \emph{consistent} surrogate losses that are ``efficient'' in the dimension of their input.
	We observe a connection between consistent surrogates and indirect property elicitation, which allows us to apply property elicitation complexity results to the minimal dimension of a consistent surrogate.
	In particular, we characterize the minimal input dimension needed in order to construct a \emph{convex}, consistent surrogate using tools from property elicitation.
\end{abstract}

\section{Introduction}\label{sec:intro}

In supervised machine learning, one often wants to make a prediction about future outcomes by training a classifier to minimize the average empirical loss of a labeled training set, where the loss is determined by the task at hand.
For example, 0-1 loss is often desired for classification tasks.
However, finite losses are typically difficult to optimize, so we construct a continuous \emph{surrogate} loss that one can more easily optimize.
In particular, we want our surrogate loss to yield guarantees so that optimal predictions for the surrogate can be linked to optimal predictions for the original (possibly discrete) loss, regardless of the underlying data distribution. 
Throughout this paper, the guarantees we desire from the condition of consistency.
We are specifically interested in \emph{consistent} and \emph{convex} surrogates yielding the same statistical guarantees as optimizing an original, possibly discrete, loss function. 

In particular, we typically use surrogate loss functions that take some input $u \in \reals^d$ and measure error against the observed outcome $y \in \Y$.
The value $d$ provides some notion of efficiency for the surrogate, as low-dimensional convex losses often improve the efficiency of the optimization algorithm.
We answer open questions from~\cite{frongillo2015elicitation} regarding a general characterization of the efficiency of \emph{convex elicitable properties}, yielding the minimum dimension $d$ for which a consistent loss can be constructed for the task at hand.

In Section~\ref{sec:related-work}, we review previous work and introduce the two main concepts used: \emph{property elicitation} and \emph{consistency}.
We draw connections between the two in Section~\ref{sec:char-convex}, presenting our main result (Theorem~\ref{thm:cvx-flats}) about the existence of consistent convex surrogates in a given dimension $d$.
In Section~\ref{sec:finite-calib}, we relate our setting to the study of \emph{convex calibration dimension} introduced by~\cite{ramaswamy2016convex}.
Our Theorem~\ref{thm:cvx-flats} generalizes the concept of feasible subspace dimension, and we can use it to simplify the proof of~\cite[Theorem \jessie{CHECK}]{ramaswamy2016convex}.
In continuous settings, one often has a prediction task for which they want to construct a consistent convex loss.
In Section~\ref{sec:contin-consis}, we address the construction of convex consistent losses by applying the main insights from Theorem~\ref{thm:cvx-flats} to this setting, answering the open question posed by~\cite[Ref?]{frongillo2015elicitation}.


\subsection{Notation}
In this paper, we take the outcome set $\Y$ and consider $\simplex$ to be the simplex over $\Y$. 
jessie{Check if $\prop{L}$ notation is in there.}

%discrete land
Let $\R$ be a (typically finite) report set and $\Y$ a finite outcome set for now.
Note that we do not necessarily have $\Y = \R$.  
For example, in ranking problems, $\R$ may be all $n!$ permutations over the $n$ outcomes forming $\Y$.
A loss $\ell : \R \times \Y \to \reals_+$ is a \emph{discrete loss} if $|\R| < \infty$, and in general, we use $\ell$ to denote our \emph{original loss} for which we want to construct a consistent surrogate.
We denote $\ell(r)_y$ as the $y^{th}$ element of the vector $\ell(r)$, and consider this to be the punishment for reporting $r$ if outcome $y$ is realized.
For a distribution $p \in \simplex$, we denote the expected discrete loss for report $r$ to be $\E_{Y \sim p} \ell(r, Y) := \ell(r; p)$.
When considering surrogate losses, we denote the loss by $L : \reals^d \times \Y \to \reals_+$, and typically denote a surrogate report with $u$.

We sometimes override the notation $\R$ to refer to a generic report set.
Throughout this paper, we use tools from \emph{property elicitation} to understand the existence of consistent surrogate functions.
Introduced in Section~\ref{subsec:properties}, a (set-valued) property $\Gamma: \simplex \to 2^\R \setminus \emptyset$ is denoted $\Gamma:\simplex \toto \R$.
We denote a finite property elicited by a discrete loss $\ell$ by $\gamma := \prop{\ell}$.
Additionally, we let $\Gamma := \prop{L}$ be the ``surrogate'' property elicited by the surrogate loss $L$.
Again, we sometimes override notation to let $\Gamma$ be a generic property; when $\Gamma: \simplex \toto \R$, we refer to both a generic property and report set.

In Section~\ref{sec:char-convex}, we will focus on characteristics of convex surrogate losses.
Here, when we say a loss $L(\cdot, \cdot)$ is \emph{convex}, we mean that it is convex in its first argument for every value of its second argument.

\section{Related work}\label{sec:related-work}
In this paper, we use tools from property elicitation to study the construction of consistent surrogate losses.
We distinguish between finite and infinite outcome settings for prediction tasks.

Property elicitation of a single property is well-understood through \cite{savage1971elicitation,osband1985information-eliciting,lambert2008eliciting, lambert2009eliciting, lambert2018elicitation}.
Recently, the elicitation of multiple properties was studied by~\cite{frongillo2015vector-valued,frongillo2015elicitation,fissler2015higher}.
In particular, these works study the \emph{minimum} dimension of a real-valued loss needed to elicit a property or vector of properties.


\subsection{Property elicitation}\label{subsec:properties}
Informally, a property is a function mapping a probability distribution $p$ over the outcome set $\Y$ to the ``suggested report'' given $p$.

\begin{definition}[Property, elicits, level set]
	\jessiet{Don't like the notation here... fix later.}
	A \emph{property} is a set-valued function $\Gamma : \simplex \toto \R$ mapping distributions to reports.
	A loss $L : \R \times \Y \to \reals_+$ \emph{elicits} the property $\Gamma$ if,
	\begin{equation}
	\forall p \in \simplex, \;\; \Gamma(p) = \argmin_{r \in \R}\inprod{p}{L(r)}
	\end{equation}
	Moreover, we call a \emph{level set} $\Gamma_r := \{p \in \simplex : r \in \Gamma(p)\}$ to be the set of distributions for which reporting $u$ minimizes the expected loss of the loss eliciting $\Gamma$.
\end{definition}
Note, moreover, that we call a property $\gamma: \simplex \toto \R$ \emph{finite} if $|\R| < \infty$.
Without much loss of generality, we assume that finite properties are \emph{non-redundant}, meaning that for each $\gamma_r$, there is no $\gamma_{r'}$ such that $\gamma_{r'} \subseteq \gamma_r$.

\cite{finocchiaro2018convex} are among the first to consider a characterization of \emph{convex} elicitable properties, in which they find that all continuous, nowhere locally constant elicitable properties (in finite outcome settings) are elicitable by a \emph{convex} loss.
However, their assumptions are more restrictive than ours; they only consider losses defined on the real line (see Section~\ref{subsec:elic-cplx} below for our generalization) and assume the properties to be identifiable: an assumption we do not need here.

In particular, when $\R \subseteq \reals^d$ for some $d \in \mathbb{Z}$, \cite{frongillo2015elicitation} introduce the notion of \emph{Elicitation Complexity} and \emph{Identification Complexity}, which uses $d$ to measure the ``efficiency'' of a loss $L$ eliciting $\Gamma$.
However, their notion of complexity relies not just on the property $\Gamma$, but instead on any property \emph{indirectly elicited} by $L$.

\begin{definition}[Indirect Elicitation]\label{def:indirectly-elicits}
\jessiet{Revisit for strongness of definition}
	A loss $L$ \emph{indirectly elicits} a property $\Gamma':\simplex \toto \R'$ if it elicits a property $\Gamma: \simplex \toto \R$ such that there is a function $\psi:\R \to \R'$ such that for all $r \in \R$, we have $\Gamma_r \subseteq \Gamma'_{\psi(r)}$.
\end{definition}


In Proposition~\ref{prop:consistent-implies-indir-elic}, we relate indirect elicitation to the consistency of a surrogate loss.

\subsubsection{Elicitation Complexity}\label{subsec:elic-cplx}
When properties are vector-valued (i.e.$\, \Gamma: \simplex \to \reals^d$),~\cite{frongillo2015elicitation} introduce the notion of \emph{elicitation complexity}.

\begin{definition}[$d$-elicitable, Elicitation Complexity]
	We say a elicitable property $\Gamma:\simplex \to \reals^d$ is $d$-elicitable if there is a loss $L : \reals^d \times \Y \to \reals_+$ eliciting it.
	The \emph{elicitation complexity} of a property $\Gamma := \elic(\Gamma)$ is the minimum dimension $d$ such that there is a property $\hat \Gamma$ that is $d$-elicitable and link $\psi$ so that $\Gamma = \psi \circ \hat \Gamma$.
	Moreover, we use $\eliccvx(\Gamma)$ to denote the elicitation complexity of $\Gamma$ when the directly elicited property $\hat \Gamma$ is elicited by a \emph{convex} surrogate loss.
\end{definition}


\iffalse
\begin{definition}[Identifiable]
	A property $\Gamma: \simplex \to \R$ is \emph{identifiable} if there is a function $V: \R \times\Y \to \reals$ such that 
	\begin{equation*}
	\forall p \in \simplex, \; r \in \Gamma(p) \iff V(r) = \vec 0~.~
	\end{equation*}
\end{definition}
\fi

\iffalse
Similarly, we can consider the notion of \emph{identifiability} for vector-valued properties.

\begin{definition}[$d$-identifiable, Identification Complexity]
	A vector-valued property $\Gamma: \simplex \to \reals^d$ is $d$-identifiable if it holds that $p \in \Gamma_u \iff V(u, p) = \vec 0 \in \reals^d$.
	Like elicitation complexity, we define the \emph{identification complexity} of a property $\Gamma$ as the minimum dimension $d$ such that there is a link $\psi$ such that $\Gamma = \psi \circ \hat \Gamma$ and $\hat \Gamma$ is $d$-identifiable.
\end{definition}
\fi



\subsection{Consistency and calibration for convex losses}\label{subsec:convex-surrogates}
\cite{zhang2004statistical,lin2004note,bartlett2006convexity,tewari2007consistency} form a characterization of consistent, calibrated surrogates for classification problems.
In particular,~\cite{bartlett2006convexity} show that if a convex surrogate is differentiable, minimal at $0$, and nonnegative, there is a consistent surrogate for binary classification problems, and ~\cite{tewari2007consistency} generalizes this result for multiclass classification. 
\cite{ramaswamy2016convex} further shows necessary and sufficient conditions for finite prediction problems to be consistent that can be applied to general loss matrices (i.e., problems where both the report and outcome set are finite.)
They additionally introduce a notion of \emph{convex calibration} dimension similar to elicitation complexity mentioned above; we discuss this further in Section~\ref{sec:finite-calib}.
Their calibration results then yield consistent surrogates for finite prediction problems such as hierarchical classification from~\cite{ramaswamy2015hierarchical} and classification with an abstain option studied by~\cite{ramaswamy2018consistent}.

\cite{steinwart2007compare} generalizes the study of consistent losses from the classification setting \ldots \jessiet{Need to come back to this.  area exam Jessie looked at this briefly, but current Jessie is a whole different person.}

While the subtleties of \emph{consistency} vary slightly with each work, we use two definitions that have the same implications, but one may be more appropriate in a given context.

\begin{definition}[Consistent: loss]
	Suppose $f_m : \X \to \R$ is the hypothesis function learned by minimizing empirical training loss over $m$ labeled examples.
	we say a loss and link $(L,\psi)$ are consistent with respect to an original loss $\ell$ if, for all distributions $D$ over input and label spaces $\X \times\Y$, 
	\begin{align*}
	\E_D L(f_m(X), Y) \to \inf_f \E_D L(f(X), Y) &\implies \E_D \ell(\psi \circ f_m(X), Y) \to \inf_f \E_D \ell(\psi \circ f(X), Y)~.~
	\end{align*}
\end{definition}


For continuous properties, such as the expected value, variance, and entropy, asking for a consistent surrogate with respect to an original \emph{loss} may not make sense.
In this setting, we also define consistency with respect to a property. 
\begin{definition}[Consistent: property]
	We say a loss $L:\reals^d \times\ Y \to \reals$ is consistent with respect to a property $\Gamma : \simplex \toto \R$ if there is a link $\psi : \reals^d \to \R$ such that, for all distributions $D$ over $\X \times \Y$ with $p = Pr[Y |X]$, we have
	\begin{equation}
	\Gamma(p) = \psi \circ \arginf_{u} \E_{Y \sim p} L(u,Y)~.~
	\end{equation}
\end{definition}


\jessie{Excess risk definition on its own?  Followed by statement that satisfying excess risk bound implies consistency.}


\section{Characteristics of Consistent Convex Surrogates}\label{sec:char-convex}

Proposition~\ref{prop:consistent-implies-indir-elic} below allows us to understand the connection between property elicitation and consistent losses.
Moreover, combining Proposition~\ref{prop:consistent-implies-indir-elic} and Theorem~\ref{thm:cvx-flats}, we provide new conditions for finding lower bounds on \emph{convex calibration dimension} for finite outcome settings in Section~\ref{sec:finite-calib}.
In infinite outcome settings, we can generalize Theorem~\ref{thm:cvx-flats}\jessie{hopefully...} in order to answer an open question about convex elicitation complexity posed by~\cite{frongillo2015elicitation}.

\begin{proposition}\label{prop:consistent-implies-indir-elic}
	If a surrogate and link pair $(L, \psi)$ is consistent with respect to $\ell$, then $L$ indirectly elicits $\gamma := \prop{\ell}$.
\end{proposition}
\begin{proof}
Since consistency must hold for all $D$ over $\X \times \Y$, let us focus on distributions $D$ such that the probability of any $x \in \X$ is a point mass, allowing us to focus on conditional distributions $p \in \simplex$ such that $p = Pr[Y = y | X = x]$, allowing us to be agnostic to $\X$.
Now, instead of focusing on sequences of optimal hypotheses, we can focus on optimal reports when given $x \in \X$ if the hypothesis class $\F$ is sufficiently rich (i.e. the optimal hypothesis is in the hypothesis class for all $x \in \X$.)

If the infimum of $L$ is always attained in report space, then for 

 \jessie{FINISH}
\end{proof}

\begin{remark}
	Both definitions of consistency tie to indirect elicitation
\end{remark}

This result allows us to apply the upper bounds from~\cite{frongillo2015elicitation} when trying to find the minimal dimension consistent surrogate for a given loss.
In particular, when $\Y$ is finite and we restrict $L$ to be convex, this translates to applications to \emph{convex calibration dimension} of~\cite{ramaswamy2016convex}.


When one restricts their focus to \emph{convex} surrogates, we know that all minima are global, and we then have $\vec 0  \in \partial L(u, y)$ if and only if $u$ minimizes $L$ in its first component.

%\begin{theorem}
%	Suppose the property $\Gamma$ is elicited by the differentiable, convex loss $L : \reals^d \times \Y \to \reals_+$ such that the infimum $L(\cdot, y)$ is attained for all $y \in \Y$.
%	Then for all $p \in \simplex$, we have $u \in \Gamma(p) \iff \vec 0 \in \nabla_{u'} \inprod{p}{L(u')}$.
%\end{theorem}
%\begin{proof}[Proof sketch]
%	Since $L$ is convex, we know that all minimizers are global. 
%	This implies that $\vec 0 \in \nabla_{u} L(u; p) \iff u \in \arginf_{u'} L(u';p)$, and the latter is true if and only if $u \in \Gamma(p)$ by definition of elicitation.
%\end{proof}
%
%In a large sense, this lets us conclude that $d$-convex elicitable properties are also $d$-identifiable, taking the identification function $V(\cdot;p)$ to be $\nabla_u L(\cdot; p)$ for all $p \in \simplex$.
%\begin{corollary}
%	Suppose the property $\Gamma$ is elicited by the convex loss $L : \reals^d \times \Y \to \reals_+$ such that the infimum $L(\cdot, y)$ is attained for all $y \in \Y$.
%	Then $\Gamma$ is $d$-identifiable.
%\end{corollary}

\begin{definition}[Flat]
	A \emph{flat} $F$ of $\reals^n$ is an affine subspace of $\reals^n$.
	In other words, $F\subseteq \reals^n$ can be written $F=\{x\in\reals^n : Wx + b = 0\}$ for some $W\in\reals^{d\times n}$ and $b\in\reals^d$ where $d\leq n$.
	By the rank-nullity theorem, the dimension of the flat $F$ is the nullity of $W$ by construction, and is given by $f := n - \mathrm{rank}(W)$.
	If $W$ is full-rank, then $f = n - d$.
%  \raft{Would suggest defining a bit more formally after, like ``In other words, $F\subseteq \reals^d$ can be written $F=\{x\in\reals^n : Wx + b = \vec 0\}$ for some $W\in\reals^{d\times n}$ and $b\in\reals^d$ where $d\leq n$.  Then you can define the dimension of $F$ easily as $d$, assuming the matrix is full rank}
\end{definition}
Unless otherwise mentioned, we will assume that $W$ is full rank.

Now we can consider specific characteristics of convex losses in order to understand the convex elicitation complexity of a given property.

\begin{theorem}\label{thm:cvx-flats}
	Suppose we are given a property $\gamma$ and distribution $p \in \simplex$.
  \raft{Made logic more direct \jessie{Done?}}
	For all $r\in\gamma(p)$, if there is no $(n - d-1)$-dimensional flat $F$ containing $p$ so that $F \cap \simplex \subseteq \gamma_r$, then there is no convex surrogate loss $L : \reals^d \times \Y \to \reals$ that indirectly elicits $\gamma$.
\end{theorem}
\begin{proof}[Proof intuition]
	We prove the contrapositive: if there is a surrogate loss $L$ indirectly eliciting $\gamma$, then for all $p \in \simplex$ and $r \in \R$ such that $r \in \gamma(p)$, there is an $(n-d-1)$-dimensional flat $F$ such that $F \cap \simplex \subseteq \gamma_r$ and $p \in F$.
%  \raft{Nice way to start.}
	
	Suppose we have $L:\reals^d \times \Y \to \reals$ eliciting $\Gamma$ and link $\psi : \reals^d \to \R$ such that for all $u \in \reals^d$, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, i.e., $L$ indirectly elicits $\gamma$.
	Fix $p \in \simplex$, and take any $u \in \Gamma(p)$ such that $\psi(u) = r \in \gamma(p)$.
	We know that $u \in \Gamma(p) \iff \vec 0 \in \partial L(u; p) \iff \vec 0 \in \oplus_y p_y \partial L(u,y)$, where $\oplus$ denotes the nonnegative weighted Minkowski sum~\cite[Theorem 4.1.1]{hiriart2012fundamentals}.  (Observe that in order for $L$ to be convex, $L(\cdot, y)$ must be convex for all $y \in \Y$.) \raft{It's a good exercise to look up the necessary results in e.g. Rockafellar (let me know if you don't have a copy; I can send a PDF).  Note that $L(\cdot,y)$ is defined on all of $\reals^d$ for all $y$, which is key.  I'd recommend spelling this out in 2-3 sentences.}.
	
	Observe that the Minkowski sum $\oplus_y p_y \partial L(u,y) = \{p_1 x_1 + \ldots + p_n x_n : x_1 \in \partial L(u,1), \ldots, x_n \in \partial L(u,n) \}$.
	In order for $\vec 0$ to be in this $p$-weighted Minkowski sum, we then have that each choice of $x_i$ is a flat for $\partial L(u, i)$.
	Moreover, with $p$ fixed, we know there is a vector of witnesses $W' = [x_1, \ldots, x_n] \in \reals^{d\times n}$ such that $\sum_y p_y x_y = \vec 0$.
%  \raft{State the dimensions of $W$}
	
%  \raft{Don't understand; this is necessary to complete the proof!  (So good thing you have it.)}
	Observe that $F' := \ker(W')$ is at most $(n-d)$ dimensions in $\reals^n$ and contains $p$ by construction.
	Since we are only concerned with a flat intersected with the simplex being a subset of $\gamma_r$, we can reduce the dimension by adding an affine simplex constraint. 
	That is, take $F := \{x \in \reals^n : W x + b = \vec 0\}$, where we have $W := [W'; \mathbbm{1}^T] \in \reals^{(d+1) \times n}$ and $b \in \reals^{d+1}$.
	The dimension of $F$ is then $n - \mathrm{rank}(W)$, which is $n - (d+1)$ if $W$ is full rank.
  \raft{Justify the dimension.  Hint: $F$ contains $\vec 0$.  You'll probably want to define some $F'$ by adding a certain something to $W$ -- check out the FSD paper.}
	We also claim that $F \cap \simplex \subseteq \gamma_r$.
	To see this, consider $q \in F$, but $q \not \in \gamma_r$.
	That would mean that $Wq + b = \vec 0 \iff W'q + b' = \vec 0$ (since $q \in \simplex$ satisfies the simplex constraints).
	This in turn implies $\vec 0 \in \partial L(u;q)$, implying $q \in \Gamma_u$.
	Therefore, $q \not \in \gamma_r$ would contradict $L$ indirectly eliciting $\gamma$.
\end{proof}


\section{Finite report and outcome settings}\label{sec:finite-calib}
\jessie{Note subtle difference between calib and consistent, but formally introduce calibration.}
\begin{definition}[Calibrated]
	Let $\ell : \R \times \Y \to \reals_+$ be a discrete loss eliciting the property $\gamma$.
	A surrogate loss $L : \reals^d \times \Y \to \reals_+$ is \emph{calibrated} with respect to $\ell$ if there exists a link function $\psi: \reals^d \to \R$ such that
	\begin{equation}
	\forall p \in \simplex: \inf_{u \in \reals^d : \psi(u) \not \in \gamma(p)} \inprod{p}{L(u)} > \inf_{u \in \reals^d} \inprod{p}{L(u)}~.~
	\end{equation}
\end{definition}

\jessie{Thankfully, \cite{ramaswamy2016convex} has swooped in to save the day with below result. (Original for binary and multiclass classification come from~\cite{bartlett2006convexity} and \cite{tewari2007consistency}, respectively.)}

\begin{theorem}[\cite{ramaswamy2016convex}]\label{thm:calib-iff-consistent}
	If $\Y$ is finite, then the surrogate loss $L:\reals^d \to \reals^\Y_+$ is calibrated with respect to $\ell: \R \times \Y \to \reals_+$ if and only if there exists a link function $\psi : \reals^d \to \R$ such that for all distributions $D$ on $\X \times\Y$ and all sequences of (vector) functions $f_m : \X \to \reals^d$,
	\begin{equation*}
	\E_D L(f_m(X), Y) \to \inf_f \E_P L(f(X), Y) \implies \E_D \ell(\psi  \circ f_m(X), Y) \to \inf_f \E_D \ell(\psi \circ f(X), Y)~.~
	\end{equation*}
\end{theorem}
In words, we have that a surrogate and link are calibrated with respect to a discrete loss if and only if any consistent sequence of hypotheses for the surrogate, when linked, is consistent for the discrete loss.

As calibration does not rely on the representation space $\X$, we can ignore this and focus instead on calibration; we focus on \jessie{marginal} probability distributions $p \in \simplex$, agnostic to $\X$.

\hrule 
\bigskip

\begin{lemma}
	Suppose we are given a discrete loss $\ell : \R \times\Y \to \reals_+$ eliciting the property $\gamma$.
	If $(L, \psi)$ is calibrated with respect to $\ell$, then $L$ indirectly elicits $\gamma$.
\end{lemma}
The proof follows from observing that Theorem~\ref{thm:calib-iff-consistent} yields consistency of $(L,\psi)$ with respect to $\ell$ from calibration, then applying Proposition~\ref{prop:consistent-implies-indir-elic}.

\iffalse
\begin{lemma}
  If a convex loss $L:\reals^d \times\Y \to \reals$ with $|\Y| < \infty$ indirectly elicits a finite property $\gamma$, then for all $p \in \simplex$ and $r \in \gamma(p)$, there is a flat $F$ containing $p$ that is $(n-d-1)$-dimensional such that $F \cap \simplex \subseteq \gamma_r$.
\end{lemma}
This is the contrapositive of Theorem~\ref{thm:cvx-flats}, and what is actually proven there.
\fi

\jessie{Ramaswamy et al introduce FSD as a technique to provide their lower bound...}
\begin{definition}[Feasible subspace dimension]
	The \emph{Feasible Subspace Dimension} of a convex set $\C \subseteq \reals^n$ at a point $p \in \C$, denoted by $\mu_\C(p)$ is defined as the dimension of the subspace $\F_\C(p) \cap (-\F_\C(p))$, where $\F_\C(p)$ is the cone of feasible directions of $\C$ at $p$\footnote{The cone of feasible directions is given by $\F_\C(p) = \{ v \in \reals^n : \exists \epsilon_0 \geq 0 $ such that $p + \epsilon v \in \C \; \forall \epsilon \in (0,\epsilon_0) \}$}.
\end{definition}
\jessiet{This is the definition from the Ramaswamy paper, but I have a small qualm with this definition... it should be fine if $\C$ is a polytope, but if $\C$ is not a polytope, it's unclear what they want from this definition.  For finite elicitable properties, obviously this doesn't matter, but yeah...}
\jessiet{I also can't tell if this is a small bug on their end, but I think they define the cone of feasible directions as the open cone, but they really want the closed cone.  I think?}

\begin{lemma}\label{lem:feas-sub-is-a-flat}
	Suppose we have the finite property $\gamma$ and distribution $p \in \simplex$.
	For ant $r$ such that $r \in \gamma(p)$, the subspace $F := \F_{\gamma_r}(p) \cap (-\F_{\gamma_r}(p))$ is a flat containing $p$, and $F \cap \simplex \subseteq \gamma_r$.
\end{lemma}
\begin{proof}
	\jessie{TODO}
\end{proof}

\iffalse
\begin{remark}
	Suppose we are given a finite property $\gamma$ and distribution $p \in \simplex$.
	If $L :\reals^d \times\Y \to \reals$ indirectly elicits $\gamma$, then
	For all $r \in \gamma(p)$, we have 
	\begin{equation}
	\mu_{\gamma_r}(p) \leq f(p)~,~
	\end{equation}
	where $f(p)$ is the maximal dimension of a flat $F$ of $L$ containing $p$ such that $F \cap \simplex \subseteq \gamma_r$. 
\end{remark}
\fi

The following is a statement from~\cite{ramaswamy2016convex} providing an upper bound on the convex calibration dimension of a given discrete loss, which now follows as a Corollary of our Theorem~\ref{thm:cvx-flats} and Lemma~\ref{lem:feas-sub-is-a-flat}.

\begin{corollary}[\cite{ramaswamy2016convex}]
	Suppose we are given a discrete loss $\ell:\R \to \reals^\Y_+$ eliciting $\gamma$.
	Take $p \in \simplex$ and $r \in \R$ such that $p \in \gamma_r$.
	\begin{equation}
	\ccdim(\ell) \geq \|p\|_0 - \mu_{\gamma_r}(p) - 1~.~
	\end{equation}
\end{corollary}



\jessie{Don't worry about this for now.}
\subsection{Elicitation complexity and convex calibration dimension}
\cite{ramaswamy2016convex} introduces \emph{Convex Calibration Dimension}, which studies the minimal dimension input $d$ to construct a convex surrogate $L : \reals^d \times \Y \to \reals_+$ that is calibrated with respect to a given discrete loss $\ell$, and is similar to elicitaiton complexity as introduced in Definition~\ref{def:elic:cplx}.

Theorem~\ref{thm:calib-iff-consistent} allows us to now relate convex elicitation complexity and convex calibration dimension introduced by~\cite[Definition 10]{ramaswamy2016convex}.
\begin{proposition}
	Consider the discrete loss $\ell : \R \times \Y \to \reals_+$ and $\gamma:= \prop{\ell}$.
	Then $\eliccvx(\gamma) \leq \ccdim(\ell)$.
	\jessie{Need to define $\eliccvx$ and $\ccdim$.}
\end{proposition}
\begin{proof}
	Let $d$ be the convex calibration dimension of $\ell$, and $(L, \psi)$ be calibrated with respect to $\ell$ with $L$ convex.
	Then $(L, \psi)$ is also consistent with respect to $\ell$, and therefore $L$ indirectly elicits $\gamma$.
\end{proof}

\cite{finocchiaro2019embedding} presents the notion of a surrogate loss \emph{embedding} a discrete loss-- typically by a polyhedral (piecewise linear and convex) surrogate.
If $\gamma$ is the finite property elicited by the discrete loss $\ell$, then for all discrete losses, $\eliccvx(\gamma) \leq \elicpoly(\gamma) \leq \elicembed(\ell)$.
However, one open question that remains from their work is if these bounds are equal. \jessie{\ldots} 


\subsection{Example: Abstain loss}
\jessie{Abstain example... maybe... hopefully...}

\section{Continuous Properties}\label{sec:contin-consis}
Theorem~\ref{thm:} allows us to address one major open question from~\cite{frongillo2015elicitation}: \emph{what are lower bounds on convex elicitation complexity?}


\newpage

\section*{Broader Impact}
\jessie{Now required for NeurIPS, but doesn't count towards page limit.}

\begin{ack}
All the thanks
\end{ack}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

\end{document}
