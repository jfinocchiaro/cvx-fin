\documentclass{article}

\usepackage{neurips_2020_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools, amsmath, amsthm, graphicx}
\newcommand{\reals}{\mathbb{R}}

\usepackage{enumitem}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

%\newcommand{\ker}{\mathrm{ker}}


\begin{document}
We thank the reviewers for their thoughtful comments and feedback.  We will reply to common themes followed by individual responses.  The minor technical notes not mentioned will also be addressed in our next revision.  

\textbf{Notation:} We will clarify notation including: $|\Y| = n$ (and that it is always finite in this paper), $\underline{L}$ = Bayes Risk($L$), and $\partial$ is the subdifferential; and set-notation in line 321.

\textbf{Examples:} We agree that earlier examples would be beneficial to help readers build intuition, and thank the reviewers for their suggestions. We will use abstain and variance as running examples starting from Section 2.

\textbf{Motivation to study prediction dimension:}  The computational (and perhaps sample) complexity of surrogate risk minimization is a function of the prediction dimension among other things.  A substantially lower prediction dimension, like the exponential reduction observed in the BEP abstain surrogate [22], can significantly improve the efficiency of the learning algorithm as a whole.  Understanding prediction dimension bounds is particularly important in structured prediction, but is also relevant in continuous estimation problems, one of our other ``quadrants’’.  Estimating risk and uncertainty are important problems in ML and statistics, and our results directly apply; see the applications to variance, entropy, distributional norms following Theorem 3.


\textbf{R1:}  We hope the earlier introduction of examples will improve clarity and improve the intuition for notation. $\ker(W)$ is the kernel of the matrix $W$. On broader impacts, we will expand on the potential negative impacts.

\textbf{R2:}  Thank you for the suggestion to add the implication interpretation of indirect elicitation.

\textbf{R3:} We hope the ``motivation of dimension’’ paragraph above helps clarify the significance of finding lower bounds.


\textbf{R5:} Thank you for all your comments and suggestions.
\begin{itemize}[itemsep=1pt,topsep=0pt]
\item 
Regarding Theorems 2 and 3, it is correct that the best lower bound is the supremum over choices of $r$ and $p$. We agree that could be a good presentation, although stylistically we had reasons for our current approach. These include: this suprema does not seem to have a nice closed form; one’s intuition often leads to the best particular choice of $r$ and $p$; and this presentation matches that of [21, Theorem 16].

\item 
$\mathrm{relint}(\Delta_\Y)$ in Theorem 2: This assumption is actually WLOG, by restricting to a face (subsimplex) $F$ such that $p\in\mathrm{relint}(F)$. Corollary 1 applies Theorem 2 in this way to remove the $\mathrm{relint}(\Delta_\Y)$ assumption.

\item 
  While straightforward, we felt it important to emphasize Theorem 1 as it is not stated clearly in the literature, and indirect elicitation is not a well known tool to reason about consistency.

\item 
Citation on line 161: more general proofs have been derived by [28] (multiclass classification) and [21] (general matrix losses), but the proof is given most rigorously by [2]; we will add the other citations. We will also cite the NeurIPS 2012 version of [21].

\item 
The notion of feasible subspace dimension comes from [21]; we can add a visual explanation in the Appendix, as well as an explanation for eliciting the first two moments.  In line 287, there was a typo that muddled our intention: we meant ``a line in two-dimensional space’’, i.e. $\reals^2$.

\item 
We only speculate that a lower prediction dimension may help lower sample complexity. Our intuition comes from, e.g. sample complexity of learning a linear mapping from $\X$ to prediction space $\reals^n$ versus to $\reals^{\log n}$. It would be very interesting to formalize, but difficult because of the importance of the rest of the context (original prediction dimension, link function, hypothesis classes, …). We will clarify this is speculation.
\end{itemize}


\textbf{R6:} Thanks for all your comments.
\begin{itemize}[itemsep=1pt,topsep=0pt]
\item 
We will emphasize approximate minimizers while giving intuition for consistency; thank you for the suggestion.  We will also direct the reader to Chapter 3 of [27] for intuition about the conditional problem.

\item 
Translating results from conditional distributions to those on $\X \times \Y$ is rigorously addressed by [2] and [28], which we can emphasize. When indirect elicitation is \emph{sufficient} for consistency is an interesting question; one example is the class of polyhedral surrogates [5]. For finite $\Y$, one would only need to show indirect elicitation implies calibration, which seems likely to hold more generally with some restrictions on the link function.

\item 
Connection to the definition of calibration in [27]: you are correct. Calibration functions are especially important when considering generalization rates. In contrast, our interest in consistency precedes discussions of rates, which we hope to study in future work.

\item 
Infinite infimum expected loss: In this paper we assume there always exists a minimizer of expected loss. For losses, this follows by definition of $\mathcal{L}$. For properties, it follows from nonemptiness. Understanding other cases is indeed interesting; we will emphasize this assumption throughout.

\item 
Thanks for the comments regarding definitions at the beginning of the paper. We seem to quickly require almost all these definitions to state results, except calibration. We felt most ML readers will wish to see how calibration relates, however.  We will carefully consider organization and these suggestions.
\end{itemize}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
