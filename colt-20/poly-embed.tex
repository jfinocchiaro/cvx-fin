\documentclass[anon]{colt2020} % Anonymized submission
% \documentclass{colt2020} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Embedding dimension]{Embedding Dimension of Polyhedral Losses}
\usepackage{times}

\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

%\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{orange!20!white}{BTW: #1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\nonnegreals}{\reals_{\geq 0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\relint}{\mathrm{relint}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\renewcommand{\P}{\mathcal{P}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}

\newcommand{\Opt}{\mathrm{Opt}}
\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathrm{int}(#1)}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}
\newtheorem{condition}{Condition}
\newtheorem{claim}{Claim}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
 \coltauthor{\Name{Jessie Finocchiaro} \Email{jefi8453@colorado.edu}\\
  \Name{Rafael Frongillo} \Email{raf@colorado.edu}\\
  \Name{Bo Waggoner} \Email{bwag@colorado.edu}\\
  \addr CU Boulder}

% Authors with different addresses:
%\coltauthor{%
% \Name{Jessie Finocchiaro} \Email{jefi8453@colorado.edu}\\
% \addr CU Boulder
% \AND
% \Name{Rafael Frongillo} \Email{Raf@colorado.edu}\\
% \addr CU Boulder
% \AND
% \Name{Bo Waggoner} \Email{bwag@colorado.edu}\\
% \addr CU Boulder%
%}

\begin{document}

\maketitle

\begin{abstract}%
  A common technique in supervised learning with discrete losses, such as $0-1$ loss, is to optimize a convex surrogate loss over $\reals^d$, calibrated with respect to the original loss.
  In particular, recent work has investigated embedding the original predictions (e.g. labels) as points in $\reals^d$.
  %Recent work has proposed the notion of designing calibrated surrogate losses for classification-like problems through the lens of \emph{embedding} the original problem into $\reals^d$ and optimizing a polyhedral loss that is calibrated with respect to the original loss.
  In this work, we study the notion of \emph{embedding dimension} for given discrete losses.
  We characterize when a given discrete loss can be embedded into the real line, as well as when a higher-dimension input to the surrogate loss is required.
  Moreover, we give a quadratic feasibility program that yields lower bounds on the embedding dimension of a given discrete loss.
\end{abstract}

\begin{keywords}%
  Calibrated surrogates, convex surrogates, proper scoring rules%
\end{keywords}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
To do...



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setting}

\subsection{Discrete losses and embeddings}

The base object of study is a \emph{discrete loss function} $\ell: \R \to \nonnegreals^{\Y}$ where $\R$ and $\Y$ are finite sets.
Here $\ell(r)_y$ is the loss of prediction $r \in \R$ (the report space) on $y \in \Y$ (the observation or label space).
For simplicity we let $\Y = [n]$ throughout (where $[n] = \{1,\ldots,n\}$).
The set of probability distributions on $\Y$ is denoted $\simplex\subseteq\nonnegreals^{\Y}$, represented as vectors of probabilities.
We write $p(y)$ for the probability of outcome $y \in \Y$ drawn from $p \in \simplex$.
As is assumed by~\cite{finocchiaro2019embedding}, we assume the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.

We denote surrogate losses in dimension $d$ by $L:\reals^d\to\nonnegreals^{\Y}$, with predictions typically written $u\in\reals^d$.
The expected losses when $Y \sim p$ can be written $\inprod{p}{\ell(r)}$ and $\inprod{p}{L(u)}$.
In order to capture how the surrogate corresponds to the original loss, we rely on the following notation from property elicitation literature \bo{cite}: $\gamma_r \subseteq \simplex$ represents those distributions where $r$ minimizes expected loss, i.e. $\gamma_r = \{p : \inprod{p}{\ell(r)} \leq \inprod{p}{\ell(r')} ~ (\forall r' \in \R) \}$.
Similarly, $\Gamma_u = \{ p : \inprod{p}{L(u)} \leq \inprod{p}{L(u')} ~ (\forall u' \in \reals^d) \}$.

For example, 0-1 loss is a discrete loss with $\R = \Y = \{-1,1\}$
given by $\ellzo(r)_y = \Ind{r \neq y}$.%, with Bayes risk $\risk{\ellzo}(p) = 1-\max_{y\in\Y} p_y$.
Two important surrogates for $\ellzo$ are hinge loss $\hinge(u)_y = (1-yu)_+$, where $(x)_+ = \max(x,0)$, and logistic loss $L(u)_y = \log(1+\exp(-yu))$ for $u\in\reals$.

Most of the surrogates $L$ we consider will be \emph{polyhedral}, meaning piecewise linear and convex; we therefore briefly recall the relevant definitions.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\citep{rockafellar1997convex}.
%
\begin{definition}[Polyhedral loss]
	A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral (piecewise linear convex) function of $u$ for each $y\in\Y$.
\end{definition}
%
For example, hinge loss is polyhedral, whereas logistic loss is not.

%We specifically study polyhedral functions in part because they have high-dimensional subgradient sets at points of nondifferentiability, which will be of interest to us later.
Polyhedral losses are motivated partly because they correspond to a natural surrogate construction technique: embedding the discrete predictions $\R$ as points in $\reals^d$.
The key condition is that $r \in \R$ should minimize expected $\ell$-loss if and only if its embedding point minimizes expected surrogate loss.
%
\begin{definition}[Embedding of a loss]
  A loss $L: \reals^d \to \nonnegreals^{\Y}$ \emph{embeds} a loss $\ell: \R \to \nonnegreals^{\Y}$ \emph{in dimension $d$} with \emph{embedding function $\phi: \R \to \reals^d$} if: (i) $\phi$ is injective; (ii) for all $r \in \R$, $\ell(r) = L(\phi(r))$; and (iii) for all $r \in \R$, $\gamma_r = \Gamma_{\phi(r)}$.
  We simply say $L$ \emph{embeds} $\ell$ (in dimension $d$) if some such embedding function exists.
\end{definition}
%
Note that embedding does not immediately imply another important property, calibration.
\begin{definition}[Calibrated surrogate]
  A surrogate loss $L: \reals^d \to \nonnegreals^{\Y}$ and \emph{link function} $\psi: \reals^d \to \R$ are \emph{calibrated} for a loss $\ell: \R \to \nonnegreals^{\Y}$ if for all $p \in \simplex$,
    \[ u \in \argmin_{u' \in \reals^d} \inprod{p}{L(u')} \implies \psi(u) \in \argmin_{r' \in \R} \inprod{p}{\ell(r)} . \]
  We simply say $L$ can be \emph{calibrated} to $\ell$ if such a link function exists.
\end{definition}
Polyhedral surrogates are motivated by the following results.
\begin{theorem}[\cite{finocchiaro2019embedding}] \label{thm:embed-iff-poly}
  If a discrete loss $\ell$ is embedded by some surrogate $L$, then (1) $L$ must be a polyhedral loss; and (2) there exists a link function calibrating $L$ to $\ell$.
\end{theorem}
It therefore suffices for us to investigate when an embedding of $\ell$ exists.
We will get calibration for free.
The next question is what desirable properties would be of a surrogate $L: \reals^d \to \nonnegreals^{\Y}$ (which we know must be polyhedral).
The most immediate is the dimensionality $d$ required.
This is the question investigated in this paper.

\subsection{Embedding dimension}

In this paper, we study the following quantity.
\begin{definition}[Embedding dimension]
  The \emph{embedding dimension} of a discrete loss $\ell: \R \to \nonnegreals^{\Y}$ is the smallest $d$ such that $\ell$ can be embedded into $d$ dimensions, i.e. where there exists a calibrated polyhedral surrogate $L: \reals^d \to \nonnegreals^{\Y}$.
  In this case, we say $\ell$ is \emph{$d$-embeddable}.
\end{definition}

A number of things are already known about embedding dimension.
Many surrogates in the literature provide upper bounds; we highlight in particular the \emph{abstain loss} given by \cite{ramaswamy2018consistent}, in which one wants to predict the most likely outcome \emph{only if} they are confident in the outcome, and otherwise \emph{abstain} from predicting an outcome for a lesser punishment than if they give a prediction and were incorrect. 
A known convex-conjugate construction generically embeds any discrete loss on $\Y = [n]$ into $n-1$ dimensions, giving a flat upper bound of $n-1$ on embedding dimension.

\begin{equation}\label{eq:abstain}
\ellabs{\alpha}(r,y) = \begin{cases}
0 & r = y\\
\alpha & r = \bot\\
1 & r \not \in \{y, \bot\}
\end{cases}
\end{equation}


Lower bounds exist but are rare.
In particular, a lower bound on the dimensionality of \emph{any} calibrated convex surrogate $L$ implies in particular a lower bound on polyhedral surrogates. \jessie{Is it this or vice versa?}
\cite{ramaswamy2016convex} give such a lower bound via the technique of \emph{feasible subspace dimension}, which is able to e.g. prove that embedding $0-1$ loss on $n$ labels requires dimension $n-1$.
However, their bounds do not work well for the $\abstain{1/2}$ property because of its geometric structure.
Their construction of lower bounds simply give $1 \leq \abstain{1/2}$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{One-dimensional embeddings}
\label{sec:1d}

In this section, we completely characterize when a discrete loss can be embedded into the real line.
Perhaps, surprisingly, we also show that $1$-embeddability is equivalent to the existence of \emph{any} convex calibrated $1$-dimensional surrogate.
We do not expect this fact to extend to higher dimensions, where separating the power of polyhedral from more general convex surrogates is an interesting open problem.

We leverage recent results in \emph{property elicitation}, which formalize the property (or statistic) of a conditional distribution $p$ that is obtained by minimizing loss.
\begin{definition}[Property, finite, level set, elicited]
  A \emph{property} is a function $g: \simplex \to 2^{R}$ where $g(p) \neq \emptyset$ for all $p$.
  It is \emph{finite} if $|\R| < \infty$.
  The \emph{level set} of $r$ is the set $g_r = \{p \in \simplex : r \in g(p)\}$.
  A loss function $f: R \to \nonnegreals^{\Y}$ \emph{elicits} the property if $g(p) = \argmin_{r \in R} \inprod{p}{f(r)}$.
  We assume all properties are non-redundant, i.e. each $g_r$ contains a $p$ not in any other $g_{r'}$.
\end{definition}
In particular, in this paper we write $\gamma$ for the property elicited by $\ell$ and $\Gamma$ for the property elicited by $L$.
For example, $0-1$ loss elicits the mode, which is formalized as a property on report space $\R = \Y$ via $g(p) = \argmax_{y \in \Y} p(y)$.

\bo{This definition actually makes sense if $g$ is not finite; do we want to only define it for finite properties?}
\begin{definition}[Intersection graph, orderable]
  Given a finite property $g: \simplex \to 2^{\R}$, the \emph{intersection graph} has vertices labeled by $\R$ with an edge $(r,r')$ if $g_r \cap g_{r'} \neq \emptyset$.
  A finite property $g$ is \emph{orderable} if its intersection graph is a path, i.e. a connected graph where every vertex has either one or two neighbors.
\end{definition}

%\begin{observation}
%  A property $g$ is orderable if and only if the set $g_r \cap g_{r'}$, for $r \neq r'$, is always the intersection of a hyperplane with the simplex.
%\end{observation}

The main result of this section is Theorem \ref{thm:orderable-1-embed}.
\begin{theorem} \label{thm:orderable-1-embed}
  Given a discrete loss $\ell: \R \to \nonnegreals^{\Y}$, the following are equivalent:
  \begin{enumerate}
    \item The property elicited by $\ell$ is orderable.
    \item $\ell$ is $1$-embeddable.
    \item $\ell$ has some polyhedral calibrated surrogate loss $L: \reals \to \nonnegreals^{\Y}$.
    \item $\ell$ has some convex calibrated surrogate loss $L: \reals \to \nonnegreals^{\Y}$.
  \end{enumerate}
\end{theorem}
This theorem implies that one can directly test if a property is $1$-embeddable via the orderable criterion.
Orderability can be tested using the Bayes risk of $\ell$, a polyhedral function defined by $p \mapsto \max_{r \in \R} \inprod{p}{\ell(r)}$.
A direct approach is to project this function down to the simplex to give the level sets $\gamma_r$, as in Figure~\ref{fig:intersection-graph}.
Another approach is to note that the intersection graph is directly given by finding those pairs $r,r'$ that simultaneously maximize the Bayes risk, i.e. cases where for some $p$, we have $r,r' \in \argmax_{r'' \in \R} \inprod{p}{\ell(r'')}$.

\begin{figure}\label{fig:intersection-graph}
	\centering
	\includegraphics[width = 0.6\linewidth]{tikz/intersection-graph.pdf}
	\caption{Intersection graph for the [truncated] expected value property on outcomes $\Y = \{1,2,3\}$.}
\end{figure}


\subsection{Proof approach: monotonicity}
\bo{Depending on space, we can give full definitions and proof sketches here, or less.}

The key idea of the proof is to define \emph{monotonicity} of a general property $g: \simplex \to 2^{R}$.
This generalizes orderability to the case where the report space $R$ is not necessarily finite
We show a finite property is orderable if and only if it is monotone.
From the definition of monotonicity, we are able to directly construct a polyhedral surrogate $L$, proving (1) $\implies$ (2) in Theorem \ref{thm:orderable-1-embed}.
The equivalence of embedding and polyhedral surrogates of \cite{finocchiaro2019embedding} (Theorem \ref{thm:embed-iff-poly}) gives (2) $\implies$ (3), and the fact that polyhedral functions are convex gives (3) $\implies$ (4).
Then, we show that monotone properties with report space $\reals$ are precisely those elicited by convex loss functions $L$.
This, along with a result that calibrated link functionss must also be in a sense ``ordered'', proves (4) $\implies$ (1).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Higher dimensions}
\bo{blurb about the purpose of this section and the layout.}
\jessie{Taking a crack at it:}
Our newfound understanding of the 1-dimensional embedding characterization leads us to realize that a large class of properties are not $1$-embeddable.
\cite{finocchiaro2019embedding} have shown a trivial upper bound that every property is $(n-1)$-embeddable, but it is an open question as to how tight we can get the bounds on embedding dimension.
Reducing embedding dimension significantly below $(n-1)$ can help provide a computational speedup in the optimization problem, which is a function of the surrogate loss dimension.
This is especially important as the number of possible outcomes $n$ grows large. 

Below, we generalize our characterization to $d$-embeddability for $d \geq 2$.
Having a finite number of embedding points and restricting to polyhedral functions allows us to study the subgradient sets of the loss at embedding points, which we formalize in Section~\ref{subsec:sub-sets}.
Using a general theory of polytopes, we can then present our general $d$-dimensional characterization in terms of optimality and monotonicity conditions.
In Section~\ref{subsec:opt-conditions}, we present a quadratic feasibility program in order to understand necessary conditions for $d$-embeddability by satisfying optimality conditions, yielding new lower bounds for embedding dimension in Corollary~\ref{cor:d-embeddable-char}.

\subsection{Setup: subgradient sets at embedding points.}\label{subsec:sub-sets}
%Recall that we have a loss function $\ell: \R \to \reals^{\Y}_{\geq 0}$, and for any prediction $r \in \R$ of the algorithm, we write $\gamma_r = \{ p : \inprod{p}{\ell(r)} \leq \inprod{p}{\ell(r')} (\forall r')\}$.
%We are interested in potential polyhedral surrogates $L: \reals^d \to \reals^{\Y}_{\geq 0}$.
%These are associated with embedding points, where $u_r \in \reals^d$ is linked back to the discrete prediction $r$.

Recall that if $\ell: \R \to \nonnegreals^{\Y}$ is embedded by $L: \reals^d \to \nonnegreals^{\Y}$, then each $r \in \R$ is embedded at some point $u_r \in \reals^d$.
The key to our approach is to study as first-class objects the sets of all subgradients\footnote{Recall that a subgradient of e.g. the convex function $L(\cdot)_y: \reals^d \to \reals$ at a point $u$ is a vector $v \in \reals^d$ such that $L(u') \geq L(u) + \inprod{v}{u'-u}$ for all $u'$. The set of subgradients at $u$ is sometimes called the subdifferential.} of $L$ at these embedding points.
To settle the question of whether a consistent surrogate exists in $d$ dimensions, it surprisingly turns out to be necessary and sufficient to consider conditions on these sets alone.
In particular, we use heavily that a convex function is minimized at a point if and only if $\vec{0}$ is in its subgradient set at that point.

Therefore, we abstract to consider collections of sets $T^r_y$, which aspire to be the subgradient sets of a calibrated polyhedral surrogate $L(\cdot)_y$ at $u_r$.
Note that if $L(\cdot)_y$ is a polyhedral function on $\reals^d$, then all of its subgradient sets are (bounded) closed polytopes.\bo{cite}

\begin{definition}[$\T$, $\T^r$, $D(\T^r)$]
  We use the following notation for a collection of closed polytopes, with implicit parameter $d$: $\T = \{T^r_y \subseteq \reals^d : r \in \R, y \in \Y\}$.
  For each fixed $r$, we write $\T^r := \{T^r_y : y \in \Y\}$.

  Given a distribution $p \in \simplex$, we write the $p$-weighted Minkowski sum of $\T^r$ as
    \[ \oplus_p \T^r := \left\{ \sum_{y \in \Y} p(y) x^r_y ~\Big|~ x^r_1 \in T^r_1, \dots, x^r_y \in T^r_y \right\} , \]
  in other words, the Minkowski sum of the scaled sets $\{p(y) T^r_y : y \in \Y\}$.

  Finally, we associate with $\T^r$ a set of distributions $D(\T^r) = \{ p : \vec 0 \in \oplus_p \T^r\}$.
\end{definition}

The importance of the $p$-weighted Minkowski sum and of $D(\T^r)$ are that they capture the distributions $p$ for which $u_r$ minimizes expected loss, assuming that $\T$ correspond to some polyhedral $L$.
\begin{proposition} \label{prop:minkowski-0-opt}
  If $T^r_y$ is the subgradient set of each $L(u_r)_y$, then $D(\T^r)$ is the set of distributions $p \in \simplex$ for which the embedding point $u_r$ minimizes expected $L$-loss.
\end{proposition}
\begin{proof}
  Any convex function $f$ is minimized at $u$ if and only if $\vec 0$ is a subgradient of $f$ at $u$.
  Let $f(u) = \inprod{p}{L(u)}$.
  Then the subgradient set of $f$ at $u$ is $\oplus_p \T^r$.
  This follows from the basic properties that if $f_1,f_2$ are convex with subgradient sets $T_1,T_2$ at $u$, then $\alpha f_1$ has subgradient set $\alpha T_1$ and $f_1 + f_2$ has subgradient set $T_1 \oplus T_2$, the Minkowski sum.
  So $\vec 0$ is a subgradient of $\inprod{p}{L(u_r)}$ if and only if $\vec 0 \in \oplus_p \T^r$, which by definition occurs if and only if $p \in D(\T^r)$.
\end{proof}
This fact will be vital for characterizing when $\ell$ is correctly embedded by some $L$ whose subgradient sets are $\T$.
We leverage it next.

\subsection{General characterization}

We now give a general characterization of when a discrete loss $\ell$ can be embedded into $d$ dimensions, i.e. when a consistent polyhedral surrogate $L: \reals^d \to \reals^{\Y}_{\geq 0}$ exists.

Two conditions are required: \emph{optimality} and \emph{monotonicity}.
Optimality enforces that the surrogate is minimized precisely when and where it should be.
It says that for each discrete prediction $r$ and set of distributions $\gamma_r$ for which it is optimal, there exists a collection of polytopes $\T^r$ (presumably the subgradient sets of $L(u_r)_y$ for each $y$) realizing Proposition \ref{prop:minkowski-0-opt}.
Monotonicity says that these individual polytopes can actually be glued together to form the subgradients of some convex loss function $L$.

%\begin{definition}[Optimality of subgradients for a level set] \label{def:opt}
%  Given a collection of polytopes $\T^r = \{T_y^r \subseteq \reals^d : y \in \Y\}$ and a polytope $C \subseteq \simplex$, the \emph{optimality condition $\Opt(\T^r, C)$ holds} if $p \in C \iff \vec{0} \in \oplus_p T^r$.
%\end{definition}

\begin{theorem} \label{thm:general-char}
  Let $\ell: \R \to \reals_{\geq 0}^{\Y}$ be a discrete loss with, for each $r \in \R$, $\gamma_r = \{p : r \in \argmin_{r'} p \cdot \ell(r)\}$.
  Then $\ell$ is $d$-embeddable if and only if there exist polytopes $\T = \{T^r_y \subseteq \reals^d : r \in \R, y \in \Y\}$ such that both of the following hold:
  \begin{enumerate}
    \item (Optimality) For all $r \in \R$, we have $D(\T^r) = \gamma_r$.
    \item (Monotonicity) There exists an injective embedding function $\phi : \R \to \reals^d$ and loss functions $\{L_y : \reals^d \to \reals^\Y_{\geq 0}\}_{y \in \Y}$ such that for all $y \in \Y$ and $r \in \R$, we have $T^r_y = \partial L_y(\phi(r))$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  TODO
\end{proof}


\subsection{Characterizing optimality}\label{subsec:opt-conditions}

We now investigate the optimality condition of Theorem \ref{thm:general-char}, for two purposes.
First, we aim to greatly narrow the search space for constructing low-dimensional surrogate loss functions for a given discrete loss.
The tools we construct in this section aid in this task by constraining or constructing feasible subgradient sets $\T^r$ given a level set $\gamma_r$.
Second, we wish to prove impossibilities, i.e. lower bounds on the embedding dimension of a given discrete loss (an apparently hard problem).

By dropping monotonicity from Theorem \ref{thm:general-char}, we obtain $|\R|$ independent optimality conditions, $D(\T^r) = \gamma_r$, that must be satisfied in order to have a $d$-dimensional embedding.
For upper bounds, one must construct a $\T^r$ satisfying $D(\T^r) = \gamma_r$ for all $r$ (and then turn to satisfying monotonicity).
For lower bounds, it suffices to prove nonexistence of any such $\T^r$, for any $r$.

To accomplish these, we construct a quadratic feasibility program (QFP, Definition \ref{def:qfp}) that takes as input a polytope $C \subseteq \simplex$ and a parameter $d \in \mathbb{N}$ and (roughly) attempts to construct $\T^r$.
We will prove the following main results.
\begin{theorem} \label{thm:opt-iff-qfp}
  Given a convex polytope $C \subseteq \simplex$, there exist polytopes $\T^r$ in $\reals^d$ such that $D(\T^r) = C$ if and only if there is a feasible solution to the QFP (Definition \ref{def:qfp}) with parameter $d$.
\end{theorem}

\begin{corollary}\label{cor:d-embeddable-char}
  If a discrete loss $\ell$ has any level set $C = \gamma_r = \{p : r \in \argmin_{r'} \inprod{p}{\ell(r')} \}$ for which the quadratic program is infeasible with parameter $d$, then $\ell$ is not $d$-embeddable.
\end{corollary}


\subsubsection{Feasibility program}

The feasibility program intuitively contains one set of necessary and one set of sufficient conditions.
The necessary conditions are derived from the representation of the polytope $C$ as an intersection of halfspaces.
The sufficient conditions are derived from its dual representation as a convex hull of vertices.

The variables are a set of normal vectors $\{v_i \in \reals^d : i \in [k]\}$ and a set of vertices $\{x^j_y \in \reals^d : j \in [\ell], y \in \Y\}$.
These correspond to a (possibly relaxed) halfspace representation and (possibly incomplete) vertex representation of the subgradient polytopes $\T^r$.

\begin{definition}[Quadratic Feasibility Program] \label{def:qfp} ~ \\
  \indent \textbf{Parameter:} $d \in \mathbb{N}$.

  \textbf{Given:} a polytope $C = \{p \in \simplex : Bp \geq \vec 0\} = \conv(\{p^1, \ldots, p^\ell\}) \subseteq \simplex$, where $B \in \reals^{k \times n}$ has a minimum number of rows.

  \textbf{Variables:} a matrix $V \in \reals^{k \times d}$, each row representing a normal vector; and a collection of matrices $X^1,\ldots,X^{\ell} \in \reals^{d \times n}$, with $X^j$ intuitively corresponding to $p^j$ with columns $x^j_y$ representing witness points.

  \textbf{Constraints:}
    \begin{align}
      V X^j                     &\leq B    & \text{(pointwise, $\forall j \in [\ell]$)}  \label{eqn:qp-constr-1} \\
      \sum_{y=1}^n p^j(y) x^j_y &= \vec 0  & \text{($\forall j \in [\ell]$)}    \label{eqn:qp-constr-2} 
    \end{align}
\end{definition}

The feasibility program can be viewed as a low-rank matrix problem, namely: do there exist a set of rank-$d$ matrices that are pointwise dominated by $B$, sharing the left factor $V$, whose right factors $X^j$ respectively satisfy a subspace constraint?
We will see \bo{in which section?} that for the important example of abstain loss, the constraints simplify into a more pure low-rank matrix problem.
In particular, for $d=n-1$ a solution always exists (e.g. \bo{cite}), found by taking the convex conjugate of the Bayes risk of $\ell$ \bo{and then what?}.


\subsubsection{Developing the proof}

To prove Theorem \ref{thm:opt-iff-qfp}, it is useful to develop a less-constructive intermediate characterization of when $D(\T^r) = C$.
The following condition formalizes a necessary condition on $\T^r$ in terms of the halfspace representation of $C$; the subsequent one formalizes a sufficient condition using the vertex representation.


\begin{condition}[Halfspace condition]\label{cond:H-condition}
	A collection of polytopes $\T^r$ and a polytope $C$ in the simplex defined by $C = \{p \in \simplex : Bp \geq \vec 0\}$ \emph{satisfy the halfspace condition} if there exist $v_1, \ldots, v_k \in \reals^d$ such that, for all $i \in [k]$ and $y \in \Y$, for all $x \in T^r_y$, we have $\inprod{v_i}{x} \leq B_{iy}$.
\end{condition}
\begin{condition}[Vertex condition]\label{cond:V-condition}
	A collection of polytopes $\T^r$ and a polytope $C$ in the simplex defined by $C = \conv(\{p^1, \ldots, p^\ell\})$ \emph{satisfy the vertex condition} if for all $j \in [\ell]$, $0 \in \oplus_{p^j} \T^r$. %% Equivalent: there exist $\{x_{jy} \in T_y : j \in [\ell]\}$ such that, for all $j \in [\ell]$, $\sum_{y \in \Y} p^j_y x_{jy} = \vec 0$.
\end{condition}

Useful lemmas:
\begin{lemma} \label{lemma:D-polytope}
  For any $\T^r$, $D(\T^r)$ is a polytope (in particular, is convex).
\end{lemma}
\begin{proof}
   Recall by definition, the notation $\oplus_p \T^r = \{\sum_y p(y) x^r_y : x^r_y \in T^r_y (\forall y)\}$.
   Each $T^r_y$ is a polytope, so $p(y) T^r_y$ is a polytope.
   The Minkowski sum of polytopes is a polytope, so $\oplus_p \T^r$ is a polytope.
  \bo{TODO}
\end{proof}
\begin{lemma}  \label{lemma:minkowski-support}
  Given polytopes $\T^r$, there exists a finite set of normal vectors $w_1,\ldots,w_K \in \reals^d$ such that, for all $p \in \simplex$, $\oplus_p \T^r = \{x : \inprod{w_i}{x} \leq \sum_{y \in \Y} p(y) \max_{x \in T^r_y} \inprod{w_i}{x} \}$.
\end{lemma}
\begin{proof}
   For each $p$, $\oplus_p \T^r$ is a polytope.

  \bo{TODO: it turns out that $p$ doesn't matter, only its support; and there are only finitely many possible supports (or even better, the normals that are complete for the uniform minkowski sum turn out to be complete for all other $p$-combinations).}
\end{proof}
\begin{lemma} \label{lemma:E-to-B}
  Let $C = \{p : Bp \geq \vec 0 \}$ where $B$ has the minimum possible number of rows to capture $C$, and suppose $C = \{p : Ep \geq \vec 0 \}$.
  Then for each row in $B$ there is some (unique) row in $E$ that is equal to $\alpha B$ for some positive $\alpha$.
\end{lemma}
\begin{proof}
  \bo{? Cite}
\end{proof}


\begin{theorem} \label{thm:vertex-halfspace-opt}
  Let the polytopes $\T^r = \{T^r_y \subseteq \reals^d : y \in \Y\}$ and $C$ be given, with $C = \conv(\{p^1,\ldots,p^{\ell}\}) = \{p: Bp \geq \vec 0\}$ for $B \in \reals^{k \times n}$.
  We have $D(\T^r) = C$ if and only if both the halfspace and vertex conditions hold.
\end{theorem}
\begin{proof}
  ($\implies$)
  Suppose $D(\T^r) = C$.
  First, we note that the vertex condition is immediate: For all $j \in [\ell]$, $p^j \in C$ which gives $p^j \in D(\T^r)$.
  To show the halfspace condition is satisfied, we first construct a matrix $E$ such that $Ep \geq 0 \iff Bp \geq 0$, then use this construction to pick out the necessary vectors $v_1,\dots,v_k$.

  By Lemma \ref{lemma:minkowski-support}, there is a finite collection of vectors $w_1,\dots,w_{K} \in \reals^d$ and such that $\vec 0 \in \oplus_p \T^r$ if and only if, for all $w_i$, $\sum_y p(y) \max_{x \in T^r_y} \inprod{w_i}{x} \geq 0$.
  Hence, each vector $w_i$ generates a row of a matrix $E \in \reals^{K \times n}$ with $E_{iy} = \max_{x \in T^r_y} \inprod{w_i}{x}$, and we have $p \in D(\T^r) \iff Ep \geq 0$.
  By assumption of $D(\T^r) = C$, then, we have $Ep \geq 0 \iff Bp \geq 0$.
  By Lemma \ref{lemma:E-to-B}, because $B$ has the minimum possible number of rows, each row of $B$ appears (scaled by some positive constant) as a different row of $E$. Taking the collection of $w_i$ corresponding to these rows and rescaling them by that positive constant, we get a collection of $k$ vectors that we can rename $v_1,\ldots,v_k \in \reals^d$, with $\max_{x \in T^r_y} \inprod{v_i}{x} = B_{iy}$, hence the halfspace condition is satisfied.

  ($\impliedby$)
  Suppose Conditions \ref{cond:H-condition} and \ref{cond:V-condition} hold.
  Then by the vertex condition, $p^j \in D(\T^r)$ for all $j \in [\ell]$.
  Because $D(\T^r)$ is convex (Lemma \ref{lemma:D-polytope}), this implies $C \subseteq D(\T^r)$.
  To show $D(\T^r) \subseteq C$, let $p \in D(\T^r)$; by definition, $0 \in \oplus_p \T^r$.
  Then in particular for each vector $v_1,\ldots,v_k$ guaranteed by the halfspace condition, we have
  \begin{align*}
    0 &\leq \max_{x \in \oplus_p \T^r} \inprod{v}{x}  \\
      &=    \sum_{y \in \Y} p(y) \max_{x \in T^r_y} \inprod{v_i}{x}  \\
      &\leq \sum_{y \in \Y} p(y) B_{iy} .
  \end{align*}
  This proves $Bp \geq 0$, so $p \in C$.
\end{proof}

Given this characterization, we prove the main result on when a solution to the quadratic program exists.
\bo{Should note that the program's solution does give candidates $\T^r$, although their usefulness is unclear...}
\bo{Actually, I guess one could run this, then use monotonicity to add constraints and find the next subgradient sets, etc.}

\begin{proof}[Proof of Theorem \ref{thm:opt-iff-qfp}]
    By Theorem \ref{thm:vertex-halfspace-opt}, it suffices to show that $\T^r$ satisfying the halfspace and vertex conditions exist if and only if the program is feasible.

  ($\implies$) 
  By the vertex condition, for each $j \in [\ell]$, there exist witnesses $\{x^j_y \in T^r_y : y \in \Y\}$ satisfying the second constraint of the quadratic program (Inequality \ref{eqn:qp-constr-2}).
  By the halfspace condition, there exist normals $v_1, \dots, v_k$ such that, for all $i$, for all $x \in T^r_y$, $\inprod{v_i}{x} \leq B_{iy}$; in particular, this applies to the above witnesses $x^j_y \in T^r_y$.
  Collecting $v_1,\dots,v_k$ as the columns of $V$, this shows that the first constraint (Inequality \ref{eqn:qp-constr-1}) is satisfied.

  \bigskip
  ($\impliedby$)
  We construct $T^r_y = \conv(\{x^1_y, \ldots, x^{\ell}_y\})$.
  The second constraint of the quadratic program immediately implies the vertex condition.
  Taking $v_1,\dots,v_k$ as the columns of $V$, the first constraint implies that for each $x^j_y$, we have $\inprod{v_i}{x^j_y} \leq B_{iy}$ for all $i,j,y$.
  Any point $x \in T^r_y$ is a convex combination of $x^1_y,\ldots,x^{\ell}_y$, so it satisfies $\inprod{v_i}{x} \leq B_{iy}$.
  This implies the halfspace condition.  % Bo: QED
\end{proof}

  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples}



% Acknowledgments---Will not appear in anonymized version
\acks{JF- Need that GRFP acknowledgement}

\bibliography{diss,extra}

\newpage
\appendix

\section{Additional proofs}


\section{Feasible subspace dimension}


\end{document}
