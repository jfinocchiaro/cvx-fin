\documentclass[anon]{colt2020} % Anonymized submission
% \documentclass{colt2020} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Embedding dimension]{Embedding Dimension of Polyhedral Losses}
\usepackage{times}

\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage[margin=1.0in]{geometry}

\usepackage{mathtools, amsmath, amssymb, graphicx, verbatim}
%\usepackage{amsthm}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

%\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{orange!20!white}{BTW: #1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\relint}{\mathrm{relint}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\renewcommand{\P}{\mathcal{P}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}

\newcommand{\Opt}{\mathrm{Opt}}
\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathrm{int}(#1)}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}
\newtheorem{condition}{Condition}
\newtheorem{claim}{Claim}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
 \coltauthor{\Name{Jessie Finocchiaro} \Email{jefi8453@colorado.edu}\\
  \Name{Rafael Frongillo} \Email{raf@colorado.edu}\\
  \Name{Bo Waggoner} \Email{bwag@colorado.edu}\\
  \addr CU Boulder}

% Authors with different addresses:
%\coltauthor{%
% \Name{Jessie Finocchiaro} \Email{jefi8453@colorado.edu}\\
% \addr CU Boulder
% \AND
% \Name{Rafael Frongillo} \Email{Raf@colorado.edu}\\
% \addr CU Boulder
% \AND
% \Name{Bo Waggoner} \Email{bwag@colorado.edu}\\
% \addr CU Boulder%
%}

\begin{document}

\maketitle

\begin{abstract}%
  A common technique in supervised learning with discrete losses, such as $0-1$ loss, is to optimize a convex surrogate loss over $\reals^d$, calibrated with respect to the original loss.
  In particular, recent work has investigated embedding the original predictions (e.g. labels) as points in $\reals^d$.
  %Recent work has proposed the notion of designing calibrated surrogate losses for classification-like problems through the lens of \emph{embedding} the original problem into $\reals^d$ and optimizing a polyhedral loss that is calibrated with respect to the original loss.
  In this work, we study the notion of \emph{embedding dimension} for given discrete losses.
  We characterize when a given discrete loss can be embedded into the real line, as well as when a higher-dimension input to the surrogate loss is required.
  Moreover, we give a quadratic feasibility program that yields lower bounds on the embedding dimension of a given discrete loss.
\end{abstract}

\begin{keywords}%
  Calibrated surrogates, convex surrogates, proper scoring rules%
\end{keywords}

\section{Introduction}
To do...

\section{Related work and background}

\subsection{Discrete losses and embeddings}

\jessie{A lot fo this is taken from the NeurIPS paper}
Let $\Y$ be a finite outcome (label) space, and throughout let $n=|\Y|$.
The set of probability distributions on $\Y$ is denoted $\simplex\subseteq\reals_+^{\Y}$, represented as vectors of probabilities.
We write $p_y$ for the probability of outcome $y \in \Y$ drawn from $p \in \simplex$.

As is assumed by~\cite{finocchiaro2019embedding}, we assume the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.

We denote surrogate losses $L:\reals^d\to\reals^\Y_+$, typically with reports written $u\in\reals^d$.
We write the corresponding expected loss when $Y \sim p$ as $\inprod{p}{\ell(r)}$ and $\inprod{p}{L(u)}$ since we are in the finite outcome setting.

For example, 0-1 loss is a discrete loss with $\R = \Y = \{-1,1\}$
given by $\ellzo(r)_y = \Ind{r \neq y}$, with Bayes risk $\risk{\ellzo}(p) = 1-\max_{y\in\Y} p_y$.
Two important surrogates for $\ellzo$ are hinge loss $\hinge(u)_y = (1-yu)_+$, where $(x)_+ = \max(x,0)$, and logistic loss $L(u)_y = \log(1+\exp(-yu))$ for $u\in\reals$.

Most of the surrogates $L$ we consider will be \emph{polyhedral}, meaning piecewise linear and convex; we therefore briefly recall the relevant definitions.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\citep{rockafellar1997convex}.
%
\begin{definition}[Polyhedral loss]
	A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral (convex) function of $u$ for each $y\in\Y$.
\end{definition}
%
For example, hinge loss is polyhedral, whereas logistic loss is not.

We specifically study polyhedral functions in part because they have high-dimensional subgradient sets at points of nondifferentiability, which will be of interest to us later.

\section{Embedding dimension}

\section{1d-characterization}
\label{sec:1d}


\section{Higher dimensions}

\paragraph{Background and notation.}
Recall that we have a loss function $\ell: \R \to \reals^{\Y}_{\geq 0}$, and for any prediction $r \in \R$ of the algorithm, we write $\gamma_r = \{ p : \inprod{p}{\ell(r)} \leq \inprod{p}{\ell(r')} (\forall r')\}$.
We are interested in potential polyhedral surrogates $L: \reals^d \to \reals^{\Y}_{\geq 0}$.
These are associated with embedding points, where $u_r \in \reals^d$ is linked back to the discrete prediction $r$.

To settle the question of whether a consistent surrogate exists in $d$ dimensions, it turns out to be necessary and sufficient to consider the possible sets of subgradients of $L$ at the embedding points.
This is useful in particular because a convex function is minimized at a point if and only if $\vec{0}$ is in its subgradient set at that point.

We will therefore abstract the relevant subgradient sets -- namely, the subgradients of $L$ at the embedding points $u_r$, given observed outcome $y$ -- as closed polytopes $T^r_y$. (The subgradient sets are bounded polytopes, because $L$ is polyhedral and defined on all of $\reals^d$.)

\begin{definition}[$\T$, $\T^r$, $D(\T^r)$]
  We use the following notation for a collection of polytopes, with implicit parameter $d$: $\T = \{T^r_y \subseteq \reals^d : r \in \R, y \in \Y\}$.
  For each fixed $r$, we write $\T^r := \{T^r_y : y \in \Y\}$.

  Given a distribution $p \in \simplex$, we write the $p$-weighted Minkowski sum of $\T^r$ as
    \[ \oplus_p \T^r := \left\{ \sum_{y \in \Y} p(y) x^r_y ~\Big|~ x^r_1 \in T^r_1, \dots, x^r_y \in T^r_y \right\} , \]
  in other words, the Minkowski sum of the scaled sets $\{p(y) T^r_y : y \in \Y\}$.

  Finally, we associate with $\T^r$ a set of distributions $D(\T^r) = \{ p : \vec 0 \in \oplus_p \T^r\}$.
\end{definition}

\begin{observation} \label{obs:subdiff-minkowski}
  Let $T^r_y$ be the subgradient set of $L(u_r)_y$.
  Then the subgradient set of the expected loss $\inprod{p}{L(u_r)}$ is $\oplus_p \T^r$.
\end{observation}
\begin{proof}
  \bo{TODO}
\end{proof}
\begin{corollary} \label{cor:minkowski-0-opt}
  If $T^r_y$ is the subgradient set of each $L(u_r)_y$, then $D(\T^r)$ is the set of distributions $p \in \simplex$ for which the embedding point $u_r$ minimizes expected $L$-loss.
\end{corollary}
\begin{proof}
  Any convex function $f$ is minimized at $u$ if and only if $\vec 0$ is a subgradient of $f$ at $u$.
  By Observation \ref{obs:subdiff-minkowski}, $\vec 0$ is a subgradient of $\inprod{p}{L(u_r)}$ if and only if $\vec 0 \in \oplus_p \T^r$, which by definition occurs if and only if $p \in D(\T^r)$.
\end{proof}
This corollary is vital because a discrete loss $\ell$ has a corresponding set of distributions $\gamma_r$ for which $r$ minimizes expected loss; and for calibration, it is necessary that these sets be the same: $\gamma_r = D(\T^r)$.
We expand on this next.

\subsection{General characterization}

We now give a general characterization of when a discrete loss $\ell$ can be embedded into $d$ dimensions, i.e. when a consistent polyhedral surrogate $L: \reals^d \to \reals^{\Y}_{\geq 0}$ exists.

Two conditions are required: \emph{optimality} and \emph{monotonicity}.
Optimality enforces that the surrogate is minimized precisely when and where it should be.
It says that for each discrete prediction $r$ and set of distributions $\gamma_r$ for which it is optimal, there exists a collection of polytopes $\T^r$ (presumably the subgradient sets of $L(u_r)_y$ for each $y$) realizing Corollary \ref{cor:minkowski-0-opt}.
Monotonicity says that these individual polytopes can actually be glued together to form the subgradients of some convex loss function $L$.

%\begin{definition}[Optimality of subgradients for a level set] \label{def:opt}
%  Given a collection of polytopes $\T^r = \{T_y^r \subseteq \reals^d : y \in \Y\}$ and a polytope $C \subseteq \simplex$, the \emph{optimality condition $\Opt(\T^r, C)$ holds} if $p \in C \iff \vec{0} \in \oplus_p T^r$.
%\end{definition}

\begin{theorem} \label{thm:general-char}
  Let $\ell: \R \to \reals_{\geq 0}^{\Y}$ be a discrete loss with, for each $r \in \R$, $\gamma_r = \{p : r \in \argmin_{r'} p \cdot \ell(r)\}$.
  Then $\ell$ is $d$-embeddable if and only if there exist polytopes $\T = \{T^r_y \subseteq \reals^d : r \in \R, y \in \Y\}$ such that both of the following hold:
  \begin{enumerate}
    \item (Optimality) For all $r \in \R$, we have $D(\T^r) = \gamma_r$.
    \item (Monotonicity) (TODO)
  \end{enumerate}
\end{theorem}
\begin{proof}
  TODO
\end{proof}


\subsection{Characterizing optimality}

We now investigate the optimality condition of Theorem \ref{thm:general-char}, for two purposes.
First, we aim to greatly narrow the search space for constructing low-dimensional surrogate loss functions for a given discrete loss.
The tools we construct in this section may aid in guiding or automating this task by constraining or constructing feasible subgradient sets $\T^r$ given a level set $\gamma_r$.
Second, we wish to prove impossibilities, i.e. lower bounds on the embedding dimension of a given discrete loss (an apparently hard problem).

By dropping monotonicity from Theorem \ref{thm:general-char}, we obtain $|\R|$ independent optimality conditions, $D(\T^r) = \gamma_r$, that must be satisfied in order to have a $d$-dimensional embedding.
For upper bounds, one must construct a $\T^r$ satisfying $D(\T^r) = \gamma_r$ for all $r$ (and then turn to satisfying monotonicity).
For lower bounds, it suffices to prove nonexistence of any such $\T^r$, for any $r$.

We next focus on constructing a feasibility program (QFP, Definition \ref{def:qfp}) that takes as input a polytope $C \subseteq \simplex$ and a parameter $d \in \mathbb{N}$ and (roughly) attempts to construct $\T^r$.
We prove the following main results.
\begin{theorem} \label{thm:opt-iff-qfp}
  Given a convex polytope $C \subseteq \simplex$, there exist polytopes $\T^r$ in $\reals^d$ such that $D(\T^r) = C$ if and only if there is a feasible solution to the QFP (Definition \ref{def:qfp}) with parameter $d$.
\end{theorem}

\begin{corollary}
  If a discrete loss $\ell$ has any level set $C = \gamma_r = \{p : r \in \argmin_{r'} \inprod{p}{\ell(r')} \}$ for which the quadratic program is infeasible with parameter $d$, then $\ell$ is not $d$-embeddable.
\end{corollary}


\subsubsection{Feasibility program}

The feasibility program intuitively contains one set of necessary and one set of sufficient conditions.
The necessary conditions are derived from the representation of the polytope $C$ as an intersection of halfspaces.
The sufficient conditions are derived from its dual representation as a convex hull of vertices.

The variables are a set of normal vectors $\{v_i \in \reals^d : i \in [k]\}$ and a set of vertices $\{X^j_y \in \reals^d : j \in [\ell], y \in \Y\}$.
These correspond to a (possibly relaxed) halfspace representation and (possibly incomplete) vertex representation of the subgradient polytopes $\T^r$.

\begin{definition}[Quadratic Feasibility Program] \label{def:qfp} ~ \\
  \indent \textbf{Parameter:} $d \in \mathbb{N}$.

  \textbf{Given:} a polytope $C = \{p \in \simplex : Bp \geq \vec 0\} = \conv(\{p^1, \ldots, p^\ell\}) \subseteq \simplex$, where $B \in \reals^{k \times n}$ has a minimum number of rows.

  \textbf{Variables:} a matrix $V \in \reals^{k \times d}$, each row representing a normal vector; and a collection of matrices $X^1,\ldots,X^{\ell} \in \reals^{d \times n}$, with $X^j$ intuitively corresponding to $p^j$ with columns $X^j_y$ representing witness points.

  \textbf{Constraints:}
    \begin{align}
      V X^j                     &\leq B    & \text{(pointwise, $\forall j \in [\ell]$)}  \label{eqn:qp-constr-1} \\
      \sum_{y=1}^n p^j(y) X^j_y &= \vec 0  & \text{($\forall j \in [\ell]$)}    \label{eqn:qp-constr-2} 
    \end{align}
\end{definition}

The feasibility program can be viewed as a low-rank matrix problem, namely: do there exist a set of rank-$d$ matrices that are pointwise dominated by $B$, sharing the left factor $V$, whose right factors $X^j$ respectively satisfy a subspace constraint?
We will see \bo{in which section?} that for the important example of abstain loss, the constraints simplify into a more pure low-rank matrix problem.
In particular, for $d=n-1$ a solution always exists (e.g. \bo{cite}), found by taking the convex conjugate of the Bayes risk of $\ell$ \bo{and then what?}.


\subsubsection{Developing the proof}

To prove Theorem \ref{thm:opt-iff-qfp}, it is useful to develop a less-constructive intermediate characterization.
The following proposition formalizes the halfspace representation of $C$ as leading to a necessary condition on $D(\T^r)$; the subsequent one formalizes the vertex representation as a sufficient condition.

\begin{proposition} \label{prop:halfspace-opt}
  Let the polytopes $\T^r = \{T^r_y \subseteq \reals^d : y \in \Y\}$ and $C$ be given, with $C = \{p: Bp \geq \vec 0\}$ for $B \in \reals^{k \times n}$.
  Then $D(\T^r) \subseteq C$ if and only if there exist $v_1,\dots,v_k \in \reals^d$ with $\inprod{v_i}{x} \leq B_{iy}$ for all $i \in [k], y \in \Y, x \in T^r_y$.
\end{proposition}
\begin{proof}
  ($\implies$)
  We are supposing that if $Bp \geq \vec{0}$, then $\vec{0} \in \oplus_p \T^r$.
  In particular, for all vectors $v \in \reals^d$, $0 \leq \sum_{y \in \Y} p(y) \max_{x \in T^r_y} \inprod{v}{x}$.
  \bo{Not sure how to argue from here, unless we get into E matrix stuff. Raf?}

  ($\impliedby$)
  Let $p \in D(\T^r)$; we show $p \in C$.
  By definition, we have $\vec{0} \in \oplus_p \T^r$.
  This implies in particular that for each of the vectors $v_i$,
  \begin{align*}
    0 &\leq \max_{x \in \oplus_p \T^r} \inprod{v}{x}  \\
      &=    \sum_{y \in \Y} p(y) \max_{x \in T^r_y} \inprod{v_i}{x}  \\
      &\leq \sum_{y \in \Y} p(y) B_{iy} .
  \end{align*}
  This proves $Bp \geq 0$, so $p \in C$.
\end{proof}

\begin{proposition} \label{prop:vertex-opt}
  Let the polytopes $\T^r = \{T^r_y \subseteq \reals^d : y \in \Y\}$ and $C$ be given, with $C = \conv(\{p^1,\ldots,p^{\ell}\})$.
  Then $D(\T^r) \supseteq C$ if and only if $\vec 0 \in \oplus_{p^j} \T^r$ for all $j \in [\ell]$.
\end{proposition}
\begin{proof}
  ($\implies$)
  Because each $p^j \in C$, we have $p^j \in D(\T^r)$, and by definition, this gives $\vec{0} \in \oplus_{p^j} \T^r$.
  
  ($\impliedby$)
  Let $p \in C$; we show $p \in D(\T^r)$.
  For each $j$, there exist $\{X^j_y \in T^r_y : y \in \Y\}$ with $\vec{0} = \sum_{y \in \Y} p^j(y) X^j_y$.
  Write $p$ as a convex combination of $p^1,\ldots,p^{\ell}$.
  Let $x_y$ be the corresponding convex combination of $X^1_y,\ldots,X^{\ell}_y$.
  Because $X^1_y,\ldots,X^{\ell}_y \in T^r_y$, a convex set, so is $x_y$.
  And $\vec{0} = \sum_{y \in \Y} p(y) x_y$, so $p \in D(\T^r)$.
\end{proof}

\begin{corollary} \label{cor:vertex-halfspace-opt}
  Let the polytopes $\T^r = \{T^r_y \subseteq \reals^d : y \in \Y\}$ and $C$ be given, with $C = \conv(\{p^1,\ldots,p^{\ell}\}) = \{p: Bp \geq \vec 0\}$ for $B \in \reals^{k \times n}$.
  We have $D(\T^r) = C$ if and only if both of the following hold:
  \begin{enumerate}
    \item (Halfspace condition) There exist $v_1,\ldots,v_k \in \reals^d$ with $\inprod{v_i}{x} \leq B_{iy}$ for all $i \in [k], y \in \Y, x \in T^r_y$.
    \item (Vertex condition) For all $j \in [\ell]$, $\vec{0} \in \oplus_{p^j} \T^r$.
  \end{enumerate}
\end{corollary}

Given this characterization, we prove the main result on when a solution to the quadratic program exists.
\bo{Should note that the program's solution does give candidates $\T^r$, although their usefulness is unclear...}
\bo{Actually, I guess one could run this, then use monotonicity to add constraints and find the next subgradient sets, etc.}

\begin{proof}[Proof of Theorem \ref{thm:opt-iff-qfp}]
    By Corollary \ref{cor:vertex-halfspace-opt}, it suffices to show that $\T^r$ satisfying the halfspace and vertex conditions exist if and only if the program is feasible.

  ($\implies$) 
  By the vertex condition, for each $j \in [\ell]$, there exist witnesses $\{X^j_y \in T^r_y : y \in \Y\}$ satisfying the second constraint of the quadratic program (Inequality \ref{eqn:qp-constr-2}).
  By the halfspace condition, there exist normals $v_1, \dots, v_k$ such that, for all $i$, for all $x \in T^r_y$, $\inprod{v_i}{x} \leq B_{iy}$; in particular, this applies to the above witnesses $X^j_y \in T^r_y$.
  Collecting $v_1,\dots,v_k$ as the columns of $V$, this shows that the first constraint (Inequality \ref{eqn:qp-constr-1}) is satisfied.

  \bigskip
  ($\impliedby$)
  We construct $T^r_y = \conv(\{X^1_y, \ldots, X^{\ell}_y\})$.
  The second constraint of the quadratic program immediately implies the vertex condition.
  Taking $v_1,\dots,v_k$ as the columns of $V$, the first constraint implies that for each $X^j_y$, we have $\inprod{v_i}{X^j_y} \leq B_{iy}$ for all $i,j,y$.
  Any point $x \in T^r_y$ is a convex combination of $X^1_y,\ldots,X^{\ell}_y$, so it satisfies $\inprod{v_i}{x} \leq B_{iy}$.
  This implies the halfspace condition.  % Bo: QED
\end{proof}

  



% Acknowledgments---Will not appear in anonymized version
\acks{JF- Need that GRFP acknowledgement}

\bibliography{diss,extra}

\newpage
\appendix

\section{Additional proofs}

\end{document}
