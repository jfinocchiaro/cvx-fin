\documentclass[anon]{colt2020} % Anonymized submission
% \documentclass{colt2020} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Embedding dimension]{Embedding Dimension of Polyhedral Losses}
\usepackage{times}

\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage[margin=1.0in]{geometry}

\usepackage{mathtools, amsmath, amssymb, graphicx, verbatim}
%\usepackage{amsthm}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
	linkcolor=darkblue,urlcolor=darkblue,
	anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

%\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
	\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{orange!20!white}{BTW: #1}}
\ifnum\Comments=1               % fix margins for todonotes
\setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\relint}{\mathrm{relint}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\renewcommand{\P}{\mathcal{P}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}

\newcommand{\Opt}{\mathrm{Opt}}
\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathrm{int}(#1)}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}
\newtheorem{condition}{Condition}
\newtheorem{claim}{Claim}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
 \coltauthor{\Name{Jessie Finocchiaro} \Email{jefi8453@colorado.edu}\\
  \Name{Rafael Frongillo} \Email{raf@colorado.edu}\\
  \Name{Bo Waggoner} \Email{bwag@colorado.edu}\\
  \addr CU Boulder}

% Authors with different addresses:
%\coltauthor{%
% \Name{Jessie Finocchiaro} \Email{jefi8453@colorado.edu}\\
% \addr CU Boulder
% \AND
% \Name{Rafael Frongillo} \Email{Raf@colorado.edu}\\
% \addr CU Boulder
% \AND
% \Name{Bo Waggoner} \Email{bwag@colorado.edu}\\
% \addr CU Boulder%
%}

\begin{document}

\maketitle

\begin{abstract}%
  A common technique in supervised learning with discrete losses, such as $0-1$ loss, is to optimize a convex surrogate loss over $\reals^d$, calibrated with respect to the original loss.
  In particular, recent work has investigated embedding the original predictions (e.g. labels) as points in $\reals^d$.
  %Recent work has proposed the notion of designing calibrated surrogate losses for classification-like problems through the lens of \emph{embedding} the original problem into $\reals^d$ and optimizing a polyhedral loss that is calibrated with respect to the original loss.
  In this work, we study the notion of \emph{embedding dimension} for given discrete losses.
  We characterize when a given discrete loss can be embedded into the real line, as well as when a higher-dimension input to the surrogate loss is required.
  Moreover, we give a quadratic feasibility program that yields lower bounds on the embedding dimension of a given discrete loss.
\end{abstract}

\begin{keywords}%
  Calibrated surrogates, convex surrogates, proper scoring rules%
\end{keywords}

\section{Introduction}
To do...

\section{Related work and background}

\subsection{Discrete losses and embeddings}

\jessie{A lot fo this is taken from the NeurIPS paper}
Let $\Y$ be a finite outcome (label) space, and throughout let $n=|\Y|$.
The set of probability distributions on $\Y$ is denoted $\simplex\subseteq\reals_+^{\Y}$, represented as vectors of probabilities.
We write $p_y$ for the probability of outcome $y \in \Y$ drawn from $p \in \simplex$.

As is assumed by~\cite{finocchiaro2019embedding}, we assume the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.

We denote surrogate losses $L:\reals^d\to\reals^\Y_+$, typically with reports written $u\in\reals^d$.
We write the corresponding expected loss when $Y \sim p$ as $\inprod{p}{\ell(r)}$ and $\inprod{p}{L(u)}$ since we are in the finite outcome setting.

For example, 0-1 loss is a discrete loss with $\R = \Y = \{-1,1\}$
given by $\ellzo(r)_y = \Ind{r \neq y}$, with Bayes risk $\risk{\ellzo}(p) = 1-\max_{y\in\Y} p_y$.
Two important surrogates for $\ellzo$ are hinge loss $\hinge(u)_y = (1-yu)_+$, where $(x)_+ = \max(x,0)$, and logistic loss $L(u)_y = \log(1+\exp(-yu))$ for $u\in\reals$.

Most of the surrogates $L$ we consider will be \emph{polyhedral}, meaning piecewise linear and convex; we therefore briefly recall the relevant definitions.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\citep{rockafellar1997convex}.
%
\begin{definition}[Polyhedral loss]
	A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral (convex) function of $u$ for each $y\in\Y$.
\end{definition}
%
For example, hinge loss is polyhedral, whereas logistic loss is not.

We specifically study polyhedral functions in part because they have high-dimensional subgradient sets at points of nondifferentiability, which will be of interest to us later.

\section{Embedding dimension}

\section{1d-characterization}
\label{sec:1d}


\section{Higher dimensions}

\paragraph{Background and notation.}
Recall that we have a loss function $\ell: \R \to \reals^{\Y}_{\geq 0}$, and for any prediction $r \in \R$ of the algorithm, we write $\gamma_r = \{ p : \inprod{p}{\ell(r)} \leq \inprod{p}{\ell(r')} (\forall r')\}$.

Recall that we are interested in potential polyhedral surrogates $L: \reals^d \to \reals^{\Y}_{\geq 0}$, along with embedding points $\{u_r \in \reals^d : r \in \R\}$ that intuitively capture the optimal prediction when the optimal discrete prediction is $r$.

To settle the question of whether a consistent surrogate exists in $d$ dimensions, it turns out to be necessary and sufficient to consider the possible sets of subgradients of $L$ at the embedding points.
\bo{This is useful for example because a convex function is minimized at a point if and only if $\vec{0}$ is in its subgradient set at that point.}

We will therefore abstract the relevant subgradient sets -- namely, the subgradients of $L$ with respect to prediction of the embedding points $u_r$, given observed outcome $y$ -- as polytopes $T^r_y$. (The subgradient sets are polytopes, possibly unbounded \bo{right? discuss}, because $L$ is polyhedral.)

\begin{definition}[$\T$, $\T^r$]
  We use the following notation for a collection of polytopes: $\T = \{T^r_y \subseteq \reals^d : r \in \R, y \in \Y\}$.
  For each fixed $r$, we write $\T^r := \{T^r_y : y \in \Y\}$.
  Given a vector $p \in \reals^n$ (for us, always a probability distribution), we write the $p$-weighted Minkowski sum of $\T^r$ as
    \[ \oplus_p \T^r := \left\{ \sum_{y \in \Y} p(y) x^r_y ~\Big|~ x^r_1 \in T^r_1, \dots, x^r_y \in T^r_y \right\} , \]
  in other words, the Minkowski sum of the sets $\{T^r_y : y \in \Y\}$ after each has been scaled by a factor $p(y)$.
\end{definition}

\begin{observation}
  Let $T^r_y$ be the subgradient set of $L(u_r)_y$.
  Then the subgradient set of the expected loss $\inprod{p}{L(u_r)}$ is $\oplus_p \T^r$.
\end{observation}
\begin{proof}
  \bo{TODO}
\end{proof}



\subsection{Generic characterization}

We now give a general characterization of when a discrete loss $\ell$ can be embedded into $d$ dimensions, i.e. when a consistent polyhedral surrogate $L: \reals^d \to \reals^{\Y}_{\geq 0}$ exists.

Two conditions are required; we term them \emph{optimality} and \emph{monotonicity}.
Optimality enforces that the surrogate is minimized precisely when and where it should be: It says that there exist subgradient sets (presumably of $L$) satisfying certain conditions, namely that $\vec 0$ is in their $p$-weighted Minkowski sum exactly when $p$ minimizes the original, discrete loss.
Monotonicity says that these subgradient sets can actually be glued together to form some convex loss function $L$.

\begin{definition}[Optimality of subgradients for a level set] \label{def:opt}
  Given a collection of polytopes $\T^r = \{T_y^r \subseteq \reals^d : y \in \Y\}$ and a polytope $C \subseteq \simplex$, the \emph{optimality condition $\Opt(\T^r, C)$ holds} if $p \in C \iff \vec{0} \in \oplus_p T^r$.
\end{definition}

\begin{theorem}
  Let $\ell: \R \to \reals_{\geq 0}^{\Y}$ be a discrete loss with, for each $r \in \R$, $\gamma_r = \{p : r \in \argmin_{r'} p \cdot \ell(r)\}$.
  Then $\ell$ is $d$-embeddable if and only if there exist polytopes $\T = \{T^r_y \subseteq \reals^d : r \in \R, y \in \Y\}$ such that both of the following hold:
  \begin{enumerate}
    \item (Optimality) For all $r \in \R$, we have $\Opt(\T^r, \gamma_r)$.
    \item (Monotonicity) (TODO)
  \end{enumerate}
\end{theorem}
\begin{proof}
  TODO
\end{proof}


\subsection{Characterizing optimality}

We now embark on an investigation of the optimality condition, for two purposes.
First, we aim to greatly narrow the search space for low-dimensional surrogate loss functions.
The tools we construct in this section may aid in automating this task by constraining the search significantly.
Second, we wish to prove impossibilities, i.e. lower bounds on the embedding dimension of a given discrete loss (an apparently hard problem).

By dropping monotonicity, we obtain $|\R|$ independent conditions, $\Opt(\T^r,\gamma_r)$, that must be satisfied in order to have a $d$-dimensional embedding.
For upper bounds, one must construct a $\T^r$ satisfying $\Opt(\T^r,\gamma_r)$ for all $r$ (and then turn to satisfying monotonicity).
For lower bounds, it suffices to prove nonexistence of any such $\T^r$, for any $r$.

Therefore, we next focus on constructing a feasibility program (QFP, Definition \ref{def:qfp}) and prove the following results.
\begin{theorem} \label{thm:opt-iff-qfp}
  Given a convex polytope $C \subseteq \simplex$, there exist polytopes $\T^r$ in $\reals^d$ such that $\Opt(\T^r, C)$ if and only if there is a feasible solution to the QFP (Definition \ref{def:qfp}) with rank parameter $d$.
\end{theorem}

\begin{corollary}
  If there exists any $r \in \R$ and corresponding $C = \gamma_r = \{p : r \in \argmin_{r'} \inprod{p}{\ell(r')} \}$ for which the quadratic program is infeasible, then $\ell$ is not $d$-embeddable.
\end{corollary}

\subsection{Proofs and key concepts}

The feasibility program intuitively contains one set of necessary and one set of sufficient conditions.
The necessary conditions are derived from the representation of the polytope $C$ as an intersection of halfspaces.
The sufficient conditions are derived from its dual representation as a convex hull of vertices.

The variables are a set of normal vectors $\{v_i \in \reals^d : i \in [k]\}$ and a set of vertices $\{X^j_y \in \reals^d : j \in [\ell], y \in \Y\}$.
These correspond to a (possibly relaxed) halfspace representation and (possibly incomplete) vertex representation of the subgradient polytopes $\T^r$.

\begin{definition}[Quadratic Feasibility Program] \label{def:qfp} ~ \\
  \indent \textbf{Parameter:} $d \in \mathbb{N}$.

  \textbf{Given:} a polytope $C = \{p \in \simplex : Bp \geq \vec 0\} = \conv(\{p^1, \ldots, p^\ell\}) \subseteq \simplex$, where $B \in \reals^{k \times n}$ has a minimum number of rows.

  \textbf{Variables:} a matrix $V \in \reals^{k \times d}$, each row representing a normal vector; and a collection of matrices $X^1,\ldots,X^{\ell} \in \reals^{d \times n}$, with $X^j$ intuitively corresponding to $p^j$ with columns $X^j_y$ representing witness points.

  \textbf{Constraints:}
    \begin{align}
      V X^j                     &\leq B    & \text{(pointwise, $\forall j \in [\ell]$)}  \label{eqn:qp-constr-1} \\
      \sum_{y=1}^n p^j(y) X^j_y &= \vec 0  & \text{($\forall j \in [\ell]$)}    \label{eqn:qp-constr-2} 
    \end{align}
\end{definition}

The feasibility program can be viewed as a low-rank matrix problem, namely: do there exist a set of rank-$d$ matrices that are pointwise dominated by $B$, sharing the left factor $V$, whose right factors $X^j$ respectively satisfy a subspace constraint?
We will see \bo{in which section?} that for the important example of abstain loss, the constraints simplifies into a more pure low-rank matrix problem.
In particular, for $d=n-1$ a solution always exists (e.g. \bo{cite}), found by taking the convex conjugate of the Bayes risk of $\ell$ \bo{and then what?}.


\bo{Do the conditions and intermediate theorem go here? I think so.}


%\begin{proof}[Proof of Theorem \ref{thm:opt-iff-qfp}]
%  By Theorem \ref{thm:nasc-optimality-conditions}, it suffices to show that there exist $\T^r$ such that the halfspace and vertex conditions (\ref{cond:H-condition}, \ref{cond:V-condition}) hold if and only if the program is feasible.
%
%  ($\implies$) 
%  By the vertex condition, for each $j \in [\ell]$, there exist witnesses $\{X^j_y \in T^r_y : y \in \Y\}$ satisfying the second constraint of the quadratic program (Inequality \ref{eqn:qp-constr-2}).
%  By the halfspace condition, there exist normals $v_1, \dots, v_k$ such that, for all $i$, for all $x \in T^r_y$, $\inprod{v_i}{x} \leq B_{iy}$; in particular, this applies to the above witnesses $X^j_y \in T^r_y$.
%  Collecting $v_1,\dots,v_k$ as the columns of $V$, this shows that the first constraint (Inequality \ref{eqn:qp-constr-1}) is satisfied.
%
%  \bigskip
%  ($\impliedby$)
%  We construct $T^r_y = \conv(\{X^1_y, \ldots, X^{\ell}_y\})$.
%  The second constraint of the quadratic program immediately implies the vertex condition.
%  Taking $v_1,\dots,v_k$ as the columns of $V$, the first constraint implies that for each $X^j_y$, we have $\inprod{v_i}{X^j_y} \leq B_{iy}$ for all $i,j,y$.
%  Any point $x \in T^r_y$ is a convex combination of $X^1_y,\ldots,X^{\ell}_y$, so it satisfies $\inprod{v_i}{x} \leq B_{iy}$.
%  This implies the halfspace condition.  % Bo: QED
%\end{proof}

  



% Acknowledgments---Will not appear in anonymized version
\acks{JF- Need that GRFP acknowledgement}

\bibliography{diss,extra}

\newpage
\appendix

\section{Additional proofs}

\end{document}
