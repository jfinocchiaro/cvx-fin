\documentclass[11pt]{article}
\usepackage[numbers, compress, sort]{natbib}
\usepackage[margin=1.1in]{geometry}

\usepackage{float}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[normalem]{ulem}
\usepackage{thm-restate}
\newcommand{\restatehack}[1]{}   % for auc-tex; don't ask
\usepackage{array}
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{mathtools, amsmath, amssymb, graphicx, verbatim, amsthm}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\definecolor{darkgreen}{rgb}{0.0,0.3,0.0}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usepackage{tikz,pgfplots,tikz-3dplot}
\usetikzlibrary{calc}
\usetikzlibrary{arrows.meta,shapes.misc,positioning,fit,calc,intersections,through,decorations.markings}
\usetikzlibrary{backgrounds}
\tdplotsetmaincoords{55}{135}        % for simplex diagrams

\tikzstyle{cell}=[dashed,thick]
\tikzstyle{simplex}=[thick]
\newcommand{\tikzfigscale}{2.2}
\newcommand{\placefiglabel}[1]{\node at (1.1,0,1.5) {#1};}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{darkgreen}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{teal}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{teal!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{BTW: #1}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}

\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\affhull}{\mathrm{affhull}}
\newcommand{\card}{\mathrm{card}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

\newcommand{\Li}[1]{L^{(#1)}}
\newcommand{\Ri}[1]{\R^{(#1)}}
\newcommand{\elli}[1]{\ell^{(#1)}}

% alphabetical order, by convention
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\OP}{\mathcal{OP}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\relint}{\mathrm{relint}}
%\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
\newcommand{\inter}{\mathrm{inter}}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\red}{\mathrm{red}}
\newcommand{\trimred}{\mathrm{trim}}
\newcommand{\trimcover}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\hyp}{\mathrm{hypo}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\regret}[3]{R_{#1}(#2,#3)}

\newcommand{\Ind}[1]{\ones\{#1\}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}{\ell_{\text{abs}}^f}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}
\newcommand{\emb}{{\tt e}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sgn}{sgn}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}

\newtheorem{definition}{Definition}
\newtheorem{construction}{Construction}
\newtheorem{condition}{Condition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}


\title{An Embedding Framework for Consistent Polyhedral Surrogates}
%\title{Consistent Polyhedral Surrogates via Embeddings}
%\title{Convex Surrogates via Polyhedral Losses}
\author{%
 Jessie Finocchiaro \\
 \texttt{jefi8453@colorado.edu}
 \and
 Rafael Frongillo\\
 \texttt{raf@colorado.edu}
 \and
 Bo Waggoner\\
 \texttt{bwag@colorado.edu}
}
\date{%                          % nice hack
  % \vspace*{-5pt}
  University of Colorado Boulder
  \\[15pt]
  \today
}
\begin{document}

\maketitle

\begin{abstract}
We formalize and study the natural approach of designing convex surrogate loss functions via embeddings\jessiet{this means nothing to a reader; do we want to say ``via embedding discrete predictions into the real coordinate space (of $d$ dimensions?)''... maybe merging with next sentence?  or ignore.}, for problems such as classification, ranking, or structured prediction. 
In this approach, one embeds each of the finitely many predictions (e.g.\ rankings) as a point in $\reals^d$, assigns the original loss values to these points, and ``convexifies'' the loss in some way to obtain a surrogate.
We establish a strong connection between this approach and polyhedral (piecewise-linear convex) surrogate losses.
Given any polyhedral loss $L$, we give a construction of a link function through which $L$ is a consistent surrogate for the loss it embeds. \jessiet{Switch order of this sentence and next?}
Conversely, we show how to construct a consistent polyhedral surrogate for any given discrete loss.
Our framework yields succinct proofs of consistency or inconsistency of various polyhedral surrogates in the literature, and for inconsistent surrogates, it further reveals the discrete losses for which these surrogates are consistent.
\raft{Insert sentence about regret bounds}
We show some additional structure of embeddings, such as the equivalence of embedding and matching Bayes risks, and the equivalence of various notions of non-redudancy.
Using these results, we establish that indirect elicitation, a necessary condition for consistency, is also sufficient when working with polyhedral surrogates.
% The framework can generally be applied to finite prediction tasks, and is demonstrated on examples including high-confidence classification, structured prediction, ordered partitions, and top-$k$ prediction.
\end{abstract}

%\begin{keywords}%
%  property elicitation, proper scoring rules, surrogate loss functions, embeddings
%\end{keywords}

\section{Introduction}\label{sec:intro}


%% surrogates are important.  in particular, consistent surrogates. (general intro)
In supervised learning, one tries to learn a hypothesis which fits some labeled data by minimizing a target loss function.
Unfortunately, minimizing the target loss directly is typically computationally intractable, especially for discrete prediction tasks like classification, ranking, and structured prediction.
Instead, one typically minimizes a surrogate loss which is convex and therefore efficiently minimized.
After learning a surrogate hypothesis, a link function then translates back to the target problem.
To be effective\jessiet{replace with ``To ensure the surrogate and link properly correspond to the target problem,''?}, the surrogate must also be \emph{consistent}, meaning that minimizing it over enough data and applying the link solves the target problem.


%% designing convex and consistent surrogates is often ad hoc.  often inefficient or not consistent (the gap)
\jessiet{I almost feel like this paragraph should be in reverse order: convex consistent studied, design often ad-hoc, for example top-k, also prediction dimension.  But also we might want to punt on prediction dimension for later.  Shifting; old version in comments.}
%A growing body of work seeks to design and analyze consistent convex surrogates for particular target loss functions.
%For example, motivated by structured prediction and other large-label domains, recent work has sought surrogates with low prediction dimension, meaning the dimension of the range of the surrogate hypothesis~\citep{frongillo2015elicitation, ramaswamy2016convex}.
%Yet in some cases these bounds are far from tight, such as for \emph{abstain loss} (classification with an abstain option)~\citep{bartlett2008classification,yuan2010classification,ramaswamy2016convex,ramaswamy2018consistent,zhang2018reject}.
%In other settings, such as top-$k$ classification, there are many proposed ``hinge-like'' surrogates which are piecewise linear and convex (henceforth, \emph{polyhedral}), which were later proved to be inconsistent~\cite{yang2018consistency}.
%Generally speaking, surrogate design has been ad-hoc, with few tools to construct consistent convex surrogates, and even sparser understanding of what types of surrogates are possible.
A growing body of work seeks to design and analyze consistent convex surrogates for particular target loss functions.
Generally speaking, surrogate design has been ad-hoc, with few tools to construct consistent convex surrogates, and even sparser understanding of what types of surrogates are possible.
For example, for top-$k$ classification, there are many proposed ``hinge-like'' surrogates which are piecewise linear and convex (henceforth, \emph{polyhedral}), which were later proved to be inconsistent~\cite{yang2018consistency}.
However, until now, there has been no unifying framework that moves from a given target problem to a convex consistent surrogate and link.


%% we introduce embeddings (the hero narrative)
To address this state of affairs, we introduce a new framework motivated by a particularly natural approach for finding convex surrogates, wherein one ``embeds'' a discrete loss.
Specifically, we say a convex surrogate $L$ embeds a discrete target loss $\ell$ if there is an injective embedding from the discrete reports (predictions) to a vector space such that (i) the original loss values are recovered, and (ii) a report is $\ell$-optimal if and only if the embedded report is $L$-optimal.
Common examples of this general construction include hinge loss as a surrogate for 0-1 loss and the abstain surrogate mentioned above~\citep{ramaswamy2018consistent}.

%% what we prove / what we do (our research question)
We prove that such an embedding scheme is intimately related to the class of polyhedral (piecewise-linear and convex) loss functions.
In particular, every discrete loss is embedded by a polyhedral surrogate.
Moreover, such an embedding gives rise to calibrated link function, and is therefore consistent with respect to the target loss.


\restatehack{
  \begin{theorem}
    \label{thm:embed-poly-main}
    \label{thm:link-main}
  \end{theorem}}

\begin{restatable}{theorem}{embedpolyinformal}\label{thm:embed-poly-main}
  Every discrete loss $\ell$ is embedded by some polyhedral loss $L$, and every polyhedral loss $L$ embeds some discrete loss $\ell$.
\end{restatable}
\begin{restatable}{theorem}{linkinformal}\label{thm:link-main}
  Given any polyhedral loss $L$, let $\ell$ be a discrete loss it embeds. There exists a link function $\psi$ such that $(L,\psi)$ is calibrated with respect to $\ell$.
\end{restatable}
\noindent
Beyond consistency, we show that any calibrated link gives rise to a linear surrogate regret bound, which allows one to translate generalization bounds from the surrogate to the target (\raf{REF}).

%% walking through results for the rest of the intro.  there are just a lot, so multiple paragraphs.
\raft{Probably should reorg next 3 or 4 paragraphs -- maybe introduce bullets or something?  Need to give an effective roadmap.  Or keep the existing flow but clarify the themes of each paragraph (tools, intuition, deeper structural results, takeaways).}
Our proofs give explicit constructions for the surrogate (\S~\ref{sec:poly-loss-embed}) and link (\S~\ref{sec:calibration}) embedding a given discrete loss.
Conversely, given an existing polyhedral surrogate, we provide tools to find the discrete losses they embed (Proposition~\ref{prop:representative-embeds-restriction}), which may or may not be the desired target.
\jessiet{Do we want the rest of this paragraph?  it kind of walks through the rest of the paper, but also doesn't?  this is just results for polyhedral losses, followed by a paragraph of intuition, then comes back to the rest of the results, which feels a little funky?}
In short, if one can identify a finite \emph{representative set} $\Sc$ of reports for a surrogate $L$, meaning one that always contains an $L$-optimal report, then $L$ embeds the loss $L|_\Sc$ ($L$ restricted to $\Sc$).
We illustrate all of these tools with several examples (\S~\ref{sec:applications}).

%% intuition for the relationship between polyhedral surrogates and discrete target losses
Underpinning our results are several observations which formalize the idea that polyhedral losses ``behave like'' discrete losses.
For example, polyhedral losses have a finite number of optimal sets (the set of reports which minimize the expected loss for some conditional label distribution).
As a result, by selecting a report from each set, one arrives at a finite representative set, which gives an embedding.
For the converse, we prove that a surrogate loss embeds a target if and only if their Bayes risks match (Proposition~\ref{prop:embed-bayes-risks}), and use the fact that discrete losses and polyhedral losses both have polyhedral Bayes risks.

We also provide several observations beyond what is needed to prove our main results, which we view as conceptual contributions (\S~\ref{sec:min-rep-sets},~\ref{sec:poly-ie-consistency}).
Using tools from property elicitation, we show an equivalence between minumum representative sets and ``non-redundancy'', wherein no report is dominated by another.
We further show that, while the minimum representative set is not always unique, the loss values associated with it are unique, giving rise to a natural ``trim'' operation on losses.
Finally, we show that when restricting to the class of polyhedral surrogates, indirect elicitation is both necessary and sufficient for consistency (Theorem~\ref{thm:poly-ie-implies-consistent}).

%% why are our results important
Taken together, we view our contributions as both conceptual and practical.
We uncover the remarkable structure of polyhedral surrogates, deepening our understanding of the relationship between surrogate and discrete target losses.
This structure leads to a powerful new framework to design and analyze surrogate losses.
As we illustrate with several examples, this framework has already been applied to solve open questions by designing new surrogates, to uncover the behavior of existing surrogates, and to construct link functions in complex structured problems.
We conclude with several exciting directions for future work.



\paragraph{Related works.}

%% connect elicitation to consistency; embedding is a special case of elicitation, but often not focused on convex elicitaiton
We study consistency through the necessary condition of \emph{indirect property elicitation}~\citep{finocchiaro2021unifying,frongillo2015elicitation}.
\citet{agarwal2015consistent} were the first to formally connect property elicitation to consistency, though their results generally do not apply to discrete prediction tasks.
The notion of embedding introduced in \S~\ref{sec:setting} is a special case of indirect property elicitation.
While property elicitation has an extensive literature by now~\citep{savage1971elicitation,osband1985information-eliciting,lambert2008eliciting,gneiting2011making,steinwart2014elicitation,frongillo2015vector-valued,fissler2016higher,lambert2018elicitation}, these works are mostly concerned with point estimation problems, as opposed to the finite set of possible predictions yielded by polyhedral embeddings. \jessiet{Not sure how to phrase this.}

\raft{Perhaps move or cut}
%% most literature for convex surrogates focuses on smooth surrogates, but there is good reason to study polyhedral surrogates
The literature on convex surrogates focuses mainly on smooth surrogate losses~\citep{crammer2001algorithmic,bartlett2006convexity,bartlett2008classification, duchi2018multiclass, williamson2016composite, reid2010composite,menon2019multilabel,zhang2020convex,bao2020calibrated}.
In practice, minimizing such surrogates often corresponds to learning the entire underlying data distribution and compute the desired target task with the full distribution in hand.
However, \citet[Section 1.2]{ramaswamy2018consistent} contend that optimizing nonsmooth losses may enable reduction of the prediction dimension while maintaining consistency relative to smooth losses, improving downstream efficiency of the learning algorithm.
This sentiment is echoed by~\citet{lapin2016loss}\jessiet{check that it's this one. it's one of the Lapin papers}, who suggests that learning the target problem of interest, rather than the entire underlying data distribution can improve performance in limited data settings.
This intuition is supported by~\citet{frongillo2021surrogate}, who show that polyhedral surrogates in general have better surrogate regret bounds than smooth surrogates. \jessiet{This connection correct?}
%Nevertheless, nonsmooth losses, such as the polyhedral losses we consider, have been proposed and studied for a variety of classification-like problems~\citep{yang2018consistency,yu2018lovasz,lapin2015top}.



\section{Setting}
\label{sec:setting}

For discrete prediction problems like classification, the given (discrete) loss is often hard to optimize directly.
Therefore, many machine learning algorithms instead minimize a surrogate loss function with better optimization qualities, such as convexity.
To ensure that this surrogate loss successfully addresses the original problem, one needs to establish statistical consistency, a minimal requirement that is a prerequisite for generalization bounds.
Consistency depends crucially on the choice of link function that maps surrogate reports (predictions) to original reports.
Consistency is often difficult to work with directly, so instead one often uses the notion of \emph{calibration} (Definition~\ref{def:calibrated}), which is equivalent to consistency in finite prediction settings~\citep{bartlett2006convexity,tewari2007consistency,ramaswamy2016convex} and depends solely on the conditional distribution over $\Y$.

\subsection{Notation and Losses}
\label{sec:notation-losses}

Let $\Y$ be a finite label space, and throughout let $n=|\Y|$.
Define $\reals^\Y_+$ to be the nonnegative orthant in $\reals^\Y$, i.e., $\reals^\Y_+ = \{x \in \reals^\Y \mid \forall y\in\Y\; x_y \geq 0 \}$.
Let $\simplex = \{p\in\reals^{\Y}_+ \mid \|p\|_1 = 1\}$ be the set of probability distributions on $\Y$, represented as vectors.
% We write $p_y$ for the probability of outcome $y \in \Y$ drawn from $p \in \simplex$.
We will primarily focus on conditional distributions $p\in\simplex$ over labels, abstracting away the feature space $\X$; see \S~\ref{subsec:calibration-links} for a discussion of the joint distribution over $\X\times\Y$.

A generic loss function, denoted $L:\R\to\reals^\Y_+$, maps a report (prediction) $r$ from a set $\R$ to the vector of loss values $L(r) = (L(r)_y)_{y\in\Y}$ for each possible outcome $y\in\Y$.
We write the corresponding expected loss when $Y \sim p$ as $\inprod{p}{L(r)}$.
The \emph{Bayes risk} of a loss $L:\R\to\reals^\Y_+$ is the function $\risk{L}:\simplex\to\reals_+$ given by $\risk{L}(p) := \inf_{r\in\R} \inprod{p}{L(r)}$.
When restricting the domain of a loss $L$ from $\R$ to $\R' \subseteq \R$, we write $L|_{\R'}$.
\btw{RF: We got some feedback from Anish and others that the way we write generic losses was confusing.  I tried to rewrite this paragraph to emphasize that we use $L$ for generic losses, including surrogates, and $\ell$ for discrete losses, where we assume $\R$ is a finite set.  That way the idea that $\R$ is always finite is perhaps dispelled.}

We assume that a given discrete prediction problem, such as classification, is given in the form of a discrete \emph{target loss} where the report set $\R$ is finite.
We will denote target losses by $\ell:\R\to\reals^\Y_+$; when $\ell$ is written we consider reports $r \in \R$, which we assume is a finite set.
%We will assume throughout that the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.
Surrogate losses will take $\R = \reals^d$ and be written $L:\reals^d\to\reals^\Y_+$, typically with reports written $u\in\reals^d$.

%\jessie{In general, we shall denote discrete reports by $r \in \R$, and real-valued reports by $u \in \U$.}
For example, 0-1 loss is a discrete loss with $\R = \Y = \{-1,1\}$
given by $\ellzo(r)_y = \Ind{r \neq y}$, with Bayes risk $\risk{\ellzo}(p) = 1-\max_{y\in\Y} p_y$.
Two important surrogates for $\ellzo$ are hinge loss $\hinge(u)_y = (1-yu)_+$, where $(x)_+ = \max(x,0)$, and logistic loss $L(u)_y = \log(1+\exp(-yu))$ for $u\in\reals$.
See Figure~\ref{fig:bayes-risks-01} for a visualization of the Bayes risks of 0-1, Hinge, and Logistic losses, respectively.

Most of the surrogate losses we consider will be \emph{polyhedral}, meaning piecewise linear and convex; we therefore briefly recall the relevant definitions.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\citep{rockafellar1997convex}.
%
\begin{definition}[Polyhedral loss]
  A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral function of $u$ for each $y\in\Y$.
\end{definition}
%
For example, hinge loss is polyhedral, whereas logistic loss is not.

\subsection{Property Elicitation}
\label{sec:property-elicitation}

To make headway, we will appeal to concepts and results from property elicitation.
This literature elevates the \emph{property}, a map from distributions to optimal reports, as a central object to study in its own right.
In our case, this map will at times be set-valued, meaning a single distribution could yield multiple optimal reports.
For example, when $p=(1/2,1/2)$, both $r=1$ and $r=-1$ optimize 0-1 loss.
We will use double arrow notation to denote a (non-empty) set-valued map, so that $\Gamma: \simplex \toto \R$ is shorthand for $\Gamma: \simplex \to 2^{\R} \setminus \{\emptyset\}$.
%See the discussion following Definition~\ref{def:elicits} for notational conventions.% regarding $\R$, $\Gamma$, $\gamma$, $L$, $\ell$, etc.

\begin{definition}[Property, level set]\label{def:property}
  A \emph{property} is a function $\Gamma:\simplex\toto\R$.
  The \emph{level set} of $\Gamma$ for report $r$ is the set $\Gamma_r := \{p \in \simplex \mid r \in \Gamma(p)\}$.
\end{definition}

Intuitively, $\Gamma(p)$ is the set of reports which should be optimal for a given distribution $p$, and $\Gamma_r$ is the set of distributions for which the report $r$ should be optimal.
By optimal, we mean minimizing an associated loss function in expectation over $p$, which we formalize shortly.
Note that our definitions align such that discrete losses elicit finite properties (those with finite range). \jessiet{What is this (prev) sentence getting at?  Can we delete?}%; both are non-redundant in the correct senses.
For example, the \emph{mode} is the %finite
property $\mode(p) = \argmax_{y\in\Y} p_y$, and captures the set of optimal reports for 0-1 loss: for each distribution over the labels, one should report the most likely label.
In this case we say 0-1 loss \emph{elicits} the mode, as we formalize below.
% This terminology comes from the information elicitation \jessiet{information vs property... do we care?}\raft{we could also say economics literature; not ``property'' though since that doesn't offer an explanation of the word ``elicits''} literature~\citep{savage1971elicitation,osband1985information-eliciting,lambert2008eliciting}, in which a report $r$ is elicited from some forecaster by scoring her with a loss on the observed outcome $y$.

\begin{definition}[Elicits]
  \label{def:elicits}
  A loss $L:\R\to\reals^\Y_+$, \emph{elicits} a property $\Gamma:\simplex \toto \R$ if
  \begin{equation}
    \forall p\in\simplex,\;\;\;\Gamma(p) = \argmin_{r \in \R} \inprod{p}{L(r)}~.
  \end{equation}
  If $L$ elicits a property, it is unique and we denote it $\prop{L}$.
\end{definition}
Since we have defined a property $\Gamma$ to be nonempty, if the minimum of expected loss $\inprod{p}{L(\cdot)}$ is not attained for some $p \in \simplex$, then $L$ does not elicit a property.
We say that a loss $L$ is \emph{minimizable} if the infimum of $\inprod{p}{L(\cdot)}$ is attained for all $p \in \simplex$.

We will typically denote general properties and losses with $\Gamma$ and $L$, respectively.
For surrogate losses and properties, recall that we typically consider the report set $\reals^d$.
For discrete target losses and properties, we will take $\R$ to be any finite set, and use lowercase notation $\gamma$ and $\ell$, respectively.
Any property $\gamma:\simplex\toto\R$ for a finite set $\R$ is called a \emph{finite property}.


\subsection{Calibration and Links}
\label{subsec:calibration-links}


To assess whether a surrogate and link function align with the original loss, we turn to the common condition of \emph{calibration}.
Roughly, a surrogate and link are calibrated if the best possible expected loss achieved by linking to an incorrect report is strictly suboptimal, which requires that the excess loss of some report is bounded by (a constant times) the excess loss of the linked report.

% \begin{definition}[Consistency]\label{def:consistency}
%   A surrogate $L:\reals^d\to\reals^\Y_+$ and link $\psi:\reals^d\to\R$ is \emph{consistent} with a discrete loss $\ell:\R\to\reals^\Y_+$ if \raf{statement about distributions on $\X\times\Y$, limits of hypotheses, and Bayes opt hypotheses}
% \end{definition}

% \begin{definition}[Separated Link]\label{def:links}
%   Let discrete loss $\ell:\R\to\reals^\Y_+$ and surrogate $L:\reals^d\to\reals^\Y_+$ be given.
%   A \emph{link function} is a map $\psi:\reals^d\to\R$.
%   We say that a link $\psi$ is \emph{$\delta$-separated} for some $\delta > 0$ if for all $p \in \simplex$ and $u\in\reals^d$, we have
%   \begin{align*}
%     \inprod{p}{L(u)} - \inf_{u' \in \reals^d} \inprod{p}{L(u')} \geq \delta\left(\inprod{p}{\ell(\psi(u))} - \min_{r \in \R} \inprod{p}{\ell(r)}\right)~.
%   \end{align*}
%   A link is \emph{separated} if it is $\delta$-separated for some $\delta>0$.
% \end{definition}

\begin{definition}
  \label{def:calibrated}
  \jessiet{Need to be careful on use of $\gamma$.}
  \raft{Replace with prop[ell] or maybe define gamma}
  Let discrete loss $\ell:\R\to\reals^\Y_+$, proposed surrogate $L:\reals^d\to\reals^\Y_+$, and link function $\psi:\reals^d\to\R$ be given.
  We say $(L,\psi)$ is \emph{calibrated} with respect to $\ell$ if
for all $p \in \simplex$,
  \begin{equation}
    \label{eq:calibrated}
  \inf_{u \in \reals^d : \psi(u) \not\in \gamma(p)} \inprod{p}{L(u)} > \inf_{u \in \reals^d} \inprod{p}{L(u)}~.
  \end{equation}
  If $(L, \psi)$ is calibrated with respect to $\ell$, we call $\psi$ a \emph{calibrated link.}
\end{definition}
It is well-known in finite-outcome settings that calibration is equivalent to \emph{consistency}, in the following sense (cf.~\citep{bartlett2006convexity,zhang2004statistical,agarwal2015consistent}).
Suppose we have the feature space $\X$ and label space $\Y$.
%For any data distribution $D \in \Delta(\X \times \Y)$, let $L^*$ be the best possible expected (over $D$) $L$-loss achieved by any hypothesis $H:\X\to\reals^d$, and $\ell^*$ the best expected $\ell$-loss for any hypothesis $h:\X\to\R$, respectively.
We say a surrogate and link pair $(L,\psi)$ is consistent with respect to $\ell$ if, for all data distributions $D \in \Delta(\X \times \Y)$, and all sequences of surrogate hypotheses $H_1,H_2,\ldots$ whose $L$-loss limits to the optimal surrogate loss $L^*$ (in expectation over $D$), the $\ell$-loss of the sequence $\psi\circ H_1,\psi \circ H_2, \ldots$ limits to the optimal target loss $\ell^*$.
\bo{edited above slightly. I believe a technical definition of consistency would have a ``for all $\D$'', but we can probably skip that.}\jessie{Tried to make sure it had this technical correctness.}
\raf{Tried to clarify/correct the quantifiers.  OLD VERSION:
Let $L^*$ be the best possible expected $L$-loss achieved by any hypothesis $H:\X\to\reals^d$, and $\ell^*$ the best expected $\ell$-loss for any hypothesis $h:\X\to\R$, respectively.
If $(L,\psi)$ is consistent, then for any sequence of surrogate hypotheses $H_1,H_2,\ldots$ whose $L$-loss limits to $L^*$, the $\ell$-loss of $\psi\circ H_1,\psi \circ H_2, \ldots$ limits to $\ell^*$ for all data distributions $D \in \Delta(\X \times \Y)$.}
%
As Definition~\ref{def:calibrated} does not involve the feature space $\X$, we will drop it for the remainder of the paper.
\jessiet{Commented out the following sentence as it was repetitive from earlier discussion. - Mar 24 22}



\subsection{Embedding}

%Several consistent convex surrogates in the literature can be thought of as \emph{embeddings} of the target loss $\ell$, wherein one maps the discrete reports of $\ell$ to a vector space, and finds a convex loss which agrees with the original loss.
%A key notion to define an embedding is that of a \emph{representative set}: a set of reports $\Sc$ such that, for all label distributions, at least one report $r\in\Sc$ minimizes expected loss.
We now formalize the sense in which a convex surrogate can \emph{embed} a target loss $\ell$.
Here one maps each report (prediction) of $\ell$ to a point in $\reals^d$, then constructs a convex loss on $\reals^d$ that agrees with $\ell$ at these points.
This approach captures several consistent surrogates in the literature (e.g.,~\citep{ramaswamy2015hierarchical,ramaswamy2016convex,lapin2015top,wang2020weston}; see \S~\ref{sec:applications}).

An important subtlety is that it is not always necessary to map \emph{all} target reports to $\reals^d$.
It is often convenient to allow $\ell$ to have reports that are ``redundant'' in some sense. (We explore redundancy further in \S~\ref{sec:min-rep-sets}; see also \citet{wang2020weston}.)
Because of this redundancy, we will only require an embedding map to be defined on a \emph{representative set}: a set of reports $\Sc$ such that, for all label distributions, at least one report $r\in\Sc$ minimizes expected loss.
\begin{definition}[Representative set]
  Let $\Gamma:\simplex\toto\R$.
  We say $\Sc \subseteq \R$ is \emph{representative for $\Gamma$} if we have $\Gamma(p) \cap \Sc \neq \emptyset$ for all $p\in \simplex$.
  We further say $\Sc$ is a \emph{minimum representative set} if it has the smallest cardinality among all representative sets.
  Given a minimizable loss $L:\R\to\reals^\Y_+$, we say $\Sc$ is a (minimum) representative set for $L$ if it is a (minimum) representative set for $\prop L$.
  % We say $\Sc \subseteq \R$ is \emph{representative for $L$} if we have $\prop{L}(p) \cap \Sc \neq \emptyset$ for all $p\in \simplex$.
  % We further say $\Sc$ is a \emph{minimum representative set} if it has the smallest cardinality among all representative sets.
\end{definition}

\citet{wang2020weston} first studies the notion of minimum representative sets under the name \emph{embedding cardinality}.

%It turns out that it will be convenient, especially in applications, to allow the target $\ell:\R\to\reals^\Y_+$ to have reports which are ``redundant'' in some sense.
%One natural sense is that $\ell$ has a representative set that is a strict subset of $\R$, i.e., $\R$ is not a minimum representative set.
%\raft{Some discussion here about minimum representative sets and \emph{maximally informative sets} from~\citet{wang2020weston} here, though maybe best to leave it until later.  BTW, I think their latest version might use different terminology.}
%Another is that some reports $r$ are ``dominated'', meaning $r$ is never optimal or there is another report $r'$ which is always optimal when $r$ is optimal.
%A priori these alternate definitions may not seem closely related.
%In fact, we show in \S~\ref{sec:min-rep-sets} that they are equivalent in our setting.

We now define an embedding.
In addition to matching loss values, as described above, we require the original reports to be optimal exactly when the corresponding embedded points are optimal.
\begin{definition}[Embedding]\label{def:loss-embed}
  A minimizable loss $L:\reals^d\to\reals^\Y_+$ \emph{embeds} a loss $\ell:\R\to\reals^\Y_+$ if there exists a representative set $\Sc$ for $\ell$ and an injective embedding $\varphi:\Sc\to\reals^d$ such that
  (i) for all $r\in\Sc$ we have $L(\varphi(r)) = \ell(r)$, and (ii) for all $p\in\simplex,r\in\Sc$ we have
  % \begin{equation}\label{eq:embed-loss}
  %   r \in \argmin_{r'\in\R} \inprod{p}{\ell(r')} \iff \varphi(r) \in \argmin_{u\in\reals^d} \inprod{p}{L(u)}~.
  % \end{equation}
  \begin{equation}\label{eq:embed-loss}
    r \in \prop{\ell}(p) \iff \varphi(r) \in \prop{L}(p)~.
  \end{equation}
  If $\Sc$ is a minimal representative set, we say $L$ \emph{tightly embeds} $\ell$.
  %if there is some representative set $\Sc$ for $\ell$ such that $L$ embeds $\ell$ over $\Sc$.
\end{definition}

To illustrate the idea of embedding, let us examine hinge loss in detail as a surrogate for 0-1 loss for binary classification.
Recall that we have $\R = \Y = \{-1, +1\}$, with $\hinge(u)_y = (1 - uy)_+$ and $\ellzo(r)_y := \Ind{r\neq y}$, typically with link function $\psi(u) = \sgn(u)$.
We will see that hinge loss embeds (2 times) 0-1 loss, via the embedding $\varphi(r) = r$.
For condition (i), it is straightforward to check that $\hinge(\varphi(r))_y = \hinge(r)_y = 2 \Ind{r \neq y} = 2\ellzo(r)_y$ for all $r,y\in\{-1,1\}$.
% First, consider that
% \begin{align*}
% 	\ellzo(-1) = (0,1) && \ellzo(1) = (1,0)\\
% 	L_{hinge}(-1) = (0,2) && L_{hinge}(1) = (2,0)\\
% \end{align*}
% Therefore, $L_{hinge}$ is twice $\ellzo$ for each $r \in \R$.
For condition (ii), let us compute the property each loss elicits, i.e., the set of optimal reports for each $p\in\simplex$:
\[
\prop{\ellzo}(p) = \begin{cases}
1 & p_1 > 1/2 \\
\{-1,1\} & p_1 = 1/2\\
-1 & p_1 < 1/2
\end{cases}
\qquad
\prop{L_{hinge}}(p) = \begin{cases}
[1,\infty) & p_1 = 1\\
1 & p_1 \in (1/2,1) \\
[-1,1] & p_1 = 1/2\\
-1& p_1 \in (0, 1/2)\\
(-\infty, -1]& p_1 = 0
\end{cases}~.
\]
% Now, take the embedding $\varphi$ to be the identity.  % := r$ if $r \in [-1,1]$, and $\sgn(r)$ otherwise.
In particular, we see that $-1 \in \prop{\ellzo}(p) \iff p_1 \in [0, 1/2] \iff -1 \in \prop{\hinge}(p)$, and $1 \in \prop{\ellzo}(p) \iff p_1 \in [1/2,1] \iff 1 \in \prop{\hinge}(p)$.
With both conditions of Definition~\ref{def:loss-embed} satisfied, we can conclude that $\hinge$ embeds $2\ellzo$.
By results in \S~\ref{subsec:match-BR}, one could also show that $\hinge$ embeds $2\ellzo$ by the fact that their Bayes risks match (Figure~\ref{fig:bayes-risks-01}).

\begin{figure}
	\begin{minipage}{0.3\linewidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figs/0-1-br.pdf}
%		\caption{Bayes risk of the 0-1 loss.}
%		\label{fig:0-1-br}
	\end{minipage}
	\hfill
	\begin{minipage}{0.3\linewidth}
	\centering		\includegraphics[width=0.95\linewidth]{figs/hinge-br.pdf}
%		\caption{Bayes risk of hinge loss.}
%		\label{fig:hinge-br}
	\end{minipage}
	\hfill
	\begin{minipage}{0.3\linewidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figs/logistic-br.pdf}
%	\caption{Bayes risk of the logistic loss.}
%	\label{fig:logistic-br}
\end{minipage}
\caption{Bayes risks $\risk L : p \mapsto \inf_u \inprod{p}{L(u)}$ of 0-1, hinge, and logistic losses, respectively, plotted as a function of $p_1 = \Pr[Y=1]$.
	Observe that the Bayes risks of 0-1 and hinge loss are both piecewise linear and concave, while the Bayes risk of logistic loss is not piecewise linear.  Proposition~\ref{prop:embed-bayes-risks} states that embedding is equivalent to matching Bayes risks, confirming that hinge loss (M) embeds 0-1 loss (L), but logistic loss (R) does not.}
\label{fig:bayes-risks-01}
\end{figure}

%% embedding yields calibration
In this particular example, it is known $(\hinge,\sgn)$ is calibrated with respect to 0-1 loss.
More generally, however, it is not clear whether an arbitrary embedding yields a calibrated link.
Indeed, apart from mapping the embedded points back to their original reports, via $\psi(\varphi(r)) = r$, how to map the remaining values is far from obvious.
Using the strong connection between embeddings and polyhedral surrogates in \S~\ref{sec:poly-loss-embed}, we give a construction to map the remaining values in \S~\ref{sec:calibration}, showing that embeddings from polyhedral surrogates always yield calibration.

While our notion of embedding is sufficient for calibration (and therefore consistency), it is worth noting that it is not \emph{necessary} for these conditions.  
For example, while logistic loss does not embed 0-1 loss, the surrogate and link for logistic loss are consistent.







\section{Embeddings and Polyhedral Losses}
\label{sec:poly-loss-embed}

In this section, we establish a tight relationship between the technique of embedding and the use of polyhedral (piecewise-linear convex) surrogate losses, showing Theorem~\ref{thm:embed-poly-main}.
We defer the question of when such surrogates are consistent to \S~\ref{sec:calibration}. 

A first observation is that if a loss $L$ elicits a property $\Gamma$, then $L$ restricted to some representative set $\Sc$, denoted $L|_\Sc$, elicits $\Gamma$ restricted to $\Sc$.
As a consequence, restricting to representative sets preserves the Bayes risk.
We will use these observations throughout.
\begin{lemma}\label{lem:loss-restrict}
  Let $L:\R\to\reals^\Y_+$ elicit $\Gamma$, and let $\Sc\subseteq\R$ be representative for $L$.
  % such that $\Gamma(p) \cap \Sc \neq \emptyset$ for all $p\in\simplex$.
  Then $L|_\Sc$ elicits $\gamma:\simplex\toto\Sc$ defined by $\gamma(p) = \Gamma(p)\cap \Sc$.
  Moreover, $\risk{L}=\risk{L|_\Sc}$.
\end{lemma}
\begin{proof}
  Let $p\in\simplex$ be fixed throughout.
  First let $r \in \gamma(p) = \Gamma(p) \cap \Sc$.
  Then $r \in \Gamma(p) = \argmin_{u\in\R} \inprod{p}{L(u)}$, so as $r\in\Sc$ we have in particular $r \in \argmin_{u\in\Sc} \inprod{p}{L(u)}$.
  For the other direction, suppose $r \in \argmin_{u\in\Sc} \inprod{p}{L(u)}$.
  As $\Sc$ is representative for $L$, we must have some $s \in \Gamma(p) \cap \Sc$.
  On the one hand, $s\in\Gamma(p) = \argmin_{u\in\R} \inprod{p}{L(u)}$.
  On the other, as $s \in \Sc$, we certainly have $s \in \argmin_{u\in\Sc} \inprod{p}{L(u)}$.
  But now we must have $\inprod{p}{L(r)} = \inprod{p}{L(s)}$, and thus $r \in \argmin_{u\in\R} \inprod{p}{L(u)} = \Gamma(p)$ as well.
  We now see $r \in \Gamma(p) \cap \Sc$.
  Finally, the equality of the Bayes risks $\min_{u\in\R} \inprod{p}{L(u)} = \min_{u\in\Sc} \inprod{p}{L(u)}$ follows immediately by the above, as $\emptyset \neq \Gamma(p)\cap\Sc \subseteq \Gamma(p)$ for all $p\in\simplex$.
\end{proof}


Lemma~\ref{lem:loss-restrict} leads to the following useful tool for finding embeddings: if a surrogate has a finite representative set, it embeds its restriction to the representative set.
\begin{proposition}\label{prop:representative-embeds-restriction}
  Let a minimizable surrogate loss $L:\reals^d \to \reals^\Y_+$ be given.
  If $L$ has a finite representative set $\Sc \subseteq \reals^d$, then $L$ embeds the discrete loss $L|_\Sc$.
\end{proposition}
\begin{proof}
  Let $\Gamma = \prop{L}$ and $\gamma = \prop{L|_\Sc}$.
  Define $\varphi : \Sc \to \Sc$ to be the identity embedding.
  Condition (i) of an embedding is trivially satisfied, as $L|_\Sc(u) = L(u)$ for all $u\in\Sc$.
  Now let $u\in\Sc$.
  From Lemma~\ref{lem:loss-restrict}, for all $p\in\simplex$ we have $u \in \gamma(p) \iff u \in \Gamma(p) \cap \Sc \iff u \in \Gamma(p)$.
  We conclude condition (ii) of an embedding.
\end{proof}


\raft{Need to combine/unify the filler text here and next paragraph}
With Proposition~\ref{prop:representative-embeds-restriction} in hand, we now shift our focus to \emph{polyhedral} (piecewise-linear and convex) surrogates.
While polyhedral surrogates cannot elicit finite properties, in the sense that they have infinitely many possible reports, they do elicit properties with a finite range, meaning they have finite representative sets.
%This observation lets us apply results about finite representative sets to understand the structure of polyhedral surrogates and the losses they embed.
See \S~\ref{app:power-diagrams} for the full proof of Lemma~\ref{lem:polyhedral-range-gamma}.

\begin{restatable}{lemma}{polyhedralrangegamma}
	\label{lem:polyhedral-range-gamma}
	Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss; then $L$ is minimizable and elicits a property $\Gamma := \prop{L}$.
	Then the range of $\Gamma$, given by $\Gamma(\simplex) = \{\Gamma(p) \subseteq \reals^d : p\in\simplex\}$, is a finite set of closed polyhedra.
\end{restatable}
\begin{proof}[Proof sketch]
	We know that $L$ is minimizable from~\citet[Corollary 19.3.1]{rockafellar1997convex} as $L$ is bounded from below.
	With $\Y$ finite, there are only finitely many supporting sets over $\simplex$.
	For $p \in \simplex$, the power diagram induced by projecting the epigraph of expected loss onto $\reals^d$ is the same for any $p$ of the same support (Lemma~\ref{lem:polyhedral-pd-same}).
	Moreover, we have $\Gamma(p)$ being exactly one of the faces of the projected epigraph since the hyperplane $u \mapsto (u, \inprod{p}{L(u)})$ supports the epigraph of the expected loss at exactly the property value; moreover, since the loss is polyhedral the supporting hyperplane must support on a face of the epigraph.
	Since this epigraph has finitely many faces (as it is polyhedral), the range of $\Gamma$ is then (a subset) of elements of a finitely generated (finite supports) set of finite elements (finite faces).
	Moreover, each element of $\Gamma(\simplex)$ is a closed polyhedron since it corresponds exactly to a closed face of a polyhedral set.
\end{proof}

\jessiet{Adding commentary about a corollary of Lemma~\ref{lem:polyhedral-range-gamma}: polyhedral losses have finite representative sets.  Not sure if we should merge with the paragraph above the lemma though.- May 3, 22}
One corollary of Lemma~\ref{lem:polyhedral-range-gamma} is that polyhedral surrogates have finite representative sets: simply, one can pick one point from each of the finitely many polyhedral optimal sets to obtain a finite (though not necessarily minimum) representative set.
This observation, combined with Lemma~\ref{lem:loss-restrict}, yields our first main result.

\begin{theorem}\label{thm:poly-embeds-discrete}
  Every polyhedral loss $L$ embeds a discrete loss.
\end{theorem}
\begin{proof}
  Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss, and $\Gamma = \prop{L}$.
  By Lemma~\ref{lem:polyhedral-range-gamma}, $\Gamma(\simplex)$ is finite set. 
  For each $U\in \Gamma(\simplex)$, select $u_U \in U$, and let $\Sc = \{u_U : U \in\Gamma(\simplex)\}$, which is again finite.
  For any $p\in\simplex$ then, let $U = \Gamma(p)$.
  We have $U \in \Gamma(\simplex)$ by definition, and thus some $u_U \in \Sc$; in particular, $u_U \in U = \Gamma(p)$.
  We conclude that $\Sc$ is representative for $L$.
  Proposition~\ref{prop:representative-embeds-restriction} now states that $L$ embeds $L|_\Sc$.
\end{proof}

We now turn to the reverse direction: which discrete losses are embedded by some polyhedral loss?
Perhaps surprisingly, we show in Theorem~\ref{thm:discrete-loss-poly-embeddable} that \emph{every} discrete loss is embeddable.
Combining this result with Theorem~\ref{thm:poly-embeds-discrete} establishes Theorem~\ref{thm:embed-poly-main}.
Further combining with Theorem~\ref{thm:link-main}, proved in the following section, this construction gives a consistent polyhedral surrogate for every discrete target loss.

The proof of Theorem~\ref{thm:discrete-loss-poly-embeddable} uses a construction via convex conjugate duality which has appeared in several different forms in the literature (e.g.\ \cite{duchi2018multiclass,abernethy2013efficient,frongillo2014general}).
We then apply a result we will prove in \S~\ref{sec:min-rep-sets}: a minimizable surrogate embeds a discrete loss if and only if their Bayes risks match (Proposition~\ref{prop:embed-bayes-risks}).

\begin{theorem}\label{thm:discrete-loss-poly-embeddable}
  Every discrete loss $\ell:\R \to \reals^\Y_+$ is embedded by a polyhedral loss.
\end{theorem}
\begin{proof}
  Let $n = |\Y|$, and let $C:\reals^n \to \reals$ be given by $(-\risk{\ell})^*$, the convex conjugate of $-\risk{\ell}$.
  From standard results in convex analysis, $C$ is polyhedral as $-\risk{\ell}$ is, and $C$ is finite on all of $\reals^\Y$ as the domain of $-\risk{\ell}$ is bounded~\cite[Corollary 13.3.1]{rockafellar1997convex}.
  Note that $-\risk{\ell}$ is a closed convex function, as the infimum of affine functions, and thus $(-\risk{\ell})^{**} = -\risk{\ell}$.
  Define $L:\reals^n\to\reals^\Y$ by $L(u) = C(u)\ones - u$, where $\ones\in\reals^\Y$ is the all-ones vector.
  As $C$ is polyhedral, so is $L$.
  We first show that $L$ embeds $\ell$, and then establish that the range of $L$ is in fact $\reals^\Y_+$, as desired.

  We compute Bayes risks and apply Proposition~\ref{prop:embed-bayes-risks} to see that $L$ embeds $\ell$.
  Observe that $\risk{\ell}$ is polyhedral as $\ell$ is discrete.
  For any $p\in\simplex$, we have
  \begin{align*}
    \risk{L}(p)
    &= \inf_{u\in\reals^n} \inprod{p}{C(u)\ones - u}\\
    &= \inf_{u\in\reals^n} C(u) - \inprod{p}{u}\\
    &= -\sup_{u\in\reals^n} \inprod{p}{u} - C(u)\\
    &= -C^*(p) = - (-\risk{\ell}(p))^{**} = \risk{\ell}(p)~.
  \end{align*}
  It remains to show $L(u)_y \geq 0$ for all $u\in\reals^n$, $y\in\Y$.
  Letting $\delta_y\in\simplex$ be the point distribution on outcome $y\in\Y$, we have for all $u\in\reals^n$, $L(u)_y \geq \inf_{u'\in\reals^n} L(u')_y = \risk{L}(\delta_y) = \risk{\ell}(\delta_y) \geq 0$, where the final inequality follows from the nonnegativity of $\ell$.
\end{proof}




\section{Consistency via Separated Links \raf{revisit title?}}
\label{sec:calibration}

We have now seen the tight relationship between polyhedral losses and embeddings; in particular, every polyhedral loss embeds some discrete loss.
The embedding itself tells us how to link the embedded points back to the discrete reports (map $\varphi(r)$ to $r$).
But it is not clear how to extend this to yield a full link function $\psi: \reals^d \to \R$, and whether such a $\psi$ can lead to consistency.
In this section, we prove Theorem~\ref{thm:link-main}, restated below, which gives a construction to generate calibrated links for \emph{any} polyhedral surrogate.

\linkinformal*
% \begin{theorem}\label{thm:eps-thick-calibrated}
%   Let $L$ be polyhedral and $\ell$ the discrete loss it embeds from Theorem~\ref{thm:poly-embeds-discrete}.
%   Then there exists a calibrated link function $\psi$ from $L$ to $\ell$.
% %Then for small enough $\epsilon > 0$, the $\epsilon$-thickened link $\psi$ is well-defined and, furthermore, is a calibrated link from $L$ to $\ell$.
% \end{theorem}

Theorem \ref{thm:link-main} will follow immediately from Theorems \ref{thm:calibrated-separated} and \ref{thm:thickened-separated}, as discussed below.
Their full proofs appear in \S~\ref{sec:equiv-sep-calib} and \ref{app:sep-link-exists} respectively.

Theorem \ref{thm:calibrated-separated} shows that calibration is equivalent to a geometric condition, which we call \emph{separation}, of a link function $\psi$.
Recall that for indirect elicitation, any point $u \in \Gamma(p)$ must link to a report $\psi(u) \in \gamma(p)$. \jessiet{We don't introduce indirect elicitation.  How do we want to address? - May 3, 22}
(In terms of losses, $u$ minimizing expected $L$-loss implies that $\psi(u)$ minimizes expected $\ell$-loss, with respect to $p$.)
The idea of separation is that points in the neighborhood of $u$ must also link to to a report in $\gamma(p)$.
Furthermore, there must be a uniform lower bound $\epsilon$ on the size of any such neighborhood.

\begin{definition}[Separated Link]\label{def:sep-link}
  Let properties $\Gamma:\simplex\toto\reals^d$ and $\gamma:\simplex\toto\R$ be given.
  We say a link $\psi:\reals^d\to\R$
  is \emph{$\epsilon$-separated with respect to $\Gamma$ and $\gamma$} if for all $u\in\reals^d$ with $\psi(u)\notin\gamma(p)$, we have $d_\infty(u,\Gamma(p)) \geq \epsilon$, where $d_\infty(u,A) \doteq \inf_{a\in A} \|u-a\|_\infty$.%
  \footnote{\citet{frongillo2021surrogate} define $\epsilon$-separation with a strict inequality $d_\infty(u,\Gamma(p)) > \epsilon$; we adopt a weak inequality as it is more convenient in examples. \raf{I felt we should write something to this effect here, but I don't love my attempt..}}
  Similarly, we say $\psi$ is $\epsilon$-separated with respect to $L$ and $\ell$ if it is $\epsilon$-separated with respect to $\prop{L}$ and $\prop{\ell}$.
\end{definition}

\begin{restatable}{theorem}{calibratedseparated} \label{thm:calibrated-separated}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given.
  Then $(L,\psi)$ is calibrated with respect to $\ell$ if and only if
  $\psi$ is $\epsilon$-separated with respect to $L$ and $\ell$ for some
  $\epsilon>0$.
\end{restatable}
\bo{Say anything about proof of above theorem?}
\raf{We definitely should for the journal submission!  Punt for now.}

To prove Theorem \ref{thm:link-main}, it now suffices to show that for any polyhedral $L$ embedding some $\ell$, there exists a \emph{separated} link $\psi$ with respect to $L$ and $\ell$.
Such separated link is given by Construction~\ref{const:eps-thick-link} below.

\restatehack{
  \begin{theorem}
    \label{thm:calibrated-separated}
    \label{thm:thickened-separated}
  \end{theorem}}

\begin{restatable}{theorem}{thickenedseparated} \label{thm:thickened-separated}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$ embed the discrete loss $\ell:\R\to\reals^\Y_+$.
  Then there exists $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, Construction~\ref{const:eps-thick-link} yields an $\epsilon$-separated link with respect to $L$ and $\ell$.
\end{restatable}

To set the stage for Construction \ref{const:eps-thick-link}, we sketch the two main steps in proving Theorem \ref{thm:thickened-separated}: \emph{(a)} showing that one can produce a link $\psi$ such that $(L,\psi)$ indirectly elicits $\ell$; \emph{(b)} ``thickening'' $\psi$ such that it is separated.

For \emph{(a)}, begin by linking each embedding point back to its original report.
Now we must determine $\psi(u)$ for non-embedding points.
%Symbolically, if $u^* \in \argmin_u \inprod{p}{L(u)} = \Gamma(p)$, then $\psi(u^*) \in \argmin_r \inprod{p}{\ell(r)} = \gamma(p)$.
The challenge is that we may have $u \in \Gamma(p) \cap \Gamma(p')$.
Because $u$ minimizes expected surrogate loss for both $p$ and $p'$, the link must satisfy $\psi(u) \in \gamma(p) \cap \gamma(p')$.
It is not even clear \emph{a priori} that these sets intersect.
We use the definition of embedding and elicitation results, discussed in \S~\ref{sec:min-rep-sets}, to show that for each such $u$ there exists $r \in \R$ such that $\Gamma_u \subseteq \gamma_r$, i.e. any $p$ satisfying $u \in \Gamma(p)$ also satisfies $r \in \gamma(p)$.
This implies that if $u \in \Gamma(p) \cap \Gamma(p')$, then there exists $r \in \gamma(p) \cap \gamma(p')$, so we may safely choose $\psi(u) = r$.
\jessiet{We write earlier that embedding is a special case of indirect elicitation, so a reader might be confused why (a) is not immediate.  I think this paragraph is saying that it is, but tries to provide intuition for why, right? - May 3, 22}

For \emph{(b)}, we show that this link can be ``thickened'' by some positive $\epsilon$, as described next.
For any $p \in \simplex$, consider the optimal surrogate report set, i.e.\ set of the form $U = \Gamma(p) = \argmin_u \inprod{p}{L(u)}$.
By indirect elicitation, $\psi$ is already correct on $U$.
Now, we ``thicken'' $U$ to obtain $U_{\epsilon} = \{u : \|u - U\| \leq \epsilon\}$.
Then we require that all points in $U_{\epsilon}$ are linked to some element of $\gamma(p) = \argmin_r \inprod{p}{\ell(r)}$.
For $\epsilon > 0$, this directly implies separation.

However, it is not clear that this linking is possible because a point $u$ may be in multiple thickened sets $U_{\epsilon}, U'_{\epsilon}$, etc.
Therefore, we need to take each possible collection $U,U'$, etc. and thicken their intersection in an analogous way.

Given $u \in U_\epsilon \cap U'_\epsilon \cap \dots$, we define a \emph{link envelope} $\Psi(u)$ which encodes the remaining legal choices for $\psi(u)$ after imposing the requirements for each such set $U_\epsilon,U'_\epsilon$, etc.
The key claim is that, for small enough $\epsilon > 0$, $\Psi(u)$ is nonempty: at least one permitted value for $\psi(u)$ remains.
This claim follows from a geometric result (Lemma \ref{lemma:thick-empty}) that, for all small enough $\epsilon$, a subset of thickenings $U_{\epsilon}$ intersect if and only if the $U$ sets themselves intersect.
When they do intersect, indirect elicitation implies that there exists a permitted choice of link for the intersection of the thickenings.
It is also important that, by Lemma~\ref{lem:polyhedral-range-gamma}, for polyhedral surrogates there are only finitely many sets of the form $U = \Gamma(p)$.
This yields a single uniform smallest $\epsilon$ such that the key claim is true for all $u \in \reals^d$.
%Given a well-defined thickened link $\psi$, calibration is almost immediate because, given $p$, any report $u$ linking to a suboptimal $r = \psi(u)$ satisfies $\inf_{u^* \in \Gamma(p)} \|u^* - u\| \geq \epsilon$.

Given the above proof sketch, the following construction is relatively straightforward.
We initialize the link using the embedding points and optimal report sets, then use $\Psi$ to narrow down to only legal choices; we then pick from $\psi(u)$ from $\Psi(u)$ arbitrarily.
Theorem \ref{thm:thickened-separated} implies that, for all small enough $\epsilon$, the resulting link $\psi$ is well-defined at all points.
\begin{construction}[$\epsilon$-thickened link] \label{const:eps-thick-link}
  Given a polyhedral $L$ that embeds some $\ell$, an $\epsilon > 0$, and a norm $\|\cdot\|$, the \emph{$\epsilon$-thickened link} $\psi$ is constructed as follows.
  First, define $\U = \{\Gamma(p) : p \in \simplex\}$.
  % For each $U \in \U$, let $R_U = \{r \in \R : \varphi(r) \in U\}$, the reports whose embedding points are in $U$. \jessiet{Move this sentence below link envelope definition?}
  First, initialize the \emph{link envelope} $\Psi: \reals^d \toto \R$ by setting $\Psi(u) = \R$ for all $u$.
  Then for each $U \in \U$, for all points $u$ such that $\inf_{u^* \in U} \|u^*-u\| < \epsilon$, update $\Psi(u) = \Psi(u) \cap R_U$, where $R_U = \{r \in \R : \varphi(r) \in U\}$ is the set of target reports that embed into $U$.
  Finally, define $\psi(u) \in \Psi(u)$, breaking ties arbitrarily.
  If $\Psi(u)$ became empty, then leave $\psi(u)$ undefined.
\end{construction}

\paragraph{Remarks.}
Construction~\ref{const:eps-thick-link} is not necessarily computationally efficient as the number of labels $n$ grows.
In practice this potential inefficiency is not typically a concern, as the family of losses typically has some closed form expression in terms of $n$, and thus the construction can proceed at the symbolic level.
We illustrate this formulaic approach in \S~\ref{sec:abstain}.

Applying the $\epsilon$-thickened link construction additionally enables one to verify the consistency of a proposed link $\psi^*$.
For a given $\epsilon$ and norm $\|\cdot\|$, suppose one follows the routine of Construction~\ref{const:eps-thick-link} until the last step in which values for the link $\psi$ are selected.
Instead, we can simply test whether the proposed link values are contained in the valid choices, i.e., if $\psi^*(u) \in \Psi(u)$ for all $u\in\reals^d$.
If so, then the proposed link $\psi^*$ is calibrated.
See \S~\ref{sec:top-k} for an illustration of this test.

\botodo{Should we comment on computing the $\epsilon$?}
\raft{Don't recall what we have to say... but this would be a great spot to do so.  Or at least make the point that the regret bounds (below) get better the higher $\epsilon$ is.}

\paragraph{A converse to Theorem \ref{thm:thickened-separated}.}
\raf{Started but did not finish; in particular, did not prove the proposition.  I'd suggest taking the last paragraph of the proof of Theorem~\ref{thm:thickened-separated} and making that one direction of the proof of this proposition, and then using the proposition to prove the theorem.}
Perhaps surprisingly, one can also show that \emph{every} calibrated link from a polyhedral surrogate to a discrete loss is produced by Construction \ref{const:eps-thick-link}.
This result follows from the fact, stated now, that Construction~\ref{const:eps-thick-link} with $\|\cdot\|_\infty$ is exactly enforcing $\epsilon$-separation; from Theorem~\ref{thm:calibrated-separated}, every calibrated link is therefore yielded by the construction for some sufficiently small $\epsilon$.

\begin{proposition}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$ and discrete loss $\ell:\R\to\reals^\Y_+$ be given.
  Let $\epsilon>0$ be given, and let $\Psi$ be the link envelope from Construction~\ref{const:eps-thick-link} for $\epsilon$ and $\|\cdot\|_\infty$.
  Then $\psi$ is $\epsilon$-separated if and only if $\psi(u) \in \Psi(u)$ for all $u\in\reals^d$.
\end{proposition}

\subsection{Surrogate regret bounds}\label{subsec:regret-bounds}
Recall that the goal of surrogate risk minimization is to learn a hypothesis $h$ that minimizes expected surrogate loss, then output hypothesis $\psi \circ h$, which hopefully minimizes expected target loss.
One may formalize this statement in terms of \emph{regret}, as follows.
Fix a data distribution $\D$.
The \emph{surrogate regret} of $h$, and the \emph{target regret} of the implied hypothesis $\psi \circ h$, are given by
\begin{align*}
  R_L(h;\D) &= \E_{(X,Y)\sim\D} L(h(X))_Y - \inf_{h':\X\to\reals^d} \E_{(X,Y)\sim\D} L(h'(X))_Y~,
  \\
  R_\ell(\psi\circ h;\D) &= \E_{(X,Y)\sim\D} \ell(\psi(h(X)))_Y - \inf_{h':\X\to\R} \E_{(X,Y)\sim\D} \ell(h'(X))_Y~,
\end{align*}
respectively,
% $R_L(h;\D) = \E_{(X,Y)\sim\D} L(h(X))_Y - \inf_{h':\X\to\reals^d} \E_{(X,Y)\sim\D} L(h'(X))_Y$,
where the infimum is taken over all measurable functions.
(One may equivalently assume the Bayes optimal hypothesis is in the function class.)
% Given $h$, the \emph{target regret} of the implied hypothesis $\psi \circ h$ is $R_\ell(\psi\circ h;\D) = \E_{(X,Y)\sim\D} \ell(\psi(h(X)))_Y - \inf_{h':\X\to\R} \E_{(X,Y)\sim\D} \ell(h'(X))_Y$.
In these terms, then, consistency means that if the surrogate regret of $h$ converges to zero, then the target regret of $\psi \circ h$ does as well; in other words, $R_L(h;\D) \to 0$ implies $R_{\ell}(\psi \circ h;\D) \to 0$.

Consistency is therefore a minimal requirement; in general, we are also interested in the \emph{rate} at which the target regret diminishes, as a function of the number of data points $n$.
Surrogate regret bounds are useful in answering this question, in that they show how a surrogate rate translates to a target rate.
We show that, for any polyhedral surrogate, the transfer is linear: if surrogate regret diminishes at a rate of $O(f(n))$, then the target rate is also $O(f(n))$.
In particular, \emph{fast} convergence in surrogate regret implies fast convergence in target regret.
\begin{theorem}
  Let $(L,\psi)$ be consistent for a discrete loss $\ell$, and $L$ polyhedral.
  Then there exists $c > 0$ such that, for all hypotheses $h$ and data distributions $\D$, we have $R_{\ell}(\psi \circ h;\D) \leq c \cdot R_L(h;\D)$.
\end{theorem}
In the proof (Appendix~\ref{app:regret-bounds}), we further show that the constant $c$ can be decomposed in terms of three constants, which depend on $L$, $\psi$, and $\ell$, respectively.
Specifically, we may write $c = C_\ell H_L / \epsilon_\psi$, where $H_L$ is the Hoffman constant for $L$, $\epsilon_\psi$ the separation of $\psi$, and $C_\ell$ the maximum loss gap of $\ell$.
See Appendix~\ref{app:regret-bounds} for details.
We also refer the reader to \citet{frongillo2021surrogate} for a discussion on how to further tighten this result, as well as a quadratic lower bound on the rate transfer for sufficiently non-polyhedral surrogates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to Specific Surrogates}\label{sec:applications}

%% overview of the applications
Our results give a framework to construct consistent polyhedral surrogates and link functions for any discrete target loss, as well as to verify consistency or inconsistency for specific surrogate and link pairs.
Below, we illustrate the power of this framework with specific examples from the literature.
To warm up, we study the abstain surrogate given by~\citet{ramaswamy2018consistent}, which is an embedding, and show how to rederive their link function and surrogate regret bounds (\S~\ref{sec:abstain}). 
We then give three examples of subsequent works that use our framework, in the context of structured binary classification (\S~\ref{sec:lovasz-hinge}), multiclass classification (\S~\ref{sec:winge}), and top-$k$ classification (\S~\ref{sec:top-k}).
In all cases, our framework illuminates the behavior of inconsistent surrogates by revealing the discrete losses they embed, i.e., the true targets for which they are consistent.
In structured binary classification and top-$k$ classification, our framework also gives new consistent surrogates and link functions which appear challenging to derive otherwise.

%% how to apply the embedding framework
% When using our framework to study the (in)consistency of an existing surrogate $L:\reals^d \to \reals^\Y_+$, often the first step is determining the loss it embeds $\ell_{\mathrm{embed}}$, in order to compare $\prop{\ell_{\mathrm{embed}}}$ to the property elicited by the proposed target $\prop{\ell}$.
% To this end, we suggest the following general approach.
% First, for each $y\in\Y$, divide $\reals^d$ into a finite number of polyhedral regions on which $L(\cdot)_y$ is an affine function.
% %Often these regions are straightforward to identify based on the form of $L$.\jessie{tbh prev sentence feels like a lie lol}
% Letting $\Sc\subset\reals^d$ be the union of vertices of these polyhedral regions, conclude that $\Sc$ is a finite representative set for $L$~\citep[Statement 3.1.7]{grunbaum2013convex}.
% By Proposition~\ref{prop:representative-embeds-restriction}, $L$ embeds $L|_\Sc$.
% From here one can further remove redundant reports until one arrives at a tight embedding if desired.
% Once the embedded discrete loss is known, the behavior of the surrogate becomes more clear: what problem it is solving, and for which restrictions on label distributions is it calibrated for the original problem (\S~\ref{sec:top-k}, \ref{sec:winge}).
% Moreover, while embedding guarantees the existence of a calibrated link function, Construction~\ref{const:eps-thick-link} provides a framework to both verify proposed link functions and construct new ones.
% A key step in Construction~\ref{const:eps-thick-link} is constructing the set-valued link candidate function $\Psi$, which yields the possible values any $\epsilon$-separated link $\psi$ could map to; this in turn provides a tool to verify the calibration of a proposed link (\S~\ref{sec:abstain}, \ref{sec:top-k}) or construct new calibrated links(\S~\ref{sec:lovasz-hinge}).

\subsection{Applying the embedding framework}
When using our framework to study the (in)consistency of an existing surrogate $L:\reals^d \to \reals^\Y_+$, often the first step is determining the loss it embeds.
To this end, we suggest the following general approach.
First, for each $y\in\Y$, divide $\reals^d$ into a finite number of polyhedral regions on which $L(\cdot)_y$ is an affine function.
Second, identify the vertices of these polyhedral regions.%
\footnote{In some cases, these regions do not have vertices, such as the top-$k$ surrogates which are invariant in the all-ones direction; here one can restrict to a subspace, or otherwise select among equivalent reports.}
Third, conclude that the union of these vertices, $\Sc\subset\reals^d$, is a finite representative set for $L$.
% ~\citep[Statement 3.1.7]{grunbaum2013convex}.
Now $L$ embeds $L|_\Sc$ from Proposition~\ref{prop:representative-embeds-restriction}.
From here one can further remove redundant reports until one arrives at a tight embedding if desired.
Once the embedded discrete loss is known, the behavior of the surrogate becomes more clear: what problem it is solving, and for which restrictions on label distributions is it consistent for the original problem.

\raf{Follow-up paragraph on prev sentence using abstain as an example}

%% also interesting to use Sec 4 to verify proposed canonical links
\raft{Rework so emphasis is more on the general construction and less on testing a given link}
With an embedding in hand, Construction~\ref{const:eps-thick-link} provides a calibrated link function.
Yet for some target problems, the search for consistent surrogates has been restricted to those accommodating a particular canonical link function, such as $k$ largest coordinates of the surrogate report in top-$k$ classification (\S~\ref{sec:top-k}).
Interestingly, our construction is also useful in this situation, where one wishes to verify the consistence of a given proposed link $\psi$. \jessiet{Redundant from \S 4 but I think that's okay.  I like it in both spots}
Recall that Construction~\ref{const:eps-thick-link} produces a set-valued link envelope $\Psi$, which yields the possible values any $\epsilon$-separated link $\psi$ could map to.
If the given $\psi$ is indeed consistent, then it is $\epsilon$-separated for sufficiently small $\epsilon$, so one can always construct such a $\Psi$ and verify that $\psi(u) \in \Psi(u)$ for all $u\in\reals^d$.
More generally, while such canonical link functions may be intuitive for a given problem, our results suggest that researchers should consider setting them aside and instead let Construction~\ref{const:eps-thick-link} determine the link.
See \S~\ref{sec:lovasz-hinge} for a somewhat intricate example.



\subsection{Consistency of abstain surrogate and link construction}
\label{sec:abstain}

Several authors consider a variant of multiclass classification, with the addition of an \emph{abstain} option~\citep{bartlett2008classification,ramaswamy2018consistent,madras2018predict,elyaniv2010foundations,cortes2016learning}.
\raf{Proposal: define $\ell$ for $1/2$, $L$ for BEP, $\psi^\infty$ for their link (bake in 1/2), $\psi^1$ for ours}
For $\alpha \in (0,1)$, \citet{ramaswamy2018consistent} study the loss $\ell^\alpha : [n] \cup \{\bot\} \to \reals^\Y_+$ defined by $\ell^\alpha(r)_y = 0$ if $r=y$, $\alpha$ if $r = \bot$, and 1 otherwise.
%\raft{Why do we have $\ell^\alpha$, $\psi^\alpha$ (superscripts) but $L_\alpha$ (subscript)?  Ideally we could make them uniform, but I wasn't sure what the considerations were}
%\jessiet{No good reason, at the very least. I changed it.  \#burn}
%\begin{align}\label{eq:abstain-discrete}
%$\ellabs{\alpha}(r)_y = \begin{cases}
%0 & r = y\\
%\alpha & r = \bot\\
%1 & \text{otherwise}
%\end{cases}~.$
%\end{align}
The report $\bot$ corresponds to ``abstaining'' for a constant loss regardless of outcome $y$. %: specifically, if no $y\in\Y$ has $p_y \geq 1-\alpha$.
For the case $\alpha=1/2$,
\citeauthor{ramaswamy2018consistent} provide a polyhedral surrogate $L^{1/2}$, which they call the \emph{binary encoded predictions (BEP)} surrogate, and link $\psi^{1/2}$ which are calibrated for $\ell^{1/2}$.
Letting $d = \ceil{\log_2(n)}$, their surrogate $L^{1/2} : \reals^d \to \reals^\Y_+$ is given by
\begin{equation}\label{eq:abstain-surrogate}
L^{1/2}(u)_y = \max_{j \in [d]} \left(1 - \varphi(y)_j u_j\right)_+~,
\end{equation}
where $\varphi:[n]\to\{-1,1\}^d$ is an injection.%
\footnote{To translate our notation to that of \citet{ramaswamy2018consistent}, take $B = -\varphi$.}
% embeds outcomes to corners of the $\pm 1$ hypercube, and the abstain report $\bot$ to the origin.
Observe that $L^{1/2}$ is exactly hinge loss when $n=2$ and thus $d=1$. \jessiet{Remove this sentence?  or new paragraph below?}
The authors show that the link $\psi^{1/2}$ is calibrated\jessiet{IIRC, they actually show this a bit more generally.  basically, they show any $\epsilon$-separated link with $\| \cdot \|_\infty$ works.  Changing in text, but happy to revert.}, where
\begin{equation}\label{eq:abstain-link}
  \psi^{\epsilon}(u) = \begin{cases}
	\bot & \min_{i \in [d]} |u_i| \leq \epsilon\\
	\varphi^{-1}(\sgn(u)) &\text{otherwise}
  \end{cases}~,
\end{equation}
and they go on to establish linear surrogate regret bounds for $(L^{1/2},\psi^{1/2})$.

Using our framework, one can show that $L^{1/2}$ embeds (2 times) $\ell^{1/2}$, with the embedding given by $\varphi$ above where we define $\varphi(\bot) = 0 \in \reals^d$.
(Following the general procedure outlined above, the regions where $L^{1/2}$ is affine all have vertices in the set $\{-1,1\}^d \cup \{0\}$, meaning it is representative, and $L^{1/2}$ restricted to that set is precisely $2\ell^{1/2} \circ \varphi^{-1}$.)
%In light of our framework, we can see that $L^{1/2}$ is an excellent example of an embedding, where $\varphi(y) = B(y)$ and $\varphi(\bot) = 0 \in \reals^d$.
Moreover, as we illustrate in Figure~\ref{fig:abstain-links}(L), the link $\psi^{\epsilon}$ proposed by \citeauthor{ramaswamy2018consistent} can be recovered from Construction~\ref{const:eps-thick-link} by choosing the norm $\|\cdot\|_\infty$ for any sufficiently small choice of $\epsilon > 0$.
% and choosing $\psi^{1/2}(u) = \bot$ for all $u$ such that $\bot \in \Psi(u)$. 
%(For all $u$ such that $\bot \not \in \Psi(u)$, there is only one choice to link, e.g., $|\Psi(u)| = 1$.)
Hence, our framework could have simplified the process of finding $\psi^{1/2}$, and the corresponding proof of consistency and surrogate regret bounds (\S~\ref{subsec:regret-bounds}\raft{check whether our bounds match}).
% can be used to verify the calibration of the prposed link.
%\jessiet{changed this sentence.  Thoughts?}\raft{Changed it back, since I think our narrative here is that our framework would have simplified their paper}
To illustrate this point further, we give an alternate link $\psi^1$ corresponding to $\|\cdot\|_1$ and $\epsilon=1$, shown in Figure~\ref{fig:abstain-links}(R),
\begin{equation}\label{eq:abstain-link-1}
  \psi^1(u) = \begin{cases}
	\bot & \|u\|_1 \leq 1\\
	\varphi^{-1}(\sgn(u)) &\text{otherwise}
  \end{cases}~.
\end{equation}
Construction~\ref{const:eps-thick-link} gives calibration of $(L^{1/2},\psi^1)$ with respect to $\ell^{1/2}$. 
%\jessiet{does it?  do we know $\epsilon$ is small enough for higher $d$?}\raft{removed ``immediately'' since you're right there are details to work out, but I'm pretty sure $\epsilon=1$ should hold for all $d$}
Aside from its simplicity, one possible advantage of $\psi^1$ is that it assigns $\bot$ to much less of the surrogate space $\reals^d$\raft{Bo, we used to say ``it appears to yield the same constant in generalization bounds as $\psi$''but I think our paper implies that the bound is worse, right? EDIT 3/16/2022: if we add surrogate regret bounds to the paper, I think it could be fun and worth working out which link actually gives the better bound, at least from our best analysis.}

% Thinking of the abstain loss $\ell^{1/2}$ as a binary structured prediction problem, \citeauthor{ourlovaszpaper} observe that the BEP surrogate $L^{1/2}$ in eq.~\eqref{eq:abstain-surrogate} is one example of a Lov\'asz hinge introduced in \S~\ref{sec:lovasz-hinge}.
% , and that $\psi^{1/2}$ in eq.~\eqref{eq:abstain-link} is an example of a calibrated link given by Construction~\ref{const:eps-thick-link} for the embedded loss.



\begin{figure}
\begin{center}
\begin{minipage}{0.32\linewidth}
\includegraphics[width=\linewidth]{tikz/abstain-link-linf.pdf}
\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
\includegraphics[width=\linewidth]{tikz/abstain-link-U-regions-l1.pdf}
\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
\includegraphics[width=\linewidth]{tikz/abstain-link-l1.pdf}
\end{minipage}\hfill
\caption{Constructing links for the abstain surrogate $L^{1/2}$ with $d=2$. The embedding is shown in bold labeled by the corresponding reports. (L) The link envelope $\Psi$ resulting from Construction~\ref{const:eps-thick-link} using $\|\cdot\|_\infty$ and $\epsilon = 1/2$, and a possible link $\psi$ which matches eq.~\eqref{eq:abstain-link} from~\cite{ramaswamy2018consistent}.  (M) An illustration of the thickened sets from Construction~\ref{const:eps-thick-link} for two sets $U, U' \in \U$, using $\|\cdot\|_1$ and $\epsilon = 1$. (R) The $\Psi$ and $\psi$ from Construction~\ref{const:eps-thick-link} using $\|\cdot\|_1$ and $\epsilon = 1$.}
\label{fig:abstain-links}
\end{center}
\end{figure}


\subsection{Lov\'asz hinge and the structured abstain problem}
\label{sec:lovasz-hinge}

\newcommand{\dis}{\mathrm{dis}}
\newcommand{\abs}{\mathrm{abs}}

%% what is structured prediction + some gap
Many structured prediction settings can be thought of as making multiple predictions at once, with a loss function that jointly measures error based on the relationship between these predictions~\cite{hazan2010direct, gao2011consistency, osokin2017structured}.
In the case of $k$ binary predictions, these settings are typically formalized by taking the predictions and outcomes to be $\R=\Y=\{-1,1\}^k$, with the $i$th coordinate giving the result for the $i$th binary prediction.
A natural family of losses are those which are functions of the misprediction or disagreement set $\dis(r,y) = \{i \in [k] \mid r_i \neq y_i\}$, meaning we may write $\ell^f(r)_y = f(\dis(r,y))$ for some set function $f:2^{[k]}\to\reals$.
For example, Hamming loss is given by $f(S) = |S|$.
In an effort to provide a general convex surrogate for these settings when $f$ is a submodular function, Yu and Blaschko~\cite{yu2018lovasz} introduce the \emph{Lov\'asz hinge} surrogate $L^f:\reals^k\to\reals^\Y_+$ which leverages the well-known convex Lov\'asz extension of submodular functions.
While the authors provide theoretical justification and experiments, they leave open whether the Lov\'asz hinge actually is consistent for $\ell^f$.

%% lovasz paper closes the gap, answering inconsistency wrt \ell^f by showing that L^f embeds structured abstain
\citet{ourlovaszpaper} use our embedding framework to resolve the consistency of $L^f$, showing that it is inconsistent with respect to $\ell^f$ outside of the trivial case where $f$ is modular, and thus $\ell^f$ is a weighted Hamming loss.
Moreover, they show that $L^f$ embeds a variant $\ellabs$ of $\ell^f$ where one is allowed to abstain on a set of indices $A \subseteq [k]$, which they call the \emph{structured abstain problem}.
The inclusion of abstain options is natural when observing that the BEP surrogate $L^{1/2}$ from \S~\ref{sec:abstain} coincides with $L^f$ for the function $f(S) = \ones\{S \neq \emptyset\}$, so the multiclass abstain problem is a special case of the Lov\'asz hinge.

To derive $\ellabs$, the authors show that the set $\V = \{-1,0,1\}^k$ is representative for $L^f$, for any choice of $f$.
From Proposition~\ref{prop:representative-embeds-restriction}, they conclude that $L^f$ embeds $\ellabs := L^f|_{\V}$.
Letting $\abs(v) = \{i\in[k] \mid v_i = 0\}$ denote the ``abstain'' set, we may write $\ellabs : \V \to \reals^\Y_+$ as
\begin{equation}
	\ellabs(v)_y = f(\dis(v,y) \setminus \abs(v)) + f(\dis(v,y))~.
\end{equation}
(Observe that $\abs(v,y) \subseteq \dis(v,y)$, since $y\in\{-1,1\}^k$.)
%They go on to show that the embedding is tight, for $f$ strictly submodular and strictly monotone, if one first disallows $v$ with $|\abs(v)| = 1$.
By Theorem~\ref{thm:link-main}, then, there is a link function such that the Lov\'asz hinge is consistent with respect to the structured abstain loss $\ellabs$.

Actually determining this link function is nontrivial.
Simple threshold links like for the BEP surrogate in \S~\ref{sec:abstain} are not always calibrated, thus casting doubt that a trial-and-error approach for finding the link would be successful.
Instead, they leverage our thickened link construction (Construction~\ref{const:eps-thick-link}) to derive two links $\psi^*$ and $\psi^\diamond$, which have somewhat intricate geometric structure (Figure~\ref{fig:lovasz-links}).
Perhaps surprisingly, by deriving the link envelope $\Psi$ which is contained in the envelopes for $L^f$ for all submodular and increasing $f$, they prove that $\psi^*(u) \subseteq \Psi(u)$ and $\psi^\diamond(u) \subseteq \Psi(u)$ for all $u \in \reals^d$.
Thus, both $(L^f, \psi^*)$ and $(L^f, \psi^\diamond)$ are simultaneously calibrated with respect to $\ellabs$ for all such $f$.

\begin{figure}
	\begin{center}
		\begin{minipage}{0.48\linewidth}
			\includegraphics[width=\linewidth]{tikz/lovasz-link-psi-star.pdf}
		\end{minipage}\hfill
		\begin{minipage}{0.48\linewidth}
			\includegraphics[width=\linewidth]{tikz/lovasz-link-psi-diamond.pdf}
		\end{minipage}\hfill
		\caption{Links $\psi^*$ and $\psi^\diamond$ such that $(L^f, \psi^*)$ and $\psi^\diamond$ are calibrated with respect to $\ellabs$ for all suitable $f$. Points in the dark green regions link to $\vec 0$; points in light green link to a permutation of $(0,\pm 1)$, and points in yellow regions link to $(\pm 1, \pm 1)$.\jessie{Not sure how to phrase this?  Would we just be better off explicitly labeling things?}\raf{Agreed that it's a little unclear, but not sure labeling would be better... let's discuss next call}}
		\label{fig:lovasz-links}
	\end{center}
\end{figure}

\subsection{Embedding ordered partitions via Weston-Watkins hinge}
\label{sec:winge}
As the hinge loss is one of the most common surrogates for binary support vector machines (SVMs), original extensions to the multiclass setting included a one-vs-all reduction to the binary problem via hinge loss, generating ${n \choose 2}$ hyperplanes for $n$ labels.
Proposing a more efficient solution, \citet{weston1999support} give an alternate surrogate for multiclass SVM prediction, defined as follows for predictions $u \in \reals^n$,
\begin{equation}\label{eq:ww-hinge}
L^{WW}(u)_y = \sum_{i \in \Y : i \neq y} (1 - (u_y - u_i))_+~,~
\end{equation}
which was later shown to be inconsistent with respect to 0-1 loss~\citep{tewari2007consistency,liu2007fisher}.

\citet{wang2020weston} use the embedding framework to show that the Weston--Watkins hinge embeds the \emph{ordered partition} loss, and in turn, recover the result of inconsistency with respect to 0-1 loss.
%An ordered partition on $[n] := \{1, \ldots, n\}$ can be defined by a nested sequence of tuples
%Let $T = \{T_1, \ldots, T_k\}$
The report space for this discrete loss can be defined in terms of nested subsets of $[n] := \{1, \ldots, n\}$, as follows.%
\footnote{To recover the partition of~\citet{wang2020weston}, one can define $S_i = T_i \setminus T_{i-1}$.}
\begin{align*}
\T = \{ (T_0,\ldots,T_s) \mid s \geq 1, \emptyset = T_0 \subsetneq T_1 \subsetneq \ldots \subsetneq T_s = [n]\}~.
\end{align*}
\noindent
The ordered partition target loss $\ell^{\OP} : \T \to \reals^\Y_+$ embedded by Weston-Watkins hinge is then defined
\begin{align*}
\ell^\OP(T)_y &= \sum_{i=1}^{s} \left(|T_{i}| \cdot \Ind{y \not \in T_{i-1}} \right) -1~.~
\end{align*}
\noindent
The ordered partition loss can be interpreted as a variation of 0-1 loss incorporating varying confidence in different outcomes: reports are a nested sequence of sets, and the punishment for the outcome $y$ is the cardinality of the first set containing $y$, plus the cardinality of all earlier sets.
%\citet{wang2020weston} show that $L^{WW}$ embeds $\ell^\OP$, with the embedding $\varphi : \T \to \reals^n$ given by $\varphi(T)_j = -(i-1)$ for all $j \in T_i \setminus T_{i-1}$.
%%\citeauthor{wang2020weston} then apply Theorem~\ref{thm:link-main} to conclude that there exists a link function $\psi$ such that $(L^{WW},\psi)$ is calibrated with respect to $\ell^\OP$; however, they do not construct the link. 


\raf{See if we can simplify / combine these paragraphs given the new 5.1}
%% one might make distributional assumptions to regain consistency
Upon showing that $L^{WW}$ embeds $\ell^\OP$, \citeauthor{wang2020weston} proceed to characterize $\prop{\ell^\OP}$ and use it to give sufficient distributions assumptions over labels in $\P \subseteq \simplex$ such that $L^{WW}$ and the canonical argmax link $\psi^1(u) : u \mapsto r \in \argmax_y \inprod{e_y}{u}$ are calibrated with respect to 0-1 loss on $\P \subseteq \simplex$ (i.e., that eq.~\ref{eq:calibrated} holds for all $p \in \P$).

%% how to understand level set simplex drawings
Sufficient constraints to recover consistency are characterized by comparing $\prop{\ell^\OP}$ and $\mode$: the property for which Weston--Watkins was originally proposed.
Figure~\ref{fig:ordered-partition} gives the cells $\{p \in \simplex \mid T \in \prop{\ell^\OP}(p)\}$ for each $T \in \T$, outlined in solid black.
These cells are juxtaposed with the cells $\{p \in \simplex \mid y \in \mode(p)\}$ for each $y \in \Y$, outlined in dashed blue, for which $L^{WW}$ was originally proposed as a surrogate.
Since each distribution in a cell of $\prop{\ell^\OP}$ corresponds to the same optimal report $T$, the choice of where to link that report must be constant.
Thus, if a cell of $\prop{\ell^\OP}$ is fully contained in a cell of $\mode$, then the corresponding $\prop{\ell^\OP}$ value can be mapped to the corresponding $\mode$ value.
Conversely, if the relative interior of a cell of $\prop{\ell^\OP}$ corresponding to $T$ spans multiple mode cells, it becomes unclear to which report the set $T$ should be linked.
To recover consistency on $\P \subseteq \simplex$, it suffices that $\P$ excludes these cells, colored in blue in Figure~\ref{fig:ordered-partition}.
\jessiet{Commented out next sentence for redundancy - May 3, 22}
%Fixing the canonical argmax link $\psi^1$ in Figure~\ref{fig:ordered-partition}, it suffices to make assumptions on the data distributions $\P$ excluding cells are depicted in blue in order to have calibration of $(L^{WW}, \psi^1)$ with respect to 0-1 loss on $\P$.


\begin{figure}[H]
	\begin{minipage}{0.47\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{tikz/ordered-partition}
	\end{minipage}
	\hfill
	\begin{minipage}{0.5\linewidth}
		\caption{Level sets of $\prop{\ell^\OP}$ (solid lines)
			%the property elicited by the ordered partition loss and embedded by $L^{WW}$, 
			juxtaposed against the level sets of $\mode$ (dashed lines).
%		The level sets of the mode (for which $L^{WW}$ is proposed as a surrogate) are given by the cells formed by the dashed blue lines.
		The level sets of $\prop{\ell^\OP}$ whose relative interiors span multiple cells of the mode cannot be properly linked to the mode.
		Here, this is demonstrated as the report corresponding to the cell has highest partition has more than one element, where in the white cells, the ``highest'' element of the partition is well-defined.}
		\label{fig:ordered-partition}
	\end{minipage}
\end{figure}






\subsection{Surrogates for top-$k$ classification}
\label{sec:top-k}
% In settings like object recognition and information retrieval, the top-$k$ classification problem arises in which one predicts a set $S$ of size $k$, and upon realizing outcome $y$, receives loss $\elltopk(S)_y = \ones\{y\notin S\}$~\citep{lapin2015top, lapin2016loss, lapin2018analysis,yang2018consistency,berrada2018smooth,rastegari2011scalable,reddi2019stochastic}.
% Many of the existing surrogates for top-$k$ classification consider the following three desiderata: convexity, calibration, and piecewise linear (``hinge-like'') structure.
% Satisfying all three desiderata simultaneously had not been observed until the work of~\citet{finocchiaro2022consistenttopk}.\raft{Not sure you can use ``observed'' like that (came up a few times)}
% The surrogates that are convex and calibrated but not polyhedral often elicit the entire distribution $p \in \simplex$, and take the link to be the top $k$ elements of $p$.
% \citet{lapin2016loss} observes that this approach learns more information than desired and often requires more samples
% % for surrogate regret to approach target regret
% ~\citep{frongillo2021surrogate}.\jessiet{right?}
% \citet{yang2018consistency} propose a piecewise linear surrogate that is calibrated for top-$k$ classification, but is not convex, generally making optimization more difficult.
% Now consider the class of polyhedral surrogates whose calibration for top-$k$ prediction is unknown~\citep{yang2018consistency,lapin2016loss,lapin2018analysis}: Theorem~\ref{thm:poly-embeds-discrete} says that every polyhedral surrogate embeds some discrete loss, and Theorem~\ref{thm:embed-poly-main} states that embedding implies calibration by some link for this discrete loss; thus, we can use embedding to analyze the (non)calibration of existing polyhedral surrogates with respect to top-$k$ by comparing the embedded losses to top-$k$.
In settings like object recognition and information retrieval, the top-$k$ classification problem arises in which one predicts a set $S$ of $k$ labels, and given the true label $y$, receives loss $\elltopk(S)_y = \ones\{y\notin S\}$~\citep{lapin2015top, lapin2016loss, lapin2018analysis,yang2018consistency,berrada2018smooth,rastegari2011scalable,reddi2019stochastic}.
In the literature on surrogates for top-$k$ classification, one goal has been to find a surrogate satisfying the following three desiderata: convexity, consistency, and piecewise linear (``hinge-like'') structure.
\citet{yang2018consistency} show that a number of previously proposed polyhedral losses, i.e., those which are convex and hinge-like, are inconsistent.
They further suggest that perhaps no surrogate could satisfy all three properties.
%\raft{Also, I cut the smooth vs hinge-like discussion since we don't really need the full motivation -- saying ``these weirdos wanted to do XYZ; who knows why'' is good enough, since our point is that our framework is useful for whatever people are trying to do \jessie{makes sense! \#burn}}

\jessiet{Commenting out the paragraph below.  It's pretty redundant and I don't think it has a super direct point.}
%Now consider the class of polyhedral surrogates whose calibration for top-$k$ prediction is unknown~\citep{yang2018consistency,lapin2016loss,lapin2018analysis}: Theorem~\ref{thm:poly-embeds-discrete} says that every polyhedral surrogate embeds some discrete loss, and Theorem~\ref{thm:embed-poly-main} states that embedding implies calibration by some link for this discrete loss; thus, we can use embedding to analyze the (non)calibration of existing polyhedral surrogates with respect to top-$k$ by comparing the embedded losses to top-$k$.

\citet{finocchiaro2022consistenttopk} apply the general approach outlined above to each of the polyhedral surrogates shown to be inconsistent by \citeauthor{yang2018consistency}, and determine the target problems they do solve, i.e., the discrete losses they embed.
%\raft{Couldn't follow this sentence (commented out)}
% None of the studied surrogates embed top-$k$ but use the properties elicited by the embedded losses to characterize (tighter\jessiet{not sure how to say this.  better bounds than YK gave, which was just the abstain cell for all the surrogates}) sufficient assumptions on the data distribution in order be calibrated with respect to top-$k$, fixing the canonical argmax link $\psi^k$.
\jessiet{do we want to drop the table here or nah?}\raft{Yeah I'd say we should try it; it's at least helpful when discussing the constraints on the conditional label distributions, and besides, it's nice to have a figure for each section.  Sentence commented out below could be useful in the caption.}
% \citeauthor{finocchiaro2022consistenttopk} consider the polyhedral surrogates proposed by \citet[Table 1, cf. $\psi_2, \psi_3, \psi_4$]{yang2018consistency} and evaluate \emph{which discrete losses these surrogates embed}, if not top-$k$.
Each of the examined surrogates embeds a discrete loss which can be viewed as a variant of the top-$k$ problem, allowing the algorithm to express varying levels of ``confidence'' on the top $k$ labels or report less than $k$ labels.
The data distributions for which these optimal reports differ from the optimal top-$k$ reports are shown in Table~\ref{tab:loss-slices} with $n=4$ and $k \in \{2,3\}$.

\begin{table*}
	\centering
	\begin{tabular}{ccccc}
		& $\prop{\Li{2}}$ & $\prop{\Li{3}}$ & $\prop{\Li{4}}$ & $\prop{L^k}$\\
		\hline \hline
		\rotatebox[origin=c]{90}{$k = 2$\hspace*{-2.5cm}} & \input{tikz/n4-k2-psi2-slice} & \input{tikz/n4-k2-psi3-slice} & \input{tikz/n4-k2-psi4-slice} & \input{tikz/n4-k2-psi6-slice}\\ 
		\rotatebox[origin=c]{90}{$k = 3$\hspace*{-2.5cm}} & \input{tikz/n4-k3-psi2-slice} & \input{tikz/n4-k3-psi3-slice} & \input{tikz/n4-k3-psi4-slice} & \input{tikz/n4-k3-psi6-slice}\\ 
	\end{tabular}
	\caption{
		Visualizations of the properties elicited by the losses (embedded by) $\Li{2}$, $\Li{3}$, $\Li{4}$ studied by \citeauthor{yang2018consistency}, and $L^k$ in eq.~\eqref{eq:topk-embedding} with $n=4$ and $k \in \{2,3\}$, fixing $p_4 = 1/4$. 
		The dashed blue lines form cells whose elements are distributions $p$ corresponding to the same $u \in \prop{\elli k}(p)$ labeling the cell. 
		As the link $\psi$ must be deterministic, in order for $(\Li{k}, \psi)$ to be consistent with respect to $\elli k$, each cell outlined in black is fully contained in exactly one cell from the dashed blue lines. 
		Regions outlined in black that are filled in blue and cross the dashed blue lines suggest where deciding how to construct a link $\psi$ is ambiguous, as the top-$k$ elements of the optimal report $u$ are ambiguous.  
		White regions are therefore where the surrogate and any top-$k$ link are consistent when restricting to data distributions whose conditional distributions are contained here. 
		On the right, $L^k$ shows our proposed surrogate that is consistent for top-$k$ classification, demonstrated by no blue regions.  
		%Each of the cells corresponds to a subset of distributions where exactly $k$ reports are optimal. 
		\jessie{I honestly think this might be more clear in this paper if we remove the labels on the cells.  Thoughts?  UPDATE:Removed them, but can add them back easily.}
	}
	\label{tab:loss-slices}
\end{table*}

For example, consider one of the surrogates, $\Li {4}(u)_y = \left(1 - u_y + \frac 1 k \sum_{i=1}^k (u_{\setminus y})_{[i]}\right)_+$, where $u_{[i]}$ denotes the $i$th largest element of $u \in \reals^n$ and $n$ is the total number of labels;
the authors show that $\Li{4}$ embeds $\ell^{(4)}(T)_y = \tfrac {k+1} {k+1-|T|} \ones\{y\notin T\}$, where $T$ is a set of at most $k$ labels.
These embedded losses may therefore be useful in top-$k$ settings where choosing smaller sets may have some benefit, such as a search engine that can use unused space for advertisements.
Using the losses each proposed surrogate embeds, \citeauthor{finocchiaro2022consistenttopk} go on to derive constraints on the label distributions under which the proposed surrogates are actually consistent for top-$k$ classification which subsume previous constraints~\citep{yang2018consistency}.

Beyond analyzing the previously proposed surrogates, \citeauthor{finocchiaro2022consistenttopk} also use our framework to derive the first consistent polyhedral surrogate for $\elltopk$,
\begin{align}\label{eq:topk-embedding}
L^k(u)_y &= \max \left(u_{[1]}, \max_{m \in \{k+1, \ldots, n\}} \left[ 1 - \frac k m + \frac 1 m \sum_{i=1}^m u_{[i]}\right] \right)- u_y~.
\end{align}
That is, they show that indeed a surrogate exists satisfying convexity, consistency, and hinge-like structure.
In light of our framework, this fact is unsurprising: Theorems~\ref{thm:embed-poly-main} and~\ref{thm:link-main} imply that \emph{every} discrete loss has a consistent polyhedral surrogate.
This new surrogate $L^k$ is given directly by the construction from the proof of Theorem~\ref{thm:discrete-loss-poly-embeddable} and applying Theorem~\ref{thm:link-main} to obtain consistency.
While Theorem~\ref{thm:link-main} guarantees the existence of some consistent link function, the authors further ask whether the canonical argmax link function $\psi^k$, which returns the $k$ largest elements of $u$, is consistent\jessiet{calibrated?}.
They indeed confirm its consistency using our framework, showing that $\psi^k$ is $\epsilon$-separated for $L^k$ and $\elltopk$, for any $\epsilon \leq \frac 1 {2n}$ \citep[Theorem 4.4]{finocchiaro2022consistenttopk}.
% confirms that indeed $(L^k,\psi^k)$ is calibrated with respect to $\elltopk$ for this link $\psi^k$, by showing that $\psi^k$ is an $\epsilon$-separated link for $\elltopk$ with any $\epsilon \leq \frac 1 {2n}$ via the construction of the link envelope $\Psi$ in Construction~\ref{const:eps-thick-link}.


% See Figure~\ref{fig:top-k} for the properties elicited by the embedded losses for previous surrogates~\citep{lapin2016loss,yang2018consistency}, and how they compare to $\prop{\elltopk}$ with $n = 3$ and $k = 2$.
%%In order to verify calibration of the canonical link they verify, for all $u \in \reals^n$, the proposed $\psi(u) \in \Psi(u)$, and thus yields an $\epsilon$-separated link.

%\begin{figure}
%	\begin{center}
%		\begin{minipage}{0.32\linewidth}
%			\includegraphics[width=\linewidth]{tikz/top-k-L2.pdf}
%		\end{minipage}\hfill
%		\begin{minipage}{0.32\linewidth}
%			\includegraphics[width=\linewidth]{tikz/top-k-L2.pdf}
%		\end{minipage}\hfill
%		\begin{minipage}{0.32\linewidth}
%			\includegraphics[width=\linewidth]{tikz/top-k-L4.pdf}
%		\end{minipage}\hfill
%		\caption{The properties elicited by previous polyhedral surrogates ($\psi_2, \psi_3, \psi_4$ in \citep[Table 1]{yang2018consistency}, respecitvely) with $n = 3, k = 2$. 
%		The dashed lines in each simplex denote $\prop{\elltopk}$, and blue regions represent cells where the top-$k$ elements of $\prop{L}$ are ambiguous, and therefore calibration cannot be guaranteed by the argmax link $\psi^k$.
%		In this setting, the third surrogate is calibrated via $\psi^k$ with respect to $\elltopk$, though this does not hold for general $k,n$.
%		\jessie{revisit}}
%		\label{fig:top-k}
%	\end{center}
%\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Structure of Embeddings}
\label{sec:min-rep-sets}

We have shown in \S~\ref{sec:poly-loss-embed} a tight connection between embeddings and polyhedral losses.
Here we go beyond polyhedral losses, showing a more general necessary condition for an embedding: a surrogate embeds a discrete loss if and only if it has a polyhedral Bayes risk, or equivalently, a finite representative set (Lemma~\ref{lem:X}).
This result implies that the embedding condition simplifies to matching Bayes risks (Proposition~\ref{prop:embed-bayes-risks}).
It also reveals some deeper structure of embeddings, even down to the geometry of the underlying properties, and the equivalence of various notions of non-redundant predictions \jessiet{What does this sentence mean?}.
In particular, we study a natural notion of a ``trimmed'' loss function (Definition~\ref{def:trim-loss}), and connect this definition to both tight embeddings and non-redundancy from property elicitation (Proposition~\ref{prop:embed-iff-trims-equal}).
 

%\raf{Add somewhere: initially explored by~\citet{wang2020weston} under the name \emph{embedding cardinality} }
%\jessie{mentioned it right under the definition of representative set way earlier}

\subsection{Structure of polyhedral Bayes risks}

%% generalizing from structure of polyhedral surrogates to surrogates with polyhedral risk (and why not necessarily the same)
While we have focused on polyhedral losses thus far, many of our results about embeddings extend to losses with polyhedral Bayes risks, a weaker condition.
(We say a concave function is polyhedral if its negation is a polyhedral convex function.)
To see that every polyhedral loss has a polyhedral Bayes risk, recall that Theorem~\ref{thm:poly-embeds-discrete} constructs a finite representative set $\Sc$ for any polyhedral loss $L$, and thus $\risk{L} = \risk{L|_\Sc}$ by Lemma~\ref{lem:loss-restrict}, which is polyhedral.
The condition is strictly weaker: a Bayes risk may be polyhedral even if the loss itself is not.
For example, a modified hinge loss $L(r)_y = \max(r^2-1,1-ry)$
as shown in Figure~\ref{fig:modified-hinge}, which matches hinge loss on the interval $[-1,1]$ but is strictly convex outside the interval $[-2,2]$, still embeds twice 0-1 loss.

\begin{figure}
	\begin{minipage}{0.47\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth]{figs/modhinge.pdf}
		%		\caption{Bayes risk of the 0-1 loss.}
		%		\label{fig:0-1-br}
	\end{minipage}
	\hfill
	\begin{minipage}{0.47\linewidth}
		\centering		\includegraphics[width=0.95\linewidth]{figs/modhinge-br.pdf}
		%		\caption{Bayes risk of hinge loss.}
		%		\label{fig:hinge-br}
	\end{minipage}
	\caption{(L) Expected modified hinge loss for fixed distribution; (R) Bayes risk of modified hinge still matches the Bayes risk of hinge.}
	\label{fig:modified-hinge}
\end{figure}



% \begin{lemma}
% 	\label{lem:poly-loss-poly-risk}
% 	If $L$ is polyhedral, $\risk{L}$ is polyhedral.
% \end{lemma}
% \begin{proof}
% 	Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss, and $\Gamma = \prop{L}$.
% 	By Lemma~\ref{lem:polyhedral-range-gamma}, $\U = \Gamma(\simplex)$ is finite. 
% 	For each $U\in \U$, select $u_U \in U$, and let $\Sc = \{u_U : U \in\U\}$.
% 	$\Sc$ is representative for $L$, so Lemma~\ref{lem:loss-restrict} gives us $\risk{L} = \risk{L|_{\Sc}}$, which is polyhedral as $\Sc$ is finite.
% \end{proof}

%% intro to Lemma X
We now present our main structural result in Lemma~\ref{lem:X}, proved in \S~\ref{app:polyhedra}, which will lay the foundation for the rest of this section.
Lemma~\ref{lem:X} observes that (minimizable) losses $L$ with polyhedral Bayes risk have finite representative sets, and derives equivalent conditions on the level sets of the $\prop{L}$ and discrete losses tightly embedded by $L$.
\raft{TODO: Brief description of the result}\jessiet{Attempted}

\begin{restatable}{lemma}{lemmaX}\label{lem:X}
  Let $L: \R \to \reals^\Y_+$ be a minimizable loss with a polyhedral Bayes risk $\risk L$.
  Then $L$ has a finite representative set.
  Furthermore, letting $\Gamma = \prop{L}$, there exist finite sets
  $\V \subseteq \reals^\Y_+$ and
  $\Theta = \{\theta_v \subseteq \simplex \mid v\in\V\}$,
  both uniquely determined by $\risk{L}$ alone,
  such that
  \begin{enumerate}
  % \item There exists $\V = \V(\risk{L}) \subseteq \reals^\Y_+$, uniquely determined by $\risk{L}$ alone, such that $\V = L(\R^*)$.\label{item:X-V}
  \item A set $\R'\subseteq\R$ is representative if and only if $\V \subseteq L(\R')$.\label{item:X-rep-V}
  \item A set $\R'\subseteq\R$ is minimum representative if and only if $L(\R') = \V$.\label{item:X-min-V}
  \item A set $\R'\subseteq\R$ is representative if and only if $\Theta \subseteq \{\Gamma_r \mid r \in \R'\}$.\label{item:X-rep-Theta}
  \item A set $\R'\subseteq\R$ is minimum representative if and only if $\{\Gamma_r \mid r \in \R'\} = \Theta$.\label{item:X-min-Theta}
  \item Every representative set for $L$ contains a minimum representative set for $L$.\label{item:X-rep-contain-min}
  \item The set of full-dimensional level sets of $\Gamma$ is exactly $\Theta$.\label{item:X-full-dim}
  \item For any $r \in \R$, there exists $\theta \in \Theta$ such that $\Gamma_r \subseteq \theta$.\label{item:X-redundant}
  \item $L$ tightly embeds $\ell:\R'\to\reals^\Y_+$ if and only if $\ell$ is injective and $\ell(\R') = \V$.\label{item:X-tight-embed}
  \end{enumerate}
\end{restatable}

As a finite representative set implies a polyhedral Bayes risk by Lemma~\ref{lem:loss-restrict}, Lemma~\ref{lem:X} shows that polyhedral Bayes risks are equivalent to having finite representative sets, which in turn gives an embedding by
Proposition~\ref{prop:representative-embeds-restriction}.
\begin{corollary}\label{cor:poly-risk-fin-rep}
  The following are equivalent for any minimizable loss $L:\R\to\reals^\Y_+$.
  \begin{enumerate}
  \item $\risk{L}$ is polyhedral.
  \item $L$ has a finite representative set.
  \item $L$ embeds a discrete loss.
  \end{enumerate}
\end{corollary}
From Corollary~\ref{cor:poly-risk-fin-rep}, $L$ having a finite representative set is an equivalent condition to $L$ being minimizable and $\risk{L}$ being polyhedral.
(Recall that having a finite representative set already implies minimizability.)
As it is also a more succinct condition, we will use the former in the sequel.
In particular, the implications of Lemma~\ref{lem:X} follow whenever $L$ has a finite representative set.


\subsection{Equivalent condition: matching Bayes risks}\label{subsec:match-BR}


Lemma~\ref{lem:X} leads to another appealing equivalent condition to our embedding condition in Definition~\ref{def:loss-embed}: a surrogate embeds a discrete loss if and only if their Bayes risks match.
\jessiet{Would it be fair to write about how this could give a quick emprical test of embedding by comparing Bayes risks at a few sampled distributions?}


\begin{proposition}\label{prop:embed-bayes-risks}
  Let discrete loss $\ell$ and minimizable loss $L$ be given.
  Then $L$ embeds $\ell$ if and only if $\risk{L}=\risk{\ell}$.
  % Let discrete loss $\ell:\R\to\reals^Y$ be given.
  % Then $L:\reals^d\to\reals^\Y$ embeds $\ell$ if and only if $\risk{L}=\risk{\ell}$.
  % A loss $L$ embeds a discrete loss $\ell$ if and only if $\risk{L}=\risk{\ell}$.
\end{proposition}
\begin{proof}
  Define $\Gamma = \prop{L}$ and $\gamma = \prop{\ell}$.
  
  $\implies$
  Suppose $L$ embeds $\ell$, so we have some $\Sc\subseteq \R$ which is representative for $\ell$ and an embedding $\varphi:\Sc\to\reals^d$; take $\U := \varphi(\Sc)$.
  Since $\Sc$ is representative for $\ell$, by embedding condition (ii) we have $\{\gamma_s \mid s\in\Sc\} = \{\Gamma_u \mid u\in\U\}$, so $\U$ is representative for $L$.
  % for all $p\in\simplex$ we have some $r \in \Sc \cap \gamma(p)$ and thus we have $\varphi(r) \in \U \cap \Gamma(p)$.
  % In particular, $\U \cap \Gamma(p)\neq \emptyset$ for all $p\in\simplex$, 
  By Lemma~\ref{lem:loss-restrict}, we have $\risk{\ell} = \risk{\ell|_{\Sc}}$ and $\risk{L} = \risk{L|_{\U}}$.
  As $L(\varphi(\cdot)) = \ell(\cdot)$ by embedding condition (i), for all $p\in\simplex$ we have
  \begin{equation*}
    \risk{\ell}(p) = \risk{\ell|_\Sc}(p) = \min_{r \in \Sc}\inprod{p}{\ell(r)} = \min_{r \in \Sc}\inprod{p}{L(\varphi(r))} = \min_{u \in \U}\inprod{p}{L(u)} = \risk{L|_\U}(p) = \risk{L}(p)~.
  \end{equation*}
  
  $\impliedby$
	For the reverse implication, assume $\risk{L} = \risk{\ell}$, which are polyhedral functions as $\ell$ is discrete.
  From Lemma~\ref{lem:X}(\ref{item:X-min-V}), we have some set $\V\subseteq\reals^\Y_+$ and minimum representative sets $\R^* \subseteq \R$ and $\U^* \subseteq \U$, for $\ell$ and $L$ respectively, such that $\ell(\R^*) = \V = L(\U^*)$.
  %\jessiet{This is a different $\Sc$ than the first paragraph, right? The first $\Sc$ is representative for $\ell$.}\raft{They are both quantified separately, so not linked; are you saying it's confusing to use $\Sc$ here though because its role is slightly different?}\jessiet{Yes... attempting to clarify.}
  As $\R^*$ and $\U^*$ are miniumum, they cannot repeat loss vectors, and thus $|\R^*|=|\ell(\R^*)|$ and $|L(\U^*)|=|\U^*|$.
  We conclude that $\R^*$ and $\U^*$ are both in bijection with $\V$.
  The map $\varphi :\R^* \to \reals^d$, given by $\varphi(r) = u \in \U^*$ where $\ell(r) = L(u)$, is therefore well-defined.
  Condition (i) of an embedding is immediate.
  From Proposition~\ref{prop:representative-embeds-restriction}, $\ell$ embeds $\ell|_{\R^*}$ and $L$ embeds $L|_{\U^*}$, both via the identity embedding.
  Using condition (ii) from both embeddings, for all $p\in\simplex$ and $r\in\R^*$, we have
  \begin{equation*}
    r \in \gamma(p) \iff r \in \prop{\ell|_{\R^*}}(p) \iff \varphi(r) \in \prop{L|_{\U^*}}(p)
    \iff \varphi(r) \in \prop{L}(p)~,
  \end{equation*}
  giving condition (ii).
\end{proof}

%\btw{RF: matching risks is not necessary for \emph{consistency}, as evidenced by logistic loss for 0-1 loss.  Maybe we should just make the observation, earlier on, that \emph{embedding} is not necessary for consistency. \jessie{Added a few sentences above, when talking about consistency vs calibration.}}

%\btw{RF: DKR (section 3.1) realized the importance of matching Bayes risks, but they could only give general results for strictly convex (concave I should say) risks, in part because they fixed the link function to be a generalization of $\sgn$.  In contrast, we focus exclusively on non-strictly-convex risks.}

Previous work from~\citet[Proposition 4]{duchi2018multiclass} realized the significance of matching Bayes risks for calibration with respect to the 0-1 loss.
Proposition~\ref{prop:embed-bayes-risks} broadens this general insight to any discrete loss.
Moreover, their result relies the Bayes risk of the surrogate being strictly concave, whereas polyhedral Bayes risks are never strictly concave.

\subsection{Trimming a loss}
\jessiet{Should this subsection be merged with the one below?}
\jessiet{Ordering suggestion: non-redundancy, trimming, (cor relating non-redundancy and trim), trimming related to properties.}

Central to the structural results in Lemma~\ref{lem:X} is the existence of a canonical set of loss vectors $\V$ which match the loss vectors of any minimum representative set.
This fact may seem surprising when one considers that losses may have many mimimum representative sets.
For example, consider hinge loss with a spurious extra dimension, i.e., $L:\reals^2\to\reals^\Y$, $L((r_1, r_2))_y = \max(0,1-r_1y)$ for $\Y = \{-1,+1\}$.
Here the minimum representative sets are exactly the two-element sets of the form $\{(-1,a),(1,b)\}$ for any $a,b\in\reals$. 
Lemma~\ref{lem:X}(\ref{item:X-min-V}) states that, while the minimum representative set is not unique, its loss vectors are.

Motivated by this observation, let us define the ``trim'' of a loss to be this unique set $\V$ of loss vectors induced by any minimum representative set, which again is well-defined by Lemma~\ref{lem:X}(\ref{item:X-min-V}).
\begin{definition}[Trim]\label{def:trim-loss}
  Given a loss $L:\R \to \reals_+^\Y$ with a finite representative set, we define $\trimcover(L) = \{L(r) \mid r \in \R^*\}$ given any minimum representative set $\R^*$ for $L$.
\end{definition}


Using this notion of trimming a loss, we can again recast our embedding condition: a loss embeds another if and only if they induce the same loss vectors, or have the same $\trimcover$.

\begin{proposition}\label{prop:embed-iff-trims-equal}
  Let $L:\reals^d\to\reals^\Y_+$ have a finite representative set, and let $\ell:\R\to\reals^\Y_+$ be a discrete loss.
  Then $L$ embeds $\ell$ if and only if $\trimcover(L) = \trimcover(\ell)$.
  Furthermore, $L$ tightly embeds $\ell$ if and only if $\ell$ is injective and $\trimcover(L) = \ell(\R)$.
\end{proposition}
\begin{proof}
  As $L$ has a finite representative set, it is minimizable.
  Proposition~\ref{prop:embed-bayes-risks} gives $L$ embeds $\ell$ if and only if $\risk L = \risk \ell$.
  If $\risk L = \risk \ell$, Lemma~\ref{lem:X}(\ref{item:X-min-V}) gives $\trim(L) = \trim(\ell)$.
  For the converse, suppose $\trim(L) = \trim(\ell) =: \V$.
  Define the discrete loss $\ell_\trim : \V \to \V, v\mapsto v$.
  Then $\ell_\trim$ is injective and $\ell_\trim(\V) = \V$, so from Lemma~\ref{lem:X}(\ref{item:X-tight-embed}), both $L$ and $\ell$ tightly embed $\ell_\trim$.
  We conclude $\risk L = \risk{\ell_\trim} = \risk \ell$ from Proposition~\ref{prop:embed-bayes-risks}.
  The second statement also follows directly from Lemma~\ref{lem:X}(\ref{item:X-tight-embed}).
\end{proof}


\subsection{Minimum representative sets and non-redundancy}

The condition that a representative set be minimum implies that one has identified exactly the ``active'' reports of a loss, in some sense.
We now relate this condition to another natural notion from the property elicitation literature: non-redundancy~\cite{frongillo2014general,lambert2018elicitation}.
Intuitively, a loss is non-redundant if no report is weakly dominated by another report.

\begin{definition}[Non-redundancy]\label{def:nonredundant}
  A loss $L : \R \to \reals^\Y_+$ eliciting $\Gamma:\simplex \toto \R$ is \emph{redundant} if there are reports $r, r' \in \R$ with $r \neq r'$ such that $\Gamma_r \subseteq \Gamma_{r'}$, and \emph{non-redundant} otherwise.
\end{definition}

From the structural result of Lemma~\ref{lem:X}, we can see that in fact these two notions are equivalent when $L$ has a polyhedral Bayes risk.
\begin{proposition}\label{prop:tfae-min-rep-nonredundant}
  Let $L:\R\to\reals^\Y_+$ have a finite representative set $\R'$.
  Then $\R'$ is a minimum representative set for $L$ if and only if $L|_{\R'}$ is non-redundant.
\end{proposition}
\begin{proof}
  Let $\Gamma = \prop{L}$.
  Suppose first that $L|_{\R'}$ is redundant.
  Then there exist $r,r' \in \R'$ such that $\Gamma_r \subseteq \Gamma_{r'}$.
  Thus, for all $p \in \Gamma_r$, we have $\{r, r'\} \subseteq \Gamma(p)$.
  Therefore $\R' \setminus \{r\}$ still a representative set, so $\R'$ is not minimum.

  Now suppose $L|_{\R'}$ is non-redundant.
  As $\R'$ is a representative set, Lemma~\ref{lem:X}(\ref{item:X-rep-contain-min}) gives some minimum representative set $\Sc \subseteq \R'$.
  Suppose we had some $r \in \R' \setminus \Sc$.
  Now Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant}) gives some $s\in\Sc$ such that $\Gamma_r \subseteq \Gamma_s$, which contradicts $L|_{\R'}$ being non-redundant.
  We conclude $L(\Sc)=L(\R')$, meaning $\R'$ is a minimum representative set.
\end{proof}

\begin{corollary}\label{cor:tight-embed-min-rep}
  Let loss $L:\R\to\reals^\Y_+$ with finite representative set $\R'$ be given.
  Then $L$ tightly embeds $L|_{\R'}$ if and only if $L|_{\R'}$ is non-redundant.
\end{corollary}

In fact, we can show something stronger: the reports in minimum representative sets are precisely those which are not strictly redundant.
To formalize this statement, given $\Gamma : \simplex \toto \R$, let $\red(\Gamma) := \{r\in\R \mid \exists r'\in\R,\; \Gamma_r \subsetneq \Gamma_{r'}\}$ be the set of strictly redundant reports.
Similarly, for minimizable $L$, let $\red(L) := \red(\prop L)$.

\begin{proposition}
  Let $L : \R \to \reals^\Y_+$ have a finite representative set.
  Let $\R'$ be the union of all minimum representative sets for $L$.
  Then $\R' = \R \setminus \red(L)$.
  \btw{Quite possibly there is a faster proof, though I do like how the proof shows something stronger.  Old version is in before-reorg-sec-3.tex.}
\end{proposition}

\begin{proof}
  Let $\Gamma = \prop L$.
  Let $\Sc$ be a minimum representative set for $L$, and let $s\in\Sc$.
  Suppose for a contradiction that $s\in\red(\Gamma)$.
  Then we have some $r\in\R$ with $\Gamma_s \subsetneq \Gamma_r$.
  From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant}) we have some $s'\in\Sc$ such that $\Gamma_r \subseteq \Gamma_{s'}$.
  But now $\Gamma_s \subsetneq \Gamma_r \subseteq \Gamma_{s'}$, contradicting $\Sc$ being minimum representative.
  Thus $\Sc \subseteq \R \setminus \red(\Gamma)$.

  For the reverse inclusion, let $r\in\R\setminus\red(\Gamma)$.
  Let $\Sc$ again be a minimum representative set for $L$.
  From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant}), we have some $s\in\Sc$ such that $\Gamma_r \subseteq \Gamma_s$.
  By definition of $\red(L)$, we conclude $\Gamma_r = \Gamma_s$.
  Now take $\Sc' = (\Sc \setminus \{s\}) \cup \{r\}$, that is, the same set of reports with $r$ replacing $s$.
  We have $\{\Gamma_s \mid s\in\Sc\} = \{\Gamma_{s'} \mid s'\in\Sc'\}$, and thus $\Sc'$ is a minimum representative for $L$ by Lemma~\ref{lem:X}(\ref{item:X-min-Theta}).
  As $r\in\Sc'$, we have $r \in \R'$ and we are done.
\end{proof}


As a corollary, we can state another characterization of $\trim$ in terms of redundant reports.
The result follows immediately from the definition of $\trim$.

\begin{corollary}\label{cor:trim-loss-red}
  Let $L : \R \to \reals^\Y_+$ have a finite representative set.
  Then $\trimcover(L) = L(\R \setminus \red(L))$.
  % $\{L(r) \mid r \notin \red(\Gamma)\}$.
  % \in \R,\; \forall r'\in\R,\; \Gamma_r = \Gamma_{r'} \textnormal{ or } \Gamma_r \not\subseteq \Gamma_{r'}\}$
  % $\mid u \in \R \textrm{ s.t. } \neg\exists u'\in\R,u'\neq u,\, \Gamma_u \subsetneq \Gamma_{u'}\}$
\end{corollary}

This result motivates the analogous definition for properties, $\trimred(\Gamma) := \{\Gamma_r \mid r \in \R\setminus\red(\Gamma)\}$.
We leverage this definition next, to study embeddings at the property level.

\subsection{A property elicitation perspective on trimmed losses}

\jessie{TODO:Revisit}
We conclude this section with a similar structural result about the properties of embedded by another property.
We say a property $\Gamma:\simplex\toto\reals^d$ embeds a finite property $\gamma:\simplex\toto\R$ if condition (ii) of Definition~\ref{def:loss-embed} holds.
In other words, $\Gamma$ embeds $\gamma$ if we have some representative set $\Sc\subseteq\R$ for $\gamma$ and embedding $\varphi:\Sc\to\reals^d$ such that for all $s\in\Sc$ we have $\gamma_s = \Gamma_{\varphi(s)}$.
% The definitions of representative sets and embeddings naturally extend to properties.
% We say a set 
% As we define embeddings from one loss to another, one can define embeddings from one property to another, where essentially condition (i) is relaxed; see \S~\ref{app:embed-props} for a precise definition.

Roughly, our result is as follows.
First, if $\Gamma$ embeds $\gamma$ and $\gamma$ is non-redundant, the level sets of $\Gamma$ must all be redundant relative to $\gamma$.
In other words, $\Gamma$ is exactly the property $\gamma$ up to relabelling reports, just with other reports filling in the gaps between the embedded reports of $\gamma$.
When working with convex losses, these extra reports often arise in the convex hull of the embedded reports.
In this sense, we can regard embedding as only a slight departure from direct elicitation: if a loss $L$ elicits $\Gamma$ which embeds $\gamma$, we can almost think of $L$ as eliciting $\gamma$ itself.
Finally, we have an important converse: if $\Gamma$ has finitely many full-dimensional level sets, or equivalently, if $\trimred(\Gamma)$ is finite, then $\Gamma$ must embed some finite elicitable property with those same level sets.
The statements about level sets make use of another corollary of Proposition~\ref{prop:embed-iff-trims-equal}, stated for properties.
\begin{corollary}\label{cor:trim-prop-red}
  Let $\Gamma : \simplex \toto \R$ be an elicitable property with a finite representative set.
  Then $\trimred(\Gamma)$ is the set of full-dimensional level sets of $\Gamma$.
\end{corollary}
\begin{proof}
  Let $L$ elicit $\Gamma$.
  From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}), for any finite minumum representative set $\Sc\subseteq\R$, the set $\{\Gamma_s\mid s\in\Sc\}$ is exactly the set of full-dimensional level sets $\Theta$ of $\Gamma$.
  From Proposition~\ref{prop:tfae-min-rep-nonredundant}, we have $r \in \R\setminus \red(\Gamma)$ if and only if $r$ is an element of some minimum representative set.
  As $\Gamma$ has at least one minimum representative set, we conclude $\trimred(\Gamma) = \{\Gamma_r \mid r\in \R\setminus\red(\Gamma)\} = \Theta$.  
\end{proof}

\begin{proposition}\label{prop:embed-trim}
  Let $\Gamma:\simplex\toto\reals^d$ be an elicitable property.
  %\jessiet{$\reals^d$ vs $\R$?}\raft{Can't recall why we needed $\reals^d$.  Probably don't... in any case, can't use $\R$ since that clashes with $\gamma$.}
  The following are equivalent:
  \begin{enumerate}\setlength{\itemsep}{0pt}
  \item $\Gamma$ embeds a elicitable finite property $\gamma:\simplex \toto \R$.
  \item $\trimred(\Gamma)$ is a finite set.%, and $\cup\,\trimred(\Gamma) = \simplex$.
  % \\ $\hat\Gamma := \trimred(\Gamma)$ is a finite property, and $\cup\,\hat\Gamma) = \simplex$.
  \item There is a finite minimum representative set $\U$ for $\Gamma$.
  \item There is a finite set of full-dimensional level sets $\hat\Theta$ of $\Gamma$, and $\cup\,\hat\Theta = \simplex$.
  \end{enumerate}
  % \raft{I see why you wanted this addition here, but (a) it's clear from the def of embedding, and (b) we don't have varphi defined.  So let's leave it out.}
  Moreover, when any of the above hold, $\trimred(\gamma) = \trimred(\Gamma) = \{\Gamma_u \mid u\in\U\} = \hat\Theta$.
\end{proposition}

\begin{proof}
  Let $L$ be a fixed loss eliciting $\Gamma$, so that in particular $\risk L$ is fixed.
  By definition of elicitable properties, $L$ is minimizable.
  In each case, we will show that $\risk L$ is polyhedral (or equivalently, that $L$ has a finite representative set), and thus Lemma~\ref{lem:X} will give us the set $\Theta$ of full-dimensional level sets of $\Gamma$, uniquely determined by $\risk L$.
  We will prove $1 \Rightarrow 2 \Rightarrow 3 \Rightarrow 4 \Rightarrow 1$, and in each case show that the relevant set of level sets is equal to $\Theta$, giving the result.

  $1 \Rightarrow 2$:
  Let $\Sc$ be the representative set for $\gamma$ and $\varphi:\Sc\to\reals^d$ the embedding.
  Since $\Sc$ is finite, $\varphi(\Sc)$ is a finite representative set for $\Gamma$ (and $L$; thus, $\risk L$ is polyhedral).
  Corollary~\ref{cor:trim-prop-red} now gives $\trimred(\Gamma) = \Theta$, which is finite, showing Case 2.

  $2 \Rightarrow 3$:
  If $\trimred(\Gamma)$ is finite, then in particular we have a finite set of reports $\Sc \subseteq \reals^d$ such that $\trimred(\Gamma) = \{\Gamma_s \mid s\in\Sc\}$.
  As $\Gamma$ is elicitable, $\reals^d$ is representative for $\Gamma$.
  By definition of $\trimred$, we have $\simplex = \cup_{r\in\reals^d} \Gamma_r = \cup \trimred(\Gamma) = \cup_{s\in\Sc} \Gamma_s$, and therefore $\Sc$ is representative for $\Gamma$ and for $L$.
  As $\Sc$ is finite, we have $\risk L$ polyhedral.
  %\jessiet{I'm a bit confused by this- how do we get $\cup_{\reals^d}\Gamma_r \subseteq \cup \trimred(\Gamma)$?}\raft{Each level set on the left is contained in one on the right}
  From Lemma~\ref{lem:X}(\ref{item:X-rep-contain-min}), we have some minimum representative set $\U\subseteq\Sc$ for $L$ and $\Gamma$, implying statement 3.
  Moreover, Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}) gives $\{\Gamma_u \mid u\in\U\} = \Theta$.

  $3 \Rightarrow 4$:
  Let $\U$ be a finite minimum representative set for $\Gamma$.
  Then $\risk L = \risk{L|_\U}$ is polyhedral.
  Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}) once again gives $\{\Gamma_u \mid u\in\U\} = \Theta$.
  We simply let $\hat\Theta = \Theta$, giving statement 4 as $\U$ is representative.

  $4 \Rightarrow 1$:
  Let $\Sc\subseteq\R$ such that $\{\Gamma_s \mid s\in\Sc\} = \hat \Theta$.
  Then $\Sc$ is representative for $\Gamma$ and $L$, as $\cup\hat\Theta = \simplex$.
  Again, this yields a finite representative set for $L$.
  Lemma~\ref{lem:loss-restrict} now states that $L$ embeds $L|_\Sc$, so $\Gamma$ embeds $\gamma := \Gamma|_\Sc$, giving Case 1.
  Finally, Corollary~\ref{cor:trim-prop-red} gives $\trimred(\gamma) = \Theta$.
  %
  % Letting $\Theta_\ell$ be the full-dimensional level sets of $\gamma$, Corollary~\ref{cor:trim-prop-red} also gives $\trimred(\gamma) = \Theta'$.
  % From Lemma~\ref{lem:X}(\ref{item:X-rep-contain-min}), we have some minimum representative set $\U\subseteq\U'$ for $L$ and $\Gamma$.
  % We have $\{\gamma_r\mid \varphi(r)\in\U\} = \{\Gamma_u \mid u\in\U\} = \Theta$ by Lemma~\ref{lem:X}(\ref{item:X-min-Theta}) and definition of embedding.
  % As $\Theta$ is a set of full-dimensional level sets of $\gamma$, we must have $\Theta \subseteq \Theta_\ell$.
  % But as $\cup \Theta = \simplex$, we must have $\Theta = \Theta_\ell$, as otherwise there exists $s\in\Sc$ with $\gamma_s \in \Theta_\ell \setminus \Theta$ and thus $\Sc \setminus \{s\}$ would still be representative.
  %
  % 
  % It remains to show $\trimred(\gamma) = \Theta$.
  % Moreover, $\Theta$ is exactly the full-dimensional level sets of $\Gamma$ by Lemma~\ref{lem:X}(\ref{item:X-full-dim}).
  % 
  % From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}) there exists some set of level sets $\Theta'$ which are the full-dimensional level sets of $\gamma$ and there exists a minimum representative set $\Sc\subseteq\R$ with $\{\gamma_s\mid s\in\Sc\} = \Theta'$.
  % Finally, Lemma~\ref{lem:X}(\ref{item:X-redundant}) gives $\trimred(\gamma) = \Theta$.
\end{proof}

As a final observation, recall that a property $\Gamma$ elicited by a polyhedral loss has a finite range, in the sense that there are only finitely many optimal sets $\Gamma(p)$ for $p\in\simplex$ (Lemma~\ref{lem:polyhedral-range-gamma}).
Proposition~\ref{prop:embed-trim} shows the inverse\jessiet{word choice?} statement: there are only finitely many level sets $\Gamma_u$ for $u\in\reals^d$.
In other words, both $\Gamma$ and $\Gamma^{-1}$ have a finite range as multivalued maps.

\section{Polyhedral Indirect Elicitation Implies Consistency}
\label{sec:poly-ie-consistency}

Our last result concerns indirect elicitation as a necessary condition for consistency when restricting to polyhedral losses.
Intuitively, a loss $L$ indirectly elicits a property $\gamma$ if we can compute $\gamma$ from $\prop L$.
To formalize the condition, we use the notion of a property refining another from~\citet{frongillo2014general}.

\begin{definition}[Refines]
  \label{def:refines}
	Let $\Gamma:\simplex \toto \R$ and $\Gamma':\simplex\toto \R'$.
	Then $\Gamma$ \emph{refines} $\Gamma'$ if for all $r \in \R$, there exists $r' \in \R'$ such that $\Gamma_{r} \subseteq \Gamma'_{r'}$.
\end{definition}
Equivalently, $\Gamma$ refines $\Gamma'$ if there is some ``link'' function $\psi:\R\to\R'$ such that $r\in\Gamma(p) \implies \psi(r) \in \Gamma'(p)$ for all $p\in\simplex$.
We will use the fact that refinement is transitive: if $\Gamma$ refines $\Gamma'$ and $\Gamma'$ refines $\Gamma''$, then $\Gamma$ refines $\Gamma''$.

\begin{definition}[Indirectly elicits]
  \label{def:indirectly-elicits}
  A loss $L$ \emph{indirectly elicits} a property $\gamma$ if $\prop L$ refines $\gamma$.
\end{definition}

It is straightforward to verify that consistency, and therefore calibration, implies indirect elicitation~\citep{finocchiaro2021unifying,agarwal2015consistent,steinwart2008support}.
Indirect elicitation may appear much weaker than calibration, since in particular it does not depend on the loss except through the property it elicits, and thus only depends on the exact minimizers of the loss.
Surprisingly, for minimizable polyhedral surrogates, we show the converse: indirect elicitation implies calibration, and therefore consistency.

A useful lemma is that for minimizable polyhedral losses, indirect elicitation must always pass through an embedding.
This result holds more generally whenever $L$ has a finite representative set, as in \S~\ref{sec:min-rep-sets}.
\begin{lemma}\label{lem:ie-iff-embeds-refinement}
  Let $L$ be a minimizable polyhedral loss.
  Then $L$ indirectly elicits a property $\gamma$ if and only if $L$ tightly embeds a discrete loss $\ell$ such that $\prop \ell$ refines $\gamma$.
\end{lemma}
\begin{proof}
  Let $L:\reals^d\to\reals^\Y_+$ be polyhedral, and $\Gamma = \prop L$.
  Then $L$ tightly embeds a discrete loss from Lemma~\ref{lem:X}(\ref{item:X-tight-embed}).
  Furthermore, Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant},\ref{item:X-tight-embed}) implies that $\prop L$ refines $\prop \ell$ for any discrete loss $\ell$ that $L$ tightly embeds.

  We claim that, for any property $\gamma$, and any loss $\ell$ that $L$ tightly embeds, $\prop L$ refines $\gamma$ if and only if $\prop \ell$ refines $\gamma$.
  If $\prop \ell$ refines $\gamma$, then $\prop L$ refines $\gamma$ by transitivity.
  For the other direction, Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-tight-embed}) shows that the level sets of $\prop \ell$ are contained in the set $\{\Gamma_u \mid u\in\reals^d\}$.
  Thus, if $\prop L$ refines $\gamma$, then in particular $\prop \ell$ refines $\gamma$.
  The result now follows immediately from the claim.
\end{proof}



\begin{theorem}\label{thm:poly-ie-implies-consistent}
	Let $L$ be a minimizable polyhedral loss which indirectly elicits a finite property $\gamma$.
  For any loss $\ell$ eliciting $\gamma$, there exists a link $\psi$ such that $(L, \psi)$ is calibrated (and consistent) with respect to $\ell$.
\end{theorem}
\begin{proof}
  %\raft{Jessie: well done with this proof!  There were a bunch of type check problems where you got the links confused but the overall thread was there.}
	Let $L:\reals^d \to \reals^\Y_+$ be a polyhedral loss indirectly eliciting $\gamma: \simplex \toto \R$, and let $\ell$ be a discrete loss eliciting $\gamma$.
  By Lemma~\ref{lem:ie-iff-embeds-refinement}, $L$ tightly embeds a discrete loss $\ell^\emb:\R^\emb\to\reals^\Y_+$ such that $\gamma^\emb := \prop{\ell^\emb}$ refines $\gamma$.
  From refinement, we can define a function $\psi^\R: \R^\emb \to \R$ such that for all $r\in\R^\emb$ and $p\in\simplex$ we have $r\in\gamma^\emb(p) \implies \psi^\R(r) \in \gamma(p)$. 
  Finally, Theorem~\ref{thm:link-main} gives a link function $\psi^\emb : \reals^d \to \R^\emb$ such that $(L,\psi^\emb)$ is calibrated with respect to $\ell^\emb$.

	
  Consider $\psi := \psi^\R \circ \psi^\emb$ and fix $p\in\simplex$.
	For any $u\in\reals^d$, if $\psi^\emb(u) \in \gamma^\emb(p)$, then $\psi(u) = \psi^\R(\psi^\emb(u)) \in \gamma(p)$ by definition of $\psi$ and $\psi^\R$.
  Contrapositively\raft{You heard me},
  $\psi(u) \notin \gamma(p) \implies \psi^\emb(u) \notin \gamma^\emb(p)$.
  Thus, we have
  \begin{equation}
    \label{eq:link-set-inclusion}
    \{u\in\reals^d \mid \psi(u) \not \in \gamma(p) \} \subseteq \{u\in\reals^d \mid \psi^\emb(u) \not \in \gamma^\emb(p) \}~.
  \end{equation}
  Combining eq.~\eqref{eq:link-set-inclusion} with the fact that $(L,\psi^\emb)$ is calibrated with respect to $\ell^\emb$, we have
	\begin{align*}
\inf_{u\in\reals^d : \psi(u) \not \in \gamma(p)} \inprod{p}{L(u)} \geq	\inf_{u\in\reals^d : \psi^\emb(u) \not \in \gamma^\emb(p)} \inprod{p}{L(u)} > \inf_{u\in\reals^d}\inprod{p}{L(u)}~,
	\end{align*}
  showing calibration of $\psi$.
	Consistency follows as calibration and consistency are equivalent in this setting~\citep{ramaswamy2016convex}.
\end{proof}

Theorem~\ref{thm:poly-ie-implies-consistent} gives a somewhat surprising result: despite the fact that indirect elicitation appears to be a somewhat weak necessary condition for consistency in general, the two conditions are equivalent for polyhedral surrogates.




\section{Conclusions} \label{sec:conclusion}

\raft{Didn't love the way any of the recaps sounded, and I also remember that some journals forbid recaps anyway, so commented out for now.  Instead I left a probably-too-terse recap (edit: also commented out just below), which we could expand slightly.}
% We have established a tight connection between polyhedral losses and embeddings.
% As detailed in \S~\ref{sec:applications}, this connection sheds light on the design and analysis of polyhedral surrogates throughout the machine learning literature.

% This paper formalizes an intuitive way to design convex surrogate losses for finite prediction problems, by embedding the reports into $\reals^d$.
% We establish a close relationship between such embeddings and polyhedral surrogates (Theorem~\ref{thm:embed-poly-main}), showing both that every polyhedral loss embeds a discrete loss and that every discrete loss is embedded by some polyhedral loss.
% We then construct a calibrated link function from any polyhedral loss to the discrete loss it embeds, giving consistency for all such losses (Theorem~\ref{thm:link-main}).
% As we detail in Section~\ref{sec:applications}


% \jessie{New version...?}
% In this paper, we introduce the embeddings tool for studying finite prediction problems, and connect this tool to the design of piecewise-linear and convex (polyhedral) surrogates (Theorem~\ref{thm:embed-poly-main}), and show that embeddings by polyhedral surrgates are consistent with respect to the given target problem (Theorem~\ref{thm:link-main}).
% We additionally show that embedding discrete prediction tasks is tightly connected to surrogates with polyhedral Bayes risks (Corollary~\ref{cor:poly-risk-fin-rep}).
% In \S~\ref{sec:applications}, this tool is applied to a variety of examples, including high-confidence classification, stuctured prediction, ordered partitions, and top-$k$ prediction.
% Finally, when restricting to polyhedral surrogate losses, we use embeddings to show that indirect property elicitation is equivalent to consistency in \S~\ref{sec:poly-ie-consistency}.

Several directions for future work remain.
% it is not clear whether one can further restrict embeddings to surrogates given by the convex envelope of embedded points.
We show in Theorem~\ref{thm:poly-ie-implies-consistent} that indirect elicitation is equivalent to consistency when restricting to the class of polyhedral surrogates; we would like to identify other classes of surrogates for which this equivalence holds.
It would also be interesting to explore embeddings through the lens of superprediction sets~\citep{williamson2014geometry}.
Finally, it is important for applications to understand the minimum prediction dimension $d$ of a consistent convex surrogate $L:\reals^d\to \reals^\Y_+$ for a given target problem, also called its elicitation complexity.
One approach to this question is to first understand the minimum $d$ for which an embedding $L$ exists, a study initiated by \citet{finocchiaro2020embedding}, and then relate this dimension to polyhedral, or general convex, elicitation complexity.

\raf{Notes on future work and conjectures -- CUT FOR ARXIV}
\begin{itemize}\small
\setlength \itemsep{0.1em}
\item %\raf{This is false I believe, from BEP surrogate $n=4$ and large (small?) $\alpha$ -- you can add a little curvature between the spokes and it still embeds.  Maybe we can rephrase / reformulate.}
%  In fact, we conjecture that \emph{any} loss embedding a discrete $\ell$ must be polyhedral on the convex hull of the embedded reports. 
%(The convex hull of the embedded reports follows since any point not in the convex hull will never minimize the expected loss.)
\btw{Modified from a conjecture Raf thought was false. Old version in comments - J, 15 Sept 21}
While the modified hinge example in Figure~\ref{fig:modified-hinge} shows that not every embedding must necessarily be polyhedral, this curvature is added on a set that is never uniquely optimal.
A natural question is to posit if an embedding my be polyhedral on the convex hull of the embedded reports; we conjecture negatively, though some conditions on linearity seem necessary.
\jessie{Do we still want this?  Maybe we rephrase now as characterizing the class of losses with polyhedral Bayes risks, or is knowing polyhedral Bayes risks sufficient?}


\item 
  Since discrete losses have a finite set of reports, and in turn, minimizers, any surrogate embedding the discrete loss must also have a finite set of unique minimizers.
  This is in turn related to another conjecture about the ``convex envelope'' of embeddings: if $L$ embeds $\ell$ by the embedding $\varphi$, the (polyhedral) surrogate $L'$ such that $L'_y$ is the convex envelope of $\{(\varphi(r),L(r)_y)\}_{r\in\R}$ also embeds $\ell$.
  \jessie{What is the significance of this conjecture? Slash I guess how does the convex envelope construction differ from the convex conjugate construction?}

\item
  Since polyhedral surrogates have a finite trimmed loss, it is an open line of work to relate embeddings to the to geometry of losses~\citep{williamson2014geometry} through superprediction sets, as these sets encapsulate losses that are dominated by elements of $\trim(L)$. \jessie{In paragraph above}
  
\item
  In \S~\ref{sec:poly-ie-consistency}, we show that indirect elicitation is equivalent to consistency when restricting to the class of polyhedral surrogates.
  It remains a line of future work to characterize other settings in which indirect elicitation and consistency are equivalent. \jessie{In paragraph above}

\item Prediction dimension in general:

While Theorem~\ref{thm:discrete-loss-poly-embeddable} constructs a consistent surrogate for any discrete loss, in some settings, such as structured prediction and information retrieval, the prediction dimension $d = n := |\Y|$ (e.g., $d$ such that $L : \reals^d \to \reals^\Y_+$) can be prohibitively large.%
\footnote{One can always reduce to $d=n-1$ in Theorem~\ref{thm:discrete-loss-poly-embeddable} via a linear transformation from $\reals^n$ to $\reals^{n-1}$ which is injective on $\simplex$; redefining the surrogate appropriately, the Bayes risks will still match.}
Recent work~\citep{ramaswamy2016convex,finocchiaro2020embedding,finocchiaro2021unifying} yield characterizations for bounding the prediction dimension $d$ for consistent convex surrogates and embeddings.
%\raft{I trimmed this discussion and footnoted the $n-1$ construction, and then folded the last paragraph above the theorem \jessie{I like this much better}}

  
\item
  \citet{finocchiaro2020embedding} introduce and present lower bounds for the notion of \emph{embedding dimension} of a target loss $\ell$: the minimum prediction dimension $d$ such that a polyhedral surrogate $L:\reals^d\to \reals^\Y_+$ embeds $\ell$.
  However, it is unclear if convex consistency dimension restricting to polyhedral losses, embedding dimension, or unrestricted convex consistency dimension are equal for all target losses. \jessie{Also above}
  %Characterizing embedding dimension \cite{finocchiaro2020embedding}; does poly dim == embedding dim?  ccdim == embedding dim?

\item
  ...

  \btw{FUTURE: we should comment in the discussion section that we probably can show that *any* loss embedding $\ell$ must be polyhedral-ish, meaning polyhedral except for stuff that is never optimal.  This theorem would then not need the ``polyhedral'' part. This is related to the ``convex envelope conjucture'', that if $L$ embeds $\ell$ via $\varphi$, you can just take the loss $L'$ such that $L_y$ is the convex envelope of points $\{(\varphi(r),L(r)_y)\}_{r\in\R}$.}
\btw{FUTURE: Link construction gives an excellent reason to focus on embeddings, since other techniques do not necessarily give you separated links for free.  Since we know we get them for free, we can just focus on the property, and study elicitation complexity; we know if we have a link at all it can be taken to be separated.  [[Is this true?]]}


\end{itemize}

%%%% Pre-neurips
%This work is part of a broader research program to understand convex surrogates through the lens of property elicitation.  % the relationship between finite losses and convex surrogates, the link functions connecting them, and the properties they elicit.
%We seek a general theory that, given a property, can prescribe when and how to construct convex surrogate losses that elicit it, and specifically, determine the minimum dimension required.
%Even more broadly, one could replace ``convex'' by any notion of ``nice'' surrogate.
%
%This work formalized the \emph{embedding} approach where labels are identified with points in $\reals^d$.
%We saw in Theorems~\ref{thm:poly-embeds-discrete} and~\ref{thm:discrete-loss-poly-embeddable} that this approach is tightly connected to the use of polyhedral (i.e. piecewise linear convex) loss functions.
%We also saw that it is a general technique that can be used for any finite elicitable property.
%
%Moreover, we established the relationship between polyhedral surrogates and \emph{calibrated links} to the discrete losses they embed.
%\jessie{Add more?}

%We then investigated the \emph{dimensionality} of $\reals^d$ required for such embeddings, giving a characterization in terms of the structure of the property in the simplex.
%This gave a complete understanding of the one-dimensional case, and a complete characterization albeit weaker understanding in higher dimensions.
%RF got here
%The two key conditions are an optimality condition that relates the structure of each level set to existence of polytopes in $\reals^d$ satisfying certain conditions; and a monotonicity condition relating such polytopes for different level sets.
%This yields new lower bounds in particular for the abstain loss.

% \paragraph{Directions.}

%There are several direct open questions involving the dimension of the surrogate loss.
%It would be interesting and perhaps practically useful to develop further techniques for upper bounds: automatically constructing embeddings in $\reals^d$ and associated polyhedral losses from a given property, with $d$ as small as possible.
%Another direction is additional lower bound techniques, or further development of our necessary conditions.

%This paper also suggests an agenda of defining more nuanced classes of surrogate losses and studying their properties.
%For example, a tangential topic in this work was the characteristics of a ``good'' link function; formalizing and exploring this question is an exciting direction.
%We would also like to move toward a full understanding of the differences between these classes.
%For example, how does embedding dimension compare in general to convex elicitation dimension (the dimensionality $d$ required of \emph{any} convex surrogate loss)?
%These questions have both theoretical interest and potential practical significance.

%\begin{conjecture}
%  $\mathrm{elic}_{embed}(\Gamma) = \mathrm{elic}_{Pcvx}(\Gamma) = \mathrm{elic}_{cvx}(\Gamma)$ for all finite elicitable $\Gamma$.
%\end{conjecture}


\subsection*{Acknowledgements}
We thank Arpit Agarwal and Peter Bartlett for many early discussions and insights,
Stephen Becker for a reference to Hoffman constants,
and Nishant Mehta, Enrique Nueve, and Anish Thilagar for other suggestions.\raft{others?}
This materialf is based upon work supported by the National Science Foundation under Grant Nos.\ CCF-1657598, IIS-2045347, and DGE-1650115.
\newpage
\bibliographystyle{plainnat}
\bibliography{diss,extra}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\newpage
\section{Power diagrams}\label{app:power-diagrams}
First, we present several definitions from Aurenhammer~\cite{aurenhammer1987power}.
\begin{definition}\label{def:cell-complex}
  A \emph{cell complex} in $\reals^d$ is a set $C$ of faces (of dimension $0,\ldots,d$) which (i) union to $\reals^d$, (ii) have pairwise disjoint relative interiors, and (iii) any nonempty intersection of faces $F,F'$ in $C$ is a face of $F$ and $F'$ and an element of $C$.
\end{definition}

\begin{definition}\label{def:power-diagram}
  Given sites $s_1,\ldots,s_k\in\reals^d$ and weights $w_1,\ldots,w_k \geq 0$, the corresponding \emph{power diagram} is the cell complex given by
  \begin{equation}
    \label{eq:pd}
    \cell(s_i) = \{ x \in\reals^d : \forall j \in \{1,\ldots,k\} \, \|x - s_i\|^2 - w_i \leq \|x - s_j\|^2 - w_j\}~.
  \end{equation}
\end{definition}

\begin{definition}\label{def:affine-equiv}
  A cell complex $C$ in $\reals^d$ is \emph{affinely equivalent} to a (convex) polyhedron $P \subseteq \reals^{d+1}$ if $C$ is a (linear) projection of the faces of $P$.
\end{definition}

Proposition~\ref{prop:embed-bayes-risks}, focuses on matching the values of Bayes Risks, while the following result from~\citet{aurenhammer1987power} allows us to move towards understanding the projection of the Bayes Risk onto the simplex $\simplex$.
In particular, one can consider the epigraph of a polyhedral convex function on $\reals^d$ and the projection down to $\reals^d$; in this case we call the resulting power diagram \emph{induced} by the convex function.

\begin{theorem}[Aurenhammer~\cite{aurenhammer1987power}]\label{thm:aurenhammer}
	A cell complex is affinely equivalent to a convex polyhedron if and only if it is a power diagram.
\end{theorem}

%\raf{Here's what I'm thinking for the text from here to Lemma~\ref{lem:poly-loss-poly-risk} (let's not implement just yet though): Let's move Lemma~\ref{lem:polyhedral-pd-same} to the appendix, and state Lemma~\ref{lem:polyhedral-range-gamma} but move the proof to the appendix.  Then we can keep the proof of Lemma~\ref{lem:poly-loss-poly-risk}.}
We extend Theorem~\ref{thm:aurenhammer} to a weighted sum of convex functions, showing that the induced power diagram is the same for any choice of strictly positive weights.

\begin{lemma}\label{lem:polyhedral-pd-same}
	Let $f_1,\ldots,f_m:\reals^d\to\reals$ be polyhedral convex functions.
	The power diagram induced by $\sum_{i=1}^m p_i f_i$ is the same for all $p \in \inter(\simplex)$.
\end{lemma}
\begin{proof}
	For any polyhedral convex function $g$ with epigraph $P$, the proof of~\citet[Theorem 4]{aurenhammer1987power} shows that the power diagram induced by $g$ is determined by the facets of $P$.
	Let $F$ be a facet of $P$, and $F'$ its projection down to $\reals^d$.
	It follows that $g|_{F'}$ is affine, and thus $g$ is differentiable on $\inter(F')$ with constant derivative $d\in\reals^d$.
	Conversely, for any subgradient $d'$ of $g$, the set of points $\{x\in\reals^d : d'\in\partial g(x)\}$ is the projection of a face of $P$; we conclude that $F = \{(x,g(x))\in\reals^{d+1} : d\in\partial g(x)\}$ and $F' = \{x\in\reals^d : d\in\partial g(x)\}$.
	
	Now let $f := \sum_{i=1}^k f_i$ with epigraph $P$, and $f' := \sum_{i=1}^k p_i f_i$ with epigraph $P'$.
	By Rockafellar~\cite{rockafellar1997convex}, $f,f'$ are polyhedral.
	We now show that $f$ is differentiable whenever $f'$ is differentiable:
	\begin{align*}
	\partial f(x) = \{d\}
	&\iff \sum_{i=1}^k \partial f_i(x) = \{d\} \\
	&\iff \forall i\in\{1,\ldots,k\}, \; \partial f_i(x) = \{d_i\} \\
	&\iff \forall i\in\{1,\ldots,k\}, \; \partial p_i f_i(x) = \{p_id_i\} \\
	&\iff \sum_{i=1}^k \partial p_if_i(x) = \left\{\sum_{i=1}^k p_id_i\right\} \\
	&\iff \partial f'(x) = \left\{\sum_{i=1}^k p_id_i\right\}~.
	\end{align*}
	From the above observations, every facet of $P$ is determined by the derivative of $f$ at any point in the interior of its projection, and vice versa.
	Letting $x$ be such a point in the interior, we now see that the facet of $P'$ containing $(x,f'(x))$ has the same projection, namely $\{x'\in\reals^d : \nabla f(x) \in \partial f(x')\} = \{x'\in\reals^d : \nabla f'(x) \in \partial f'(x')\}$.
	Thus, the power diagrams induced by $f$ and $f'$ are the same.
	The conclusion follows from the observation that the above held for any strictly positive weights $p$, and $f$ was fixed.
\end{proof}

We now include the full proof of Lemma~\ref{lem:polyhedral-range-gamma}.

\polyhedralrangegamma*
\begin{proof}
	First, observe that $L: \reals^d \to \reals^\Y_+$ is finite and bounded from below (by $\vec 0$), and thus its infimum is finite. 
	Therefore, we can apply \citet[Corollary 19.3.1]{rockafellar1997convex} to conclude that its infimum is attained for all $p \in \simplex$ and is therefore minimizable; thus, elicits a property.
	
	For all $p$, let $P(p)$ be the epigraph of the convex function $u\mapsto \inprod{p}{L(u)}$.
	From Lemma~\ref{lem:polyhedral-pd-same}, we have that the power diagram $D_\Y$ induced by the projection of $P(p)$ onto $\reals^d$ is the same for any $p\in\inter(\simplex)$.
	Let $\F_\Y$ be the set of faces of $D_\Y$, which by the above are the set of faces of $P(p)$ projected onto $\reals^d$ for any $p\in\inter(\simplex)$.
	
	We claim for all $p\in\inter(\simplex)$, that $\Gamma(p) \in \F_\Y$.
	To see this, let $u \in \Gamma(p)$, and $u' = (u,\inprod{p}{L(u)}) \in P(p)$.
	The optimality of $u$ is equivalent to $u'$ being contained in the face $F$ of $P(p)$ exposed by the normal $(0,\ldots,0,-1)\in\reals^{d+1}$.
	Thus, $\Gamma(p) = \argmin_{u\in\reals^d} \inprod{p}{L(u)}$ is a projection of $F$ onto $\reals^d$, which is an element of $\F_\Y$.
	
	Now for $p \not \in \inter(\simplex)$, consider $\Y'\subsetneq \Y$, $\Y'\neq\emptyset$.
	Applying the above argument, we have a similar guarantee: a finite set $\F_{\Y'}$ such that $\Gamma(p) \in \F_{\Y'}$ for all $p$ with support exactly $\Y'$.
	Taking $\F = \bigcup\{\F_{\Y'} | \Y'\subseteq\Y, \Y'\neq\emptyset\}$, we have for all $p\in\simplex$ that $\Gamma(p) \in \F$, giving $\U \subseteq \F$.
	As $\F$ is finite, so is $\U$, and the elements of $\U$ are closed polyhedra as faces of $D_{\Y'}$ for some $\Y'\subseteq\Y$.
\end{proof}


%\section{Embedding properties}\label{app:embed-props}
%\jessie{Do we use this anymore?}
%
%While Definition~\ref{def:loss-embed} gives the notion of one \emph{loss} embedding another, we now define the notion of one \emph{property} embedding another.
%\begin{definition}\label{def:prop-embed}
%	A property $\Gamma : \simplex \toto \reals^d$ embeds a property $\gamma:\simplex \toto \R$ if there exists some injective embedding $\varphi:\R \to \reals^d$ such that for all $p \in \simplex$ and $r \in \R$, we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
%\end{definition}
%
%By condition (ii.) of Definition~\ref{def:loss-embed}, we then have $L$ embedding $\ell$ implies $\prop{L}$ embeds $\prop{\ell}$.
%
%This also prompts us to think about redundancy in properties.
%\begin{definition}[Non-redundant property]\label{def:nonredundant-prop}
%	A property $\Gamma:\simplex \toto \R$ is \emph{redundant} if there are reports $r, r' \in \R$ such that $\Gamma_r \subseteq \Gamma_{r'}$, and \emph{non-redundant} otherwise.
%\end{definition}
%
%
%%It is often convenient to work directly with properties and set aside the losses which elicit them.
%%To this end, we say a property to embeds another if eq.~\eqref{eq:embed-loss} holds.
%%We begin with the notion of redundancy.
%%\begin{definition}[Finite property, non-redundant]
%%  A property $\gamma:\simplex\toto\R$ is \emph{redundant} if for some $r,r'\in\R$ with $r \neq r'$, we have $\gamma_r \subseteq \gamma_{r'}$, and \emph{non-redundant} otherwise.
%%  $\gamma$ is \emph{finite} if it is non-redundant and $\R$ is a finite set.
%%\end{definition}
%
%%With the terminology of properties in hand, we can restate our definition of embedding.
%%First, we formalize the notion of embedding properties.
%%\begin{definition}
%%  A property $\Gamma : \simplex \toto \reals^d$ \emph{embeds} a property $\gamma : \simplex \toto \R$ if there exists some injective embdedding $\varphi:\R\to\reals^d$ such that for all $p\in\simplex,r\in\R$ we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
%%  Similarly, we say a loss $L:\reals^d\to\reals^\Y$ embeds $\gamma$ if $\prop{L}$ embeds $\gamma$.
%%\end{definition}
%%We can now see that a surrogate $L:\reals^d\to\reals^\Y$ embeds $\ell:\R\to\reals^\Y$ if and only if $\prop{L}$ embeds $\prop{\ell}$ via $\varphi$ and for all $r\in\R$ we have $L(\varphi(r)) = \ell(r)$.
%
%When working with convex losses which are not strictly convex, one quickly encounters redundant properties: if $\inprod{p}{L(\cdot)}$ is minimized by a point where $p\cdot L$ is flat, then there will be an uncountable set of reports which also minimize the loss.
%As results in property elicitation typically assume non-redundant properties (e.g.~\cite{frongillo2014general,frongillo2015elicitation}), it is useful to consider a transformation which removes redundant level sets.
%We capture this transformation as the trim operator presented below.
%
%\begin{definition}\label{def:trim-prop-nonred}
%  Given an elicitable property $\Gamma:\simplex \toto\R$, we define $\trimred(\Gamma) = \{\Gamma_u \mid \neg \exists u' \neq u$ s.t. $\Gamma_u \subsetneq \Gamma_{'u}\}$.
%  %$\trimcover(\Gamma) = \{\Gamma_u : u \in \U \}$ as the set of maximal level sets of $\Gamma$ for any minimum representative set $\U \subseteq \R$.
%\end{definition}
%
%\begin{definition}\label{def:trim-prop-cover}
%	Given an elicitable property $\Gamma:\simplex \toto\R$, we define $\trimcover(\Gamma) = \{\Gamma_u : u \in \U \}$ as the set of maximal level sets of $\Gamma$ for any minimum representative set $\U \subseteq \R$.
%\end{definition}
%
%The following corollary follows from Corollary~\ref{cor:trim-loss-condition}.
%\begin{corollary}
%	Let $\Gamma$ be an elicitable property with a finite minimum representative set.
%	$\trimcover(\Gamma) = \trimred(\Gamma)$.
%\end{corollary}
%
%\btw{RF: Note for later: should be able to show that the union of trim is the simplex.\jessie{This is part of the proposition statement 2 now.}}
%Take note that the unlabeled property $\trim(\Gamma)$ is non-redundant, meaning that for any $\theta \in \trim(\Gamma)$, there is no level set $\theta' \in \trim(\Gamma)$ such that $\theta \subset \theta'$.


\iffalse
\hrule
\bigskip
\jessie{Add results (subsection?) relating trim and positive normal sets.  From here until the hrule}
\subsection{Relation to Positive Normal Sets}

\proposedadd{The concept of $\trim$ is closely related to the \emph{positive normal set} of Ramaswamy et al.~\cite{ramaswamy2016convex}.
However, they define positive normal sets in terms of the loss vector, agnostic to the report yielding such a loss vector.
In constructing the $\trim$ of a property, we yield the positive normal sets of the loss eliciting such a property, without the attachment to loss vectors.
This allows us to frame necessary and sufficient conditions for constructing a calibrated surrogate in terms of finite properties.}


\begin{definition}
	Let $L:\reals^d \to \reals^n_+$, and define $\Sc_L := \conv(L(\reals^d))$ as in~\cite[Definition 8]{ramaswamy2016convex}.
	For $z \in \Sc_L$, we define the \emph{positive normal set} of $L$ at $z$ as
	\begin{equation}
	\N^L(z) = \left\{ p \in \simplex: \inprod{p}{z} = \inf_{z' \in \Sc_L} \inprod{p}{z} \right\}~.~
	\end{equation}
\end{definition}

\begin{conjecture}
	For $L$ convex and $p \in \simplex$, we have $\inf_{z' \in \Sc_L} \inprod{p}{z'} = \inf_{z' \in L(\reals^d)}\inprod{p}{z'}$.
\end{conjecture}
\begin{proof}
	First, we have $\inf_{z' \in \Sc_L} \inprod{p}{z'} \leq \inf_{z' \in L(\reals^d)}\inprod{p}{z'}$ since $L(\reals^d) \subseteq \Sc_L$.
	
	Now, we want to show $\inf_{z' \in \Sc_L} \inprod{p}{z'} \geq \inf_{z' \in L(\reals^d)}\inprod{p}{z'}$.
	Consider that for all $z \in \Sc_L$, we have $z \in \inter(\Sc_L) \implies z \not \in \arginf_{z' \in \Sc_L}\inprod{p}{z'}$, where $\partial \Sc_L$ is the boundary of $\Sc_L$.
	Therefore, we have $\inf_{z' \in \Sc_L}\inprod{p}{z'} = \inf_{z' \in \partial\Sc_L}\inprod{p}{z'}$.
	
	If we can then show $L(\reals^d) \supseteq \partial \Sc_L$, then we have $\inf_{z' \in \Sc_L} \inprod{p}{z'} \geq \inf_{z' \in L(\reals^d)}\inprod{p}{z'}$.

	\jessie{??? Not sure why this should be true, so it probably isn't, but I can't think of a counterexample.}
\end{proof}

\begin{proposition}
	Consider the loss $L:\reals^d \to \reals^n_+$ and $\Gamma := \prop{L}$ nondegenerate.
	Fix a finite set of $\{z_i\}_{i=1}^k$ so that each $z_i \in \Sc_L$ and $\cup_{z_i} \N^L(z_i) = \simplex$.
	Then $\N^L(\mathcal{Z}) = \{\N^L(z_i)\}_{i=1}^k = \trim(\Gamma)$.
\end{proposition}
\begin{proof}
  \jessie{still a conjecture.}
  Take $\theta \in \N^L(\mathcal{Z})$.
  For all $p \in \theta$, we then have 
  \begin{align*}
  p \in \theta \iff \inprod{p}{z} &= \inf_{z' \in \Sc_L} \inprod{p}{z'}\\
  &= \inf_{z' \in L(\reals^d)} \inprod{p}{z'}\\
  &= \inf_{u \in \reals^d} \inprod{p}{L(u)}\\
  &= \inf_{u\in \reals^d} \E_p L(u, Y)\\
  &\iff p \in \Gamma_u~.~
  \end{align*}
  \jessie{Not quite... we have $\inf_{z' \in \Sc_L} \inprod{p}{z'} \leq \inf_{z' \in L(\reals^d)} \inprod{p}{z'}$, by $L(\reals^d) \subseteq \Sc_L$, but we need the fact that $\N^L(z) = \emptyset$ for all $z \in \inter(\Sc_L)$ or something similar for the first to second lines of equality.  Otherwise we just have $\Gamma_u \subseteq \N^L(z)$.  i.e. we need Conjecture 1.
  }
  Since this is true for all $p \in \theta$, we have $\theta = \Gamma_u$.
  As this is true for a finite set of $\mathcal{Z}$ whose positive normal sets union to the simplex, we have $\N^L(\mathcal{Z}) = \trim(\prop{L})$.
\end{proof}

\begin{corollary}
	\jessie{The necessary and sufficient conditions for calibrated surrogates from Ramaswamy \cite{ramaswamy2016convex}, but in terms of the unlabeled property.}
\end{corollary}
\hrule

Before we state the Proposition needed to prove many of the statements in Section~\ref{sec:poly-loss-embed}, we will need to general lemmas about properties and their losses.
The first follows from standard results relating finite properties to power diagrams (see Theorem~\ref{thm:aurenhammer}), and its proof is omitted.
The second is closely related to the trim operator: it states that if some subset of the reports are always represented among the minimizers of a loss, then one may remove all other reports and elicit the same property (with those other reports removed).

\begin{lemma}\label{lem:finite-full-dim}
  Let $\gamma$ be a finite (non-redundant) property elicited by a loss $L$.
  Then the negative Bayes risk $G$ of $L$ is polyhedral, and the level sets of $\gamma$ are the projections of the facets of the epigraph of $G$ onto $\simplex$, and thus form a power diagram.
  In particular, the level sets of $\gamma$ are full-dimensional in $\simplex$ (i.e.,\ of dimension $n-1$).
\end{lemma}

\begin{lemma}\label{lem:loss-restrict}
  Let $L$ elicit $\Gamma:\simplex\toto\R_1$, and let $\R_2\subseteq\R_1$ such that $\Gamma(p) \cap \R_2 \neq \emptyset$ for all $p\in\simplex$.
  Then $L|_{\R_2}$ ($L$ restricted to $\R_2$) elicits $\gamma:\simplex\toto\R_2$ defined by $\gamma(p) = \Gamma(p)\cap \R_2$.
  Moreover, the Bayes risks of $L$ and $L|_{\R_2}$ are the same.
\end{lemma}
\begin{proof}
  Let $p\in\simplex$ be fixed throughout.
  First let $r \in \gamma(p) = \Gamma(p) \cap \R_2$.
  Then $r \in \Gamma(p) = \argmin_{u\in\R_1} \inprod{p}{L(u)}$, so as $r\in\R_2$ we have in particular $r \in \argmin_{u\in\R_2} \inprod{p}{L(u)}$.
  For the other direction, suppose $r \in \argmin_{u\in\R_2} \inprod{p}{L(u)}$.
  By our assumption, we must have some $r^* \in \Gamma(p) \cap \R_2$.
  On the one hand, $r^*\in\Gamma(p) = \argmin_{u\in\R_1} \inprod{p}{L(u)}$.
  On the other, as $r^* \in \R_2$, we certainly have $r^* \in \argmin_{u\in\R_2} \inprod{p}{L(u)}$.
  But now we must have $\inprod{p}{L(r)} = \inprod{p}{L(r^*)}$, and thus $r \in \argmin_{u\in\R_1} \inprod{p}{L(u)} = \Gamma(p)$ as well.
  We now see $r \in \Gamma(p) \cap \R_2$.
  Finally, the equality of the Bayes risks $\min_{u\in\R_1} \inprod{p}{L(u)} = \min_{u\in\R_2} \inprod{p}{L(u)}$ follows immediately by the above, as $\emptyset \neq \Gamma(p)\cap\R_2 \subseteq \Gamma(p)$ for all $p\in\simplex$.
\end{proof}

We now state a useful result for proving the existence of an embedding loss, which shows remarkable structure of embeddable properties, and the properties that embed them.
First, we conclude that any embeddable property must be elicitable.
We also conclude that if $\Gamma$ embeds $\gamma$, the level sets of $\Gamma$ must all be redundant relative to $\gamma$.
In other words, $\Gamma$ is exactly the property $\gamma$, just with other reports filling in the gaps between the embedded reports of $\gamma$.
(When working with convex losses, these extra reports are typically the convex hull of the embedded reports.)
In this sense, we can regard embedding as a minor departure from direct elicitation: if a loss $L$ elicits $\Gamma$ which embeds $\gamma$, we can think of $L$ as essentially eliciting $\gamma$ itself.
Finally, we have an important converse: if $\Gamma$ has finitely many full-dimensional level sets, or if $\trim(\Gamma)$ is finite, then $\Gamma$ must embed some finite elicitable property with those same level sets.





\jessie{Added from COLT-19 but not discussed in COLT-20}

\begin{definition}
	We say a link $\psi:\reals^d \to \R$ is \emph{calibrated from $\Gamma$ to $\gamma$} if there is a calibrated link from $L$ to $\ell$, where $\Gamma := \prop{L}$ and $\gamma:= \prop{\ell}$.
\end{definition}

\begin{proposition}
	Let $L$ elicit $\Gamma:\simplex \toto \reals^d$ which embeds a finite property $\gamma$.
	Then there is a calibrated link from $\Gamma$ to $\gamma$.
\end{proposition}
\begin{proof}
	Let $\gamma: \simplex \toto \R$.
	Proposition~\ref{prop:embed-trim} gives us that $\trim(\Gamma) = \{\gamma_r : r \in \R\}$.
	We conclude that for any $u \in \reals^d$, there is a calibrated link from $\Gamma$ to $\gamma$.\jessie{Added definition for calibrated link for properties above.}
\end{proof}
\jessie{Not sure what we want to say about calibrated links, if anything.}

\subsection{Refining properties}

\begin{definition}
	Let $\Gamma:\simplex \toto \R$ and $\Gamma':\simplex\toto \R'$.
	Then $\Gamma'$ \emph{refines} $\Gamma$ if for all $r' \in \R'$, we have $\Gamma'_{r'} \subseteq \Gamma_r$ for some $r \in \R$.
	That is, the cells of $\Gamma'$ are all contained in the cells of $\Gamma$.
\end{definition}

\begin{theorem}
	Every polyhedral loss embeds a finite elicitable property.
	Moreover, a polyhedral loss $L$ indirectly elicits a finite elicitable property $\gamma$ if and only if $\gamma$ is finite and $L$ embeds a property which refines $\gamma$.
\end{theorem}
\begin{proof}
	Let $L:\reals^d\to\reals^\Y_+$ be a polyhedral loss.
	For all $p$, let $P(p)$ be the epigraph of the convex function $u\mapsto \inprod{p}{L(u)}$.
	From Lemma~\ref{lem:polyhedral-pd-same}, we have that the power diagram induced by the projection of $P(p)$ onto $\reals^d$ is constant whenever $p\in\inter(\simplex)$.
	Let $q\in\inter\simplex$ be the uniform distribution on $\Y$, and $V_\Y$ be the set of vertices of $P(q)$ projected onto $\reals^d$.
	By the above, this set is the same had we replaced $q$ by any $p\in\inter\simplex$.
	
	Now let $\Gamma := \Gamma[L]$.
	We claim for all $p\in\inter(\simplex)$, that $\Gamma(p) \cap V_\Y \neq \emptyset$.
	To see this, let $u \in \Gamma(p)$, and $u' = (u,\inprod{p}{L(u)}) \in P(p)$.
	The optimality of $u$ is equivalent to $u$ being contained in the face exposed by the normal $(0,\ldots,0,-1)\in\reals^{d+1}$, which is a face of $P(p)$.
	Let $v'\in\reals^{d+1}$ be a vertex on such a face, and $v\in V_\Y$ its projection onto $\reals^d$.
	Then $v$ is also optimal, and therefore $v\in\Gamma(p)$.
	
	Now consider $\Y'\subset \Y$.
	Applying the above argument on distributions $p$ with support exactly $\Y'$, we have a similar guarantee: a finite set $V_{\Y'}$ such that $\Gamma(p) \cap V_{\Y'} \neq \emptyset$ for all $p$ with support exactly $\Y'$.
	(When $\Y' = \{y\}$ is a singleton, we simply take the projected vertices of $L(\cdot)_y$.)
	
	Thus, taking $V = \bigcup_{\Y'\subseteq\Y} V_{\Y'}$, we have for all $p\in\simplex$ that $\Gamma(p) \cap V \neq \emptyset$.
	This implies that $\trim(\Gamma) \subseteq \{\Gamma_v : v\in V\}$, which is finite, so Proposition~\ref{prop:embed-trim} now gives the conclusion.
	\jessiet{Do we need to show the preimage of $\Gamma$ is $V$?}
	
	\raft{I might be delusional, but this second part ended up being much slicker than I'd thought, by essentially chaining definitions and maps.  Please check!}
	For the second part, let $\gamma':\simplex\toto\R'$ be the finite elicitable property embedded by $L$, with embedding $\varphi:\R'\to\reals^d$, and let $\psi$ be a calibrated link to a non-redundant elicitable property $\gamma:\simplex\toto\R$.
	Then letting $\psi' = (\psi \circ \varphi):\R'\to\R$, we see that $\psi'$ is a calibrated link from $\gamma'$ to $\gamma$:
	for all $r'\in\R'$, we have $\gamma'_{r'} = \prop{L}_{\varphi(r')} \subseteq \gamma_{\psi(\varphi(r'))}$.
	In particular, $\gamma'$ refines $\gamma$, and as $\gamma'$ is finite, $\gamma$ must be finite.
\end{proof}

\jessie{Other results on refined properties? Assuming we don't want the embedding dimension conjecture brought up on 02.03.2020 in here.}
\begin{conjecture}
	Let $\gamma'$ be a refinement of $\gamma$.
	Then a loss $\ell$ embedding for $\gamma'$ also embeds $\gamma$. \jessie{Not quite.  I think something similar is true, but we need more assumptions. See below for a related statement though.}
\end{conjecture}

\begin{proposition}
	Take $\prop{\ell} =: \gamma : \simplex \toto \R$ and $\prop{\ell'} =:\gamma' : \simplex \toto \R'$.
	If $\gamma'$ refines $\gamma$ and $L'$ is calibrated with respect to  the discrete loss $\ell'$, then there exists a link $\psi$ so that $(L', \psi)$ is calibrated with respect to $\ell$.
\end{proposition}
\begin{proof}
	Let us construct the link $\psi$ such that, for all $r' \in \R'$ consider $r \in \R$ so that $\gamma'_{r'} \subseteq \gamma_r$.
	Define $\psi$ so that $\psi'(u) = r' \implies \psi(u) = r$.
	
	To see this link and surrogate are calibrated with respect to $\ell$, consider that for any fixed $p \in \simplex$, $\{u \in \reals^d : \psi(u) \not \in \gamma(p)\} \subseteq \{u \in \reals^d : \psi'(u) \not \in \gamma'(p)\}$, which in turn implies that the infimum over the first term of the expected loss is at least the infimum over the second term, which is strictly greater than the Bayes Risk of $L'$ at $p$ by calibration of $(L', \psi')$.
	
	\jessiet{Probably need to be more thorough on the subset argument.}
	That is, 
	\begin{align*}
		\{u \in \reals^d : \psi(u) \not \in \gamma(p)\} &\subseteq \{u \in \reals^d : \psi'(u) \not \in \gamma'(p)\}\\
		\implies
		\inf_{u \in \reals^d : \psi(u) \not \in \gamma(p)} \inprod{p}{L'(u)} &\geq \inf_{u \in \reals^d : \psi'(u) \not \in \gamma'(p)} \inprod{p}{L'(u)} > \inf_u \inprod{p}{L'(u)}\\
		\implies 		\inf_{u \in \reals^d : \psi(u) \not \in \gamma(p)} \inprod{p}{L'(u)} &> \inf_u \inprod{p}{L'(u)}~.~
	\end{align*}
	Thus, as $p$ is arbitrary, we observe $(L', \psi)$ is calibrated with respect to $\ell$.
\end{proof}

\begin{conjecture}
	Let $\gamma'$ refine $\gamma = \prop{\ell}$ and $L'$ embeds $\gamma'$ by the injection $\varphi'$.
	Define $\phi : \R \to \R'$ such that $\phi(r) = r' \implies \gamma'_{r'} \subseteq \gamma_r$.
	Let $\varphi:\R \to \reals^d = \phi \circ \varphi'$
	Then $(-\risk{L'|_{\varphi(R)}})^*$ embeds $\ell$.
\end{conjecture}
\fi

%\section{Polyhedral losses}\label{app:polyhedral-losses}


%\raft{The following statement is true I believe, but low priority: ``An elicitable property $\Gamma:\simplex\toto\reals$ is convex elicitable (elicited by a convex $L : \reals \to \reals^\Y$) if and only if it is monotone.''  Start of the proof commented out.  Just need to show that $b$ is the upper limit of $a$ and $a$ the lower of $b$; should follow from elicitability of $\Gamma$.}
%\begin{lemma}\label{lem:prop-L-monotone}
%  For any convex $L : \reals \to \reals^\Y_+$, the property $\prop{L}$ is monotone.
%\end{lemma}
%\begin{proof}
%  If $L$ is convex and elicits $\Gamma$, let $a,b$ be defined by $a(r)_y = \partial_- L(r)_y$ and $b(r) = \partial_+ L(r)_y$, that is, the left and right derivatives of $L(\cdot)_y$ at $r$, respectively.
%  Then $\partial L(r)_y = [a(r)_y,b(r)_y]$.
%  We now have $r \in \prop{L}(p) \iff 0 \in \partial \inprod{p}{L(r)} \iff \inprod{a(r)}{p} \leq 0 \leq \inprod{b(r)}{p}$, showing the first condition.
%  The second condition follows as the subgradients of $L$ are monotone functions (see e.g.~\citet[Theorem 24.1]{rockafellar1997convex}).
%  % Conversely, given such an $a,b$, we appeal to~\citet[Theorem 24.2]{rockafellar1997convex}, which gives us that $L(u)_y := \int_0^u a(u)_y$ is convex, and
%\end{proof}

%\newcommand{\Pbar}{\overline P}
%\begin{lemma}\label{lem:pbar}
%  Let $\gamma:\simplex\toto\R$ be a finite elicitable property, and suppose there is a calibrated link $\psi$ from an elicitable $\Gamma$ to $\gamma$.
%  For each $r\in\R$, define $P_r = \bigcup_{u\in\psi^{-1}(r)} \Gamma_u \subseteq \simplex$, and let $\Pbar_r$ denote the closure of the convex hull of $P_r$.
%  Then $\gamma_r = \Pbar_r$ for all $r\in\R$.
%\end{lemma}
%\begin{proof}
%  As $P_r \subseteq \gamma_r$ by the definition of calibration, and $\gamma_r$ is closed and convex, we must have $\Pbar_r \subseteq \gamma_r$.
%  Furthermore, again by calibration of $\psi$, we must have $\bigcup_{r\in\R} P_r = \bigcup_{u\in\reals} \Gamma_u = \simplex$, and thus $\bigcup_{r\in\R} \Pbar_r = \simplex$ as well.
%  Suppose for a contradiction that $\gamma_r \neq \Pbar_r$ for some $r\in\R$.
%  From Lemma~\ref{lem:finite-full-dim}, $\gamma_r$ has nonempty interior, so we must have some $p\in\inter\gamma_r \setminus \Pbar_r$.
%  But as $\bigcup_{r'\in\R} \Pbar_{r'} = \simplex$, we then have some $r'\neq r$ with $p\in\Pbar_{r'} \subseteq \gamma_{r'}$.
%  By Theorem~\ref{thm:aurenhammer}, the level sets of $\gamma$ form a power diagram, and in particular a cell complex, so we have contradicted point (ii) of Definition~\ref{def:cell-complex}: the relative interiors of the faces must not be disjoint.
%  Hence, for all $r\in\R$ we have $\gamma_r = \Pbar_r$.
%\end{proof}



%\begin{proof}[Proof of Theorem~\ref{thm:polyhedral-embed-prop}]
%  Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss, and $\Gamma = \prop{L}$.
%  By Lemma~\ref{lem:polyhedral-range-gamma}, $\U = \Gamma(\simplex)$ is finite.
%  For any $U \in \U$, let $\Gamma_U = \{p\in\simplex | \Gamma(p) = U\}$, which is nonempty by definition.
%  Observe that for any $p\in\simplex$ and $u\in\reals^d$, we have $p \in \Gamma_u \iff u \in \Gamma(p) \iff U = \Gamma(p) \land u \in U \iff p\in\Gamma_U \land u \in U$.
%  Thus, we have for all $u\in\reals^d$ that $\Gamma_u = \cup\{\Gamma_U | U\in\U,u\in U\}$.
%  Now $\trim(\Gamma)$ is finite because the powerset of $\U$ is finite, and we apply Proposition~\ref{prop:embed-trim}.
%
%%
%%  \raf{I might be delusional, but this second part ended up being much slicker than I'd thought, by essentially chaining definitions and maps.  Please check!}
%%  For the second part, let $\gamma':\simplex\toto\R'$ be the finite elicitable property embedded by $L$, with embedding $\varphi:\R'\to\reals^d$, and let $\psi$ be a calibrated link to a non-redundant elicitable property $\gamma:\simplex\toto\R$.
%%  Then letting $\psi' = (\psi \circ \varphi):\R'\to\R$, we see that $\psi'$ is a calibrated link from $\gamma'$ to $\gamma$:
%%  for all $r'\in\R'$, we have $\gamma'_{r'} = \prop{L}_{\varphi(r')} \subseteq \gamma_{\psi(\varphi(r'))}$.
%%  In particular, $\gamma'$ refines $\gamma$, and as $\gamma'$ is finite, $\gamma$ must be finite.
%\end{proof}

\section{Equivalence of Separation and Calibration for Polyhedral Surrogates}
\label{sec:equiv-sep-calib}

We recall that Theorem \ref{thm:link-main} states that, if a polyhedral $L$ embeds a discrete $\ell$, then there exists a calibrated link $\psi$.
Theorem \ref{thm:link-main} is directly implied by the combination of Theorem \ref{thm:calibrated-separated}, that calibration is equivalent to separation (Definition \ref{def:sep-link}); and Theorem \ref{thm:thickened-separated}, existence of a separated link.
Theorem \ref{thm:calibrated-separated} is proven in this section and Theorem \ref{thm:thickened-separated} is proven in Appendix \ref{app:sep-link-exists}.

Throughout we will work with the two \emph{regret} functions:
the \emph{surrogate regret} $R_L(u,p) = \inprod{p}{L(u)} - \risk{L}(p)$, and similarly the \emph{target regret} $R_{\ell}(r,p) = \inprod{p}{\ell(r)} - \risk{\ell}(p)$.
In fact, the results in this section can be extended to surrogate regret bounds; see \citet{frongillo2021surrogate}.

We first show one direction: any calibrated link from a polyhedral surrogate to a discrete target must be $\epsilon$-separated.
The proof follows a similar argument to that of~\citet[Lemma 6]{tewari2007consistency}.
\begin{lemma}\label{lemma:calibrated-eps-sep}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is calibrated with respect to $\ell$.
  Then there exists $\epsilon>0$ such that $\psi$ is $\epsilon$-separated with respect to   $\prop{L}$ and $\prop{\ell}$.
\end{lemma}
\begin{proof}
  Let $\Gamma := \prop{L}$ and $\gamma := \prop{\ell}$.
  Suppose that $\psi$ is not $\epsilon$-separated for any $\epsilon>0$.
  Then letting $\epsilon_i := 1/i$ we have sequences $\{p_i\}_i \subset \simplex$ and  $\{u_i\}_i \subset \reals^d$ such that for all $i\in\mathbb N$ we have both $\psi(u_i) \notin \gamma(p_i)$ and $d_\infty(u_i,\Gamma(p_i)) < \epsilon_i$.
  First, observe that there are only finitely many values for $\gamma(p_i)$ and $\Gamma(p_i)$, as $\R$ is finite and $L$ is polyhedral (from Lemma~\ref{lem:polyhedral-range-gamma}).
  Thus, there must be some $p\in\simplex$ and some infinite subsequence indexed by $j\in J \subseteq \mathbb N$ where
  for all $j\in J$, we have $\psi(u_j) \notin \gamma(p)$ and $\Gamma(p_j) = \Gamma(p)$.

  Next, observe that, as $L$ is polyhedral, the expected loss $\inprod{p}{L(u)}$ is $\beta$-Lipschitz in $\|\cdot\|_\infty$ for some $\beta>0$.
  \raft{Could write $\beta$ explicitly; it's in my notes}
  Thus, for all $j\in J$, we have
  \begin{align*}
    d_\infty(u_i,\Gamma(p)) < \epsilon_j
    &\implies \exists u^*\in\Gamma(p)\; \|u_j-u^*\|_\infty < \epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \inprod{p}{L(u^*)} \right| < \beta\epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \risk{L}(p) \right| < \beta\epsilon_j~.
  \end{align*}
  Finally, for this $p$, we have
  \begin{align*}
    \inf_{u:\psi(u)\notin\gamma(p)} \inprod{p}{L(u)}
    \leq
    \inf_{j\in J} \inprod{p}{L(u_j)}
    =
    \risk{L}(p)~,
  \end{align*}
  contradicting the calibration of $\psi$.
\end{proof}

For the other direction, we will make use of Hoffman constants for systems of linear inequalities.
See \citet{zalinescu2003sharp} for a modern treatment.
\begin{theorem}[Hoffman constant \cite{hoffman1952approximate}]
  \label{thm:hoffman}
  Given a matrix $A\in\reals^{m\times n}$, there exists some smallest $H(A)\geq 0$, called the \emph{Hoffman constant} (with respect to $\|\cdot\|_\infty$), such that for all $b\in\reals^m$ and all $x\in\reals^n$,
  \begin{equation}
    \label{eq:hoffman}
    d_\infty(x,S(A,b)) \leq H(A) \|(A x - b)_+\|_\infty~,
  \end{equation}
  where $S(A,b) = \{x\in\reals^n \mid A x \leq b\}$ and $(u)_+ := \max(u,0)$ component-wise.
\end{theorem}

\begin{lemma}\label{lemma:hoffman-polyhedral}
  Let $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral loss with $\Gamma = \prop{L}$.
  Then for any fixed $p$, there exists some smallest constant $H_{L,p} \geq 0$ such that $d_{\infty}(u,\Gamma(p)) \leq H_{L,p} R_L(u,p)$ for all $u \in \reals^d$.
\end{lemma}
\begin{proof}
  Since $L$ is polyhedral, there exist $a_1,\ldots,a_m \in \reals^d$ and $c\in\reals^m$ such that we may write $\inprod{p}{L(u)} = \max_{1\leq j\leq m} a_j \cdot u + c_j$.
  Let $A \in \reals^{m\times d}$ be the matrix with rows $a_j$, and let $b = \risk{L}(p)\ones - c$, where $\ones\in\reals^m$ is the all-ones vector.
  Then we have
  \begin{align*}
    S(A,b)
    &:= \{u\in\reals^d \mid A u \leq b\}
    \\
    &= \{u\in\reals^d \mid A u + c \leq \risk{L}(p)\ones\}
    \\
    &= \{u\in\reals^d \mid \forall i\, (A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \max_i \;(A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \inprod{p}{L(u)} \leq \risk{L}(p)\}
    \\
    & = \Gamma(p)~.
  \end{align*}
  Similarly, we have $\max_i\; (A u - b)_i = \inprod{p}{L(u)} - \risk{L}(p) = \regret{L}{u}{p} \geq 0$.
  Thus,
  \begin{align*}
    \|(Au - b)_+\|_\infty
    &= \max_i\; ((Au - b)_+)_i
    \\
    &= \max((Au - b)_1,\ldots,(Au - b)_m, 0)
    \\
    &= \max(\max_i\; (Au - b)_i, \, 0)
    \\
    &= \max_i\; (Au - b)_i
    % \\
    % &= \inprod{p}{L(u)} - \risk{L}(p)
    \\
    &= \regret{L}{u}{p}~.
  \end{align*}
  Now applying Theorem~\ref{thm:hoffman}, we have
  \begin{align*}
    d_\infty(u,\Gamma(p))
    &=    d_\infty(u,S(A,b))
    \\
    &\leq H(A) \|(Au-b)_+\|_\infty
    \\
    &= H(A) \regret{L}{u}{p}~.\qedhere
  \end{align*}
\end{proof}


We are now ready to prove Theorem \ref{thm:calibrated-separated} as desired.
\calibratedseparated*
\begin{proof}
  Let $\gamma=\prop{\ell}$ and $\Gamma=\prop{L}$.
  From Lemma~\ref{lemma:calibrated-eps-sep}, calibration implies $\epsilon$-separation.
  For the converse, suppose $\psi$ is $\epsilon$-separated with respect to $L$ and $\ell$.
  Fix $p\in\simplex$.
  To show calibration, it suffices to find a positive lower bound for $R_L(u,p)$ that holds for all $u\in\reals^d$ with $\psi(u) \notin \gamma(p)$.
  
  Applying the definition of $\epsilon$-separated and Lemma~\ref{lemma:hoffman-polyhedral}, $\psi(u) \notin \gamma(p)$ implies
  \begin{align*}
    \epsilon &\leq    d_{\infty}(u,\Gamma(p)) \leq H_{L,p} R_L(u,p) \implies 1 \leq \frac{H_{L,p}}{\epsilon} R_L(u,p)~.
  \end{align*}
  Let $C_{\ell} = \max_{r,p} R_{\ell}(r,p)$.
  Then $R_{\ell}(\psi(u),p) \leq C_{\ell} \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p)$.

  If $H_{L,p} = 0$, then for all $u\in\reals^d$ we have $R_\ell(\psi(u),p) = 0$, so calibration for this $p$ is trivial.
  \raft{In fact, we must also have $\Gamma(p) = \reals^d$ by Lemma~\ref{lemma:hoffman-polyhedral}, since every $u\in\reals^d$ would have $d_\infty(u,\Gamma(p)) = 0$ and $\Gamma(p)$ is closed.}
  Similarly, if $C_\ell = 0$, then $R_\ell(r,p) = 0$ for all $r\in\R$, so again $R_\ell(\psi(u),p) = 0$ for all $u\in\reals^d$.

  Now assume $C_\ell > 0$ and $H_{L,p} > 0$.
  Let $C'_{\ell,p} \doteq \min_{r \notin \gamma(p)} R_\ell(r,p) > 0$.
  (As we assume $C_\ell > 0$, we must have $\gamma(p) \neq \R$, so the minimum is attained.)
  Then for all $u$ such that $\psi(u) \notin \gamma(p)$, we have $R_\ell(\psi(u),p) \geq C'_{\ell,p}$.
  Rearranging, we have
  \[ \psi(u) \notin \gamma(p) \implies R_L(u,p) \geq \frac{C'_{\ell,p} \epsilon}{C_\ell H_{L,p}} > 0~.\]
  Thus, $\inf_{u : \psi(u) \notin \gamma(p)} \inprod{L(u)}{p} > \risk{L}(p)$.
  Since the above holds for all $p\in\simplex$, $\psi$ is calibrated.
\end{proof}

%\section{Omitted Proofs}\label{app:omitted-proofs}
% While Definition~\ref{def:loss-embed} gives the notion of one \emph{loss} embedding another, we now generalize the notion of one \emph{property} embedding another. \jessie{Merge with earlier definitions if we keep this section}
% \begin{definition}\label{def:prop-embed}
%   A property $\Gamma : \simplex \toto \reals^d$ embeds a property $\gamma:\simplex \toto \R$ on a set $\Sc$ if there exists some injective embedding $\varphi:\R \to \reals^d$ such that for all $p \in \simplex$ and $r \in \R$, we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
%   If there is a representative set $\Sc$ such that $\Gamma$ embeds $\gamma$ on $\Sc$, then we simply say $\Gamma$ embeds $\gamma$.
%   Moreover, we say a loss embeds a property when it embeds a loss eliciting the property.
% \end{definition}
% 
% 
% By condition (ii.) of Definition~\ref{def:loss-embed}, we then have $L$ embedding $\ell$ implies $\prop{L}$ embeds $\prop{\ell}$.
% However, Definitions~\ref{def:loss-embed} and~\ref{def:prop-embed} are not immediately equivalent because property embedding does not capture the requirement of matching losses on embedded points; i.e., that $L(\varphi(r)) = \ell(r)$ for all $r \in \R$.  
% However, Lemma~\ref{lem:embed-defs-equiv} shows an equivalence follows without loss of generality.
% 
% \begin{lemma}\label{lem:embed-defs-equiv}
%	% \raft{Introduce $L$}
%	Let $L : \reals^d \to \reals^\Y_+$ be a loss function whose infimum is attained in expectation for all $p \in \simplex$.
%	If the property $\Gamma := \prop{L}$ embeds the finite property $\gamma : \simplex \toto \R$, then there is a discrete loss $\ell:\R \to \reals^\Y_+$ such that $\ell$ elicits $\gamma$ and $L$ embeds $\ell$.
% \end{lemma}
% \begin{proof}
%   \raft{All the important pieces are here, but take it slower.  State $\ell$ as a definition (let $\ell$ be given by...) and show why $L$ embeds it.\jessie{Took another pass but changed technique, adding in this detail.  Now very similar to the proof of Prop 1 forward direction.}}
%   Consider the embedding $\varphi$ given by the property embedding and take the discrete loss $\ell:r \mapsto L(\varphi(r))$.
%   Now, we claim that $L$ embeds $\ell$, and by Proposition~\ref{prop:embed-bayes-risks}, we can simply show $\risk{L} = \risk{\ell}$, and the risks being polyhedral follows from $\ell$ being discrete.
%   
%   First, we show that for all $p \in \simplex$, we have $(1) \inf_{u \in \reals^d}\inprod{p}{L(u)} = (2) \inf_{r \in \R}\inprod{p}{L(\varphi(r))}$, and the equality of risks follows.
%   $(1) \leq (2)$ follows from the fact that $\varphi(\R) \subset \reals^d$ and definition of infimum. 
%   Consider that if we had $(1) < (2)$ for some $p \in \simplex$, then there would be no $r$ such that $\varphi(r) \in \Gamma(p)$, and therefore, we would have $\gamma(p) = \emptyset$, yielding a contradiction. 
%   This gives us $(1) = (2)$, so we have $\risk{L} = \risk{L|_{\varphi(\R)}} = \risk{\ell}$.
%	% 
%	% Now consider the property $\Gamma' := p \mapsto \Gamma(p) \cap \varphi(\R)$.
%	% We have $\Gamma'$ nondegenerate since $\Gamma$ embedding $\gamma$ implies that, for all $p \in \simplex$, there is some $r \in \R$ such that $\varphi(r) \in \Gamma(p)$.
%	% Therefore, for all $p \in \simplex$, we have $\inf_{r \in \R}\inprod{p}{L(\varphi(r))} = \inf_{r \in \R} \inprod{p}{\ell(r)}$, which yields $\risk{L|_{\varphi(\R)}} = \risk{\ell}$.
%	% Chaining the two equalities, we now have $\risk{L} = \risk{\ell}$.
%	
%	% To verify $\ell$ elicits $\gamma$, consider $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$ by embedding, $ \iff \varphi(r) \in \argmin_{u \in \reals^d} \inprod{p}{L(u)}$ by $L$ eliciting $\Gamma$, and $ \iff r \in \argmin_{r \in \R} \inprod{p}{\ell(r)}$ by applying the definition of properties to the embedding definition.
%	% \hrule
%	%	If $\Gamma$ embeds a finite $\gamma$, $L$ must embed a discrete loss $\ell$ such that $\ell(r) = L(\varphi(r))$ for all $r \in \R$.  
%	%	Moreover, we can see $\ell$ elicits $\gamma$ since we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p) \iff \varphi(r) \in \argmin_{u \in \reals^d} \inprod{p}{L(u)} \iff r \in \argmin_{r \in \R} \inprod{p}{\ell(r)}$.
%	% 
%	% \raft{Spell this out; I don't think these distributions are enough, for the same reason as above.  \jessie{Took a different approach of using Prop 1 to show the risks are equal.}}
%	%	The final condition of matching loss values is satisfied by considering the equality of the expected loss on each $\delta_y$ distribution.
% \end{proof}


%When working with convex surrogates which are not strictly convex, one quickly encounters redundant properties: if $\inprod{p}{L(\cdot)}$ is minimized by a point where $\inprod{p}{L}$ is differentiable and flat, then there will be an uncountably infinite set of reports which also minimize the expected loss.
%As results in property elicitation typically assume properties are non-redundant (e.g.~\cite{frongillo2014general,frongillo2015elicitation}), we must be careful when considering redundant reports.
% it is useful to consider a transformation which removes redundant level sets, captured by the \emph{trim} operation.

%\begin{definition}\label{def:trim}
%  Given an elicitable property $\Gamma:\simplex \toto\R$, we define $\trim(\Gamma) = \{\Gamma_u : u \in \R \text{ s.t. } \neg\exists u'\in\R,u'\neq u,\, \Gamma_u \subsetneq \Gamma_{u'}\}$ as the set of maximal level sets of $\Gamma$.
%\end{definition}
%\jessie{Do we want to re-define $\trim(\Gamma) = \{\Gamma_r \mid r \in \Sc\}$ for any minimum representative set $\Sc$?}

% \btw{RF: Note for later: should be able to show that the union of trim is the simplex.\jessie{This is part of the proposition statement 2 now.}}
% Take note that the unlabeled property $\trim(\Gamma)$ is non-redundant, meaning that for any $\theta \in \trim(\Gamma)$, there is no level set $\theta' \in \trim(\Gamma)$ such that $\theta \subset \theta'$; this is a corollary of Proposition~\ref{prop:embed-trim}.


%Before we state Proposition~\ref{prop:embed-trim}, we will need to general lemmas about properties and their losses.
%The first follows from standard results relating finite properties to power diagrams (see Theorem~\ref{thm:aurenhammer}), and its proof is omitted.
%The second is closely related to the definition of the trim operator: it states that if some subset of the reports are always represented among the minimizers of a loss, then one may remove all other reports and elicit the same property (with those other reports removed).
%
%\begin{lemma}\label{lem:finite-full-dim}
%  Let $\gamma$ be a finite (non-redundant) property elicited by a loss $L$.
%  Then the negative Bayes risk $G$ of $L$ is polyhedral, and the level sets of $\gamma$ are the projections of the facets of the epigraph of $G$ onto $\simplex$, and thus form a power diagram.
%  In particular, the level sets of $\gamma$ are full-dimensional in $\simplex$ (i.e.,\ of dimension $n-1$).
%\end{lemma}
%\jessie{Commented out 27 Sep 21, duplicate in above section}

%\btw{Commented out result about old trim def = new trim def - Jessie 2 June 21}
%\btw{JESSIE: Figures/an example might help clarify this}
%When a property $\Gamma$ embeds a finite property $\gamma$, we can show that the level sets of $\gamma$ correspond exactly to their embedded level sets of $\Gamma$, and that these embedded level sets are exactly $\trim(\Gamma)$.
%\jessiet{This lemma new to journal version}
%\begin{lemma}\label{lem:embedded-level-sets-trim}
%  Let $\Gamma$ be an elicitable property.
%  If $\Gamma$ embeds a (non-redundant) finite property $\gamma : \simplex \toto \R$ by the injection $\varphi$, then $\{\gamma_r : r \in \R\} = \{\Gamma_u : u \in \varphi(\R)\} = \trim(\Gamma)\proposedadd{=\trim(\gamma)}$.
%  \proposedadd{RETRY: Let $\Gamma$ be an elicitable property.
%    If $\Gamma$ embeds a finite property $\gamma : \simplex \toto \R$ by the injection $\varphi$, then for any minimum representative set $\Sc$ for $\gamma$, we have $\{\gamma_r : r \in \Sc\} = \{\Gamma_u : u \in \varphi(\Sc)\} = \trim(\Gamma)=\trim(\gamma)$.}
%\end{lemma}
%\begin{proof}
%  Let $L$ elicit $\Gamma$.
%  Take $\ell$ to be the loss embedded by $L$ from Lemma~\ref{lem:embed-defs-equiv}.
%  Moreover, by Proposition~\ref{prop:embed-bayes-risks}, we have $\risk{L} = \risk{\ell}$.
%  % \raft{Implicitly? Justify the claim.  \jessie{In definition of finite properties, we say that we assume we are talking about non-redundant.}}
%  As $\gamma$ is finite (and non-redundant by assumption), we know that each level set of $\gamma$ must be full-dimensional in $\affhull(\simplex)$. 
%  For all $r \in \R$, we know $\gamma_r = \Gamma_{\varphi(r)}$, so we must also have $\Gamma_{\varphi(r)}$ full-dimensional.
%  Moreover, we have $\{\gamma_r : r \in \R\} = \{\Gamma_{\varphi(r)} : r \in \R\}$ as a corollary since this is true for each $r \in \R$.
%  \raft{The main action is here.  (1) Epigraph of $-\risk{L}$. (2) There is really only one case: every level set is a face of the power diagram, which must be contained (weakly, so $\subseteq$) in a facet. (3) To show that level sets are faces, you'll want to appeal to some piece of a proof in the prev section.}
%  Since the cell $\Gamma_{\varphi(r)}$ is the projection of a facet of the epigraph of $-\risk{L}$, which is convex, any other level set intersecting $\Gamma_{\varphi(r)}$ can be considered as faces of the epigraph of $-\risk{L}$, and therefore a face of the induced power diagram.
%  Therefore, we remove lower-dimensional faces, as they are proper subsets of the level sets of embedded reports, and conclude such $\Gamma_u \not \in \trim(\Gamma)$.
%  % in one of two cases:
%  % First, if the level set $\Gamma_u$ (for $u \not \in \varphi(\R)$) is a projection of a lower-dimensional face of $\risk{L}$, which is contained in a facet of the epigraph.
%  % Therefore, we must have $\Gamma_u \subset \Gamma_{\varphi(r)}$ for some $r \in \R$, and therefore $\Gamma_u \not \in \trim(\Gamma)$.
%  
%  Second, $\Gamma_u = \Gamma_{\varphi(r)}$ for some $r \in \R$, then the level set is in both $\{\Gamma_{\varphi(r)} : r \in \R\}$ and $\trim(\Gamma)$.
%
%  \proposedadd{First, construct $\Sc := \Sc(L)$ as Definition~\ref{cons:rep-set}\jessiet{Argue there is a bijection from this set to any other minimum representative set}, which is a minimum representative set by Corollary~\ref{cor:poly-risk-fin-rep}. 
%    Moreover, we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$ by embedding, and therefore $\gamma_r = \Gamma_{\varphi(r)}$, yielding $\{\gamma_r \mid r \in \Sc\} = \{\Gamma_{\varphi(r)} \mid r \in \Sc\}$ as this is true for all $r \in \Sc$.}
%
%  \proposedadd{Now let $T := \{\gamma_r \mid r \in \Sc\} = \trim(\gamma)$. 
%    First, if $\gamma_r \in T$, then it is not a subset of any level set as $\gamma_r$ is full-dimensional in the simplex as it corresponds to a facet of the epigraph of the negative risk $E_\ell$, and the supporting hyperplane is therefore unique.
%    Now, if $\theta \in \trim(\gamma)$, then there is some report $r'$ such that $\theta = \gamma_{r'}$.
%    If $r' \not \in \Sc$, we want to claim there is some $r$ such that $\gamma_{r'} = \gamma_r$.
%    As $\theta$ is a maximal level set, it has a nonempty interior.
%    Therefore, for any $p \in \inter{\theta}$ (and one such $p$ must exist), the hyperplane $p \mapsto (p, \ell(r'))$ uniquely supports the epigraph of negative risk.
%    If $r' \not \in \Sc$, then there must be some $r \in \Sc$ such that $p \mapsto (p, \ell(r))$ also supports the same facet of $E_\ell$; if no such $r \in \Sc$ existed, we would contradict our claim that $\Sc$ is representative for $\ell$.
%    Similarly, we have $\{\Gamma_u \mid u \in \varphi(\Sc)\} = \trim(\Gamma)$ by the same logic.}
%\end{proof}

%\jessiet{03.01.21 - Old version of Prop~\ref{prop:embed-trim} moved to comment}
% \jessiet{Can we remove this?}
% \begin{proof}[Original]
%   Let $L$ elicit $\Gamma$.
%   
%   1 $\Rightarrow$ 2:
%   By the embedding condition, taking $\R_1 = \reals^d$ and $\R_2 = \varphi(\R)$ satisfies the conditions of Lemma~\ref{lem:loss-restrict}: for all $p\in\simplex$, as $\gamma(p) \neq \emptyset$ by definition, we have some $r\in\gamma(p)$ and thus some $\varphi(r) \in \Gamma(p)$.
%   Let $G(p) := -\min_{u\in\reals^d} \inprod{p}{L(u)}$ be the negative Bayes risk of $L$, which is convex, and $G_{\R}$ that of $L|_{\varphi(\R)}$.
%   By the Lemma, we also have $G = G_\R$.
%   As $\gamma$ is finite, $G$ is polyhedral.
%   Moreover, the projection of the epigraph of $G$ onto $\simplex$ forms a power diagram, with the facets projecting onto the level sets of $\gamma$, the cells of the power diagram.
%   (See Theorem~\ref{thm:aurenhammer}.)
%   As $L$ elicits $\Gamma$, for all $u\in\reals^d$, the hyperplane $p\mapsto \inprod{p}{L(u)}$ is a supporting hyperplane of the epigraph of $G$ at $(p,G(p))$ if and only if $u\in\Gamma(p)$.
%   This supporting hyperplane exposes some face $F$ of the epigraph of $G$, which must be contained in some facet $F'$.
%   Thus, the projection of $F$, which is $\Gamma_u$, must be contained in the projection of $F'$, which is a level set of $\gamma$.
%   We conclude that $\Gamma_u \subseteq \gamma_r$ for some $r\in\R$.
%   Hence, $\trim(\Gamma) = \{\gamma_r : r\in\R\}$, which is finite, and unions to $\simplex$.
%   
%   2 $\Rightarrow$ 3: let $\R = \{u_1,\ldots,u_k\} \subseteq\reals^d$ be a set of distinct reports such that $\trim(\Gamma) = \{\Gamma_{u_1},\ldots,\Gamma_{u_k}\}$.
%   Now as $\cup\,\trim(\Gamma) = \simplex$, for any $p\in\simplex$, we have $p\in\Gamma_{u_i}$ for some $u_i\in\R$, and thus $\Gamma(p) \cap \R \neq \emptyset$.
%   We now satisfy the conditions of Lemma~\ref{lem:loss-restrict} with $\R_1 = \reals^d$ and $\R_2 = \R$.
%   The property $\gamma:p\mapsto\Gamma(p)\cap\R$ is non-redundant by the definition of $\trim$, finite, and elicitable.
%   Now from Lemma~\ref{lem:finite-full-dim}, the level sets $\Theta = \{\gamma_r:r\in\R\}$ are full-dimensional, and union to $\simplex$.
%   Statement 3 then follows from the fact that $\gamma_r = \Gamma_r$ for all $r\in\R$.
%   
%   3 $\Rightarrow$ 1: let $\Theta = \{\theta_1,\ldots,\theta_k\}$.
%   For all $i\in\{1,\ldots,k\}$ let $u_i\in\reals^d$ such that $\Gamma_{u_i} = \theta_i$.
%   Now define $\gamma:\simplex\toto\{1,\ldots,k\}$ by $\gamma(p) = \{i : p\in\theta_i\}$, which is non-degenerate as $\cup\,\Theta = \simplex$.
%   By construction, we have $\gamma_i = \theta_i = \Gamma_{u_i}$ for all $i$, so letting $\varphi(i) = u_i$ we satisfy the definition of embedding, namely statement 1.
% \end{proof}






% \begin{lemma}\label{lem:fdls-in-trim}
%   If $\theta$ is a full-dimensional level set (in the simplex) of the elicitable property $\Gamma$, then $\theta \in \trim(\Gamma)$.
% \end{lemma}
% \begin{proof}
%   Let $u$ be the report such that $\theta = \Gamma_u$.
%   If there is a $u'$ so that $\Gamma_u = \Gamma_{u'}$, one of the level sets is in $\trim(\Gamma)$, and then we have $\theta$ equal to that level set.
%   If $\Gamma_u \subsetneq \Gamma_{u'}$, then either $\Gamma_u$ is not a full-dimensional level set or $\Gamma_u \cap \Gamma_{u'}$ is full-dimensonal.
%   If $\Gamma_u$ is not full-dimensional, then we contradict our assumption.
%   If $\Gamma_u \cap \Gamma_{u'}$ is full dimensional, then we contradict our elicitability condition (Lemma~\ref{lem:elicitable-level-sets}).
%   Therefore, full-dimensional level sets of an elicitable property must be in the trim operation of the same property.
% \end{proof}



\section{Surrogate Regret Bounds} \label{app:regret-bounds}

The following is a direct result of Jensen's inequality, using the fact that $R_L(h;\D) = \E_X R_L(h(X),p_X)$ where $p_X$ is the conditional distribution on $Y$ given $X$.
\begin{observation} \label{obs:transfer}
  If $(L,\psi)$ guarantee a conditional regret transfer of $\zeta$ for $\ell$, and $\zeta$ is concave, then $(L,\psi)$ guarantee a regret transfer of $\zeta$ for $\ell$.
\end{observation}

\begin{lemma}\label{lemma:linear-on-levelset}
  Suppose $(L,\psi)$ indirectly elicits $\ell$ and let $\Gamma = \prop{L}$.
  Then for any fixed $u,u^* \in \reals^d$ and $r \in \R$, the functions $R_L(u,\cdot)$ and $R_{\ell}(r,\cdot)$ are linear in their second arguments on $\Gamma_{u^*}$.
\end{lemma}
\begin{proof}
  Let $u^* \in \reals^d$ and $p \in \Gamma_{u^*}$.
  By definition, for all $p \in \Gamma_{u^*}$, $\risk{L}(p) = \inprod{p}{L(u^*)}$.
  So for fixed $u$,
    \[ R_L(u,p) = \inprod{p}{L(u)} - \inprod{p}{L(u^*)} = \inprod{p}{L(u) - L(u^*)} , \]
  a linear function of $p$ on $\Gamma_{u^*}$.
  Next, by Definitions~\ref{def:refines} and~\ref{def:indirectly-elicits}, there exists $r^*$ such that $\Gamma_{u^*} \subseteq \gamma_{r^*}$.
  By the same argument, for fixed $r$, $R_{\ell}(r,p) = \inprod{p}{\ell(r) - \ell(r^*)}$, a linear function of $p$ on $\gamma_{r^*}$ and thus on $\Gamma_{u^*}$.
\end{proof}

\begin{lemma}\label{lemma:separated-constant-p}
  Let $\ell: \R \to \reals_+^{\Y}$ be a discrete target loss, $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral surrogate loss, and $\psi: \reals^d \to \R$ a link function.
  If $(L,\psi)$ indirectly elicit $\ell$ and $\psi$ is $\epsilon$-separated, then for all $u$ and $p$,
    \[ R_{\ell}(\psi(u),p) \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p) . \]
\end{lemma}
\begin{proof}
  If $\psi(u) \in \gamma(p)$, then $R_{\ell}(u,p) = 0$ and we are done.
  Otherwise, applying the definition of $\epsilon$-separated and Lemma~\ref{lemma:hoffman-polyhedral},
  \begin{align*}
    \epsilon &<    d_{\infty}(u,\Gamma(p))  \\
             &\leq H_{L,p} R_L(u,p) .
  \end{align*}
  So $R_{\ell}(\psi(u),p) \leq C_{\ell} \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p)$.
\end{proof}

\begin{theorem}\label{thm:separated-constant}
  Let $\ell: \R \to \reals_+^{\Y}$ be discrete, $L: \reals^d \to \reals_+^{\Y}$ polyhedral, and $\psi: \reals^d \to \R$.
  If $(L,\psi)$ are consistent for $\ell$, then
    \[ (\forall h,\D) \quad R_{\ell}(\psi \circ h ; \D) \leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(h ; \D) ~. \]
\end{theorem}
\begin{proof}
  Let $\Gamma = \prop{L}$.
  From Lemma~\ref{lem:X}, there is a finite set $U \subset \reals^d$ of predictions such that (a) for each $u \in U$, the level set $\Gamma_u$ is a polytope (see e.g.\ Lemma~\label{lem:level-set-is-projected-face}), and (b) $\cup_{u \in U} \Gamma_u = \simplex$.
  For each $u\in U$ let $\mathcal{Q}_u \subset \simplex$ be the finite set of vertices of the polytope $\Gamma_u$, and define the finite set $\mathcal{Q} = \cup_{u \in U} \mathcal{Q}_u$.
  
  By Lemma~\ref{lemma:calibrated-eps-sep}, $\psi$ is separated and $\epsilon_{\psi}$ well-defined.
  By Lemma~\ref{lemma:separated-constant-p}, for each $p \in \mathcal{Q}$, $R_{\ell}(\psi(u),p) \leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(u,p)$ for all $u\in\reals^d$.
  Now consider a general $p \in \simplex$, which is in some full-dimensional polytope level set $\Gamma_u$.
  Write $p = \sum_{q \in \mathcal{Q}_u} \beta(q) q$ for some convex combination $\beta \in \Delta_{\mathcal Q_u}$.
  By Lemma~\ref{lemma:linear-on-levelset}, $R_L$ and $R_{\ell}$ are linear in the second argument on $\Gamma_u$, so for any $u'\in\reals^d$,
  \begin{align*}
    R_{\ell}(\psi(u'),p)
    &=    \sum_{q \in \mathcal{Q}_u} \beta(q) R_{\ell}(\psi(u'), q)  \\
    &\leq \sum_{q \in \mathcal{Q}_u} \beta(q) \frac{C_{\ell} H_{L,p}}{\epsilon_{\psi}} R_L(u', q)  \\
    &\leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} \sum_{q \in \mathcal{Q}_u} \beta(q) R_L(u', q)  \\
    &\leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(u', p) .
  \end{align*}

  \raf{Replace this last line (and preamble above) with a direct calculation -- don't need the observation, since we are actually linear!}

  By Observation~\ref{obs:transfer}, this conditional regret transfer implies a full regret transfer, with the same constant.
\end{proof}



\section{Existence of a Separated Link} \label{app:sep-link-exists}
In this section, we prove Theorem \ref{thm:thickened-separated}, as discussed at the beginning of Appendix \ref{sec:equiv-sep-calib}.

We define some notation and assumptions to be used throughout this section.
Let some norm $\|\cdot\|$ on finite-dimensional Euclidean space be given.
Given a set $T$ and a point $u$, let $d(T,u) = \inf_{t \in T} \|t-u\|$.
Given two sets $T,T'$, let $d(T,T') = \inf_{t\in T, t' \in T'} \|t-t'\|$.
Finally, let the ``thickening'' $B(T,\epsilon)$ be defined as
  \[ B(T,\epsilon) = \{u \in \R' : d(T,u) < \epsilon \} . \]

\begin{assumption} \label{assume:cal}
  $\ell: \R \times \Y \to \reals^{\Y}_+$ is a loss on a finite report set $\R$, eliciting the property $\gamma: \simplex \toto \R$.
  It is embedded by $L: \reals^d \times \Y \to \reals^{\Y}_+$, which elicits the property $\Gamma: \simplex \toto \reals^d$.
  The embedding points are $\{\varphi(r) : r \in \R\}$.
\end{assumption}

Given Assumption \ref{assume:cal}, let $\mathcal{S} \subseteq 2^{\R}$ be defined as $\mathcal{S} = \{\gamma(p) : p \in \Delta_{\Y}\}$.
In other words, for each $p$, we take the set of optimal reports $R = \gamma(p) \subseteq \R$, and we add $R$ to $\mathcal{S}$.
Let $\U \subseteq 2^{\reals^d}$ be defined as $\U = \{\Gamma(p) : p \in \Delta_{\Y}\}$.
For each $U \in \U$, let $R_U = \{r: \varphi(r) \in U\}$.

The next lemma shows that if a subset of $\U$ intersect, then their corresponding report sets intersect as well.
\begin{lemma} \label{lemma:calibrated-pos}
  Let $\U' \subseteq \U$.
  If $\cap_{U\in\U'} U \neq \emptyset$ then $\cap_{U\in\U'} R_U \neq \emptyset$.
\end{lemma}
\begin{proof}
  Let $u \in \cap_{U\in\U'} U$.
  Our first claim is that there exists $r$ such that $\Gamma_u \subseteq \gamma_r$.
  This follows from Proposition \ref{prop:embed-trim}, which shows that $\trim(\Gamma) = \{ \gamma_r : r \in \R\}$.
  Each $\Gamma_u$ is either in $\trim(\Gamma)$ or is contained in some set in $\trim(\Gamma)$, by definition, proving the first claim.
  Our second claim is that $r \in \cap_{U\in\U'} R_U$, which proves the lemma.
  To prove the second claim, take any $U \in \U'$.
  There is some $p$ such that $U = \Gamma(p)$, and we have in particular $p \in \Gamma_u$.
  By the first claim, $p \in \gamma_r$.
  By definition of embedding, $p \in \gamma_r \implies \varphi(r) \in \Gamma(p) = U$, so $r \in R_U$.
\end{proof}
Lemma \ref{lemma:calibrated-pos} implies that there exists a $\psi$ such that $(L,\psi)$ indirectly elicits $\ell$: for each $u$, let $\U' = \{U\in\U : u \in U\}$ be the optimal sets that contain it; choose $r$ from the nonempty set $\cap_{U \in\U'} R_U$; and set $\psi(u) = r$.

The main problem now is to prove a ``thickened'' analogue of Lemma \ref{lemma:calibrated-pos} that extends this link to points $u$ that are up to $\epsilon$ far from an optimal set $U$.
Namely, Lemma \ref{lemma:thick-empty} will show that if $\epsilon$ is small enough, then the $\epsilon$-thickenings of all $U \in \U'$ intersect if and only if the $U$ sets themselves intersect.
Thus, if $u \in \cap_{U \in \U'} B(U,\epsilon)$, then $u \in \cap_{U \in \U'} U$, and Lemma \ref{lemma:calibrated-pos} gives some legal target report $\psi(u) = r \in \cap_{U \in \U'} R_U$.

The next few geometric results build to Lemma \ref{lemma:thick-empty}.
Then, the main proof will be completed as we have just sketched.

\begin{lemma} \label{lemma:enclose-halfspaces}
  Let $D$ be a closed, convex polyhedron in $\reals^d$.
  For any $\epsilon > 0$, there exists an \emph{open}, convex set $D'$, the intersection of a finite number of open halfspaces, such that
    \[ D \subseteq D' \subseteq B(D,\epsilon) . \]
\end{lemma}
\begin{proof}
  Let $S$ be the standard open $\epsilon$-ball $B(\{\vec{0}\},\epsilon)$.
  Note that $B(D,\epsilon) = D + S$ where $+$ is the Minkowski sum.
%  Now let $S' = \{u : \|u\|_1 < \delta\}$ be the open $\delta$ ball in $L_1$ norm.
%  By equivalence of norms in Euclidean space, \bo{cite} we can take $\delta$ small enough yet positive such that $S' \subseteq S$.
%  By Lemma \ref{lemma:open-plus-closed-poly}, the Minkowski sum $D' = D + S'$ is an open polyhedron, i.e. the intersection of a finite number of open halfspaces.
  Now let $S' = \{u : \|u\|_1 \leq \delta\}$ be the closed $\delta$ ball in $L_1$ norm.
  By equivalence of norms in Euclidean space~\cite[Appendix A.1.4]{boyd2004convex}, we can take $\delta$ small enough yet positive such that $S' \subseteq S$.
  By standard results, the Minkowski sum of two closed, convex polyhedra, $D'' = D + S'$ is a closed polyhedron, i.e. the intersection of a finite number of closed halfspaces. (A proof: we can form the higher-dimensional polyhedron $\{(x,y,z) : x \in D, y \in S', z = x+y\}$, then project onto the $z$ coordinates.)

  Now, if $T' \subseteq T$, then the Minkowksi sum satisfies $D + T' \subseteq D + T$.
  In particular, because $\emptyset \subseteq S' \subseteq S$, we have
    \[ D \subseteq D'' \subseteq B(D,\epsilon) . \]
  Now let $D'$ be the interior of $D''$, i.e. if $D'' = \{x : Ax \leq b\}$, then we let $D' = \{x: Ax < b\}$.
  We retain $D' \subseteq B(D,\epsilon)$.
  Further, we retain $D \subseteq D'$, because $D$ is contained in the interior of $D'' = D + S'$.
  (Proof: if $x \in D$, then for some $\gamma$, $x + B(\{\vec{0}\},\gamma) = B(x,\gamma)$ is contained in $D + S'$.)
  This proves the lemma.
\end{proof}

\begin{lemma} \label{lemma:thick-nonempty}
  Let $\{U_j : j \in \mathcal{J}\}$ be a finite collection of closed, convex sets with $\cap_{j\in\mathcal{J}} U_j \neq \emptyset$.
  Let $\delta > 0$ be given.
  Then there exists  $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, $\cap_j B(U_j,\epsilon) \subseteq B(\cap_j U_j, \delta)$.
\end{lemma}
\begin{proof}
  We induct on $|\mathcal{J}|$.
  If $|\mathcal{J}|=1$, set $\epsilon = \delta$.
  If $|\mathcal{J}|>1$, let $j\in\mathcal{J}$ be arbitrary, let $U' = \cap_{j'\neq j} U_{j'}$, and let $C(\epsilon) = \cap_{j' \neq j} B(U_{j'},\epsilon)$.
  Let $D = U_j \cap U'$.
  We must show that $B(U_j,\epsilon) \cap C(\epsilon) \subseteq B(D,\delta)$.
  By Lemma \ref{lemma:enclose-halfspaces}, we can enclose $D$ strictly within a polyhedron $D'$, the intersection of a finite number of open halfspaces, which is itself strictly enclosed in $B(D,\delta)$.
  (For example, if $D$ is a point, then enclose it in a hypercube, which is enclosed in the ball $B(D,\delta)$.)
  We will prove that, for all small enough $\epsilon$, $B(U_j,\epsilon) \cap C(\epsilon)$ is contained in $D'$.
  This implies that it is contained in $B(D,\delta)$.

  For each halfspace defining $D'$, consider its complement $F$, a closed halfspace.
  We prove that $F \cap B(U_j,\epsilon) \cap C(\epsilon) = \emptyset$.
  Consider the intersections of $F$ with $U$ and $U'$, call them $G$ and $G'$.
  These are closed, convex sets that do not intersect (because $D$ in contained in the complement of $F$).
  So $G$ and $G'$ are separated by a nonzero distance, so $B(G,\gamma) \cap B(G',\gamma) = \emptyset$ for all small enough $\gamma$.
  And $B(G,\gamma) = F \cap B(U_j,\gamma)$ while $B(G',\gamma) = F \cap B(U',\gamma)$.
  This proves that $F \cap B(U_j,\gamma) \cap B(U',\gamma) = \emptyset$.
  By inductive assumption, $C(\epsilon) \subseteq B(U',\gamma)$ for small enough $\epsilon = \epsilon_F$.
  So $F \cap B(U_j,\gamma) \cap C(\epsilon) = \emptyset$.
  We now let $\epsilon_0$ be the minimum over these finitely many $\epsilon_F$ (one per halfspace).
\end{proof}

\begin{figure}
\caption{Illustration of a special case of the proof of Lemma \ref{lemma:thick-nonempty} where there are two sets $U_1,U_2$ and their intersection $D$ is a point. We build the polyhedron $D'$ inside $B(D,\delta)$. By considering each halfspace that defines $D'$, we then show that for small enough $\epsilon$, $B(U_1,\epsilon)$ and $B(U_2,\epsilon)$ do not intersect outside $D'$. So the intersection is contained in $D'$, so it is contained in $B(D,\delta)$.}
\includegraphics[width=0.24\textwidth]{figs/separated-proof-2} \hfill
\includegraphics[width=0.24\textwidth]{figs/separated-proof-3} \hfill
\includegraphics[width=0.24\textwidth]{figs/separated-proof-4} \hfill
\includegraphics[width=0.24\textwidth]{figs/separated-proof-5}
\end{figure}

\begin{lemma} \label{lemma:thick-empty}
  Let $\{U_j : j \in \mathcal{J}\}$ be a finite collection of nonempty closed, convex sets with $\cap_{j\in\mathcal{J}} U_j = \emptyset$.
  Then there exists  $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, $\cap_{j\in\mathcal{J}} B(U_j,\epsilon) = \emptyset$.
\end{lemma}
\begin{proof}
  By induction on the size of the family.
  Note that the family must have size at least two.
  Let $U_j$ be any set in the family and let $U' = \cap_{j' \neq j} U_{j'}$.
  There are two possibilities.

  The first possibility, which includes the base case where the size of the family is two, is the case $U'$ is nonempty.
  Because $U_j$ and $U'$ are non-intersecting closed convex sets, they are separated by some distance $\delta$.
  So $B(U_j, \delta/3) \cap B(U', \delta/3) = \emptyset$.
  By Lemma \ref{lemma:thick-nonempty}, there exists $\epsilon'_0 > 0$ such that $\cap_{j'\neq j} B(U_{j'},\epsilon) \subseteq B(U', \delta/3)$ for all $0 < \epsilon \leq \epsilon'_0$.
  Pick $\epsilon_0 = \min\{\epsilon'_0,\delta/3\}$.
  Then for all $0 < \epsilon \leq \epsilon_0$, the intersection of $\epsilon$-thickenings is contained in the $(\delta/3)$-thickening of the intersection, which is disjoint from the $(\delta/3)$-thickening of $U_j$, which contains the $\epsilon$-thickening of $U_j$.

  The second possibility is that $U'$ is empty.
  This implies we are not in the base case, as the family must have three or more sets.
  By inductive assumption, for all small enough $\epsilon$ we have $\cap_{j' \neq j} B(U_{j'},\epsilon) = \emptyset$, which proves this case.
\end{proof}


\begin{corollary} \label{cor:thick-intersect}
  There exists $\epsilon_0 > 0$ such that, for any $0 < \epsilon \leq \epsilon_0$, for any subset $\{U_j : j \in \mathcal{J}\}$ of $\U$, if $\cap_j U_j = \emptyset$, then $\cap_j B(U_j,\epsilon) = \emptyset$.
\end{corollary}
\begin{proof}
  For each subset, Lemma \ref{lemma:thick-empty} gives an $\epsilon_0 > 0$.
  We take the minimum over these finitely many subsets of $\U$.
\end{proof}

\begin{theorem} \label{thm:small-eps-thick}
  For all small enough $\epsilon$, the epsilon-thickened link $\psi$ (Construction \ref{const:eps-thick-link}) is a well-defined link function from $\R'$ to $\R$, i.e. $\psi(u) \neq \bot$ for all $u$.
\end{theorem}
\begin{proof}
  Fix a small enough $\epsilon$ as promised by Corollary \ref{cor:thick-intersect}.
  Consider any $u \in \R'$.
  If $u$ is not in $B(U,\epsilon)$ for any $U \in \U$, then we have $\Psi(u) = \R$, so it is nonempty.
  Otherwise, let $\{U_j : j \in \mathcal{J}\}$ be the family whose thickenings intersect at $u$.
  By Corollary \ref{cor:thick-intersect}, because of our choice of $\epsilon$, the family themselves has nonempty intersection.
  By Lemma \ref{lemma:calibrated-pos}, their corresponding report sets $\{R_j : j \in \mathcal{J}\}$ also intersect at some $r$, so $\Psi(u)$ is nonempty.
\end{proof}

Theorem \ref{thm:thickened-separated}, which we restate here, is now almost immediate.
\thickenedseparated*
%% Bo: copying the text in a comment here for reference
%\begin{restatable}{theorem}{thickenedseparated} \label{thm:thickened-separated}
%  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$ embed the discrete loss $\ell:\R\to\reals^\Y_+$.
%  Then there exists $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, Construction~\ref{const:eps-thick-link} yields an $\epsilon$-separated link with respect to $L$ and $\ell$.
%\end{restatable}
\begin{proof}
  We create $\psi$ using Construction \ref{const:eps-thick-link} with the $L_{\infty}$ norm.
  By Theorem \ref{thm:small-eps-thick}, for all small enough $\epsilon$, $\psi$ is well-defined everywhere.

  To prove separation, suppose $u$ and $p$ are given such that $d_{\infty}(u,U) \leq \epsilon$, where $U = \Gamma(p)$.
  Then in Construction \ref{const:eps-thick-link}, $\psi(u) \in \Psi(u) \subseteq R_U = \{r : \varphi(r) \in U\}$.
  By definition of embedding, $\varphi(r) \in U = \Gamma(p) \implies r \in \gamma(p)$.
  So we obtain $\psi(u) \in \gamma(p)$ whenever $d_{\infty}(u,\Gamma(p)) \leq \epsilon$, which proves $\epsilon$-separation of the link $\psi$.
\end{proof}

\section{General characteristics of polyhedra}
\label{app:polyhedra}
\subsection{Definitions and preliminaries}
\label{app:polyhedra:defs}
\begin{definition}[Closed halfspace]
  A closed halfspace is a set of the form $H_{(w,b)}^+ := \{ x \in \reals^d \mid \inprod{x}{w} \geq b\}$ for any $(w,b) \in \reals^d \times \reals$.
  % Similarly let $H_{(w,b)}^- := \{ x \in \reals^d \mid \inprod{x}{w} \leq b\}$.
\end{definition}
\begin{definition}[Hyperplane]
  A hyperplane is a set of the form $H_{(w,b)} := \{ x \in \reals^d \mid \inprod{x}{w} = b\}$ for any $(w,b)\in\reals^d \times \reals$.
  % The halfspaces corresponding to a hyperplane $H = H_{(w,b)}$ are the sets $H_{(w,b)}^+$ and $H_{(w,b)}^-$.
\end{definition} 
Observe that $H_{(w,b)} = \partial H^+_{(w,b)}$, meaning the hyperplane $H_{(w,b)}$ is the boundary of $H^+_{(w,b)}$.
Thus, for any halfspace $H^+$, we have that $H^+$ is one of the two halfspaces corresponding to the hyperplane $\partial H^+ = H$.


\begin{definition}[Polyhedron halfspace representation]
	\btw{Def from Ziegler page 28}
	A \emph{polyhedron} $P$ is an intersection of a finite set of closed halfspaces $\H$
  % = \{H^+_i\}_{i=1}^k$
  presented in the form $P = \cap \H$.
\end{definition}
Observe that by the halfspace representation, a polyhedron need not be bounded.


\begin{definition}[Valid, Supports]
	A halfspace $H^+$ is \emph{valid} for $P$ if $P \subseteq H^+$.
	A hyperplane $H$ \emph{supports} the polyhedron $P$ if 
	% (i) $P \subseteq H^+$ for a halfspace $H^+$ corresponding to $H$
	(i) $P \subseteq H^+$ for a halfspace $H^+$ with $H = \partial H^+$, and
	(ii)$H \cap \partial P \neq \emptyset$.
	Moreover, $H$ supports $P$ at $x$ if $x \in H \cap \partial P$.
\end{definition}


\begin{definition}[Face, facet]\label{def:face}
  Let $P \subseteq \reals^d$ be a convex polyhedron.
  A (non-trivial)\jessiet{word choice} \emph{face} $F$ of the polytope $P$ is any set of the form
  \begin{equation*}
    F = P \cap H~,
  \end{equation*}
  for a hyperplane $H$ supporting $P$.
  %valid halfspace $H^+_{(w,b)}$.
  The dimension of a face $F$ is the dimension of its affine hull $\dim(F) := \dim(\affhull(F))$.
  A face $F$ with $\dim(F) = \dim(\affhull(P)) - 1$ is called a facet.

%Let $P \subseteq \reals^d$ be a convex polyhedron.
%A halfspace $H^+_{(w,b)}$ is \emph{valid} for $P$ if $P \subseteq H^+_{(w,b)}$.
%% A linear inequality $\inprod{w}{x} \geq b$ is \emph{valid} for $P$ if it is satisfied for all $x \in P$.
%A \emph{face} $F_{(w,b)}$ of the polytope $P$ is any set of the form
%\begin{equation*}
%F_{(w,b)} = P \cap H_{(w,b)}~,~
%\end{equation*}
%for any valid halfspace $H^+_{(w,b)}$.
%The dimension of a face $F$ is the dimension of its affine hull $\dim(F) := \dim(\affhull(F))$.
%A face $F$ with $\dim(F) = \dim(\affhull(P)) - 1$ is called a facet.

\end{definition}

Observe that $P$ is a trivial face of itself, and cannot be written by the above definition.
Throughout, we restrict our focus to non-trivial faces, and omit mentioning non-trivial henceforth.
\jessiet{is this clear?}

\begin{claim}
  A face $F$ of the polyhedron $P$ such that $F = P \cap H$ 
  %such that $F_{(w,b)} = P \cap H_{(w,b)}$ 
  is nonempty if and only if $H$ is a supporting hyperplane of $P$.
\end{claim}

It is often useful to understand polyhedra in terms of their halfspace representations and the set of hyperplanes generating facets of $P$.
To find this set, we must first establish when a halfspace representation is irredundant for a given polyhedron.

\begin{definition}[{\citet{gallier2008notes}}]\label{def:irredundant}
	Let $P = \cap \H$ for a finite set of halfspaces $\H$ be a polyhedron.
	We say that $\cap \H$ is an \emph{irredundant decomposition} for $P$ (and $\H$ is irredundant for $P$) if $P$ cannot be expressed as $P = \cap \H'$ for some $\H'$ such that $|\H'| < |\H|$.
\end{definition}

\citet{gallier2008notes} shows that every full-dimensional (i.e. $\dim(\affhull(P)) = d$) polyhedron $P \subseteq \reals^d$ has a unique and irredundant halfspace representation $\H^*$, and each $H^+ \in \H^*$ generates a facet of $P$.

\begin{theorem}\label{thm:polyhedron-uniquely-gen-facets}
  Given a $d$-dimensional polyhedron $P \subseteq \reals^d$, 
  (i) there is a unique irredundant and finite set of closed halfspaces $\H^*$ such that $P = \cap \H^*$, 
  (ii) $\{H \cap P \mid H^+ \in \H^*, H = \partial H^+\}$ is the set of facets of $P$, and
  (iii) for all finite sets of closed halfspaces $\H$ such that $P = \cap \H$, we have $\H^* \subseteq \H$, .
\end{theorem}
\begin{proof}
	Since $P$ is $d$-dimensional in $\reals^d$, it therefore has nonempty interior.
	We claim that $P$ must have some irredundant representation $P = \cap \H$ for a finite set $\H$.
	As $P$ has a finite halfspace representation, it must have a smallest halfspace representation $\H^*$.
	That is, $|\H^*| = \min \{\cap \H \mid P = \cap \H, \H$ finite$\}$.
	As the smallest halfspace representation, $\H^*$ is irredundant; if it was redundant, this would imply there is a smaller representation $\H'$ so that $P = \cap \H'$ and $|\H'| < |\H^*|$, contradicting $\H^*$ as the smallest representation.
%	Since $P$ has a (finite) halfspace representation, if any representation $\cap \H$ was redundant, then its redundant representation would either be irredundant or have a smaller representation.
%	This can only iterate up to $|\H^*|$ times, so there must be some finite irredunant representation for $P$.\jessie{This feels way too spelled out for its lack of detail.}
	\citet[Proposition 4.5(i)]{gallier2008notes} then states that the irredundant representation $\H^*$ is unique up to ordering, allowing us to conclude (i).
	Additionally, (ii) is shown by~\citep[Proposition 4.5(ii)]{gallier2008notes}.
	

	It is just left to show (iii).
	%Suppose for contradiction that there was a set of closed halfspaces $\H$ such that $P = \cap \H$, but there was a $H^+_{(w,b)} \in \H^* \setminus \H$, and therefore $\H^* \not \subseteq \H$.
	By (i), we know that each $H^+ \in \H^*$ uniquely determines a facet of $P$.
	Moreover, by (ii), define $F := P \cap H$, where $H = \partial H^+$ and $H^+ \in \H^*$, which is a facet of $P$ and of dimension $d-1$.
	The facet $F$ can then be defined by $d$ affinely independent points (contained in $P$), whose affine hull is $H$.
	As halfspaces are uniquely determined, so is the facet $F = P \cap H$.
	As polyhedron are uniquely determined by their facets (by Minkowski's uniqueness theorem, cf.,~\citep{klain2004minkowski}), we must have $H^+ \in \H^*$. 
	\jessiet{This is not as precise as I'd like... but also I think it follows more immediately from Minkowski's uniqueness theorem}
		
%  \jessiet{\url{https://arxiv.org/abs/0805.0292} Prop 4.5? Ziegler Lectures on Polytopes Theorem 2.15 (7) also kind of close to this, but on polytopes not polyhedron.}
\end{proof}

\subsection{Notation}\label{appsubsec:notation}
Within this appendix, we use some self-contained notation.
We will later consider losses over a finite set of outcomes $\Y$; to make notation consistent, we use $\reals^\Y_+$ throughout as shorthand for $\reals^{|\Y|}_+$, and let $d := |\Y|+1$.

Fix a set $\V \subseteq \reals^\Y_+$, and consider the concave function $g_\V : x \mapsto \inf_{v \in \V}\inprod{v}{x} - \delta(x \mid \reals^\Y_+)$.
We denote the hypograph of $g_\V$ by $\hyp(g_\V) = \{(x,c) \mid c \leq g(x)\} \subseteq \reals^\Y_+ \times \reals$.

Given any $v \in \V \subseteq \reals^\Y_+$, define $H^+_v := H^+_{(v, -1)} = \{(x, c) \in \reals^{\Y}_+ \times \reals \mid \inprod{v}{x} = c\}$.
%, where $(v, -1)\in\reals^\Y_+\times\reals$ defines a halfspace.
Similarly, we denote $H_y^+ := H_{(e_y, 0)}^+$ for any $y \in \Y$; the latter will help us restrict a constructed polyhedron to the nonnegative orthant. 
Extending to hyperplanes, we construct $H_v := H_{(v,-1)}$ and observe that $H_v = \partial H^+_v$ for $v \in \reals^\Y_+$ and define $H_y := H_{(e_y, 0)}$ so that $H_y = \partial H^+_y$.
Given a polyhedron $P$, we denote the face $F^P_v := H_v \cap P$.
If $P$ is understood from context, we simply denote this face $F_v$. 

Finally, given a set $\V \subseteq \reals^d$, we let $\H_{\V} = \{H_v^+ \mid v\in\V\}$ denote the set of halfspaces generated by $\V$, $\H_\Y = \{H_y^+ \mid y\in\Y\}$.
If $\V$ and $\Y$ are understood from context, we may denote $\H := \H_\V \cup\H_\Y$.


\subsection{Finitely generated polyhedra}\label{appsubsec:phase1}
Throughout, we will work with a (minimizable) function $g_\V$ generated by a set $\V \subseteq \reals^\Y_+$ of the following form.

\begin{definition}\label{def:g-finite}
  Given a set $\V \subseteq \reals^\Y_+$, define the function $g_\V : \reals_+^\Y \to \reals_+$ by
  \begin{align*}
    g_\V(x) = \inf_{v\in\V} \inprod{x}{v} - \delta(x \mid \reals_+^\Y)~. % = \min_{v\in\V} \inprod{x}{v} - \delta(x \mid \reals_+^\Y)~.~
  \end{align*}
\end{definition}

We first observe that the region generated by the intersection of the $H^+_y$ halfspaces restricts the hypograph of any $g_\V$ to be finite only on the nonnegative orthant.
\begin{lemma}\label{lem:x-nonneg-orthant-iff-intersection-HY}
  $\cap \H_\Y = \reals^\Y_+ \times \reals$.
\end{lemma}
\begin{proof}
  The result follows if we show $x \in \reals^\Y_+ \iff (x,c) \in \cap \H_\Y$ for all $c \in \reals$.

  $\implies$
  Fix any $c \in \reals$.
  $x \in \reals^\Y_+ \iff x_y \geq 0$ for all $y \in \Y$.
  This means that for any $y \in \Y$, $(x,c) \in \{(x,c) \mid x_y \geq 0\} = H^+_y$.
  As $y$ and $c$ were arbitrary, this shows the forward direction.
  
  $\impliedby$
  $(x,c) \in \cap \H_\Y$ implies $x_y \geq 0$ for all $y \in \Y$, and therefore $x \in \reals^d_+$.	
\end{proof}

\subsection{Infinitely generated polyhedra with finite representation}\label{appsubsec:phase2}

%In \S~\ref{appsubsec:phase1}, we discussed the function $g_\V$, where $\V \subset \reals^\Y_+$ was finite, but we move towards considering an infinite $\V$ in some settings relevant to embeddings.
We now contextualize the setting of the previous section.
Suppose $L : \R \to \reals^\Y_+$ is a minimizable loss function.
For $x\in\reals^\Y_+$, consider the $1$-homogeneous extension of Bayes risk  $\risk{L}_+(x) := \inf_{r\in\R} \inprod{x}{L(r)} - \delta(x \mid \reals^\Y_+)$, which we assume is polyhedral throughout.


%As before, take $G$ to be the hypograph of $g$.
We now consider $L(\R) \subseteq \reals^\Y_+$ and $\H_{L(\R)} = \{H_v^+ \mid v\in L(\R)\}$.  
Observe that $L(\R)$ and $\H_{L(\R)}$ may be infinitely generated sets.  
Now let $\H = \H_\Y \cup \H_{L(\R)}$; again, this may be infinitely generated.
We find the existence of a finite $\V \subseteq L(\R)$ such that $g_\V = g_{L(\R)}$, and proceed to work with such a $\V$.
\begin{claim}\label{claim:gV-equals-riskL}
	Given a minimizable $L : \R \to \reals^\Y_+$ with polyhedral extended risk $\risk L_+$, consider $\H = \H_\Y \cup \H_{L(\R)}$.
	Then $\hyp(g_{L(\R)}) = \cap\H$.
	Moreover, there is a finite $\V \subseteq L(\R)$ such that $g_{L(\R)} = g_{\V}$. %$\hyp(g_{L(\R)}) = \cap (\H_{\V} \cup \H_\Y) = \hyp(g_{\V})$ and . 
\end{claim}
\begin{proof}
	Observe that $x \in \reals^\Y_+ \iff (x, c) \in \cap \H_\Y$.
	Let $x \in \reals^\Y_+$.
	\begin{align*}
	(x,c) \in \hyp(g_{L(\R)})
	&\iff g_{L(\R)}(x) \geq c & \text{ Definition of hypograph;}\\
	&\iff \risk L_+(x) \geq c & \text{$g_{L(\R)}=\risk L_+$ for $x \in \reals^\Y_+$;}\\
	&\iff \inprod{v}{x} \geq c \,\, \forall v \in {L(\R)} & \text{by def of $\risk L_+$ as the infimum over $v \in {L(\R)}$ of}\\
	& & \text{the inner product with $x$ and minimizable;} \\
	&\iff (x,c) \in H^+_{v} \,\, \forall v \in {L(\R)} & \text{by definition of each halfspace;}\\
	&\iff (x,c) \in \cap \H_{L(\R)} & \text{since true for all $v \in {L(\R)}$.}  
	\end{align*}
	Combining the two equalities (e.g., $\cap \H = (\cap \H_\Y) \cap (\cap \H_{L(\R)})$), we have $\hyp(g_{L(\R)}) = \cap \H$.
	The existence of a finite $\V \subseteq {L(\R)}$ follows as $\risk L_+$ polyhedral implies $\hyp(\risk L_+)$ is a polyhedron, which has a finite representation by definition. \jessiet{Omitting details on why $\V \subseteq {L(\R)}$.}
	%Finally, $\hyp(\risk L_+) = \hyp(g_{L(\R)}) = \hyp(g_{\V})$ if and only if $\risk L_+ = g_{L(\R)} = g_{\V}$. 
\end{proof}

This claim allows us to proceed while considering the finite set $\V$ rather than the full range $L(\R)$.
We now evaluate the structure of $g_{L(\R)} = g_\V$ and its hypograph through $\V$.

\subsection{Structure of $g_\V$}

%Throughout this section let $G:= \hyp(g_\V)$ for a fixed finite set $\V$.
We will assume $\V \subset \reals^\Y_+$ is finite; if $L(\R)$ is finite, then take $\V = L(\R)$.
Otherwise, take any finite $\V \subseteq L(\R)$ as in Claim~\ref{claim:gV-equals-riskL}.
Now, we can define $\hyp(g_\V)$ as the intersection of halfspaces generated by $\V$ on the nonnegative orthant.
\jessiet{Claim vs lemma? also kinda not necessary now as it follow from a corollary of Claim~\ref{claim:gV-equals-riskL}}
\begin{claim}\label{claim:G-intersection-H}
  Given a finite set $\V \subset \reals^\Y_+$, define $\H = \H_\V \cup \H_\Y$.
  Then $\hyp(g_\V) = \cap \H$.
\end{claim}
\begin{proof}
  $(x,c) \in \hyp(g_\V) \iff g(x) - c \geq 0 \iff \min_{v \in \V}\inprod{v}{x} - c \geq 0$ and $x \in \reals_+^\Y$, which is true if and only if $\inprod{v}{x} - c \geq 0 \,\forall v \in \V$ and $x_y \geq 0$ for all $y$.
  In turn, this statement holds if and only if $(x,c) \in H^+_v$ for all $v \in \V$ and in $H^+_y$ for all $y \in \Y$, so $(x,c) \in \cap \H$.
\end{proof}

We proceed with some observations about facets and dimension of $\hyp(g_\V)$ in order to finite the \emph{smallest} halfspace representation for $g_{L(\R)} = g_\V$.

\begin{lemma}\label{lem:G-full-dimensional}
  Given a finite, nonempty set $\V \subset \reals^\Y_+$, $\hyp(g_\V)$ is $d$-dimensional.  %\raf{$g$ is nonnegative on $\reals^\Y_+$, so $G$ contains $\{(x,c) \mid x\in\reals^\Y_+, c \leq 0\}$, which is full-dim.} 
\end{lemma}
\begin{proof}
  Since $g_\V$ is nonnegative on $\reals_+^\Y$, $\hyp(g_\V)$ therefore contains $\{(x,c) \mid x\in\reals^\Y_+, c \leq 0\}$, which is $(|\Y| + 1)$-dimensional.
  Recall $d= (|\Y| + 1)$.
\end{proof}

\jessiet{move theorem~\ref{thm:polyhedron-uniquely-gen-facets} here?}

Lemma~\ref{lem:G-full-dimensional} allows us to apply Theorem~\ref{thm:polyhedron-uniquely-gen-facets} to observe a unique set of halfspaces $\H^*$ generating $\hyp(g_\V)$.

\begin{lemma}\label{lem:G-unique-facets-Hstar}
  Given a finite set $\V \subset \reals^\Y_+$, define $\H = \H_\V \cup \H_\Y$.
  %\raf{This is the important general polyhedra result to cite/state.  Note: we'll have to work half-spaces of the form $H_{(w,\beta)}^+ = \{ z \in \reals^{n+1} \mid \inprod{z}{w} - \beta \geq 0\}$.}  
  There is some unique $\H^* \subseteq \H$ such that $\hyp(g_\V) = \cap \H^*$. 
  Moreover, for each $H^+ \in\H^*$ and $H$ such that $H = \partial H^+$, the face $F = \hyp(g_\V) \cap H$ is a facet. 
\end{lemma}
\begin{proof}
  Since $\hyp(g_\V)$ is full-dimensional by Lemma~\ref{lem:G-full-dimensional}, this follows immediately from Theorem~\ref{thm:polyhedron-uniquely-gen-facets}(i) and (iii).
\end{proof}

We now show that the set $\H_\Y$ is contained in $\H^*$ so that we can separate the facets generated by $\H^*$ into a partition of vertical and non-vertical facets of $\hyp(g_\V)$.

\begin{lemma}\label{lem:HY-subset-Hstar}
  Given a finite set $\V \subset \reals^\Y_+$, consider the unique finite set $\H^*$ as given by Lemma~\ref{lem:G-unique-facets-Hstar}. % such that $\hyp(g_\V) = \cap \H^*$ and $\H^* \subseteq (\H_\V \cup \H_\Y)$.
  $\H_\Y \subseteq \H^*$. 
\end{lemma}
\begin{proof}
  If there was a $y \in \Y$ such that $H^+_y = \{(x,c) \mid x_y \geq 0\}$ was not in $\H^*$, then we would either have some $c_1 > 0$ such that $\{(x,c) \mid x_y \geq c_1\} \in \H^*$, or we have a point $x$ such that $x_y < 0$ but $g(x) > -\infty$.
  The first cannot happen as we take $g_\V$ is finite at $x = e_y \in \reals^\Y_+$ and is concave.
  Moreover, the second cannot be true by construction of $g_\V$ since $\V$ is finite including the $0-\infty$ indicator on $\reals^\Y_+$.	
\end{proof}

\begin{corollary}\label{cor:unique-set-loss-vectors-defining-facets}
  Suppose we are given a finite set $\V \subset \reals^\Y_+$, and consider the unique irredundant set $\H^*$ given by Lemma~\ref{lem:G-unique-facets-Hstar}. %such that $\hyp(g_\V) = \cap \H^*$.
  There is a unique finite set $\V^* \subseteq \reals^\Y_+$ such that $\H^* = \H_\Y \cup \H_{\V^*}$.
  Moreover, $F_v$ is a facet of $\hyp(g_\V)$ for each $v\in\V^*$.
\end{corollary}
\begin{proof}
%  \raf{Argue that all facets of $G$ are an $H_y \cap G$ or some $H_v \cap G$.  Thus, we can define $\V^*$ by $\H_\Y \cup \H_{\V^*} = \H^*$.  (I.e., $\H^* \setminus \H_\Y = \H_{\V^*}$ for some $\V^*$.)}
%
  Since $\hyp(g_\V)$ is full-dimensional, the facets of $\hyp(g_\V)$ are uniquely determined by the hyperplanes $H$ such that $H = \partial H^+$ and $\H^* = \{H^+\}$ by Lemma~\ref{lem:G-unique-facets-Hstar}.
  Any facet must then be some intersection of an $H_y \cap \hyp(g_\V)$ or $H_v \cap \hyp(g_\V)$.
  Consider $\H_{\V^*} := \H^* \setminus \H_{\Y}$, and $\V^*$ the unique set generating $\H_{\V^*}$, since $\H_\Y \subseteq \H^*$ by Lemma~\ref{lem:HY-subset-Hstar}.
  ($\V^*$ is unique as halfspaces are uniquely determined.)
  Moreover, $H_v \in \H_{\V^*} \subseteq \H^*$ generates the facet $F_v$ of $\hyp(g_\V)$ by Lemma~\ref{lem:G-unique-facets-Hstar}.
%
%  The moreover follows immediately since $\H_{\V^*} \subseteq \H^*$ and every $H^+ \in \H^*$ defines a facet by Lemma~\ref{lem:G-unique-facets-Hstar}.
\end{proof}



\begin{corollary}\label{cor:anything-gen-G-subset-Hstar}
  Let $\V \subset \reals^\Y_+$ be a finite set, and $\V^*$ the unique finite set from Corollary~\ref{cor:unique-set-loss-vectors-defining-facets} such that $\H^* = \H_\Y \cup \H_{\V^*}$. %such that $\H := \H_\Y \cup \H_{\V}$ satisfies $\hyp(g_{\V}) = \cap \H$, and take $\V^*$ the unique set such that irredundant (wrt. $\hyp(g_\V)$) set $\H^* = \H_\Y \cup \H_{\V^*}$.  
  Then $\V^* \subseteq \V$.
\end{corollary}
\begin{proof}
  $\H_\Y \cup \H_{\V^*} = \H^*$ by Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}, and $\H^* \subseteq \H = \H_\Y \cup \H_{\V}$ by Lemma~\ref{lem:HY-subset-Hstar}, ergo $\V^* \subseteq \V$.
\end{proof}

Thus, we will introduce our first assumption for the existence of $\V^*$ given a finite $\V \subset \reals^\Y_+$.

%\begin{assumption}\label{assum:V-star-exists-finite}
%	Suppose we are given a finite set $\V \subset \reals^\Y_+$, and consider the unique irredundant set $\H^*$ from Corollary~\ref{cor:unique-set-loss-vectors-defining-facets} such that $\hyp(g_\V) = \cap \H^*$.
%	Moreover, $\V^* \subseteq \V$ is the unique finite set from Corollary~\ref{cor:anything-gen-G-subset-Hstar} such that $\H^* = \H_\Y \cup \H_{\V^*}$ is irredundant.
%\end{assumption}
%


We now show that we can equivalently construct $g_\V$ through the unique finite set $\V^*$ instead of the given set of vectors $\V$, and in turn, loss vectors $L(\R)$.
\begin{lemma}\label{claim:g-gen-by-Vstar}
	Given a finite set $\V \subset \reals^\Y_+$, consider $\V^* \subseteq \V$ as in Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}.
  Then $g_\V(x) = \min_{v \in \V^*}\inprod{v}{x} - \delta(x \mid \reals_+^\Y) = g_{\V^*}(x)$
\end{lemma}
\begin{proof}
  The result holds if $\hyp(g_\V) = \hyp(g_{\V^*})$.
  By construction, $\hyp(g_\V) = \cap (\H_\Y \cup \H_\V)= \cap \H^* = \cap (\H_\Y \cup \H_{\V^*}) = \{(x,c) \in \reals_+^\Y \times \reals \mid \inprod{v^*}{x} \geq c$ for all $v^* \in \V^* \}$ where the first equality follows as $\H^* \subseteq \H$.
  This means $g_\V$ can be written as $g_\V(x) = \min_{v \in \V^*}\inprod{v}{x} - \delta(x \mid \reals^\Y_+) = g_{\V^*}(x)$. 
\end{proof}


\begin{corollary}
	Given minimizable $L : \R \to \reals^\Y_+$ such that $\risk L_+$ is polyhedral, there exists a (unique) finite $\V^* \subseteq L(\R)$ such that $g_\V = g_{\V^*}$ and $\hyp(g_{L(\R)}) = \hyp(g_{\V^*}) = \cap(\H_{\V^*} \cup \H_\Y)$ is irredundant.
\end{corollary}

This paves the way for our primary assumption for the rest of this appendix.

\jessiet{another assumption commented out.  I think current one is more than sufficient}
\begin{assumption}\label{assum:V-star-exists-infinite}
	$L : \R \to \reals^\Y_+$ is a minimizable loss function such that $\risk L_+ = g_{L(\R)}$ is polyhedral.
	$\V \subseteq L(\R)$ is a finite set such that $g_\V = g_{L(\R)}$.
	Finally, $\V^* \subseteq \V \subseteq L(\R)$ is the (unique) finite irredundant set such that $g_{L(\R)} = g_\V = g_{\V^*}$ and $\hyp(g_{L(\R)}) = \hyp(g_\V) = \hyp(g_{\V^*}) = \cap(\H_{\V^*} \cup \H_\Y)$, the last of which is irredundant.
\end{assumption}
%Observe that we can use $\V$ from Claim~\ref{claim:gV-equals-riskL} to apply results from the finitely generated polyhedra case in \S~\ref{appsubsec:phase1}, in particular \ref{lem:G-unique-facets-Hstar}, and \ref{lem:HY-subset-Hstar}, and  Corollaries~\ref{cor:unique-set-loss-vectors-defining-facets} and \ref{cor:anything-gen-G-subset-Hstar} to justify the assumption of $\V^* \subseteq \V' \subseteq \V$.
%However, once we have $\V^*$ via Assumption~\ref{assum:V-star-exists-infinite}, this set $\V^*$ also suffices to apply these statements.



%%\raf{Need to use minimizable}
%If follows that for $\risk L_+$ polyhedral, that $\hyp(g_{L(\R)})$ is a polyhedron, and $(d+1)$-dimensional as $\{(x,c) \mid x \in \reals^\Y_+, c \leq 0\} \subseteq \hyp(g_{L(\R)})$.
%As any polyhedron, by definition, has a finite halfspace representation, we apply Theorem~\ref{thm:polyhedron-uniquely-gen-facets} to conclude that $\hyp(g_\V)$ has a unique irredundant halfspace representation $\cap \H^*$, where $\H^* = \H_\Y \cup \H_{\V^*}$ for the unique finite set $\V^*$.
%Moreover, we have $g_\V = g_{\V^*}$, as $\H^*$ is the unique minimum halfspace representation for $\hyp(g_\V)$.


With this assumption in hand, we can show a few more statements involving $\V^*$ and how it relates to $\V$.
For intuition, the construction of $\V^*$ will be helpful to consider as a minimum representative set in the proof of Lemma~\ref{lem:X}.


\begin{claim}\label{claim:finite-rep-set}
	Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, there is a finite set $\R^* \subseteq \R$ such that $L(\R^*) = \V^*$ (without duplicates).
\end{claim}
\begin{proof}
	This follows immediately from the Assumption~\ref{assum:V-star-exists-infinite} as $\V^* \subseteq L(\R)$.
\end{proof}


\begin{claim}\label{claim:vstar-supporting-G}
	\jessiet{We don't need $L$ in here for the result, but $\V$ follows from it... how to state?}
	Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, for all $x\in\reals^\Y_+$, there exists $v^*\in\V^*$ such that $H_{v^*}$ supports $\hyp(g_\V)$ at $(x,g_\V(x))$.
\end{claim}
\begin{proof}
	% \jessie{Notes in meeting:
	%   For any $x \in \reals^d_+$, consider $v^* \in \argmin_{v \in \V} \inprod{v}{x}$.
	%   Argue $G \subseteq H^+_{v^*}$ by definition.
	%   Argue support: $g(x) = \inprod{v^*}{x}$.  WTS $(x,g(x)) \in G$.  Already have $(x,g(x)) \in H_{v^*}$.}
	%
	By Assumption~\ref{assum:V-star-exists-infinite}, we have $g_\V(x) = g_{\V^*}(x) = \inf_{v \in \V^*}\inprod{v}{x} = \min_{v \in \V^*}\inprod{v}{x}$ for $x \in \reals^\Y_+$.
	In particular, consider a normal $v^* \in \argmin_{v \in \V^*}\inprod{v}{x}$; we claim that the hyperplane $H_{v^*}$ such that $H_{v^*} = \partial H^+_{v^*}$ supports $\hyp(g_\V)$ at $(x,g(x))$.
	First, $\hyp(g_\V) \subseteq H^+_{v^*}$ by definition of $\hyp(g_\V)$ as the intersection of halfspaces including $H^+_{v^*}$.
	Thus, it is just left to show that $(x, \inprod{v^*}{x}) \in H_{v^*} \cap \hyp(g_\V)$.
	%Recall from \S~\ref{appsubsec:notation} that $H_{v^*} = \{(x,c) \mid \inprod{v^*}{x} = c\}$.
	By definition of $g_\V$, we have $g_\V(x)= g_{\V^*}(x) = \inprod{v^*}{x}$, so $(x,g_\V(x)) \in H_{v^*}$.
	Moreover, $(x,g_\V(x)) \in \hyp(g_\V) = \{(x,c) \mid g_\V(x) \geq c\}$ trivially since $g_\V(x) \geq g_\V(x)$.
\end{proof}



\subsection{Projecting from $\reals^d_+$ to $\reals^\Y_+$}\label{subsec:project-pi}

We now define the projection $\pi:\reals^\Y\times \reals \to \reals^\Y, (x,c) \mapsto x$.
The projected faces generated by $\V^*$ cover the nonnegative orthant.

\begin{corollary}\label{cor:projected-Vstar-faces-cover-pos-orthant}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For each $v \in \V^*$, let $F_v = F_v^{\hyp(g_{\V^*})}= H_v \cap \hyp(g_{\V^*})$. \jessiet{redundant?}
	Then $\cup_{v\in\V^*} \pi(F_v) = \reals^\Y_+$.
\end{corollary}

Moreover, the projection $\pi$ preserves dimension of faces.

\begin{claim}\label{claim:pi-preserves-dim}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}. %, let $F_v = H_v \cap \hyp(g_\V)$.
	For all $v \in L(\R)$, define $F_v$ as the face of $g_\V$ generated by $v \in \reals^\Y_+$.
	Then $\dim(F_v) = \dim(\pi(F_v))$.
\end{claim}
\begin{proof}
	\jessiet{Can use a careful check}
	\btw{See \url{http://homepages.cae.wisc.edu/~linderot/classes/ie418/lecture11.pdf} for def of AI}
	%\jessie{just need to argue that you never needed $c$ (i.e., $c$ was always just $g(x)$).  Maybe: take an affine basis, project, and show it's an affine basis.}
	Recall from Definition~\ref{def:face} that the dimension of a polytope to be the dimension of its affine hull.
	Suppose we are given $|\Y|+1$ affinely independent vectors $z_i$ in $F_v$. 
	We claim their projections $\{\pi(z_i)\}$ are affinely independent.
	Let $a_1 + \ldots + a_{|\Y|+1} = 0$, such that $\sum_i a_i \pi(z_i) = 0$.
	We want to conclude that we must have $a_i = 0$ for all $i$, meaning they are affinely independent.
	
	Observe $z_i = (x_i, \inprod{v}{x_i})$ for all $i$; therefore, if $z_i \in F_v$ (e.g., $F_v$ supports $\hyp(g_\V)$ at $(x, \inprod{v}{x})$), then we also have $z_i \in H_v$.
	So $0 = \sum_i a_i \pi(z_i) = \sum_i a_i x_i$.
	Moreover, the sum $\sum_i a_i z_i = \sum_i a_i (x_i, \inprod{v}{x_i}) = (\sum_i a_i x_i, \inprod{v}{\sum_i a_i x_i}) = (\vec 0,0) = \vec 0$.
	Thus, since $a_i = 0$ for all $i$, the set $\{z_i\}$ is affinely independent and the dimensions of the affine hulls are therefore equal.
\end{proof}



Since we preserve the dimension of these projected spaces, we can now study equivalence of projected faces of the hypograph and regions of support of $g_\V$ for any $v \in \L(\R)$.

\begin{lemma}\label{lem:projected-faces-iff-support-iff-argmin}
Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, fix $x \in \reals^\Y_+$.
	For any $v \in L(\R)$, the following are equivalent: 
	
	
	(1) $(x,g_\V(x)) \in F_v^{\hyp(g_\V)}$; % H_v \cap \hyp(g_\V)$
	
	(2) $\inprod{v}{x}= g_\V(x)$;
	
	(3) $v \in \argmin_{v' \in \V} \inprod{v'}{x}$; and
	
	(4) $x \in \pi(F_v^{\hyp(g_\V)})$~.
\end{lemma}
\begin{proof}
	For $v \in L(\R)$, define $F_v := F_v^{\hyp(g_\V)}$.
	\begin{align*}
	(1) \quad \quad (x,g(x)) \in F_v
	&\iff (x,g_\V(x)) \in \{(x',c) \in \hyp(g_\V) \mid \inprod{v}{x'} = c\} & \\
	&\iff  \inprod{v}{x} = g_\V(x) & (2)\\
	&\iff \inprod{v}{x} = \min_{v' \in \V}\inprod{v'}{x} & \\
	&\iff v \in \argmin_{v' \in \V}\inprod{v'}{x}~. & (3)
	\end{align*}
	This covers $1 \iff 2 \iff 3$.
	
	For $1 \iff 4$, the forward implication follows trivially by applying the definition of the projection $\pi$.
	For the reverse implication, consider some $x \in \pi(F_v)$.
	There must be a $c \in \reals$ so that $(x,c) \in F_v$.
	Expanding, this is actually saying $(x,c) \in \{(x',c') \in \hyp(g_\V) \mid \inprod{v}{x'} = c\}$.
	In particular, this is true when $c = \inprod{v}{x}$, which defines a face of $\hyp(g_\V)$ at $x$ if any only if $\inprod{v}{x} = g_\V(x)$.
	Therefore, we have $(x, g_\V(x)) \in F_v$.
\end{proof}


Taking $L,\V,\V^*$ as in Assumption~\ref{assum:V-star-exists-infinite} and a face of $\hyp(g_\V)$, $F_{v^*} = H_{v^*} \cap \hyp(g_\V)$ for $v^* \in \V^*$, the projection $\pi$ preserves full-dimensionality.
\begin{claim}\label{claim:projected-Vstar-faces-full-dim}
	Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, define $F_v := F^{\hyp(g_\V)}_v = H_v \cap \hyp(g_\V)$.
	For all $v \in \V^*$, $\pi(F_v)$ is full dimensional in $\reals_+^\Y$.
	\btw{$\Lambda$ is exactly the set of FDLS; Will lead to $\Theta_{\V^*}$ are exactly the full dimensional level sets over simplex (6).}
\end{claim}
\begin{proof}
	By Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}, $F_v$ is a facet of $\hyp(g_{\V^*}) = \hyp(g_\V)$ in $\reals^d_+$, meaning it is $(d - 1)$-dimensional.
	Moreover, Claim~\ref{claim:pi-preserves-dim} states that the dimension of $F_v$ is preserved for each $v \in \V^*$.
	Thus, $\dim(F_v) = \dim(\pi(F_v)) = |\Y|$.
\end{proof}



Now we can observe a set of normals $\V'$ generates faces of $g_{\V}$ whose projections cover $\reals^\Y_+$ if and only if the set contains $\V^*$.
This will translate to a set being representative for a loss if and only if it contains a finite minimum representative set (in settings where one exists.)
%\raf{For Lemma X: NTS $\cup_{v\in\V} \pi(F_v) = \reals^\Y_+ \implies \V^* \subseteq \V$.}
%\raf{If $G = \cap( \H_\Y \cup \H_\V)$...}

\begin{claim}\label{claim:Vprime-projected-faces-cover-iff-Vstar-subset-Vprime}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For $\V' \subseteq L(\R)$, we have
	$\cup_{v\in\V'} \pi(F_v) = \reals^\Y_+ \iff \V^* \subseteq \V'$.\btw{Will get us to REP $\iff$ $\V^* \subseteq L(R')$ (3).}
\end{claim}
\begin{proof}
	\jessiet{Revisit- just rewrote (Mar 10)}
	For any $v \in \V$, define $F_v := F^{\hyp(g_\V)}_v = H_v \cap \hyp(g_\V)$.
	%We would like to show $\cup_{v\in\V'} \pi(F_v) = \reals^\Y_+ \iff \cap(\H_\Y \cup \H_{\V'}) = \hyp(g_\V) := G$, then the result follows by a corollary of the assumption. %apply Corollary~\ref{cor:anything-gen-G-subset-Hstar}.
	
	
	%Take any $x\in\reals^\Y_+$.
	%\raf{Argue that we have $\cap(\H_\Y \cup \H_{\V'}) \subseteq G$}
	%\raf{Another approach: assume $v^* \in \V^* \setminus \V'$.  Take $z = (x,g(x)) \in \relint(F_{v^*})$.  Argue that $x \not\in \cup_{v\in\V'} \pi(F_v)$.}
	
	($\implies$) 
	For contraposition, suppose $\V^* \not \subseteq \V'$.
	Then $\exists v \in \V^* \setminus \V'$.
	Observe that $\V^*$ is unique and irredundant (by assumption) and $\pi(F^{\hyp(g_\V)}_v)$ is full-dimensional in $\reals^\Y_+$ by Claim~\ref{claim:projected-Vstar-faces-full-dim}.
	Moreover, $\pi(F_v) \not \in \cup_{v' \in \V'} \pi(F_v)$, which implies $\cup_{v' \in \V'} \pi(F_v) \neq \cup_{v^* \in \V^*} \pi(F_{v^*}) = \reals^\Y_+$.
	
%Commented out March 10 2022 and replaced with above- Jessie
%	Suppose $\cup_{v\in\V'} \pi(F_v) = \reals^\Y_+$.
%	Fix $x \in \reals^\Y_+$.
%	Since $\V' \subseteq \V$, and $G = \cap (\H_\Y \cup \H_\V)$, we immediately have $G \subseteq \cap (\H_\Y \cup \H_{\V'})$.
%	Therefore, we just need to show the other direction of inclusion.
%	%  \raf{Need to add logic: know $G \subseteq \cap(\H_\Y \cup \cap \H_{\V'})$, so only need to show $\cap(\H_\Y \cup \cap \H_{\V'}) \subseteq G$}
%	By Lemma~\ref{lem:x-nonneg-orthant-iff-intersection-HY} we have $(x,c) \in \cap \H_\Y$ for all $c \in \reals$, so it is left to show that $(x,c) \in \cap \H_{\V'}$, which yields the desired result.
%	First, $(x,c) \in \cap \H_{\V'}$ implies $(x,c) \in H^+_{v'}$ for all $v' \in \V'$.
%	This implies $c \leq \inprod{v'}{x}$ for all $v' \in \V'$.
%	By Lemma~\ref{lem:projected-faces-iff-support-iff-argmin}, there exists a $v' \in \V'$ such that $(x,g_\V(x)) \in F_{v'}$, and $\inprod{v'}{x} = g_\V(x)$.
%	Therefore, we have $c \leq \inprod{v'}{x} = g_\V(x)$, and $(x,c) \in \hyp(g_\V)$ follows.
%	
%	%\raf{Want to show $c \leq g(x)$, then done by def $G$}
%	%\raf{Now $(x,c) \in \cap \H_{\V'} \implies (x,c) \in H^+_{v'} \implies c \leq \inprod{v'}{x} = g(x) \implies (x,c) \in G$}
%	
%	% ...$\leq \inprod{v}{x}$ for all $v \in \V$, so $(x,g(x)) \in \cap \H_\V \implies (x,g(x)) \in G = \cap (\H_\Y \cup \H_\V)$.
	
	($\impliedby$)
	Since $\V^* \subseteq \V'$, we immediately have $\cup_{v \in \V^*} \pi(F_v) \subseteq \cup_{v' \in \V'} \pi(F_{v'})$.
	Moreover, $\cup_{v \in \V^*} \pi(F_v) = \reals^\Y_+$ by Corollary~\ref{cor:projected-Vstar-faces-cover-pos-orthant}, so $\reals^\Y_+ \subseteq \cup_{v' \in \V'} \pi(F_{v'})$.
	As $g_\V$ is only finite on $\reals^\Y_+$ by construction, equality follows.
%commented out Mar 11 - Jessie
%	$\hyp(g_\V) = \cap (\H_\Y \cup \H_{\V'})$ if and only if $\hyp(g_\V) = \cap \H_{\V'}$ when restricting to $x \in \reals_+^\Y$.
%	This implies that for all $x \in \reals_+^\Y$, there exists a $v' \in \V'$ such that $(x,g(x)) \in H_{v'} \cap \hyp(g_\V) = F_{v'} \implies x \in \pi(F_{v'})$.
%	As this is true for all $x \in \reals_+^\Y$, we have $\cup_{v \in \V'} \pi(F_{v}) = \reals_+^\Y$.
	
\end{proof}



We now claim that a set of projected faces $\{F^{\hyp(g_\V)}_v\}_{v \in \V'}$ for some $\V'$ will cover $\reals^\Y_+$ if and only if $\V^* \subseteq \V'$.
Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, denote $\Lambda_{S} := \{\pi(F_v) \mid v \in S\}$\jessie{To reduce notation, let's get rid of $\Lambda$?} as the set of projected facets generated by $\V^*$.

\begin{claim}\label{claim:Vprime-projected-faces-cover-iffprojected-faces-subsets}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $\R' \subseteq \R$ with $\V' := L(\R')$. 
	%For $\V' \subseteq \V$, 
	We have $\cup_{v \in \V'} \pi(F_v) = \reals_+^\Y \iff  \{ \pi(F_v) \mid v \in \V^*\} \subseteq \{ \pi(F_v) \mid v \in \V'\}$.
	\btw{$1$-homogeneous version of rep iff $\Theta_{\V^*}$ subset of level sets generated by $\R'$.}
\end{claim}
\begin{proof}
	$\implies$
	The result follows if $\V^* \subseteq \V'$, which follows from the forward implication of Claim~\ref{claim:Vprime-projected-faces-cover-iff-Vstar-subset-Vprime}.
	Explicitly, for all $v \in \V^*$ we also have $v \in \V'$, so, $\pi(F_v) \in \Lambda_{\V^*} \cap \Lambda_{\V'} = \Lambda_{\V^*}$.
	%$\pi(F_v) \in \Lambda \cap \{\pi(F_v) \mid v \in \V'\} = \Lambda$.
	
	$\impliedby$
	If $\{\pi(F_v) \mid v \in \V^*\} \subseteq \{ \pi(F_v) \mid v \in L(\R') \}$, then $\cup \{\pi(F_v) \mid v \in \V^*\} \subseteq \cup_{v \in \V'} \pi(F_v)$.
	By Corollary~\ref{cor:projected-Vstar-faces-cover-pos-orthant}, we have $\cup \{\pi(F_v) \mid v \in \V^*\} = \reals_+^\Y$, so $\reals_+^\Y \subseteq \cup_{v \in \V'} \pi(F_v)$.
	The other direction of subset inequality following from $\hyp(g_\V)$ being finite only on $\reals_+^\Y$.	
\end{proof}

%Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, consider $v = L(r)$ for any $r \in \R$.
%As $H_{v}^+ \in \H$, it supports $\hyp(g_\V)$ or it is redundant in $\H$ (or both).

\begin{claim}\label{claim:faces-contained-in-facets}
	Given $L, \V$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, if some $H_v \in \H$ supports $\hyp(g_\V)$, then $F_v := H_{v} \cap \hyp(g_\V)$ is a nonempty face of $\hyp(g_\V)$, and thus a subset of a facet. 
\end{claim}
\begin{proof}
	Since $H_v$ supports $\hyp(g_\V)$, we know that $\hyp(g_\V) \subseteq H_v^+$ and $F$ is not empty.
	Moreover, $H^+_v$ is valid, and we have $F_v = H_v \cap \hyp(g_\V)$ is a face of $\hyp(g_\V)$ by definition.
	
	% subset of facet
	Moreover, the faces of $\hyp(g_\V)$ are all convex polyhedra as $\hyp(g_\V)$ is a polyhedron.
	Any face of $\hyp(g_\V)$ is must then be a lower-dimensional face of a facet, and therefore a subset.
	\btw{Gr\"unbaum Page 35, Section 3.1 statement 7}
\end{proof}


\begin{claim}\label{claim:vface-subset-some-vstar-face}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $v \in \V$, the face $F_{v} \subseteq F_{v^*}$ for some $v^* \in \V^*$. \jessiet{Need to clarify that face of $\hyp(g_\V)$?} %must be a subset of one of the $\V^*$ facets $F_{v^*}$ (as opposed to the vertical ones)
\end{claim}
\begin{proof}
	\jessie{I'm not sure if this is clear, but this is a new approach to the proof that I generally like better. I can clarify if the wording is not clear though.}
	As $\hyp(g_{\V^*})$ is a polyhedron, each of its faces are convex polyhedra, and is also a face of some facet of $\hyp(g_{\V^*})$; these facets are defined by $\V^*$ and $\Y$ via Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}.
	
	It then suffices to show that if $F_v := H_v \cap \hyp(g_\V)$ is a face of the facet $F_y := H_y \cap \hyp(g_\V)$ for some $y\in \Y$, then it must also be a face of $F_{v^*}$ for a $v \in \V^*$; it suffices to show $F_v$ is not a facet, and thus $F_v \neq F_y$.
	Recall from the definition of a face that $F_v = \hyp(g_\V) \cap H_v$ and $F_y = \hyp(g_\V) \cap H_y$.
	As facets of a polyhedron are (uniquely) determined by hyperplanes and $F_y$ is a facet, then if $F_v$ is a facet we must have $v \in \V^*$, and constructed $\V^*$ such that $\H_{\V^*} \cap \H_\Y = \emptyset$.
	Thus, $F_v \neq F_y$ by unique determination of facets.
	If $F_v$ is not a facet of $\hyp(g_\V)$, then $F_v \neq F_y$ is immediate as $F_y$ is a facet of $\hyp(g_\V)$. 
	In both cases, we have $F_v \neq F_y$, and the result follows.

	
	\jessiet{Old version in comments}
	%Commented out 7 Jan 2022
%	\jessiet{Can use a careful read}
%	$\{F_{v^*}\}_{v^* \in \V^*}$ is exactly the set of facets of $\hyp(g_{\V^*})$ on $\reals_+^d$ by Theorem~\ref{thm:polyhedron-uniquely-gen-facets}.
%	Any $k$-face is contained in a facet by Claim~\ref{claim:faces-contained-in-facets}.
%	Since $\H = \cap (\H_{\V^*} \cup \H_\Y)$, we are done if $F_v$ is contained in a face of $\H_{\V^*}$ and need to consider the case where $F_v \in \H_\Y$.
%	
%	If $F_v$ is a subset of a vertical facet $F_y$, we would have $F_v = H_v \cap G \subseteq H_y \cap G$ for some $y \in \Y$.
%	Expanding, this is $\{(x,g(x)) \mid \inprod{v}{x} = g(x)\} \subseteq \{(x,g(x)) \mid x_y = 0 \}$.
%	While this can happen, it must be on the boundary of $\reals_+^d$, and must be at the intersection of $H_y$ and some other $H_{v^*}$ for $v^* \in \V^*$, since $g$ is continuous on $\reals^\Y_{+}$, and determined by $\V^*$ on $\reals^\Y_{++}$.
%	\jessiet{Justification: $g$ is continuous on $\reals_+^\Y$, and there has to be a unique $v^*$ on $\reals^\Y_{++}$. More hand-wavy than I'd like}
\end{proof}

\begin{corollary}\label{cor:projected-faces-subset-projected-facet}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $F_v := H_v \cap \hyp(g_\V)$ be a face of $\hyp(g_\V)$.
	For $v,v^*$ such that $F_v \subseteq F_{v^*}$ and $v^*\in \V^*$, $\pi(F_{v}) \subseteq \pi(F_{v^*})$. 
\end{corollary}

Now, we can conclude that projected facets generated by $\V$ contain all other projected faces of $\hyp(g_\V)$.
\begin{corollary}\label{cor:exists-vstar-projected-face-subset}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For $v \in L(\R)$, let $F_v := H_v \cap \hyp(g_\V)$ be a face of $\hyp(g_\V)$. 
	For any $v \in \V$, there is a $v^* \in \V^*$ such that $\pi(F_v) \subseteq \pi(F_{v^*})$.
	\btw{shows (7) on $\reals^\Y_+$}
\end{corollary}
\begin{proof}
	This is exactly Claim~\ref{claim:vface-subset-some-vstar-face} and Corollary~\ref{cor:projected-faces-subset-projected-facet} chained together.
\end{proof}


% \begin{claim}
%   \jessie{This should probably move down to translating for properties since representative sets are at the core, about properties}
%   $\R'$ is a representative set for $L$ if and only if $\V^* \subseteq L(\R')$.
% \end{claim}
% \begin{proof}
%   Fix $v^* \in \V^*$.
%   If $\R'$ is representative, then for any $p \in \simplex$ such that , 
%   As $g$ is $1$-homogeneous and minimizable, for every $v^* \in \V^*$, there must be some $p \in \simplex$ such that $\inprod{v^*}{p} = \inf_v \inprod{v}{x}$.
%   Thus, it suffices to show subset inclusion for any $p \in \simplex$.
%   Moreover, since $\R'$ is representative, there is some $r' \in \R'$ such that $\inprod{L(r')}{p} = \inf_{v} \inprod{v}{x}$.
%   
%   For the reverse implication, $\V^*$ is representative (since $\cup_{v^* \in \V^*} F_{v^*} = \reals_+^\Y$ in Corollary 8\jessie{make reference}), so any set containing $\V^*$ is also representative.
% \end{proof}


% \begin{corollary}
%   $\R^*$ is a min rep set for $L$ if and only if $\V^* = L(\R^*)$. \raf{(2)}
% \end{corollary}




\subsection{Translating to properties: projecting from $\reals^\Y_+$ to $\simplex$}\label{subsec:project-f}

Let $f_\V:\reals^\Y\to\reals_+\cup\{-\infty\}$ be a polyhedral concave function with $\dom(f_\V) = \simplex$. % such that $f_\V(x) = g_\V(x)$ for all $x \in \simplex$.
\begin{claim}\label{claim:f-min-affine}
	There is some finite set $\V \subseteq \reals^\Y_+$ such that $f_\V(p) = \min_{v\in\V} \inprod{p}{v} - \delta(p \mid \simplex)$.  %\raf{show you can do this using the $h$ and $\delta$ representation from Rockafellar -- argue that because $p\in\simplex$ we can write $\inprod{p}{b_i}-\beta_i = \inprod{p}{b_i-\beta_i\ones}$.}
\end{claim}
\begin{proof}
  We will think of $f_\V$ as defined $f_\V:\reals^\Y\to\reals_+\cup\{-\infty\}$  with $\dom(f_\V) = \simplex$.
  For $p \in \simplex$, we know $\sum_i p_i = 1$, and can write any inner product $\inprod{p}{b} - \beta = \inprod{p}{b} - \inprod{p}{\beta \ones} = \inprod{p}{b - \beta \ones}$.
  If $p \not \in \simplex$, then $f_\V(p) = -\infty$ and inner products are not used to compute $f_\V$.
  Moreover, since $f_\V$ is polyhedral, it is finitely generated \citep[Proposition 19.1.2]{rockafellar1997convex} and can be written 
	\begin{align*}
	f_\V(p) %&= h(p) - \delta(p \mid C)\\
		 &= \min(\inprod{p}{b_1} - \beta_1, \ldots, \inprod{p}{b_k} - \beta_k) - \delta(p \mid \simplex)\\
		 &= \min(\inprod{p}{b_1 - \beta_1 \ones}, \ldots, \inprod{p}{b_k - \beta_k \ones}) - \delta(p \mid \simplex)~.~
	\end{align*}
\end{proof}

This allows us to project $g_\V$ from $\reals^\Y_+$ to the $\simplex$.
\begin{lemma}\label{cor:f-matches-g-on-simplex}
  Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
  For all polyhedral concave $f_\V : \reals^\Y_+ \to \reals_+ \cup \{-\infty\}$ with $\dom(f_\V) = \simplex$, the polyhedral concave function $g_\V : \reals^\Y_+ \to \reals_+$ with $\dom(g_\V) = \reals^\Y_+$ matches $f_\V(p) = g_\V(p)$ for all $p \in \simplex$.
\end{lemma}
Given the function $f_\V$, we consider $g_\V$ to be its extension and $L$ such that $\risk L_+ = g_\V$ and $\risk L = f_\V$.
% so that we may use the tools in \S~\ref{appsubsec:phase1} and~\ref{appsubsec:phase2} to draw conclusions about the property $\Gamma := \prop{L}$ defined on the simplex.
Moreover, define the function $\theta(v) = \{p \in \simplex \mid \inprod{v}{p} = f_\V(p)\}$ as the level sets of the loss vector $v \in \V$. %, and the set $\Theta_{\V^*} := \{\theta(v) \mid v \in \V^*\}$ to be the set of level sets uniquely defined by a given set of loss vectors.

\begin{claim}\label{claim:gV-and-fV}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	Then $\risk L$ is polyhedral (on the simplex) and $f_\V = \risk L$.
	Moreover, $f_\V(p) = g_\V(p)$ for all $p \in \simplex$.
\end{claim}
\jessie{Less of an assumption and more of a claim...}

\begin{claim}\label{claim:theta-is-pi-cap-simplex}
  Consider $L, \V, \V^*$ as in Assumption~\ref{assum:V-star-exists-infinite}.
  For all $v \in \V$, consider the face $F_v$ of $\hyp(g_\V)$.
  Then $\theta(v) = \pi(F_v) \cap \simplex$.
\end{claim}
\begin{proof}
  % Argue $\theta(v)$ is full-dim in $\simplex$ if and only if $\pi(F_v)$ is $|\Y|$-dim, if and only if $F_v$ is a facet. \raf{(2)}
  \btw{Moved previous proof outline to comment}

  Fix $p \in \simplex$.
  \begin{align*}
    p \in \theta(v)
    &\iff \inprod{v}{p} = f_\V(p) & \text{Definition of $\theta$}\\
    &\iff \inprod{v}{p} = \min_{v' \in \V}\inprod{v'}{p} & \text{$f_\V=g_\V$ on $\simplex$ (Cor.~\ref{cor:f-matches-g-on-simplex})}\\
    &\iff v \in \argmin_{v' \in \V}\inprod{v'}{p} &\\
    &\iff p \in \pi(F_v)~. & \text{Lemma~\ref{lem:projected-faces-iff-support-iff-argmin}}
  \end{align*}
\end{proof}


\subsection{Moving from $\simplex$ to $\reals^\Y_+$}
Now that we have translated from $\reals^d_+$ to $\reals^\Y_+$ in \S~\ref{subsec:project-pi} and from $\reals^\Y_+$ to $\simplex$ in \S~\ref{subsec:project-f}, we take some final steps to prove Lemma~\ref{lem:X} by showing equivalences from $\simplex$ to $\reals^\Y_+$.
\begin{lemma}\label{lem:level-set-is-projected-face}
  Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
  For all $r \in \R$ with $v = L(r)$, $\Gamma_r = \theta(v) = \pi(F_{v}) \cap \simplex$.
\end{lemma}
\begin{proof}
  % Follows immediately from Lemma~\ref{lem:projected-faces-iff-support-iff-argmin}.
  Let us rewrite
  \begin{align*}
    \Gamma_r
    &= \{p \in \simplex \mid r \in \argmin_{r' \in \R} \inprod{L(r')}{p}\}\\
    &= \{p \in \simplex \mid v \in \argmin_{v' \in \V} \inprod{v'}{p}\}\\
    &= \{p \in \simplex \mid \inprod{v}{p} = \min_{v' \in \V}\inprod{v'}{p}\}\\
    &= \{p \in \simplex \mid \inprod{v}{p} = f(p) \}\\
    &= \theta(v)~.
  \end{align*}
  The rest of the result follows from Claim~\ref{claim:theta-is-pi-cap-simplex}.
%  Now, we have $p \in \theta(v)$ if and only if,
%  \begin{align*}
%    p \in \theta(v)
%    &\iff p \in \{p' \in \simplex \mid \inprod{v}{p'} = f(p')\} & \text{Definition of $\theta$}\\
%    &\iff p \in \{p' \in \simplex \mid \inprod{v}{p'} = g(p')\} &\text {$f = g$ on $\simplex$ (Cor.~\ref{cor:f-matches-g-on-simplex})} \\
%    &\iff p \in \{p' \in \reals_+^\Y \mid \inprod{v}{p'} = g(p')\} \cap \simplex & \\
%    &\iff p \in \pi(F_v) \cap \simplex & \text{Definition of $\pi$, $F_v$}
%  \end{align*}
%  The result follows.
   
\end{proof}


\begin{lemma}\label{lem:g-1-homog}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	Then $g_{\V^*}(x) = \min_{v \in \V^*}\inprod{v}{x}$ is (positively) $1$-homogeneous.
\end{lemma}
\begin{proof}
	If $x \not \in \reals^\Y_+$, then $c g(x) = -\infty = g(cx)$ for any $c > 0$.
	If $x \in \reals^\Y_+$, then we have $g(cx) = \min_{v \in \V^*}\inprod{v}{cx} = c \min_{v \in \V^*}\inprod{v}{x} = c g(x)$ for any $c > 0$ by linearity of the inner product.
\end{proof}


Every minimizable loss $L$ elicits a unique property $\Gamma := \prop{L}$, and we can define the \emph{extended level set} $\bar \Gamma_r := \{x \in \reals^\Y_+ \mid \inprod{L(r)}{x} = \risk L_+(x)\}$.

\begin{lemma}\label{lem:levelset-to-extended-levelset}
	Consider $L: \R \to \reals^\Y_+,\V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $r \in \R$ and $c > 0$, if $p \in \Gamma_r$, then $cp \in \bar \Gamma_r$. 
\end{lemma}
\begin{proof}
	Fix $r \in \R$ and $c > 0$.
	We have
	\begin{align*}
	p \in \Gamma_r
	&= \{p' \in \simplex \mid r \in \argmin_{r' \in \R} \inprod{L(r')}{p'} \} & \text {Definition of level set} \\
	&= \{p' \in \simplex \mid v \in \argmin_{v' \in L(\R)} \inprod{v'}{p'} \} &  \\
	&= \{p' \in \simplex \mid \inprod{v}{p'} = \min_{v' \in L(\R)} \inprod{v'}{p'} \} & \text {$L$ minimizable} \\
	&= \{p' \in \simplex \mid \inprod{v}{p'} = g_\V(p') \} & \text {Assumption~\ref{assum:V-star-exists-infinite}: $g_{L(\R)} = g_\V$} \\
	&= \{p' \in \simplex \mid c \inprod{v}{p'} = c g_\V(p') \} &  \\
	&= \{p' \in \simplex \mid  \inprod{v}{cp'} = g_\V(cp') \} & \text {Lemma~\ref{lem:g-1-homog}} \\
	\implies cp
	&\in \{x \in \reals^\Y_+ \mid \inprod{v}{x} = g_\V(x)\}\\
	&= \bar \Gamma_r~.~
	\end{align*}
\end{proof}

\begin{lemma}\label{lem:extended-levelset-equals-projected-face}
	Consider $L : \R \to \reals^\Y_+, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $v \in L(\R)$, define the face $F_v$ of $\hyp(g_\V)$.
	For any $r\in \R$ with $v = L(r)$, $\bar \Gamma_r = \pi(F_v)$.  
\end{lemma}
\begin{proof}
	\begin{align*}
	\bar \Gamma_r
	&= \{x \in \reals^\Y_+ \mid \inprod{L(r)}{x} = \risk L_+(x)\} & \text{Definition of $\bar \Gamma_r$}\\
	&= \{x \in \reals^\Y_+ \mid \inprod{L(r)}{x} = g_\V(x)\} & \text{Assumption~\ref{assum:V-star-exists-infinite}: $\risk L_+(x) = g_\V(x)$}\\
	&= \{x \in \reals^\Y_+ \mid \inprod{v}{x} = g_\V(x)\} & \text{$v = L(r)$}\\
	&= \pi(F_v) & \text{Since $F_v = \{(x,g_\V(x)) \mid \inprod{v}{x} = g_\V(x)\}$}\\
	\end{align*}
\end{proof}


\begin{claim}\label{claim:projected-faces-cover-RY-iff-representative}
	Consider $L : \R \to \reals^\Y_+, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $v \in L(\R)$, denote the face $F_v := H_v \cap \hyp(g_\V)$.
	A set $\R' \subseteq \R$ with $\V' := L(\R')$ is representative for $L$ if and only if $\cup_{v \in \V'} \pi(F_v) = \reals^\Y_+$.  
\end{claim}
\begin{proof}
	%\raf{Need 1-homog here -- suggest separating out statement like cone(simplex cap pi(Fv)) = pi(Fv)}
	$\implies$
	This proof follows from three lemmas: first, we observe that $g$ is $1$-homogeneous via Lemma~\ref{lem:g-1-homog}.
	Then we extend the notion of a level set $\Gamma_r$ to the nonnegative orthant $\bar \Gamma_r$, and show that any scalar transformation of a distribution in the level set is contained in the same (extended) level set via Lemma~\ref{lem:levelset-to-extended-levelset}.
	Finally, we show the extended level set is exactly the projection $\pi(F_v)$ in Lemma~\ref{lem:extended-levelset-equals-projected-face}.
	As a corollary, we chain the results to observe $\cup_{r \in \R'} \Gamma_r = \simplex \implies \cup_{r \in \R'} \bar \Gamma_r = \reals_+^\Y = \cup_{v \in L(\R')}\pi(F_v) = \reals^\Y_+$, yielding the forward implication.
	
	
	$\impliedby$
	Fix $p \in \simplex \subseteq \reals^\Y_+$.
	By the assumption, there is a $v \in \V'$ such that $p \in \pi(F_v)$.
	By Lemma~\ref{lem:level-set-is-projected-face}, we have $p \in \pi(F_v) \cap \simplex = \Gamma_r$ for the $r \in \R'$ such that $v = L(r)$.
	As this is true for all $p \in \simplex$, we have $\R'$ representative.
	
	
\end{proof}

\subsection{Proving Lemma~\ref{lem:X}}\label{subsec:lem-X-proof}
We now proceed with a few final lemmas that ultimately yield the proof of Lemma~\ref{lem:X}.

\begin{lemma}\label{lem:lemX1-rep-iff-subset-vectors}
	\btw{(1)}
	Consider $L : \R \to \reals^\Y_+, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	A finite set $\R' \subseteq \R$ with $\V' = L(\R')$ is representative if and only if $\V^* \subseteq \V'$.
\end{lemma}
\begin{proof}
	Chain Claim~\ref{claim:projected-faces-cover-RY-iff-representative} and Claim~\ref{claim:Vprime-projected-faces-cover-iff-Vstar-subset-Vprime} to yield the result.
\end{proof}

\begin{lemma}\label{lem:lemX-3-rep-iff-FDLS-subsets}
	\btw{(3)}
	Consider $L : \R \to \reals^\Y_+, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	A finite set $\R' \subseteq \R$ with $\V' = L(\R')$ is representative if and only if $\Theta_{\V^*} \subseteq \{\theta(v) \mid v \in \V'\}$.
\end{lemma}
\begin{proof}
	Chain Claim~\ref{claim:projected-faces-cover-RY-iff-representative} and Claim~\ref{claim:Vprime-projected-faces-cover-iffprojected-faces-subsets} to yield the result.
\end{proof}

Define $\Theta_{\V^*} := \{\theta(v) \mid v \in \V^*\}$; it follows that this set is exactly the set of level sets of the property elicited by $L$.
Moreover, let $\R^*$ be the finite set of reports given by Claim~\ref{claim:finite-rep-set}.

\begin{corollary}
	Consider $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $\R^*$ such that $\V^* = L(\R^*)$ as in Claim~\ref{claim:finite-rep-set}.
	Moreover, suppose $L$ elicits $\Gamma$.
	$\Theta_{\V^*} = \{\Gamma_r \mid r \in \R^*\}$.
\end{corollary}
%Observe that $\R^*$ exists by Claim~\ref{claim:finite-rep-set}.

\begin{lemma}\label{lem:fdls-exactly-theta}
	\btw{(6)}
	Consider $L : \R \to \reals^\Y_+, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	Moreover, let $\Gamma := \prop{L}$.
	$\Theta_{\V^*} = \{\Gamma_r \mid r\in\R, \dim(\Gamma_r) = |\Y|-1\}$.
	\raft{The set of full-dimensional level sets of $\Gamma$ is exactly $\Theta$.}
\end{lemma}
\begin{proof}
	From Claim~\ref{claim:projected-Vstar-faces-full-dim}, we know $\Lambda_{\V^*}$ is exactly the set of full-dimensional level sets in $\reals^\Y_+$.
	Each element of $\Lambda_{\V^*}$ is $\pi(F_v)$ for some $v \in \V^*$.
	Take $r \in \R^*$ so that $v = L(r)$.
	By Lemma~\ref{lem:level-set-is-projected-face}, we have $\theta(v) = \Gamma_r = \pi(F_v) \cap \simplex$ is full-dimensional relative to the simplex.
	The result follows.
\end{proof}


\begin{lemma}\label{lem:any-levelset-contained-in-minlevelset}
  Consider $L : \R \to \reals^\Y_+, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $\R^* \subseteq \R$ the set such that $\V^* = L(\R^*)$ as in Claim~\ref{claim:finite-rep-set}.
  Moreover, consider $\Gamma := \prop{L}$.
  For any $r \in \R$, there exists a $v^* \in \V^*$ such that $\Gamma_r \subseteq \theta(v^*)$.\btw{(7)}
\end{lemma}
\begin{proof}
  Take $v = L(r)$.
  %It suffices to show there is a $v^* \in \V$ such that $\theta(v) \subseteq \theta(v^*)$ by their equality in Claim~\ref{claim:level-set-is-projected-face}.
  By Corollary~\ref{cor:exists-vstar-projected-face-subset}, there is a $v^* \in \V^* \subseteq \V$ such that $\pi(F_v) \subseteq \pi(F_{v^*})$.
  Therefore, $\pi(F_v) \cap \simplex \subseteq \pi(F_{v^*})\cap \simplex$.  
  We know $\theta(v) = \pi(F_v) \cap \simplex$ and similarly for $\theta(v^*)$ by Lemma~\ref{lem:level-set-is-projected-face}.
  The result follows.
  % Let $v = L(r) \in \V$.
  % By Claim \ref{claim:vface-subset-some-vstar-face}, the face $F_v$ must be a subset of some $F_{v^*}$ for $v^* \in \V^*$.
  % We must then have $\pi(F_v) \cap \simplex = \Gamma_r \subseteq \pi(F_{v^*}) \cap \simplex = \theta(v^*)$.
\end{proof}



We are now ready to apply the above to prove Lemma~\ref{lem:X}.
In particular, any loss $L$ satisfying the assumptions of Lemma~\ref{lem:X} has some $g_{L(\R)} = \risk{L}_+$ as in this section. 


\lemmaX*
\begin{proof}
%As in this appendix, we consider $\V = L(\R)$ such that $g_\V = \risk L_+$.
Consider $\risk L = f_\V$ for a finite set $\V$ by Claim~\ref{claim:f-min-affine}.
There is a polyhedral concave function $g_\V$ on $\reals^\Y_+$ matching $f_\V$ on $\simplex$ by Corollary~\ref{cor:f-matches-g-on-simplex}.
Moreover, consider $g_\V = \risk L_+$\jessiet{is it clear enough that $g_\V = \risk L_+$?}, and observe that $\risk L_+$ matches $\risk L$ on $\simplex$ as well.
By Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}, we then have a finite set $\V^*$ of smallest cardinality such that $f_\V = f_{\V^*}$ and $g_\V = g_{\V^*}$.
Consider $\R^* \subseteq \R$ such that $\V^* = L(\R^*)$ as in Claim~\ref{claim:finite-rep-set}.
First, observe that $\R^*$ is representative for $L$ as a corollary of Claim~\ref{claim:projected-faces-cover-RY-iff-representative}.
Moreover, consider the following set of level sets of $f_{\V^*}$, $\Theta_{\V^*} = \{\theta(v) \mid v \in \V^*\}$.

Now that we have the preliminaries, consider the itemized statements.
For $f_{\V^*}$, Lemma~\ref{lem:lemX1-rep-iff-subset-vectors} is exactly statement \eqref{item:X-rep-V}.
This immediately implies statement~\eqref{item:X-min-V}.
Moreover, Lemma~\ref{lem:lemX-3-rep-iff-FDLS-subsets} is exactly statement~\eqref{item:X-rep-Theta}, and again statement~\eqref{item:X-min-Theta} immediately follows.
Statement~\eqref{item:X-rep-contain-min} is a corollary of the existence of a finite representative set, as shown in Claim~\ref{claim:finite-rep-set}.
Statement~\eqref{item:X-full-dim} is exactly Lemma~\ref{lem:fdls-exactly-theta}.
Statement~\eqref{item:X-redundant} is exactly Lemma~\ref{lem:any-levelset-contained-in-minlevelset}.
Finally, Statement~\eqref{item:X-tight-embed} follows as a corollary of statement~\eqref{item:X-min-V} and Corollary~\ref{cor:tight-embed-min-rep}.
\end{proof}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
