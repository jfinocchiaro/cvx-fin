\documentclass[twoside,11pt]{article}
\usepackage[natbib]{jmlr2e}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage[margin=1.1in]{geometry}

\usepackage{float}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[normalem]{ulem}
\usepackage{thm-restate}
\newcommand{\restatehack}[1]{}   % for auc-tex; don't ask
\usepackage{array}
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{mathtools, amsmath, amssymb, graphicx, verbatim}
%\usepackage{amsthm}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\definecolor{darkgreen}{rgb}{0.0,0.3,0.0}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usepackage{tikz,pgfplots,tikz-3dplot}
\usetikzlibrary{calc}
\usetikzlibrary{arrows.meta,shapes.misc,positioning,fit,calc,intersections,through,decorations.markings}
\usetikzlibrary{backgrounds}
\tdplotsetmaincoords{55}{135}        % for simplex diagrams

\tikzstyle{cell}=[dashed,thick]
\tikzstyle{simplex}=[thick]
\newcommand{\tikzfigscale}{2.5}
\newcommand{\placefiglabel}[1]{\node at (1.1,0,1.5) {#1};}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\CommentsR}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\reviewerresponse}[2]{\ifnum\CommentsR=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{darkgreen}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{teal}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{teal!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\reviewer}[1]{\reviewerresponse{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{BTW: #1}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}

\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\affhull}{\mathrm{affhull}}
\newcommand{\card}{\mathrm{card}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\BEP}{L_{\text{\tiny BEP}}}
\newcommand{\LWW}{L_{\text{\tiny WW}}}
\newcommand{\ellabstain}{\ell_{\text{abs}}}
\newcommand{\ellOP}{\ell_{\text{\tiny OP}}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

\newcommand{\Li}[1]{L^{(#1)}}
\newcommand{\Ri}[1]{\R^{(#1)}}
\newcommand{\elli}[1]{\ell^{(#1)}}

% alphabetical order, by convention
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\I}{\mathcal{I}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\relint}{\mathrm{relint}}
%\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
\newcommand{\inter}{\mathrm{inter}}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\red}{\mathrm{red}}
\newcommand{\trimred}{\mathrm{trim}}
\newcommand{\trimcover}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\epi}{\mathrm{epi}}
\newcommand{\hyp}{\mathrm{hypo}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\regret}[3]{R_{#1}(#2,#3)}

\newcommand{\Ind}[1]{\ones\{#1\}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}{\ell_{\text{abs}}^f}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}
\newcommand{\emb}{{\tt e}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sgn}{sgn}

%\newtheorem*{theorem*}{Theorem}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
%\newtheorem{proposition}{Proposition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}

%\newtheorem{definition}{Definition}
\newtheorem{construction}{Construction}
\newtheorem{condition}{Condition}
%\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\jmlrheading{1}{2022}{1-55}{7/22}{7/22}{finocchiaro22a}{Jessie Finocchiaro, Rafael M. Frongillo, and Bo Waggoner}

% Short headings should be running head and authors last names

\ShortHeadings{Embeddings for Consistent Polyhedral Surrogates}{Finocchiaro, Frongillo, and Waggoner}
\firstpageno{1}


%\title{An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates}
%\author{%
% Jessie Finocchiaro \\
% \texttt{jefi8453@colorado.edu}\\
% University of Colorado Boulder
% \and
% Rafael Frongillo\\
% \texttt{raf@colorado.edu}\\
%  University of Colorado Boulder
% \and
% Bo Waggoner\\
% \texttt{bwag@colorado.edu}\\
%  University of Colorado Boulder
%}
%%\date{%                          % nice hack
%%  % \vspace*{-5pt}
%%  University of Colorado Boulder
%%  \\[15pt]
%%  \today
%%}
\begin{document}
\title{An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates}
\author{\name Jessie Finocchiaro \email jefi8453@colorado.edu \\
	\addr Department of Computer Science\\
	University of Colorado Boulder\\
	Boulder, CO, USA
	\AND
	\name Rafael M. Frongillo \email raf@colorado.edu \\
	\addr Department of Computer Science\\
	University of Colorado Boulder\\
	Boulder, CO, USA
	\AND
	\name Bo Waggoner \email bwag@colorado.edu \\
	\addr Department of Computer Science\\
	University of Colorado Boulder\\
	Boulder, CO, USA
}

\editor{Tong Zhang}

\maketitle

\begin{abstract}%
  We formalize and study the natural approach of designing convex surrogate loss functions via embeddings, for discrete problems such as classification, ranking, or structured prediction. 
  In this approach, one embeds each of the finitely many predictions (e.g.\ rankings) as a point in $\reals^d$, assigns the original loss values to these points, and ``convexifies'' the loss in some way to obtain a surrogate.
  We establish a strong connection between this approach and polyhedral (piecewise-linear convex) surrogate losses:
  every discrete loss is embedded by some polyhedral loss, and every polyhedral loss embeds some discrete loss.
  Moreover, an embedding gives rise to a consistent link function as well as linear surrogate regret bounds.
  Our results are constructive, as we illustrate with several examples.
  In particular, our framework gives succinct proofs of consistency or inconsistency for existing polyhedral surrogates, and for inconsistent surrogates, it further reveals the discrete losses for which these surrogates are consistent.
  We go on to show additional structure of embeddings, such as the equivalence of embedding and matching Bayes risks, and the equivalence of various notions of non-redudancy.
  Using these results, we establish that indirect elicitation, a necessary condition for consistency, is also sufficient when working with polyhedral surrogates.
  % The framework can generally be applied to finite prediction tasks, and is demonstrated on examples including high-confidence classification, structured prediction, ordered partitions, and top-$k$ prediction.
\end{abstract}

\begin{keywords}%
  Statistical consistency, surrogate loss functions, calibration, property elicitation, embeddings
\end{keywords}

\section{Introduction}\label{sec:intro}


%% surrogates are important.  in particular, consistent surrogates. (general intro)
In supervised learning, one tries to learn a hypothesis which fits labeled data as judged by a target loss function.
Minimizing the target loss directly is typically computationally intractable for discrete prediction tasks like classification, ranking, and structured prediction.
Instead, one typically minimizes a surrogate loss which is convex and therefore efficiently minimized.
After learning a surrogate hypothesis, a link function then translates back to the target problem.
To ensure the surrogate and link properly correspond to the target problem, the surrogate must be \emph{consistent}, meaning that minimizing the surrogate loss over enough data will also minimize the target loss.

%% designing convex and consistent surrogates is often ad hoc.  often inefficient or not consistent (the gap)
While a growing body of work seeks to design and analyze consistent convex surrogates for particular target loss functions, to date much of this work has been ad-hoc.
We lack general tools to systematically construct consistent convex surrogates, much less an understanding of the full class of consistent surrogates.
For example, in multiclass and top-$k$ classification, several proposed surrogates were proposed and adopted but later proved to be inconsistent~\cite{yang2018consistency,crammer2001algorithmic,rifkin2004defense}.
This state of affairs is even more dire for structured prediction, where in addition to convexity and consistency, one often requires a low \emph{prediction dimension} (the dimension of the surrogate prediction space) as the label set can grow exponentially large.
Clever constructions like the binary-encoded predictions (BEP) surrogate for multiclass classification with an abstain option~\cite{ramaswamy2018consistent}, which achieves logarithmic prediction dimension, are the exception rather than the rule.
In all of these settings, we lack a unifying framework that moves from a given target problem to a convex consistent surrogate and link function.


%% we introduce embeddings (the hero narrative)
To address this shortcoming, we introduce a new framework motivated by a particularly natural approach for finding convex surrogates, wherein one ``embeds'' a discrete loss.
Specifically, we say a convex surrogate $L$ embeds a discrete target loss $\ell$ if there is an injective embedding from the discrete reports (predictions) to $\reals^d$ such that (i) the original loss values are recovered, and (ii) a report is $\ell$-optimal if and only if the embedded report is $L$-optimal.
(See \S~\ref{sec:embedding}.)
Common examples of this general construction include hinge loss as a surrogate for 0-1 loss and the BEP surrogate mentioned above~\citep{ramaswamy2018consistent}.

Using this framework, we give several constructive results to design new consistent surrogates, as well as a suite of tools to analyze existing surrogates.
As a first step, in \S~\ref{sec:poly-loss-embed}, we show that such an embedding scheme is intimately related to the class of \emph{polyhedral} loss functions, i.e., those that are piecewise-linear and convex.
\begin{restatable}{theorem}{embedpolyinformal}\label{thm:embed-poly-main}
  Every discrete loss $\ell$ is embedded by some polyhedral loss $L$, and every polyhedral loss $L$ embeds some discrete loss $\ell$.
\end{restatable}%\vskip-5pt
\noindent
Crucially, we go on in \S~\ref{sec:calibration} to show that an embedding gives rise to a calibrated link function, and is therefore consistent with respect to the target loss.
\begin{restatable}{theorem}{linkinformal}\label{thm:link-main}
  Given any polyhedral loss $L$, let $\ell$ be a discrete loss it embeds. There exists a link function $\psi$ such that $(L,\psi)$ is calibrated with respect to $\ell$.
\end{restatable}
\noindent
Beyond consistency, we show in \S~\ref{subsec:regret-bounds} that any polyhedral surrogate achieves a linear surrogate regret bound, which allows one to translate generalization bounds from the surrogate to the target.
Our results are constructive: given a discrete target loss, we show how to define a surrogate and construct a calibrated link, and given a polyhedral surrogate, we show how to find a discrete loss that it embeds.

We demonstrate the constructiveness of our framework in \S~\ref{sec:applications} with several applications, many of which are subsequent to our work.
In addition to constructing new surrogates, we illustrate the power of our framework to analyze previously proposed polyhedral surrogates.
For example, while we know that the inconsistent top-$k$ polyhedral surrogates mentioned above do not solve top-$k$ classification, our framework illuminates the problem they \emph{do} solve, and what restrictions on the underlying distribution would render them top-$k$ consistent (\S~\ref{sec:top-k}).

%% intuition for the relationship between polyhedral surrogates and discrete target losses
Underpinning our results are several observations, outlined in \S~\ref{sec:min-rep-sets}, which formalize the idea that polyhedral losses ``behave like'' discrete losses.
In particular, any polyhedral loss $L$ has a finite \emph{representative set} $\Sc$ of reports, such that for all distributions, some report in $\Sc$ is $L$-optimal.
We show that $L$ embeds the discrete loss $\ell = L|_\Sc$ given by restricting $L$ to just the reports in $\Sc$.
To go from a discrete loss to a polyhedral surrogate, we prove that the conditions of an embedding are equivalent to matching Bayes risks (Proposition~\ref{prop:embed-bayes-risks}), and use the fact that discrete losses and polyhedral losses both have polyhedral Bayes risks.

Finally, we also provide several observations beyond what is needed to prove our main results, which we view as conceptual contributions (\S~\ref{sec:min-rep-sets},~\ref{sec:poly-ie-consistency}).
Using tools from property elicitation, we show an equivalence between minimum representative sets (those of minimum cardinality) and ``non-redundancy'', wherein no report is dominated by another.
We further show that, while a minimum representative set is not always unique, the loss values associated with it are unique, giving rise to a natural ``trim'' operation on losses.
The paper concludes with an interesting observation (Theorem~\ref{thm:poly-ie-implies-consistent}): while indirect property elicitation is generally a strictly weaker condition than consistency, the two are equivalent when restricting to the class of polyhedral surrogates.

Taken together, we view our contributions as both conceptual and practical.
We uncover the remarkable structure of polyhedral surrogates, deepening our understanding of the relationship between surrogate and discrete target losses.
This structure leads to a powerful new framework to design and analyze surrogate losses.
As we illustrate with several examples, this framework has already been applied to solve open questions by designing new surrogates, to uncover the behavior of existing surrogates, and to construct link functions in complex structured problems.
We conclude with several directions for future work.





\section{Setting}
\label{sec:setting}

For discrete prediction problems like classification, the given (discrete) loss is often computationally intractable to optimize directly.
Therefore, many machine learning algorithms instead minimize a surrogate loss function with better optimization qualities, such as convexity.
To ensure that this surrogate loss successfully addresses the original target problem, one needs to establish statistical consistency, a minimal requirement that is a prerequisite for generalization bounds.
Consistency roughly says that, in the limit as one obtains more and more data, the learned hypothesis approaches the best possible.
Consistency also depends crucially on the choice of link function that maps surrogate reports (predictions) to target reports; see the discussion following Definition~\ref{def:indirect-elic}.

In this section, we introduce notation and concepts related to consistency that we use throughout.
Consistency is often a difficult condition to work with directly, but in finite prediction settings, it is equivalent the simpler notion of \emph{calibration} (Definition~\ref{def:calibrated}) which depends solely on the conditional distribution over the labels~\citep{bartlett2006convexity,tewari2007consistency,ramaswamy2016convex}.
Even simpler than calibration indirect elicitation, a weaker condition only requiring that optimal surrogate reports link to optimal target reports.
Finally, we introduce a new precise notion of embedding, a special case of indirect elicitation, which forms the backbone of our approach.


\subsection{Notation and Losses}
\label{sec:notation-losses}

Let $\Y$ be a finite label space, and throughout let $n=|\Y|$.
Define $\reals^\Y_+$ to be the nonnegative orthant in $\reals^\Y$, i.e., $\reals^\Y_+ = \{x \in \reals^\Y : \forall y\in\Y\; x_y \geq 0 \}$.
Let $\simplex = \{p\in\reals^{\Y}_+ : \|p\|_1 = 1\}$ be the set of probability distributions on $\Y$, represented as vectors.
% We write $p_y$ for the probability of outcome $y \in \Y$ drawn from $p \in \simplex$.
We will primarily focus on conditional distributions $p\in\simplex$ over labels, abstracting away the feature space $\X$; see \S~\ref{subsec:calibration-links} for a discussion of the joint distribution over $\X\times\Y$.

A generic loss function, denoted $L:\R\to\reals^\Y_+$, maps a report (prediction) $r$ from a set $\R$ to the vector of loss values $L(r) = (L(r)_y)_{y\in\Y}$ for each possible outcome $y\in\Y$.
We write the corresponding expected loss when $Y \sim p$ as $\inprod{p}{L(r)}$.
The \emph{Bayes risk} of a loss $L:\R\to\reals^\Y_+$ is the function $\risk{L}:\simplex\to\reals_+$ given by $\risk{L}(p) := \inf_{r\in\R} \inprod{p}{L(r)}$.
When restricting the domain of a loss $L$ from $\R$ to $\R' \subseteq \R$, we write $L|_{\R'}$.

We assume that the target prediction problem is given in the form of a \emph{target loss} $\ell:\R\to\reals^\Y_+$ for some report set $\R$.
A \emph{discrete (target) loss} is such an $\ell$ where $\R$ is a finite set.
%We will assume throughout that the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.
Surrogate losses will take $\R = \reals^d$ and be written $L:\reals^d\to\reals^\Y_+$, typically with reports written $u\in\reals^d$.

%\jessie{In general, we shall denote discrete reports by $r \in \R$, and real-valued reports by $u \in \U$.}
For example, 0-1 loss is a discrete loss with $\R = \Y = \{-1,1\}$
given by $\ellzo(r)_y = \Ind{r \neq y}$, with Bayes risk $\risk{\ellzo}(p) = 1-\max_{y\in\Y} p_y$.
Two widely-used surrogates for $\ellzo$ are hinge loss $\hinge(u)_y = (1-yu)_+$, where $(x)_+ = \max(x,0)$, and logistic loss $L(u)_y = \log(1+\exp(-yu))$ for $u\in\reals$.
See Figure~\ref{fig:bayes-risks-01} for a visualization of the Bayes risks of 0-1, hinge, and logistic losses, respectively.

Many of the surrogate losses we consider will be \emph{polyhedral}, meaning piecewise linear and convex; we briefly recall the relevant definitions.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\citep{rockafellar1997convex}.
%
\begin{definition}[Polyhedral loss]
  A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral function of $u$ for each $y\in\Y$.
\end{definition}
\noindent
In the example above, hinge loss is polyhedral, whereas logistic loss is not.

\subsection{Property Elicitation}
\label{sec:property-elicitation}

We will frequently appeal to concepts and results from property elicitation~\citep{savage1971elicitation,osband1985information-eliciting,lambert2008eliciting,gneiting2011making,steinwart2014elicitation,frongillo2015vector-valued,lambert2018elicitation}.
Here one studies \emph{properties}, maps from (conditional) label distributions to reports, and asks when a property characterizes the reports that exactly minimize a loss.
In our case, this map will at times be set-valued, meaning a single distribution could yield multiple optimal reports.
For example, when $p=(1/2,1/2)$, both $r=1$ and $r=-1$ optimize 0-1 loss.
We will use double arrow notation to denote a (non-empty) set-valued map, so that $\Gamma: \simplex \toto \R$ is shorthand for $\Gamma: \simplex \to 2^{\R} \setminus \{\emptyset\}$, where $2^\R$ denotes the power set of $\R$.
%See the discussion following Definition~\ref{def:elicits} for notational conventions.% regarding $\R$, $\Gamma$, $\gamma$, $L$, $\ell$, etc.

\begin{definition}[Property, level set]\label{def:property}
  A \emph{property} is a function $\Gamma:\simplex\toto\R$.
  The \emph{level set} of $\Gamma$ for report $r$ is the set $\Gamma_r := \{p \in \simplex \mid r \in \Gamma(p)\}$.
  If $\R$ is finite, we call $\Gamma$ a \emph{finite property}.
\end{definition}

Intuitively, $\Gamma(p)$ is the set of reports which should optimize expected loss for a given distribution $p$, and $\Gamma_r$ is the set of distributions for which the report $r$ should be optimal.
For example, the \emph{mode} is the %finite
property $\mode(p) = \argmax_{y\in\Y} p_y$, and captures the set of optimal reports for 0-1 loss: for each distribution over the labels, one should report the most likely label.
In this case we say 0-1 loss (directly) \emph{elicits} the mode, as we formalize below.
% This terminology comes from the information elicitation \jessiet{information vs property... do we care?}\raft{we could also say economics literature; not ``property'' though since that doesn't offer an explanation of the word ``elicits''} literature~\citep{savage1971elicitation,osband1985information-eliciting,lambert2008eliciting}, in which a report $r$ is elicited from some forecaster by scoring her with a loss on the observed outcome $y$.

\begin{definition}[Directly Elicits]
  \label{def:elicits}
  A loss $L:\R\to\reals^\Y_+$, \emph{(directly) elicits} a property $\Gamma:\simplex \toto \R$ if
  \begin{equation}
    \forall p\in\simplex,\;\;\;\Gamma(p) = \argmin_{r \in \R} \inprod{p}{L(r)}~.
  \end{equation}
  If $L$ elicits a property, that property is unique and we denote it $\prop{L}$.
\end{definition}
Since we have defined a property $\Gamma$ to be nonempty, if the infimum of expected loss $\inprod{p}{L(\cdot)}$ is not attained for some $p \in \simplex$, then $L$ does not elicit a property.
We say that a loss $L$ is \emph{minimizable} if the infimum of $\inprod{p}{L(\cdot)}$ is attained for all $p \in \simplex$.
For example, hinge loss is minimizable, whereas logistic loss is not (take $p=(0,1)$ or $(1,0)$).

We will typically denote general properties and losses with $\Gamma$ and $L$, respectively.
For surrogate losses and properties, recall that we typically consider the report set $\reals^d$.
For discrete target losses and properties, we will take $\R$ to be a finite set, and use lowercase notation $\gamma$ and $\ell$, respectively.


\subsection{Calibration and Links}
\label{subsec:calibration-links}


To assess whether a surrogate and link function align with the original loss, we turn to the common condition of \emph{calibration}.
Roughly, a surrogate and link are calibrated if the best possible expected loss achieved by linking to an incorrect report is strictly suboptimal.



% \begin{definition}[Consistency]\label{def:consistency}
%   A surrogate $L:\reals^d\to\reals^\Y_+$ and link $\psi:\reals^d\to\R$ is \emph{consistent} with a discrete loss $\ell:\R\to\reals^\Y_+$ if \raf{statement about distributions on $\X\times\Y$, limits of hypotheses, and Bayes opt hypotheses}
% \end{definition}

% \begin{definition}[Separated Link]\label{def:links}
%   Let discrete loss $\ell:\R\to\reals^\Y_+$ and surrogate $L:\reals^d\to\reals^\Y_+$ be given.
%   A \emph{link function} is a map $\psi:\reals^d\to\R$.
%   We say that a link $\psi$ is \emph{$\delta$-separated} for some $\delta > 0$ if for all $p \in \simplex$ and $u\in\reals^d$, we have
%   \begin{align*}
%     \inprod{p}{L(u)} - \inf_{u' \in \reals^d} \inprod{p}{L(u')} \geq \delta\left(\inprod{p}{\ell(\psi(u))} - \min_{r \in \R} \inprod{p}{\ell(r)}\right)~.
%   \end{align*}
%   A link is \emph{separated} if it is $\delta$-separated for some $\delta>0$.
% \end{definition}

\begin{definition}
  \label{def:calibrated}
  Let discrete loss $\ell:\R\to\reals^\Y_+$, proposed surrogate $L:\reals^d\to\reals^\Y_+$, and function $\psi:\reals^d\to\R$ be given.
  In this context, $\psi$ is called a \emph{link function}.
  We say $(L,\psi)$ is \emph{calibrated} with respect to $\ell$ if
for all $p \in \simplex$,
  \begin{equation}
    \label{eq:calibrated}
  \inf_{u \in \reals^d : \psi(u) \not\in \prop{\ell}(p)} \inprod{p}{L(u)} > \inf_{u \in \reals^d} \inprod{p}{L(u)}~.
  \end{equation}
  If $(L, \psi)$ is calibrated with respect to $\ell$, we call $\psi$ a \emph{calibrated link.}
\end{definition}


It is well-known in finite-outcome settings that calibration is equivalent to \emph{consistency}, in the following sense (cf.~\citep{bartlett2006convexity,zhang2004statistical,agarwal2015consistent}).
Suppose we have the feature space $\X$ and label space $\Y$.
%For any data distribution $D \in \Delta(\X \times \Y)$, let $L^*$ be the best possible expected (over $D$) $L$-loss achieved by any hypothesis $H:\X\to\reals^d$, and $\ell^*$ the best expected $\ell$-loss for any hypothesis $h:\X\to\R$, respectively.
We say a surrogate and link pair $(L,\psi)$ is consistent with respect to $\ell$ if, for all data distributions $D \in \Delta(\X \times \Y)$, and all sequences of surrogate hypotheses $h_1,h_2,\ldots$ whose $L$-loss limits to the optimal surrogate loss $L^*$ (in expectation over $D$), the $\ell$-loss of the sequence $\psi\circ h_1,\psi \circ h_2, \ldots$ limits to the optimal target loss $\ell^*$.
As Definition~\ref{def:calibrated} does not involve the feature space $\X$, we will drop it for the remainder of the paper.


Like the use of a surrogate and link pair in the calibration definition, one can also extend the earlier definition of property elicitation to \emph{indirect (property) elicitation}, in which one applies a link to an elicited property to obtain a related property of interest.
\begin{definition}\label{def:indirect-elic}
	Let minimizable loss $L : \reals^d \to \reals^\Y_+$ and link $\psi : \reals^d \to \R$ be given.
  The pair $(L, \psi)$ indirectly elicits a property $\gamma : \simplex \toto \R$ if for all $u \in \reals^d$, we have $\Gamma_u \subseteq \gamma_{\psi(u)}$, where $\Gamma = \prop{L}$.
	Moreover, we say $L$ indirectly elicits $\gamma$ if such a $\psi$ exists, i.e., if for all $u\in\reals^d$ there exists $r\in\R$ such that $\Gamma_u \subseteq \gamma_r$.%
  \footnote{The elicitation literature often refers to this latter condition as one property ``refining'' another~\citep{frongillo2015elicitation}.}
\end{definition}

\raft{FYI: Added this paragraph since we needed it in \S~\ref{sec:poly-ie-consistency}}
Indirect elicitation is a weaker condition than calibration~\citep{steinwart2008support,agarwal2015consistent,finocchiaro2021unifying}; we briefly sketch the argument.
Suppose $L$ is minimizable and $(L,\psi)$ is calibrated with respect to $\ell$, and set $\Gamma=\prop{L}$ and $\gamma=\prop{\ell}$.
Let $u\in\reals^d$ and $p\in\Gamma_u$.
From eq.~\eqref{eq:calibrated}, if $\psi(u) \notin \gamma(p)$, then we would have $u\notin\Gamma(p)$, a contradiction to $p\in\Gamma_u$.
Thus, $\psi(u) \in \gamma(p)$, so $p\in\gamma_{\psi(u)}$.
In fact, indirect elicitation is strictly weaker; take hinge loss with the link $\psi(u) = -1$ for $u < 1$ and $\psi(u) = 1$ for $u\geq 1$.
\reviewer{Explaining why $(L, \psi)$ is not calibrated w.r.t. $0-1$ loss.}
While this pair indirectly elicits the mode, we can show it is not calibrated with respect to $0-1$ loss. 
Suppose $ p  = (0, 1)$ and consider the sequence $\{1 - \frac 1 {n+1}\}_{n=1}^\infty$, which limits to $1$, but $\psi(u) = -1$ for all $u$ in the sequence.
Therefore, the infimum of hinge loss over reports not linking to $1$ approaches the infimum over all reports.
\citet{agarwal2015consistent} were the first to formally connect property elicitation to calibration, though their results generally do not apply to discrete prediction tasks.

% At times, the elicitation literature studies indirect elicitation through the notion of one property ``refining'' another~\citep{frongillo2015elicitation}, where a property $\Gamma: \simplex \toto \R$ refines $\Gamma': \simplex \toto \R'$ if, for all $r \in \R$, there exists an $r' \in \R'$ such that $\Gamma_r \subseteq \Gamma'_{r'}$.
% In particular, a loss $L$ indirectly elicits a property $\gamma$ if and only if $\prop L$ refines $\gamma$.




\subsection{Embedding}
\label{sec:embedding}

%Several consistent convex surrogates in the literature can be thought of as \emph{embeddings} of the target loss $\ell$, wherein one maps the discrete reports of $\ell$ to a vector space, and finds a convex loss which agrees with the original loss.
%A key notion to define an embedding is that of a \emph{representative set}: a set of reports $\Sc$ such that, for all label distributions, at least one report $r\in\Sc$ minimizes expected loss.
We now formalize the sense in which a convex surrogate can \emph{embed} a target loss $\ell$.
Here one maps each report (prediction) of $\ell$ to a point in $\reals^d$, then constructs a convex loss on $\reals^d$ that agrees with $\ell$ at these points.
This approach captures several consistent surrogates in the literature (e.g.,~\citep{ramaswamy2015hierarchical,ramaswamy2016convex,lapin2015top,wang2020weston}; see \S~\ref{sec:applications}).

An important subtlety is that it is not always necessary to map \emph{all} target reports to $\reals^d$.
It is often convenient to allow $\ell$ to have reports that are ``redundant'' in some sense. (We explore redundancy further in \S~\ref{sec:min-rep-sets}; see also \citet{wang2020weston}.)
Because of this redundancy, we will only require an embedding map to be defined on a \emph{representative set}: a set of reports $\Sc$ such that, for all (conditional) label distributions, at least one report $r\in\Sc$ minimizes the conditional expected loss.
\begin{definition}[Representative set]
  Let $\Gamma:\simplex\toto\R$.
  We say $\Sc \subseteq \R$ is \emph{representative for $\Gamma$} if we have $\Gamma(p) \cap \Sc \neq \emptyset$ for all $p\in \simplex$.
  We further say $\Sc$ is a \emph{minimum representative set} if it has the smallest cardinality among all representative sets.
  Given a minimizable loss $L:\R\to\reals^\Y_+$, we say $\Sc$ is a (minimum) representative set for $L$ if it is a (minimum) representative set for $\prop L$.
  % We say $\Sc \subseteq \R$ is \emph{representative for $L$} if we have $\prop{L}(p) \cap \Sc \neq \emptyset$ for all $p\in \simplex$.
  % We further say $\Sc$ is a \emph{minimum representative set} if it has the smallest cardinality among all representative sets.
\end{definition}

\noindent
\citet{wang2020weston} first study the notion of minimum representative sets under the name \emph{embedding cardinality}.

%It turns out that it will be convenient, especially in applications, to allow the target $\ell:\R\to\reals^\Y_+$ to have reports which are ``redundant'' in some sense.
%One natural sense is that $\ell$ has a representative set that is a strict subset of $\R$, i.e., $\R$ is not a minimum representative set.
%\raft{Some discussion here about minimum representative sets and \emph{maximally informative sets} from~\citet{wang2020weston} here, though maybe best to leave it until later.  BTW, I think their latest version might use different terminology.}
%Another is that some reports $r$ are ``dominated'', meaning $r$ is never optimal or there is another report $r'$ which is always optimal when $r$ is optimal.
%A priori these alternate definitions may not seem closely related.
%In fact, we show in \S~\ref{sec:min-rep-sets} that they are equivalent in our setting.

We now define an embedding, which is a special case of indirect property elicitation.
(This fact is non-trivial; see Lemma~\ref{lem:embedding-implies-indirect-elic}.)
In addition to matching loss values, as described above, we require the original reports to be $\ell$-optimal exactly when the corresponding embedded points are $L$-optimal.
As we discuss following Proposition~\ref{prop:representative-embeds-restriction}, this latter condition can be more simply stated: the representative set for the target must embed into a representative set for the surrogate.
\raft{FYI: Added this last sentence}
\begin{definition}[Embedding]\label{def:loss-embed}
  A minimizable loss $L:\reals^d\to\reals^\Y_+$ \emph{embeds} a loss $\ell:\R\to\reals^\Y_+$ if there exists a representative set $\Sc$ for $\ell$ and an injective embedding $\varphi:\Sc\to\reals^d$ such that
  (i) for all $r\in\Sc$ we have $L(\varphi(r)) = \ell(r)$, and (ii) for all $p\in\simplex,r\in\Sc$ we have
  % \begin{equation}\label{eq:embed-loss}
  %   r \in \argmin_{r'\in\R} \inprod{p}{\ell(r')} \iff \varphi(r) \in \argmin_{u\in\reals^d} \inprod{p}{L(u)}~.
  % \end{equation}
  \begin{equation}\label{eq:embed-loss}
    r \in \prop{\ell}(p) \iff \varphi(r) \in \prop{L}(p)~.
  \end{equation}
  If $\Sc$ is a minimal representative set, we say $L$ \emph{tightly embeds} $\ell$.
  %if there is some representative set $\Sc$ for $\ell$ such that $L$ embeds $\ell$ over $\Sc$.
\end{definition}

To illustrate the idea of embedding, let us closely examine hinge loss as a surrogate for 0-1 loss in binary classification.
Recall that we have $\R = \Y = \{-1, +1\}$, with $\hinge(u)_y = (1 - uy)_+$ and $\ellzo(r)_y := \Ind{r\neq y}$, typically with link function $\psi(u) = \sgn(u)$, where $\sgn(0) = 1$ without loss of generality.
We will see that hinge loss embeds (2 times) 0-1 loss, via the identity embedding $\varphi(r) = r$.
For condition (i), it is straightforward to check that $\hinge(\varphi(r))_y = \hinge(r)_y = 2 \Ind{r \neq y} = 2\ellzo(r)_y$ for all $r,y\in\{-1,1\}$.
% First, consider that
% \begin{align*}
% 	\ellzo(-1) = (0,1) && \ellzo(1) = (1,0)\\
% 	L_{hinge}(-1) = (0,2) && L_{hinge}(1) = (2,0)\\
% \end{align*}
% Therefore, $L_{hinge}$ is twice $\ellzo$ for each $r \in \R$.
For condition (ii), let us compute the property each loss elicits, i.e., the set of optimal reports for each $p\in\simplex$:
\[
\prop{\ellzo}(p) = \begin{cases}
1 & p_1 > 1/2 \\
\{-1,1\} & p_1 = 1/2\\
-1 & p_1 < 1/2
\end{cases}
\qquad
\prop{L_{hinge}}(p) = \begin{cases}
[1,\infty) & p_1 = 1\\
1 & p_1 \in (1/2,1) \\
[-1,1] & p_1 = 1/2\\
-1& p_1 \in (0, 1/2)\\
(-\infty, -1]& p_1 = 0
\end{cases}~.
\]
% Now, take the embedding $\varphi$ to be the identity.  % := r$ if $r \in [-1,1]$, and $\sgn(r)$ otherwise.
In particular, we see that $-1 \in \prop{\ellzo}(p) \iff p_1 \in [0, 1/2] \iff -1 \in \prop{\hinge}(p)$, and $1 \in \prop{\ellzo}(p) \iff p_1 \in [1/2,1] \iff 1 \in \prop{\hinge}(p)$.
With both conditions of Definition~\ref{def:loss-embed} satisfied, we can conclude that $\hinge$ embeds $2\ellzo$ with the representative set $\Sc = \{-1,1\}$.
By results in \S~\ref{subsec:match-BR}, one could also show that $\hinge$ embeds $2\ellzo$ by the fact that their Bayes risks match (Figure~\ref{fig:bayes-risks-01}).

\begin{figure}
	\begin{minipage}{0.3\linewidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figs/0-1-br.pdf}
%		\caption{Bayes risk of the 0-1 loss.}
%		\label{fig:0-1-br}
	\end{minipage}
	\hfill
	\begin{minipage}{0.3\linewidth}
	\centering		\includegraphics[width=0.95\linewidth]{figs/hinge-br.pdf}
%		\caption{Bayes risk of hinge loss.}
%		\label{fig:hinge-br}
	\end{minipage}
	\hfill
	\begin{minipage}{0.3\linewidth}
	\centering
	\includegraphics[width=0.95\linewidth]{figs/logistic-br.pdf}
%	\caption{Bayes risk of the logistic loss.}
%	\label{fig:logistic-br}
\end{minipage}
\caption{Bayes risks $\risk L : p \mapsto \inf_u \inprod{p}{L(u)}$ of 0-1, hinge, and logistic losses, respectively, plotted as a function of $p_1 = \Pr[Y=1]$.
	Observe that the Bayes risks of 0-1 and hinge loss are both piecewise linear and concave, while the Bayes risk of logistic loss is not piecewise linear.  Proposition~\ref{prop:embed-bayes-risks} states that embedding is equivalent to matching Bayes risks, confirming that hinge loss (M) embeds 0-1 loss (L), but logistic loss (R) does not.}
\label{fig:bayes-risks-01}
\end{figure}

%% embedding yields calibration
In this particular example, it is known $(\hinge,\sgn)$ is calibrated with respect to 0-1 loss~\citep[Example 4]{bartlett2006convexity}.
Beyond this simple case, however, it is not clear whether an embedding will always yield a calibrated link.
Indeed, while it is intuitively clear that embedded points should link back to their original reports, via $\psi(\varphi(r)) = r$, how to map the remaining values is far from obvious.
Using the strong connection between embeddings and polyhedral surrogates in \S~\ref{sec:poly-loss-embed}, we give a construction to map the remaining values in \S~\ref{sec:calibration}, showing that embeddings from polyhedral surrogates always yield calibration.

While our notion of embedding is sufficient for calibration (and therefore consistency), it is worth noting that an embedding is not \emph{necessary} for these conditions.  
For example, while logistic loss does not embed 0-1 loss, logistic loss and the sign link are still consistent for 0-1 loss.
When working with polyhedral surrogates, however, embeddings are necessary for calibration in a strong sense, as we discuss in \S~\ref{sec:poly-ie-consistency}: if a polyhedral surrogate $L$ has a calibrated link to some target $\ell$, then $L$ must embed some discrete target $\hat\ell$ which can then be linked to $\ell$.



\section{Embeddings and Polyhedral Losses}
\label{sec:poly-loss-embed}

In this section, we establish a tight relationship between the technique of embedding and the use of polyhedral (piecewise-linear convex) surrogate losses, culminating in Theorem~\ref{thm:embed-poly-main}.
We defer the question of when such surrogates are consistent to \S~\ref{sec:calibration}. 

A first observation is that if a loss $L$ elicits a property $\Gamma$, then $L$ restricted to some representative set $\Sc$, denoted $L|_\Sc$, elicits $\Gamma$ restricted to $\Sc$.
As a consequence, restricting to representative sets preserves the Bayes risk.
We will use these observations throughout.
\begin{lemma}\label{lem:loss-restrict}
  Let $L:\R\to\reals^\Y_+$ elicit $\Gamma$, and let $\Sc\subseteq\R$ be representative for $L$.
  % such that $\Gamma(p) \cap \Sc \neq \emptyset$ for all $p\in\simplex$.
  Then $L|_\Sc$ elicits $\gamma:\simplex\toto\Sc$ defined by $\gamma(p) = \Gamma(p)\cap \Sc$.
  Moreover, $\risk{L}=\risk{L|_\Sc}$.
\end{lemma}
\begin{proof}
  Let $p\in\simplex$ be fixed throughout.
  First let $r \in \gamma(p) = \Gamma(p) \cap \Sc$.
  Then $r \in \Gamma(p) = \argmin_{u\in\R} \inprod{p}{L(u)}$, so as $r\in\Sc$ we have in particular $r \in \argmin_{u\in\Sc} \inprod{p}{L(u)}$.
  For the other direction, suppose $r \in \argmin_{u\in\Sc} \inprod{p}{L(u)}$.
  As $\Sc$ is representative for $L$, we must have some $s \in \Gamma(p) \cap \Sc$.
  On the one hand, $s\in\Gamma(p) = \argmin_{u\in\R} \inprod{p}{L(u)}$.
  On the other, as $s \in \Sc$, we certainly have $s \in \argmin_{u\in\Sc} \inprod{p}{L(u)}$.
  But now we must have $\inprod{p}{L(r)} = \inprod{p}{L(s)}$, and thus $r \in \argmin_{u\in\R} \inprod{p}{L(u)} = \Gamma(p)$ as well.
  We now see $r \in \Gamma(p) \cap \Sc$.
  Finally, the equality of the Bayes risks $\min_{u\in\R} \inprod{p}{L(u)} = \min_{u\in\Sc} \inprod{p}{L(u)}$ follows immediately by the above, as $\emptyset \neq \Gamma(p)\cap\Sc \subseteq \Gamma(p)$ for all $p\in\simplex$.
\end{proof}


Lemma~\ref{lem:loss-restrict} leads to the following useful tool for finding embeddings: if a surrogate has a finite representative set, it embeds its restriction to the representative set.
\begin{proposition}\label{prop:representative-embeds-restriction}
  Let a minimizable surrogate loss $L:\reals^d \to \reals^\Y_+$ be given.
  If $L$ has a finite representative set $\Sc \subseteq \reals^d$, then $L$ embeds the discrete loss $L|_\Sc$.
\end{proposition}
\begin{proof}
  Let $\Gamma = \prop{L}$ and $\gamma = \prop{L|_\Sc}$.
  Define $\varphi : \Sc \to \Sc$ to be the identity embedding.
  Condition (i) of an embedding is trivially satisfied, as $L|_\Sc(u) = L(u)$ for all $u\in\Sc$.
  Now let $u\in\Sc$.
  From Lemma~\ref{lem:loss-restrict}, for all $p\in\simplex$ we have $u \in \gamma(p) \iff u \in \Gamma(p) \cap \Sc \iff u \in \Gamma(p)$.
  We conclude condition (ii) of an embedding.
\end{proof}

\raft{FYI: I added this paragraph since I used it in the Lov\'asz hinge talk (so I didn't have to define properties), and realized that maybe it should have been our definition this whole time!  I was terse with the details, but we could spell it out more if it's not clear.}
Proposition~\ref{prop:representative-embeds-restriction} reveals an equivalent definition of an embedding which can be more convenient.
Given a representative set $\Sc$ for $\ell$, an injection $\varphi:\Sc\to\reals^d$ is an embedding if: (i) the loss values match as in Definition~\ref{def:loss-embed}(i), and (ii) $\varphi(\Sc)$ is representative for $L$.
This new definition is clearly implied by Definition~\ref{def:loss-embed}; for the converse, Proposition~\ref{prop:representative-embeds-restriction} states that $L$ embeds $L|_{\varphi(\Sc)}$, which by (i) is the same loss as $\ell$ up to relabeling via $\varphi$.

With Proposition~\ref{prop:representative-embeds-restriction} in hand, we now shift our focus to \emph{polyhedral} (piecewise-linear and convex) surrogates.
While polyhedral surrogates do not directly elicit finite properties, as their report sets are infinite, they do elicit properties with a finite range, meaning the set of possible optimal report sets is finite.
%This observation is the statement of Lemma~\ref{lem:polyhedral-range-gamma} below.


\begin{restatable}{lemma}{polyhedralrangegamma}
	\label{lem:polyhedral-range-gamma}
	Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss.
  Then $L$ is minimizable and elicits a property $\Gamma := \prop{L}$.
	Moreover, the range of $\Gamma$, given by $\Gamma(\simplex) = \{\Gamma(p) \subseteq \reals^d : p\in\simplex\}$, is a finite set of closed polyhedra.
\end{restatable}
\begin{proof}[Proof sketch]
	See \S~\ref{app:power-diagrams} for the full proof.
	As $L$ is bounded from below, $L$ is minimizable from~\citet[Corollary 19.3.1]{rockafellar1997convex} .
	With $\Y$ finite, there are only finitely many supporting sets over $\simplex$.
	For $p \in \simplex$, the power diagram induced by projecting the epigraph of expected loss onto $\reals^d$ is the same for any $p$ of the same support (Lemma~\ref{lem:polyhedral-pd-same}).
	Moreover, we have $\Gamma(p)$ is exactly one of the faces of the projected epigraph since the hyperplane $u \mapsto (u, \inprod{p}{L(u)})$ supports the epigraph of the expected loss at exactly the property value; moreover, since the loss is polyhedral the supporting hyperplane must support a face of the epigraph.
	Since this epigraph has finitely many faces as it is polyhedral, the range of $\Gamma$ is then a subset of elements of a finitely generated (finite supports) set of finite elements (finite faces).
	Moreover, each element of $\Gamma(\simplex)$ is a closed polyhedron since it corresponds exactly to a closed face of a polyhedral set.
\end{proof}

From Lemma~\ref{lem:polyhedral-range-gamma}, one can simply select a point from each of the finitely many optimal sets to obtain a finite representative set.
% This observation lets us apply results about finite representative sets to understand the structure of polyhedral surrogates and the losses they embed.
Plugging this finite representative set into Proposition~\ref{prop:representative-embeds-restriction} then yields an embedding.


\begin{theorem}\label{thm:poly-embeds-discrete}
  Every polyhedral loss $L$ embeds a discrete loss.
\end{theorem}
\begin{proof}
  Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss, and $\Gamma = \prop{L}$.
  By Lemma~\ref{lem:polyhedral-range-gamma}, $\Gamma(\simplex)$ is a finite set. 
  For each $U\in \Gamma(\simplex)$, select $u_U \in U$, and let $\Sc = \{u_U : U \in\Gamma(\simplex)\}$, which is again finite.
  For any $p\in\simplex$ then, let $U = \Gamma(p)$.
  We have $U \in \Gamma(\simplex)$ by definition, and thus some $u_U \in \Sc$; in particular, $u_U \in U = \Gamma(p)$.
  We conclude that $\Sc$ is representative for $L$.
  Proposition~\ref{prop:representative-embeds-restriction} now states that $L$ embeds $L|_\Sc$.
\end{proof}

We now turn to the reverse direction: which discrete losses are embedded by some polyhedral loss?
Perhaps surprisingly, we show in Theorem~\ref{thm:discrete-loss-poly-embeddable} that \emph{every} discrete loss is embeddable by a polyhedral surrogate.
In the proof, we apply a result we will prove in \S~\ref{sec:min-rep-sets}: a minimizable surrogate embeds a discrete loss if and only if their Bayes risks match (Proposition~\ref{prop:embed-bayes-risks}).

\begin{theorem}\label{thm:discrete-loss-poly-embeddable}
  Every discrete loss $\ell:\R \to \reals^\Y_+$ is embedded by a polyhedral loss.
\end{theorem}
\begin{proof}
  Let $n = |\Y|$, and let $C:\reals^n \to \reals$ be given by $(-\risk{\ell})^*$, the convex conjugate of $-\risk{\ell}$.
  From standard results in convex analysis, $C$ is polyhedral as $-\risk{\ell}$ is, and $C$ is finite on all of $\reals^\Y$ as the domain of $-\risk{\ell}$ is bounded~\cite[Corollary 13.3.1]{rockafellar1997convex}.
  Note that $-\risk{\ell}$ is a closed convex function, as the infimum of affine functions, and thus $(-\risk{\ell})^{**} = -\risk{\ell}$.
  Define $L:\reals^n\to\reals^\Y$ by $L(u) = C(u)\ones - u$, where $\ones\in\reals^\Y$ is the all-ones vector.
  As $C$ is polyhedral, so is $L$.
  We first show that $L$ embeds $\ell$, and then establish that the range of $L$ is in fact $\reals^\Y_+$, as desired.

  We compute Bayes risks and apply Proposition~\ref{prop:embed-bayes-risks} to see that $L$ embeds $\ell$.
  Observe that $\risk{\ell}$ is polyhedral as $\ell$ is discrete.
  For any $p\in\simplex$, we have
  \begin{align*}
    \risk{L}(p)
    &= \inf_{u\in\reals^n} \inprod{p}{C(u)\ones - u}\\
    &= \inf_{u\in\reals^n} C(u) - \inprod{p}{u}\\
    &= -\sup_{u\in\reals^n} \inprod{p}{u} - C(u)\\
    &= -C^*(p) = - (-\risk{\ell}(p))^{**} = \risk{\ell}(p)~.
  \end{align*}
  It remains to show $L(u)_y \geq 0$ for all $u\in\reals^n$, $y\in\Y$.
  Letting $\delta_y\in\simplex$ be the point distribution on outcome $y\in\Y$, we have for all $u\in\reals^n$, $L(u)_y \geq \inf_{u'\in\reals^n} L(u')_y = \risk{L}(\delta_y) = \risk{\ell}(\delta_y) \geq 0$, where the final inequality follows from the nonnegativity of $\ell$.
\end{proof}

Combining this result with Theorem~\ref{thm:poly-embeds-discrete} establishes Theorem~\ref{thm:embed-poly-main}.
Further combining with Theorem~\ref{thm:link-main}, proved in the following section, this construction gives a calibrated polyhedral surrogate for every discrete target loss.

\raf{Restate Thm 1 and give a short proof, either in text since it's SO simple, or formal proof environment}

The proof of Theorem~\ref{thm:discrete-loss-poly-embeddable} uses a construction via convex conjugate duality similar to many constructions in the literature. 
%(e.g.\ \cite{duchi2018multiclass,abernethy2013efficient,frongillo2014general}).
For example, the min-max objective in the literature on adversarial prediction~\citep{asif2015adversarial,farnia2016minimax,fathony2016adversarial,fathony2018consistent} is a special case of this construction when one unfolds the definition of the convex conjugate of $-\risk \ell$.
% reducing to  playing the role of their is a maximum of affine functions, and the conjugate takes the infimum (which is always attained) so $C = (-\risk \ell)^*$ used in the proof of Theorem~\ref{thm:discrete-loss-poly-embeddable} lines up with their min-max objective.  
%\raft{TODO: We said ``poses the construction in Theorem~\ref{thm:discrete-loss-poly-embeddable}'' -- is it exactly the same?  We just have to say how it relates, like ``when viewing X as Y, one arrives at the same construction'', or maybe one is a special case of the other.}\raf{TODO: Update 6/12/2022: looks like this sentence changed, but now isn't tied to Theorem~\ref{thm:discrete-loss-poly-embeddable}...}
%\jessie{If I understand the papers correctly, the cited works take a loss matrix (often for adversarial classification or prediction) and use the dual to obtain a consistent surrogate for the task, so it's just a specific application of this construction.}
\citet{reid2012convexity} construct a canonical link function for proper losses with differentiable Bayes risks; the link maps a report $p\in\simplex$ to the gradient of the Bayes risk at $p$, which uses the same duality as above.
\citet[Proposition 3]{duchi2018multiclass} give essentially the same construction as ours, but only comment on the calibration of surrogates under such constructions for multiclass classification tasks given by strictly concave losses, excluding polyhedral surrogates.
Finally, a similar construction also appears in the design of prediction markets \citep{abernethy2013efficient} and in connections between proper losses and mechanism design \citep{frongillo2014general}.


\section{Consistency and Linear Regret Transfer via Separated Links}
\label{sec:calibration}

We have seen that every polyhedral loss embeds some discrete loss.
The embedding itself tells us how to link the embedded points back to the discrete reports: link $\varphi(r)$ back to $r$.
But it is not clear how to extend this to yield a full link function $\psi: \reals^d \to \R$, and whether such a $\psi$ can lead to consistency.
In this section, we prove Theorem~\ref{thm:link-main}, restated below, via a construction to generate calibrated links for \emph{any} polyhedral surrogate.
Recalling that calibration is equivalent to consistency for discrete targets, this result implies that an embedding always yields consistency.

The key idea behind our construction is the notion of \emph{separation}, a condition which is equivalent to calibration for discrete prediction problems.
Roughly, given a surrogate $L$ and discrete target $\ell$, a link is $\epsilon$-separated if the distance between any $L$-optimal point in $\reals^d$, and any point that links to an $\ell$-suboptimal report, is at least $\epsilon$.
\raft{FYI: previous statement was not correct I think: ``Roughly, a link is $\epsilon$-separated if the distance between any point in $\reals^d$ that links to an $\ell$-optimal report, and any point that links to a non-optimal report, is at least $\epsilon$.''}
We also show how this characterization also leads to linear \emph{regret transfer} or \emph{surrogate regret} bounds.

\subsection{Separation}

%Theorem \ref{thm:calibrated-separated} shows that calibration is equivalent to a geometric condition, which we call \emph{separation}, of a link function $\psi$.
Recall that for indirect elicitation, any point $u \in \Gamma(p)$ must link to a report $\psi(u) \in \gamma(p)$. 
(In terms of losses, $u$ minimizing expected $L$-loss implies that $\psi(u)$ minimizes expected $\ell$-loss, with respect to $p$.)
The idea of separation is that points in the neighborhood of $u$ must also link to to a report in $\gamma(p)$.
Furthermore, there must be a uniform lower bound $\epsilon$ on the size of any such neighborhood.


\begin{definition}[Separated Link]\label{def:sep-link}
  Let properties $\Gamma:\simplex\toto\reals^d$ and $\gamma:\simplex\toto\R$ be given.
  We say a link $\psi:\reals^d\to\R$
  is \emph{$\epsilon$-separated with respect to $\Gamma$ and $\gamma$} if for all $u\in\reals^d$, $p\in\simplex$ with $\psi(u)\notin\gamma(p)$, we have $d_\infty(u,\Gamma(p)) \geq \epsilon$, where $d_\infty(u,A) \doteq \inf_{a\in A} \|u-a\|_\infty$.%
  \footnote{\citet{frongillo2021surrogate} define $\epsilon$-separation with a strict inequality $d_\infty(u,\Gamma(p)) > \epsilon$; we adopt a weak inequality as it is more natural in applications.  For example, taking hinge loss for binary classification, the sign link is 1-separated under the weak inequality, but only $(1-\delta)$-separated for $\delta > 0$ under the strict inequality.}
  Similarly, we say $\psi$ is $\epsilon$-separated with respect to $L$ and $\ell$ if it is $\epsilon$-separated with respect to $\prop{L}$ and $\prop{\ell}$.
  \raft{FYI, this def, along with Theorem~\ref{thm:discrete-loss-poly-embeddable} and Theorem~\ref{thm:calibrated-separated}, are referenced specifically from the top-k paper, so hopefully the numbers don't change by the time we post on arXiv}
\end{definition}

\begin{restatable}{theorem}{calibratedseparated} \label{thm:calibrated-separated}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given.
  Then $(L,\psi)$ is calibrated with respect to $\ell$ if and only if
  $\psi$ is $\epsilon$-separated with respect to $L$ and $\ell$ for some
  $\epsilon>0$.
\end{restatable}
Intuitively, calibration of a polyhedral surrogate and separated link follows from two facts.
First, Lemma~\ref{lem:polyhedral-range-gamma} states that a polyhedral surrogate has only a finite number of ``optimal report sets'' $\{\Gamma(p) : p \in \simplex\}$.
Second, for a given $p$, the expected surrogate loss for a suboptimal point scales with the distance from the optimal set $\Gamma(p)$ at some minimum linear rate $\alpha > 0$; this rate is related to \emph{Hoffman constants}.
Combining with the first fact gives a universal minimum constant $\alpha$ for all $p$.
Now bringing in $\epsilon$-separation, any surrogate report linking to a suboptimal target report has expected surrogate loss at least $\alpha \cdot \epsilon > 0$.
On the other hand, if a link is not separated, then the same two facts imply that, for a sequence of surrogate reports that get arbitrarily close to an optimal report set while linking to a suboptimal target report, this sequence has conditional expected loss approaching optimal, violating calibration.
See \S~\ref{sec:equiv-sep-calib} for the proof.

\subsection{Consistency}

\restatehack{
  \begin{theorem}
    \label{thm:calibrated-separated}
    \label{thm:thickened-separated}
  \end{theorem}}

To prove Theorem \ref{thm:link-main}, that embedding implies calibration, we now show how to construct a calibrated link from an embedding.
In light of Theorem~\ref{thm:calibrated-separated}, it now suffices to show that for any polyhedral $L$ embedding some $\ell$, there exists a \emph{separated} link $\psi$ with respect to $L$ and $\ell$.
Construction~\ref{const:eps-thick-link}, discussed next, ``thickens'' a given embedding to produce a link.
Theorem \ref{thm:thickened-separated} states that, for a small enough choice of $\epsilon$, that link is separated.
See \S~\ref{app:sep-link-exists} for the proof.
\begin{restatable}{theorem}{thickenedseparated} \label{thm:thickened-separated}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$ embed the discrete loss $\ell:\R\to\reals^\Y_+$.
  Then there exists $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, Construction~\ref{const:eps-thick-link} for $L,\ell,\epsilon,\|\cdot\|$ produces a nonempty set of links, all of which are $\epsilon$-separated with respect to $L$ and $\ell$.
\end{restatable}

\raf{Restate theorem 2 and give formal proof (probably two lines) with no additional filler text}

To set the stage for Construction \ref{const:eps-thick-link}, we sketch the two main steps in proving Theorem \ref{thm:thickened-separated}: (i) showing that one can define a link $\psi$ on all possible optimal points of $L$; (ii) ``thickening'' $\psi$ so that it is separated.

For (i), given the embedding $\varphi: \Sc\to\reals^d$, begin by linking each embedding point back to its original report, so that $\psi(\varphi(r)) = r$.
Now we wish to determine $\psi(u)$ for non-embedding points $u\in\reals^d$ which optimize $L$ for some distribution.
Let $\Gamma = \prop L$ and define $\U = \{\Gamma(p) \mid p\in\simplex\}$ to be the ``range'' of $\Gamma$, i.e., all possible optimal sets.
For each $U\in\U$, we can define $R_U = \{ r\in\Sc \mid \varphi(r) \in U\}$ to be the set of target reports which, by definition of embedding, are $\ell$-optimal when $U$ is $L$-optimal.
We would like to restrict $\psi(U) \subseteq R_U$, so that optimal surrogate reports are mapped to optimal target reports.
The challenge is that we could have a point $u\in U\cap U'$ for two optimal sets $U,U'\in\U$, and \emph{a priori}, it could be that $R_U \cap R_{U'} = \emptyset$.
The first step of the proof is to show that this scenario cannot arise.
\begin{equation}
  \label{eq:intersection-property-embedding}
  \forall \U'\subseteq\U, \; \cap_{U\in\U'} U \neq \emptyset \implies \cap_{U\in\U'} R_U \neq \emptyset~.
\end{equation}
\noindent
Thus, for any $u\in\cup\U$, we have a nonempty set of valid choices for $\psi(u)$.
(Eq.~\eqref{eq:intersection-property-embedding} may appear similar to indirect elicitation; in fact the two conditions are equivalent, as we discuss in \S~\ref{sec:poly-ie-consistency}.)

For (ii), we show that this link can be ``thickened'' by some positive $\epsilon$, as described next.
Let $U\in\U$.
By the above, $\psi$ is already correct on $U$.
Now, we ``thicken'' $U$ to obtain $U_{\epsilon} = \{u : \|u - U\| \leq \epsilon\}$.
Then we require that all points in $U_{\epsilon}$ are linked to some element of $R_U$.
For $\epsilon > 0$, this condition directly implies separation.

It is not clear that this linking is possible, however, because a point $u$ may be in the intersection of several thickened sets $U_{\epsilon}, U'_{\epsilon}$, etc., corresponding to $\Gamma(p), \Gamma(p')$, etc.
Therefore, we need to take each possible collection $U,U'$, etc., show that their intersection (if nonempty) contains a legal choice for the link, and then thicken their intersection in an analogous way.

To do so, given $u \in U_\epsilon \cap U'_\epsilon \cap \dots$, we define a \emph{link envelope} $\Psi(u)$ which encodes the remaining legal choices for $\psi(u)$ after imposing the requirements for each such set $U_\epsilon,U'_\epsilon$, etc.
The key claim is that, for small enough $\epsilon > 0$, $\Psi(u)$ is nonempty: at least one permitted value for $\psi(u)$ remains.
This claim follows from a geometric result (Lemma~\ref{lem:thick-intersect}) that, for all small enough $\epsilon$, a subset of thickenings $U_{\epsilon}$ intersect if and only if the $U$ sets themselves intersect.
When they do intersect, eq.~\eqref{eq:intersection-property-embedding} implies that there exists a permitted choice of link for the intersection of the thickenings.
It is crucial that, by Lemma~\ref{lem:polyhedral-range-gamma}, polyhedral surrogates only have finitely many sets of the form $U = \Gamma(p)$.
Together, these observations yield a single uniform smallest $\epsilon$ such that the key claim is true for all $u \in \reals^d$.
%Given a well-defined thickened link $\psi$, calibration is almost immediate because, given $p$, any report $u$ linking to a suboptimal $r = \psi(u)$ satisfies $\inf_{u^* \in \Gamma(p)} \|u^* - u\| \geq \epsilon$.

Given the above proof sketch, the following construction is relatively straightforward.
We initialize the link using the embedding points and optimal report sets, then adjust $\Psi$ to narrow down to only legal choices; we then pick from $\psi(u)$ from $\Psi(u)$ arbitrarily.
Theorem \ref{thm:thickened-separated} implies that, for all small enough $\epsilon$, the resulting link $\psi$ is well-defined at all points, and $\epsilon$-separated.
\begin{construction}[$\epsilon$-thickened link] \label{const:eps-thick-link}
  Let $L:\reals^d\to\reals^\Y_+$, $\ell:\R\to\reals^\Y_+$, $\epsilon > 0$, and a norm $\|\cdot\|$ be given, such that $L$ is polyhedral and embeds $\ell$ via the embedding $\varphi: \Sc \to \reals^d$ for a representative set $\Sc\subseteq\R$.
  Define $\Gamma = \prop L$ and $\U = \{\Gamma(p) \mid p \in \simplex\}$.
  For all $U \in \U$, define $R_U = \{r \in \Sc \mid \varphi(r) \in U\}$.
  The \emph{$\epsilon$-thickened link} $\psi$ is constructed as follows.
  First, initialize the \emph{link envelope} $\Psi: \reals^d \to 2^{\Sc}$ by setting $\Psi(u) = \Sc$ for all $u$.
  Then for each $U \in \U$, for all points $u$ such that $\inf_{u^* \in U} \|u^*-u\| < \epsilon$, update $\Psi(u) = \Psi(u) \cap R_U$.
  If we have $\Psi(u)\neq\emptyset$ for all $u\in\reals^d$, then the construction \emph{produces a link} $\psi \in \Psi$ pointwise, breaking ties arbitrarily.
\end{construction}
In \S~\ref{sec:poly-ie-consistency} we generalize this construction beyond embeddings, where we only require that $L$ indirectly elicits $\prop\ell$.
There we will see that, perhaps surprisingly, this construction recovers every possible calibrated link.

Applying Construction \ref{const:eps-thick-link} also enables one to verify the consistency of a given proposed link $\psi^*$.
For a given $\epsilon$ and norm $\|\cdot\|$, suppose one follows the routine of Construction~\ref{const:eps-thick-link} until the last step in which values for the link $\psi$ are selected.
Instead, we can simply test whether the proposed link values are contained in the valid choices, i.e., if $\psi^*(u) \in \Psi(u)$ for all $u\in\reals^d$.
If so, then the proposed link $\psi^*$ is calibrated.
See \S~\ref{sec:top-k} for an illustration of this test.
On the other hand, if $\psi^*$ cannot be produced from Construction \ref{const:eps-thick-link}, then it cannot be a calibrated link.
This impossibility can be shown, for example, if there exists a point $u$ where $\Psi(u)$ is empty for all $\epsilon > 0$.

Construction~\ref{const:eps-thick-link} is not necessarily computationally efficient as the number of labels $n$ grows.
In practice this potential inefficiency is not typically a concern, as the family of losses typically has some closed form expression in terms of $n$, and thus the construction can proceed at the symbolic level.
We illustrate this formulaic approach in \S~\ref{sec:abstain}.
%\jessiet{I'm a little confused by (the intention of) this paragraph, but could just be me.}
%\raft{Yeah, we could cut it, but given that we wrote it in response to a reviewer back in the day, I think it's fine to keep too. \#burn}

\subsection{Surrogate regret bounds}\label{subsec:regret-bounds}
Recall that the approach of surrogate risk minimization is to learn a hypothesis $h$ that minimizes expected surrogate loss, then output hypothesis $\psi \circ h$, which hopefully minimizes expected target loss.
We would like surrogates where a bound on surrogate loss of $h$ immediately implies a bound on target loss of $\psi \circ h$.
One can formalize this problem in terms of \emph{regret}, as follows.
Fix a data distribution $\D$.
The \emph{surrogate regret} $R_L$ of $h$ and the \emph{target regret} $R_{\ell}$ of the implied hypothesis $\psi \circ h$, are given by
\begin{align*}
  R_L(h;\D) &= \E_{(X,Y)\sim\D} L(h(X))_Y - \inf_{h':\X\to\reals^d} \E_{(X,Y)\sim\D} L(h'(X))_Y~,
  \\
  R_\ell(\psi\circ h;\D) &= \E_{(X,Y)\sim\D} \ell(\psi(h(X)))_Y - \inf_{g':\X\to\R} \E_{(X,Y)\sim\D} \ell(g'(X))_Y~,
\end{align*}
% $R_L(h;\D) = \E_{(X,Y)\sim\D} L(h(X))_Y - \inf_{h':\X\to\reals^d} \E_{(X,Y)\sim\D} L(h'(X))_Y$,
where the infimum is taken over all measurable functions.
The infimum represents the risk of the Bayes optimal hypothesis, so regret can be viewed as \emph{excess risk} under the assumption that the learner's hypothesis class contains the Bayes optimal.
%(One may equivalently assume the Bayes optimal hypothesis is in the function class.)
% Given $h$, the \emph{target regret} of the implied hypothesis $\psi \circ h$ is $R_\ell(\psi\circ h;\D) = \E_{(X,Y)\sim\D} \ell(\psi(h(X)))_Y - \inf_{h':\X\to\R} \E_{(X,Y)\sim\D} \ell(h'(X))_Y$.

Consistency means that if the surrogate regret of $h$ converges to zero, then the target regret of $\psi \circ h$ does as well; in other words, $R_L(h;\D) \to 0$ implies $R_{\ell}(\psi \circ h;\D) \to 0$.
Consistency is therefore a minimal requirement; in general, we are also interested in the \emph{rate} at which the target regret diminishes, as a function of the number of data points $n$.
A \emph{regret transfer bound}, also called a \emph{surrogate regret bound}, gives a guarantee on the relationship between the rates of convergence of $R_L$ and $R_{\ell}$.
For example, a surrogate with a fast rate of convegence $R_L \to 0$ as $n \to \infty$ is not very useful if we nevertheless have a slow rate $R_{\ell} \to 0$.

We show that, for any polyhedral surrogate, the transfer is linear: if surrogate regret diminishes at a rate of $O(f(n))$, then the target rate is also $O(f(n))$.
In particular, fast convergence in surrogate regret implies fast convergence in target regret.
\begin{theorem}
  \label{thm:linear-regret-bound}
  Let $(L,\psi)$ be consistent for a discrete loss $\ell$, and $L$ polyhedral.
  Then there exists $c > 0$ such that, for all hypotheses $h$ and data distributions $\D$, we have $R_{\ell}(\psi \circ h;\D) \leq c \cdot R_L(h;\D)$.
\end{theorem}
In the proof (\S~\ref{app:regret-bounds}), we further show that the constant $c$ can be decomposed in terms of three constants, which depend on $L$, $\psi$, and $\ell$, respectively.
Specifically, we may write $c = C_\ell H_L / \epsilon_\psi$, where $H_L$ is the Hoffman constant for $L$, $\epsilon_\psi$ the separation of $\psi$, and $C_\ell$ the maximum loss gap of $\ell$.
This expression gives the intuition that larger link separation is generally better for performance.
In some cases, this bound can be tightened, as we discuss in \S~\ref{sec:regret-tighter-bounds}.
See \citet{frongillo2021surrogate} for a quadratic lower bound on the rate transfer for sufficiently non-polyhedral surrogates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to Specific Surrogates}\label{sec:applications}

%% overview of the applications
Our results give a framework to construct consistent polyhedral surrogates and link functions for any discrete target loss, as well as to verify consistency or inconsistency for specific surrogate and link pairs.
Below, we illustrate the power of this framework with specific examples from the literature.
To warm up, we study the abstain surrogate given by~\citet{ramaswamy2018consistent}, which is an embedding, and show how to rederive their link function and surrogate regret bounds (\S~\ref{sec:abstain}). 
We then give three examples of subsequent works that use our framework, in the context of structured binary classification (\S~\ref{sec:lovasz-hinge}), multiclass classification (\S~\ref{sec:winge}), and top-$k$ classification (\S~\ref{sec:top-k}).
In each of these latter three examples, our framework illuminates the behavior of inconsistent surrogates by revealing the discrete losses they embed, i.e., the true targets for which they are consistent.
In structured binary classification and top-$k$ classification, our framework also gives new consistent surrogates and link functions which appear challenging to derive otherwise.

%% how to apply the embedding framework
% When using our framework to study the (in)consistency of an existing surrogate $L:\reals^d \to \reals^\Y_+$, often the first step is determining the loss it embeds $\ell_{\mathrm{embed}}$, in order to compare $\prop{\ell_{\mathrm{embed}}}$ to the property elicited by the proposed target $\prop{\ell}$.
% To this end, we suggest the following general approach.
% First, for each $y\in\Y$, divide $\reals^d$ into a finite number of polyhedral regions on which $L(\cdot)_y$ is an affine function.
% %Often these regions are straightforward to identify based on the form of $L$.\jessie{tbh prev sentence feels like a lie lol}
% Letting $\Sc\subset\reals^d$ be the union of vertices of these polyhedral regions, conclude that $\Sc$ is a finite representative set for $L$~\citep[Statement 3.1.7]{grunbaum2013convex}.
% By Proposition~\ref{prop:representative-embeds-restriction}, $L$ embeds $L|_\Sc$.
% From here one can further remove redundant reports until one arrives at a tight embedding if desired.
% Once the embedded discrete loss is known, the behavior of the surrogate becomes more clear: what problem it is solving, and for which restrictions on label distributions is it calibrated for the original problem (\S~\ref{sec:top-k}, \ref{sec:winge}).
% Moreover, while embedding guarantees the existence of a calibrated link function, Construction~\ref{const:eps-thick-link} provides a framework to both verify proposed link functions and construct new ones.
% A key step in Construction~\ref{const:eps-thick-link} is constructing the set-valued link candidate function $\Psi$, which yields the possible values any $\epsilon$-separated link $\psi$ could map to; this in turn provides a tool to verify the calibration of a proposed link (\S~\ref{sec:abstain}, \ref{sec:top-k}) or construct new calibrated links(\S~\ref{sec:lovasz-hinge}).

\subsection{Applying the embedding framework}
\label{sec:apply-embedd-fram}

When using our framework to study the consistency or inconsistency of an existing surrogate $L:\reals^d \to \reals^\Y_+$, often the first step is determining the loss it embeds.
To do so, we suggest the following general approach.
First, for each $y\in\Y$, divide $\reals^d$ into a finite number of polyhedral regions on which $L(\cdot)_y$ is an affine function.
Second, identify the vertices of these polyhedral regions.%
\footnote{In some cases, these regions do not have vertices, such as the top-$k$ surrogates in \S~\ref{sec:top-k} which are invariant in the all-ones direction; here one can restrict to a subspace, or otherwise select among equivalent reports.}
Third, conclude that the union of these vertices, $\Sc\subset\reals^d$, is a finite representative set for $L$.
% ~\citep[Statement 3.1.7]{grunbaum2013convex}.
Now $L$ embeds $L|_\Sc$ from Proposition~\ref{prop:representative-embeds-restriction}.
From here one can further remove redundant reports until arriving at a tight embedding if desired, or re-label embedded reports to a more intuitive form; call this resulting embedded loss $\hat \ell$.

Once the embedded discrete loss $\hat\ell$ is known, the behavior of the surrogate $L$ becomes more clear.
In particular, we learn what problem $L$ is actually solving, as captured by $\hat\ell$.
If this problem $\hat\ell$ is not the desired target problem $\ell$, we can still derive restrictions on conditional distributions (e.g., $\P \subseteq \simplex$) for which $L$ is $\P$-calibrated for $\ell$.
Any level set of the embedded property $\hat\gamma = \prop{\hat\ell}$ which spans multiple level sets of the target property $\gamma=\prop{\ell}$ will lead to inconsistency for $\ell$ (Figure~\ref{fig:abstain-intuition-inconsistent}).
To obtain consistency with respect to a desired target, therefore, it suffices to restrict to the union of level sets of $\hat\gamma$ which are each fully contained in some level set of $\gamma$.

\begin{figure}
	\begin{minipage}{0.28\linewidth}
    \input{tikz/abstain-intuition}
\end{minipage}
\begin{minipage}{0.72\linewidth}
	\caption{Using an embedding to show inconsistency.
    Let $L$ be a surrogate embedding $\ell$, and let $\ell$ be a desired target; here $L=\BEP$, $\hat\ell = \ellabstain$ (\S~\ref{sec:abstain}) and $\ell$ is 0-1 loss for multiclass classification.
    Let $\Gamma = \prop{L}$, $\hat\gamma = \prop{\hat\ell}$, and $\gamma = \prop{\ell}$ be the properties elicited by these losses; here $\gamma = \mode$, as 0-1 loss elicits the mode.
    The level sets of $\hat\gamma$ are given in solid black lines, and those of $\gamma$ in dashed blue lines.
    To exhibit non-calibration, take distributions $p, p'$ in the relative interior of the blue cell $\hat \gamma_{\hat r}$ (here $\hat r=\bot$ for $\ellabstain$) but in different cells of $\gamma$.
    These distributions will satisfy $\hat\gamma(p) = \hat\gamma(p')$, and thus $\Gamma(p) = \Gamma(p')$ by definition of embedding, but $\gamma(p) \cap \gamma(p') = \emptyset$.
    Taking $u\in\Gamma(p)=\Gamma(p')$, it is impossible to define $\psi(u)$ to satisfy calibration, as $\psi(u)$ cannot be in $\gamma(p)$ and $\gamma(p')$ simultaneously.
    In particular, even though $u$ is $L$-optimal for both $p$ and $p'$, $\psi(u)$ will be $\ell$-suboptimal for at least one, violating calibration.
    We may impose restrictions on the conditional distributions to remove the blue cell, however, and thus satisfy calibration.
    For this example, the $\BEP$ is classification-calibrated if one restricts to the set of distributions where at least one label $y\in\Y$ has probability $p_y \geq \tfrac 1 2$.
	}
	\label{fig:abstain-intuition-inconsistent}
	\end{minipage}
\end{figure}

With an embedding in hand, Construction~\ref{const:eps-thick-link} provides a calibrated link function from $L$ to $\hat\ell$.
This construction is especially beneficial in cases where the most intuitive link functions are not calibrated, and no known calibrated link is known; see \S~\ref{sec:lovasz-hinge} for a somewhat intricate example.
Surrogate regret bounds then follow from Theorem~\ref{thm:linear-regret-bound}, as we illustrate in \S~\ref{sec:abstain}.
In particular, our results imply the existence of linear regret transfer bounds bounds for several applications where no such bounds were known (\S~\ref{sec:lovasz-hinge},~\ref{sec:winge},~\ref{sec:top-k}).

Finally, our link construction can even be useful in cases where the search for consistent surrogates has been restricted to those accommodating a particular canonical link function $\psi$.
For example, one typically uses the sign link for binary classification, and the argmax link (the $k$ largest coordinates) for top-$k$ classification (\S~\ref{sec:top-k}).
As we show in Proposition~\ref{prop:link-converse}, Construction~\ref{const:eps-thick-link} fully characterizes the set of possible calibrated link functions for a polyhedral embedding via the link envelope $\Psi$, so $\psi$ is calibrated if and only if it is contained in $\Psi$ for some $\epsilon>0$.
% From Theorem~\ref{thm:calibrated-separated}, if $\psi$ is consistent, then it is $\epsilon$-separated for some $\epsilon>0$.
% From Theorem\raf{fill in converse theorem}, we can therefore test whether $\psi$ is contained in the link envelope $\Psi$ from Construction~\ref{const:eps-thick-link} for sufficiently small $\epsilon>0$.
We demonstrate this approach for top-$k$ classification in \S~\ref{sec:top-k}.
More generally, however, while such canonical link functions may be intuitive for a given problem, our results suggest that researchers should consider setting them aside and instead let Construction~\ref{const:eps-thick-link} determine the link.


\subsection{Consistency of abstain surrogate and link construction}
\label{sec:abstain}

Several authors consider a variant of binary and multiclass classification, with the addition of an \emph{abstain} option~\citep{bartlett2008classification,ramaswamy2018consistent,madras2018predict,elyaniv2010foundations,cortes2016learning}.
\citet{ramaswamy2018consistent} study the loss $\ellabstain: \Y \cup \{\bot\} \to \reals^\Y_+$ defined by $\ellabstain(r)_y = 0$ if $r=y$, $1/2$ if $r = \bot$, and 1 otherwise.
The report $\bot$ corresponds to ``abstaining'' to predict, in exchange for a constant loss regardless of outcome $y$. 
\citeauthor{ramaswamy2018consistent} give the polyhedral \emph{binary encoded predictions (BEP)} surrogate $\BEP$, and the link $\psi^\infty$ which they show is calibrated for $\ellabstain$.
Letting $d = \ceil{\log_2 |\Y|}$, their surrogate $\BEP : \reals^d \to \reals^\Y_+$ is given by
\begin{equation}\label{eq:abstain-surrogate}
\BEP(u)_y = \max_{j \in [d]} \left(1 - \varphi(y)_j u_j\right)_+~,
\end{equation}
where $\varphi:\Y\to\{-1,1\}^d$ is an injection.%
\footnote{To translate our notation to that of \citet{ramaswamy2018consistent}, take $B = -\varphi$.}
Observe that $\BEP$ is exactly hinge loss when $|\Y|=2$ and thus $d=1$. 
The authors show that the link $\psi^{\infty}$ is calibrated, where
\begin{equation}\label{eq:abstain-link}
  \psi^{\infty}(u) = \begin{cases}
	\bot & \min_{i \in [d]} |u_i| \leq 1/2\\ %prev: \epsilon\\
	\varphi^{-1}(\sgn(u)) &\text{otherwise}
  \end{cases}~,
\end{equation}
and they go on to establish linear surrogate regret bounds for $(\BEP,\psi^{\infty})$.

Using our framework, one can show that $\BEP$ embeds (2 times) $\ellabstain$, with the embedding given by $\varphi$ above where we define $\varphi(\bot) = 0 \in \reals^d$.
(Following the general procedure outlined above, the regions where $\BEP$ is affine all have vertices in the set $\{-1,1\}^d \cup \{0\}$, meaning it is representative, and $\BEP$ restricted to that set is precisely $2\ellabstain \circ \varphi^{-1}$.)

%% giving intuition for abstain figure: how to use fig to find consistency restrictions
As an illustration, one can use the fact that $\BEP$ embeds $\ellabstain$ to verify that $\BEP$ is inconsistent for multiclass classification, i.e., with respect to 0-1 loss.
In particular, since the abstain report $\bot$ is $\ellabstain$-optimal whenever $\max_{y\in\Y} p_y \leq 1/2$, by the definition of embedding, the origin $0\in\reals^d$ is $\BEP$-optimal for the same distributions.
Recalling that 0-1 loss elicits the mode, one can now find two distributions with different modes but for which $0$ is $\BEP$-optimal, violating calibration (Figure~\ref{fig:abstain-intuition-inconsistent}).
% , one can observe that the BEP surrogate $\BEP$ does not indirectly elicit the mode, as the level sets $\Gamma_{\vec 0} = \hat \gamma_\bot$ are equal by the definition of embedding, yet one can observe $\prop{\ellabstain}_\bot \not \subseteq \mode_r$ for any $r$. 
% As embedding is a special form of indirect elicitation, we additionally know the BEP surrogate does not embed the mode.
% The intuition described by Figure~\ref{fig:abstain-intuition-inconsistent} additionally gives a procedure to validate (in)consistency with respect to a target task.

Moreover, as we illustrate in Figure~\ref{fig:abstain-links}(L), the link $\psi^{\infty}$ proposed by \citeauthor{ramaswamy2018consistent} can be recovered from Construction~\ref{const:eps-thick-link} by choosing the norm $\|\cdot\|_\infty$ and $\epsilon=1/2$ (or smaller).
Hence, our framework could have simplified the process of finding $\psi^\infty$, and the corresponding proof of consistency.
It also could have simplified the derivation of surrogate regret bounds (\S~\ref{subsec:regret-bounds}); we show how to recover the tight bound of \citeauthor{ramaswamy2018consistent} for the BEP surrogate in \S~\ref{sec:regret-tighter-bounds}.

To illustrate these points further, consider the alternate link $\psi^1$
in Figure~\ref{fig:abstain-links}(R),
given by
\begin{equation}\label{eq:abstain-link-1}
  \psi^1(u) = \begin{cases}
	\bot & \|u\|_1 \leq 1\\
	\varphi^{-1}(\sgn(u)) &\text{otherwise}
  \end{cases}~.
\end{equation}
This link is the result of Construction~\ref{const:eps-thick-link} for norm $\|\cdot\|_1$ and the choice $\epsilon=1$, which proves calibration of $(\BEP,\psi^1)$ with respect to $\ellabstain$.
Aside from its simplicity, one possible advantage of $\psi^1$ is that it assigns $\bot$ to much less of the surrogate space $\reals^d$.



\begin{figure}
\begin{center}
\begin{minipage}{0.32\linewidth}
\includegraphics[width=\linewidth]{tikz/abstain-link-linf.pdf}
\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
\includegraphics[width=\linewidth]{tikz/abstain-link-U-regions-l1.pdf}
\end{minipage}\hfill
\begin{minipage}{0.32\linewidth}
\includegraphics[width=\linewidth]{tikz/abstain-link-l1.pdf}
\end{minipage}\hfill
\caption{Designing links for $\BEP$ with $d=2$ using Construction~\ref{const:eps-thick-link}. The embedding is shown in bold labeled by the corresponding reports. (L) The link envelope $\Psi$ resulting from Construction~\ref{const:eps-thick-link} using $\|\cdot\|_\infty$ and $\epsilon = 1/2$, and a possible link $\psi$ which matches eq.~\eqref{eq:abstain-link} from~\cite{ramaswamy2018consistent}.  (M) An illustration of the thickened sets for two sets $U, U' \in \U$, using $\|\cdot\|_1$ and $\epsilon = 1$. (R) The envelope $\Psi$ and link $\psi$ %from Construction~\ref{const:eps-thick-link}
  using $\|\cdot\|_1$ and $\epsilon = 1$.}
\label{fig:abstain-links}
\end{center}
\end{figure}


\subsection{Lov\'asz hinge and the structured abstain problem}
\label{sec:lovasz-hinge}

\newcommand{\dis}{\mathrm{dis}}
\newcommand{\abs}{\mathrm{abs}}

%% what is structured prediction + some gap
Many structured prediction settings can be thought of as making multiple predictions at once, with a loss function that jointly measures error based on the relationship between these predictions~\cite{hazan2010direct, gao2011consistency, osokin2017structured}.
In the case of $k$ binary predictions, these settings are typically formalized by taking the predictions and outcomes to be $\R=\Y=\{-1,1\}^k$, with the $i$th coordinate giving the result for the $i$th binary prediction.
A natural family of losses are those which are functions of the misprediction or disagreement set $\dis(r,y) = \{i \in [k] \mid r_i \neq y_i\}$, meaning we may write $\ell^f(r)_y = f(\dis(r,y))$ for some set function $f:2^{[k]}\to\reals$.
For example, Hamming loss is given by $f(S) = |S|$.
In an effort to provide a general convex surrogate for these settings when $f$ is a submodular function,~\citet{yu2018lovasz} introduce the \emph{Lov\'asz hinge} surrogate $L^f:\reals^k\to\reals^\Y_+$ which leverages the well-known convex Lov\'asz extension of submodular functions.
While the authors provide theoretical justification and experiments, they leave open whether the Lov\'asz hinge is actually consistent for $\ell^f$.

%% lovasz paper closes the gap, answering inconsistency wrt \ell^f by showing that L^f embeds structured abstain
\citet{ourlovaszpaper} use our embedding framework to resolve the consistency of $L^f$, showing that it is inconsistent with respect to $\ell^f$ outside of the trivial case where $f$ is modular (in which case $\ell^f$ is a weighted Hamming loss).
Moreover, they show that $L^f$ embeds a variant $\ellabs$ of $\ell^f$ where one is allowed to abstain on a set of indices $A \subseteq [k]$, which they call the \emph{structured abstain problem}.
The inclusion of abstain options is natural when observing that the BEP surrogate $\BEP$, for multiclass classification with an abtain option (\S~\ref{sec:abstain}), is the special case of $L^f$ where $f(S) = \ones\{S \neq \emptyset\}$.

To derive the discrete loss $\ellabs$ that $L^f$ embeds, the authors follow an approach similar to \S~\ref{sec:apply-embedd-fram} to show that the set $\V = \{-1,0,1\}^k$ is representative for $L^f$, for any choice of $f$.
From Proposition~\ref{prop:representative-embeds-restriction}, they conclude that $L^f$ embeds $\ellabs := L^f|_{\V}$.
Letting $\abs(v) = \{i\in[k] \mid v_i = 0\}$ denote the ``abstain'' set, we may write $\ellabs : \V \to \reals^\Y_+$ as
\begin{equation}
	\ellabs(v)_y = f(\dis(v,y) \setminus \abs(v)) + f(\dis(v,y))~.
\end{equation}
(Observe that $\abs(v,y) \subseteq \dis(v,y)$, since $y\in\{-1,1\}^k$.)
%They go on to show that the embedding is tight, for $f$ strictly submodular and strictly monotone, if one first disallows $v$ with $|\abs(v)| = 1$.
By Theorem~\ref{thm:link-main}, then, there is a link function such that the Lov\'asz hinge is consistent with respect to the structured abstain loss $\ellabs$.

As \citeauthor{ourlovaszpaper} observe, actually determining a calibrated link function in this case is nontrivial.
Simple threshold links like for the BEP surrogate in \S~\ref{sec:abstain} are not always calibrated, thus casting doubt that a trial-and-error approach for finding the link would be successful.
Instead, the authors leverage our thickened link construction (Construction~\ref{const:eps-thick-link}) to derive two links $\psi^*$ and $\psi^\diamond$, which have somewhat intricate geometric structure (Figure~\ref{fig:lovasz-links}).
Perhaps surprisingly, by deriving the link envelope $\Psi$ which is contained in the envelopes for $L^f$ for all submodular and increasing $f$, they prove that $\psi^*(u) \subseteq \Psi(u)$ and $\psi^\diamond(u) \subseteq \Psi(u)$ for all $u \in \reals^d$.
Thus, both $(L^f, \psi^*)$ and $(L^f, \psi^\diamond)$ are simultaneously calibrated with respect to $\ellabs$ for all such $f$.

\begin{figure}
	\begin{center}
		\begin{minipage}{0.32\linewidth}
			\includegraphics[width=\linewidth]{tikz/lovasz-link-psi-star.pdf}
		\end{minipage}\hfill
		\begin{minipage}{0.32\linewidth}
			\includegraphics[width=\linewidth]{tikz/lovasz-link-psi-diamond.pdf}
		\end{minipage}\hfill
		\begin{minipage}{0.32\linewidth}
		\includegraphics[width=\linewidth]{tikz/lovasz-link-envelope.pdf}
		\end{minipage}\hfill
		\caption{Links $\psi^*$ and $\psi^\diamond$ such that $(L^f, \psi^*)$ and $\psi^\diamond$ are calibrated with respect to $\ellabs$ for all suitable $f$. Points in each region link to the embedding point contained in the region. Both are constructed via the link envelope $\Psi$ from Construction~\ref{const:eps-thick-link}, which yields possible choices for calibrated links.
			}
			%Points in the dark green regions link to $\vec 0$; points in light green link to a permutation of $(0,\pm 1)$, and points in yellow regions link to $(\pm 1, \pm 1)$.\jessie{Not sure how to phrase this?  Would we just be better off explicitly labeling things?}\raf{Agreed that it's a little unclear, but not sure labeling would be better... let's discuss next call}}
		\label{fig:lovasz-links}
	\end{center}
\end{figure}

\subsection{Embedding ordered partitions via Weston-Watkins hinge}
\label{sec:winge}
As the hinge loss is one of the most common surrogates for binary support vector machines (SVMs), original extensions to the multiclass setting included a one-vs-all reduction to the binary problem via hinge loss, generating ${n \choose 2}$ hyperplanes for $n$ labels.
Proposing a more efficient solution, \citet{weston1999support} give an alternate surrogate for multiclass SVM prediction, defined as follows for predictions $u \in \reals^n$,
\begin{equation}\label{eq:ww-hinge}
\LWW(u)_y = \sum_{i \in \Y : i \neq y} (1 - (u_y - u_i))_+~.
\end{equation}
This surrogate $\LWW$ was later shown to be inconsistent with respect to 0-1 loss~\citep{tewari2007consistency,liu2007fisher}.

\citet{wang2020weston} use our embedding framework to show that the Weston--Watkins hinge embeds the \emph{ordered partition} loss $\ellOP$, as defined below.
In turn, they recover the result of inconsistency with respect to 0-1 loss.
%An ordered partition on $[n] := \{1, \ldots, n\}$ can be defined by a nested sequence of tuples
%Let $T = \{T_1, \ldots, T_k\}$
The report space for $\ellOP$ can be defined in terms of nested subsets of $[n] := \{1, \ldots, n\}$, as follows.%
\footnote{To recover the partition of~\citet{wang2020weston}, one can define $S_i = T_i \setminus T_{i-1}$.}
\begin{align*}
\T = \{ (T_0,\ldots,T_s) \mid s \geq 1, \emptyset = T_0 \subsetneq T_1 \subsetneq \ldots \subsetneq T_s = [n]\}~.
\end{align*}
\noindent
The ordered partition target loss $\ellOP : \T \to \reals^\Y_+$ is then defined
\begin{align*}
\ellOP(T)_y &= \sum_{i=1}^{s} \left(|T_{i}| \cdot \Ind{y \not \in T_{i-1}} \right) -1~.~
\end{align*}
\noindent
The loss $\ellOP$ can be interpreted as a variation of 0-1 loss incorporating confidence: reports are a nested sequence of sets, and the penalty upon seeing label $y$ is the cardinality of the first set containing $y$, plus the cardinality of all earlier sets.
%\citet{wang2020weston} show that $\LWW$ embeds $\ellOP$, with the embedding $\varphi : \T \to \reals^n$ given by $\varphi(T)_j = -(i-1)$ for all $j \in T_i \setminus T_{i-1}$.
%%\citeauthor{wang2020weston} then apply Theorem~\ref{thm:link-main} to conclude that there exists a link function $\psi$ such that $(\LWW,\psi)$ is calibrated with respect to $\ellOP$; however, they do not construct the link. 


%% one might make distributional assumptions to regain consistency with geometric drawing
Upon showing that $\LWW$ embeds $\ellOP$, \citeauthor{wang2020weston} then characterize $\prop{\ellOP}$.
In the same manner as Figure~\ref{fig:abstain-intuition-inconsistent}, knowledge of the level sets of $\prop{\ellOP}$ clarifies which conditional distributions are the source of inconsistency for classification.
Removing these distributions gives a set $\P \subseteq \simplex$
such that $\LWW$ and the canonical link $\psi(u) : u \mapsto r \in \argmax_y \inprod{e_y}{u}$ are calibrated with respect to 0-1 loss on $\P \subseteq \simplex$ (i.e., such that eq.~\eqref{eq:calibrated} holds for all $p \in \P$).
%Sufficient constraints to recover consistency are characterized by comparing $\prop{\ellOP}$ and $\mode$: the property for which Weston--Watkins was originally proposed.
See Figure~\ref{fig:ordered-partition} for an illustration.
% gives the level sets $\gamma_T =\{p \in \simplex \mid T \in \prop{\ellOP}(p)\}$ for each $T \in \T$\jessie{notation} juxtaposed with the level sets of the mode.
%$\mode_r = \{p \in \simplex \mid r \in \mode(p)\}$ for each $r \in \Y$.
%Since each distribution in a cell of $\prop{\ellOP}$ corresponds to the same optimal report $T$, the choice of where to link that report must be constant.
%Thus, if a cell of $\prop{\ellOP}$ is fully contained in a cell of $\mode$, then the corresponding $\prop{\ellOP}$ value can be mapped to the corresponding $\mode$ value.
%Conversely, if the relative interior of a cell of $\prop{\ellOP}$ corresponding to $T$ spans multiple mode cells, it becomes unclear to which report the set $T$ should be linked.
% To recover consistency on $\P \subseteq \simplex$, it suffices that $\P$ excludes the cells colored in blue in Figure~\ref{fig:ordered-partition}.


\begin{figure}[t]
	\begin{minipage}{0.47\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{tikz/ordered-partition}
	\end{minipage}
	\hfill
	\begin{minipage}{0.5\linewidth}
		\caption{Level sets of $\prop{\ellOP}$ (solid lines)
			%the property elicited by the ordered partition loss and embedded by $\LWW$, 
			juxtaposed against the level sets of $\mode$ (dashed lines).
%		The level sets of the mode (for which $\LWW$ is proposed as a surrogate) are given by the cells formed by the dashed blue lines.
      For the same reason as in Figure~\ref{fig:abstain-intuition-inconsistent}, the level sets of $\prop{\ellOP}$ whose relative interiors span multiple cells of the mode, colored blue, cannot be properly linked to the mode.
      These offending cells correspond to reports whose highest partition has more than one element, where in the white cells, the ``highest'' element of the partition is well-defined.}
		\label{fig:ordered-partition}
	\end{minipage}
\end{figure}






\subsection{Surrogates for top-$k$ classification}
\label{sec:top-k}
% In settings like object recognition and information retrieval, the top-$k$ classification problem arises in which one predicts a set $S$ of size $k$, and upon realizing outcome $y$, receives loss $\elltopk(S)_y = \ones\{y\notin S\}$~\citep{lapin2015top, lapin2016loss, lapin2018analysis,yang2018consistency,berrada2018smooth,rastegari2011scalable,reddi2019stochastic}.
% Many of the existing surrogates for top-$k$ classification consider the following three desiderata: convexity, calibration, and piecewise linear (``hinge-like'') structure.
% Satisfying all three desiderata simultaneously had not been observed until the work of~\citet{finocchiaro2022consistenttopk}.\raft{Not sure you can use ``observed'' like that (came up a few times)}
% The surrogates that are convex and calibrated but not polyhedral often elicit the entire distribution $p \in \simplex$, and take the link to be the top $k$ elements of $p$.
% \citet{lapin2016loss} observes that this approach learns more information than desired and often requires more samples
% % for surrogate regret to approach target regret
% ~\citep{frongillo2021surrogate}.\jessiet{right?}
% \citet{yang2018consistency} propose a piecewise linear surrogate that is calibrated for top-$k$ classification, but is not convex, generally making optimization more difficult.
% Now consider the class of polyhedral surrogates whose calibration for top-$k$ prediction is unknown~\citep{yang2018consistency,lapin2016loss,lapin2018analysis}: Theorem~\ref{thm:poly-embeds-discrete} says that every polyhedral surrogate embeds some discrete loss, and Theorem~\ref{thm:embed-poly-main} states that embedding implies calibration by some link for this discrete loss; thus, we can use embedding to analyze the (non)calibration of existing polyhedral surrogates with respect to top-$k$ by comparing the embedded losses to top-$k$.
In settings like object recognition and information retrieval, it is natural to predict a set $S$ of labels.
In top-$k$ classification, one requires $|S|=k$, and given the true label $y$, the target loss is $\elltopk(S)_y = \ones\{y\notin S\}$~\citep{lapin2015top, lapin2016loss, lapin2018analysis,yang2018consistency,berrada2018smooth,rastegari2011scalable,reddi2019stochastic}.
In the literature on surrogates for top-$k$ classification, one goal has been to find a surrogate satisfying the following three desiderata: convexity, consistency, and piecewise linear (``hinge-like'') structure.
\citet{yang2018consistency} show that a number of previously proposed polyhedral losses, i.e., those which are convex and hinge-like, are inconsistent.
They further suggest that perhaps no surrogate could satisfy all three properties.
%\raft{Also, I cut the smooth vs hinge-like discussion since we don't really need the full motivation -- saying ``these weirdos wanted to do XYZ; who knows why'' is good enough, since our point is that our framework is useful for whatever people are trying to do \jessie{makes sense! \#burn}}

%\jessiet{Commenting out the paragraph below.  It's pretty redundant and I don't think it has a super direct point.}
%Now consider the class of polyhedral surrogates whose calibration for top-$k$ prediction is unknown~\citep{yang2018consistency,lapin2016loss,lapin2018analysis}: Theorem~\ref{thm:poly-embeds-discrete} says that every polyhedral surrogate embeds some discrete loss, and Theorem~\ref{thm:embed-poly-main} states that embedding implies calibration by some link for this discrete loss; thus, we can use embedding to analyze the (non)calibration of existing polyhedral surrogates with respect to top-$k$ by comparing the embedded losses to top-$k$.

\citet{finocchiaro2022consistenttopk} apply the general approach outlined above to each of the polyhedral surrogates shown to be inconsistent by \citeauthor{yang2018consistency}, and determine the target problems they do solve, i.e., the discrete losses they embed.
%\raft{Couldn't follow this sentence (commented out)}
% None of the studied surrogates embed top-$k$ but use the properties elicited by the embedded losses to characterize (tighter\jessiet{not sure how to say this.  better bounds than YK gave, which was just the abstain cell for all the surrogates}) sufficient assumptions on the data distribution in order be calibrated with respect to top-$k$, fixing the canonical argmax link $\psi^k$.
%\jessiet{do we want to drop the table here or nah?}\raft{Yeah I'd say we should try it; it's at least helpful when discussing the constraints on the conditional label distributions, and besides, it's nice to have a figure for each section.  Sentence commented out below could be useful in the caption.}
% \citeauthor{finocchiaro2022consistenttopk} consider the polyhedral surrogates proposed by \citet[Table 1, cf. $\psi_2, \psi_3, \psi_4$]{yang2018consistency} and evaluate \emph{which discrete losses these surrogates embed}, if not top-$k$.
Each of the examined surrogates embeds a discrete loss which can be viewed as a variant of the top-$k$ problem, allowing the algorithm to express varying levels of ``confidence'' on the top $k$ labels or report fewer than $k$ labels.
The conditional distributions for which these optimal reports differ from the optimal top-$k$ reports are shown in Table~\ref{tab:loss-slices} with $n=4$ and $k \in \{2,3\}$.
(Recall $n=|\Y|$, the number of labels.)

\begin{table*}
	\centering
	\begin{tabular}{ccccc}
		& $\prop{\Li{2}}$ & $\prop{\Li{3}}$ & $\prop{\Li{4}}$ & $\prop{L^k}$\\
		\hline \hline
		\rotatebox[origin=c]{90}{$k = 2$\hspace*{-2.5cm}} & \input{tikz/n4-k2-psi2-slice} & \input{tikz/n4-k2-psi3-slice} & \input{tikz/n4-k2-psi4-slice} & \input{tikz/n4-k2-psi6-slice}\\ 
		\rotatebox[origin=c]{90}{$k = 3$\hspace*{-2.5cm}} & \input{tikz/n4-k3-psi2-slice} & \input{tikz/n4-k3-psi3-slice} & \input{tikz/n4-k3-psi4-slice} & \input{tikz/n4-k3-psi6-slice}\\ 
	\end{tabular}
	\caption{
		Visualizations of the properties elicited by the losses (embedded by) $\Li{2}$, $\Li{3}$, $\Li{4}$ studied by \citet{yang2018consistency} and \citet{finocchiaro2022consistenttopk}, and $L^k$ in eq.~\eqref{eq:topk-embedding}.
    We take $n=4$ and $k \in \{2,3\}$, and visualize in 2 dimensions by fixing $p_4 = 1/4$. 
		%The dashed blue lines form cells whose elements are distributions $p$ corresponding to the same $u \in \prop{\elli k}(p)$ labeling the cell. 
		%As the link $\psi$ must be deterministic, in order for the $(\Li{k}, \psi)$ to be consistent with respect to $\elli k$, each cell outlined in black is fully contained in exactly one cell from the dashed blue lines. 
		The blue-filled regions are cells of the surrogate property which cross the dashed blue lines of the target property, exhibiting inconsistency (see Figure~\ref{fig:abstain-intuition-inconsistent}).
    Intuitively, the inconsistency arises from ambiguity in the top-$k$ elements of the optimal surrogate report.  
		% See the caption for Figure~\ref{fig:ordered-partition} on how to interpret what the colored regions say about inconsistency with respect to top-$k$ classification.
	}
	\label{tab:loss-slices}
\end{table*}

For example, one of the surrogates is $\Li {4}(u)_y = \left(1 - u_y + \frac 1 k \sum_{i=1}^k (u_{\setminus y})_{[i]}\right)_+$, where $u_{[i]}$ denotes the $i$th largest element of $u \in \reals^n$.
The authors show that $\Li{4}$ embeds $\ell^{(4)}(T)_y = \tfrac {k+1} {k+1-|T|} \ones\{y\notin T\}$, where $T$ is a set of at most $k$ labels.
These embedded losses may therefore be useful in top-$k$ settings where choosing smaller sets may have some benefit, such as a search engine that can use unused space for advertisements.
Using the losses each proposed surrogate embeds, using the same technique from Figure~\ref{fig:abstain-intuition-inconsistent}, the authors go on to derive constraints on the conditional distributions under which the proposed surrogates are actually consistent for top-$k$ classification; these constraints are tighter than previous constraints~\citep{yang2018consistency}.

Beyond analyzing the previously proposed surrogates, \citeauthor{finocchiaro2022consistenttopk} also use our framework to derive the first consistent polyhedral surrogate for $\elltopk$,
\begin{align}\label{eq:topk-embedding}
L^k(u)_y &= \max \left(u_{[1]}, \max_{m \in \{k+1, \ldots, n\}} \left[ 1 - \frac k m + \frac 1 m \sum_{i=1}^m u_{[i]}\right] \right)- u_y~.
\end{align}
That is, they show that a hinge-like surrogate does exist which is both convex and consistent.
In light of our framework, this fact is unsurprising: Theorems~\ref{thm:embed-poly-main} and~\ref{thm:link-main} imply that \emph{every} discrete loss has a consistent polyhedral surrogate.
This new surrogate $L^k$ is given directly by the construction from the proof of Theorem~\ref{thm:discrete-loss-poly-embeddable} and applying Theorem~\ref{thm:link-main} to obtain consistency.
While Theorem~\ref{thm:link-main} guarantees the existence of some consistent link function, the authors further ask whether the canonical argmax link function $\psi^k$, which returns the $k$ largest elements of $u$, is calibrated.
They indeed confirm its consistency using our framework, showing that $\psi^k$ is $\epsilon$-separated for $L^k$ and $\elltopk$, for any $\epsilon \leq \frac 1 {2n}$ \citep[Theorem 4.4]{finocchiaro2022consistenttopk}.
% confirms that indeed $(L^k,\psi^k)$ is calibrated with respect to $\elltopk$ for this link $\psi^k$, by showing that $\psi^k$ is an $\epsilon$-separated link for $\elltopk$ with any $\epsilon \leq \frac 1 {2n}$ via the construction of the link envelope $\Psi$ in Construction~\ref{const:eps-thick-link}.


% See Figure~\ref{fig:top-k} for the properties elicited by the embedded losses for previous surrogates~\citep{lapin2016loss,yang2018consistency}, and how they compare to $\prop{\elltopk}$ with $n = 3$ and $k = 2$.
%%In order to verify calibration of the canonical link they verify, for all $u \in \reals^n$, the proposed $\psi(u) \in \Psi(u)$, and thus yields an $\epsilon$-separated link.

%\begin{figure}
%	\begin{center}
%		\begin{minipage}{0.32\linewidth}
%			\includegraphics[width=\linewidth]{tikz/top-k-L2.pdf}
%		\end{minipage}\hfill
%		\begin{minipage}{0.32\linewidth}
%			\includegraphics[width=\linewidth]{tikz/top-k-L2.pdf}
%		\end{minipage}\hfill
%		\begin{minipage}{0.32\linewidth}
%			\includegraphics[width=\linewidth]{tikz/top-k-L4.pdf}
%		\end{minipage}\hfill
%		\caption{The properties elicited by previous polyhedral surrogates ($\psi_2, \psi_3, \psi_4$ in \citep[Table 1]{yang2018consistency}, respecitvely) with $n = 3, k = 2$. 
%		The dashed lines in each simplex denote $\prop{\elltopk}$, and blue regions represent cells where the top-$k$ elements of $\prop{L}$ are ambiguous, and therefore calibration cannot be guaranteed by the argmax link $\psi^k$.
%		In this setting, the third surrogate is calibrated via $\psi^k$ with respect to $\elltopk$, though this does not hold for general $k,n$.
%		\jessie{revisit}}
%		\label{fig:top-k}
%	\end{center}
%\end{figure}



\raf{TODO: add paragraph on multiclass case, where $k=1$!}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Structure of Embeddings}
\label{sec:min-rep-sets}

We have shown in \S~\ref{sec:poly-loss-embed} a close connection between embeddings and polyhedral losses.
Here we go beyond polyhedral losses, showing a more general necessary condition for an embedding: a surrogate embeds a discrete loss if and only if it has a polyhedral Bayes risk, or equivalently, a finite representative set (Lemma~\ref{lem:X}).
This result implies that the embedding condition simplifies to matching Bayes risks (Proposition~\ref{prop:embed-bayes-risks}).
We also use this result to understand deeper structure of embeddings, and the geometry of the underlying properties. %, and the equivalence of various notions of non-redundant predictions.
In particular, we study a natural notion of a ``trimmed'' loss function (Definition~\ref{def:trim-loss}), and connect this notion to tight embeddings, and to non-redundancy from property elicitation (Proposition~\ref{prop:embed-iff-trims-equal}).
 

%\raf{Add somewhere: initially explored by~\citet{wang2020weston} under the name \emph{embedding cardinality} }
%\jessie{mentioned it right under the definition of representative set way earlier}

\subsection{Structure of polyhedral Bayes risks}

%% generalizing from structure of polyhedral surrogates to surrogates with polyhedral risk (and why not necessarily the same)
While we have focused on polyhedral losses thus far, many of our results extend to losses with polyhedral Bayes risks, a strictly weaker condition.
(We say a concave function is polyhedral if its negation is a polyhedral convex function.)
To see that every polyhedral loss has a polyhedral Bayes risk, recall that Theorem~\ref{thm:poly-embeds-discrete} constructs a finite representative set $\Sc$ for any polyhedral loss $L$, and thus $\risk{L} = \risk{L|_\Sc}$ by Lemma~\ref{lem:loss-restrict}, which is polyhedral.
Conversely, however, a Bayes risk may be polyhedral even if the loss itself is not.
For example, a modified hinge loss $L(r)_y = \max(r^2-1,1-ry)$
as shown in Figure~\ref{fig:modified-hinge}, which matches hinge loss on the interval $[-1,1]$ but is strictly convex outside the interval $[-2,2]$, still embeds twice 0-1 loss.

%% intro to Lemma X
Much of our embedding framework relies on the existence of finite representative sets.
Our main structural result is that a minimizable loss has a finite representative sets if and only if its Bayes risk is polyhedral.
The proof looks at the facets (full-dimensional faces) of the Bayes risk, and argues that each facet is generated by the loss at a particular report, and the (finite) set of these reports is representative.
Along the way, we identify several other useful facts deriving from this same geometry; for example, a discrete loss tightly embedded by a loss are unique up to relabeling, any set-wise minimal representative set must be minimum in cardinality, and the level sets of the corresponding property are unique and full-dimensional.
Together, these facts form Lemma~\ref{lem:X}, which we use throughout this section.
See \S~\ref{app:polyhedra} for ommitted proofs.

\begin{figure}[t]
	\begin{minipage}{0.47\linewidth}
		\centering
		\includegraphics[width=0.95\linewidth]{figs/modhinge.pdf}
		%		\caption{Bayes risk of the 0-1 loss.}
		%		\label{fig:0-1-br}
	\end{minipage}
	\hfill
	\begin{minipage}{0.47\linewidth}
		\centering		\includegraphics[width=0.95\linewidth]{figs/modhinge-br.pdf}
		%		\caption{Bayes risk of hinge loss.}
		%		\label{fig:hinge-br}
	\end{minipage}
	\caption{(L) Expected modified hinge loss for fixed distribution; (R) Bayes risk of modified hinge still matches the Bayes risk of hinge.}
	\label{fig:modified-hinge}
\end{figure}

\begin{restatable}{lemma}{lemmaX}\label{lem:X}
  Let $L: \R \to \reals^\Y_+$ be a minimizable loss with a polyhedral Bayes risk $\risk L$.
  Then $L$ has a finite representative set.
  Furthermore, letting $\Gamma = \prop{L}$, there exist finite sets
  $\V \subseteq \reals^\Y_+$ and
  $\Theta = \{\theta_v \subseteq \simplex \mid v\in\V\}$,
  both uniquely determined by $\risk{L}$ alone,
  such that
  \begin{enumerate}
  % \item There exists $\V = \V(\risk{L}) \subseteq \reals^\Y_+$, uniquely determined by $\risk{L}$ alone, such that $\V = L(\R^*)$.\label{item:X-V}
  \item A set $\R'\subseteq\R$ is representative if and only if $\V \subseteq L(\R')$.\label{item:X-rep-V}
  \item A set $\R'\subseteq\R$ is minimum representative if and only if $L(\R') = \V$.\label{item:X-min-V}
  \item A set $\R'\subseteq\R$ is representative if and only if $\Theta \subseteq \{\Gamma_r \mid r \in \R'\}$.\label{item:X-rep-Theta}
  \item A set $\R'\subseteq\R$ is minimum representative if and only if $\{\Gamma_r \mid r \in \R'\} = \Theta$.\label{item:X-min-Theta}
  \item Every representative set for $L$ contains a minimum representative set for $L$.\label{item:X-rep-contain-min}
  \item The set of full-dimensional level sets of $\Gamma$ is exactly $\Theta$.\label{item:X-full-dim}
  \item For any $r \in \R$, there exists $\theta \in \Theta$ such that $\Gamma_r \subseteq \theta$.\label{item:X-redundant}
  \item $L$ tightly embeds $\ell:\R'\to\reals^\Y_+$ if and only if $\ell$ is injective and $\ell(\R') = \V$.\label{item:X-tight-embed}
  \end{enumerate}
\end{restatable}

As a finite representative set implies a polyhedral Bayes risk by Lemma~\ref{lem:loss-restrict}, Lemma~\ref{lem:X} shows that polyhedral Bayes risks are equivalent to having finite representative sets, which in turn gives an embedding by
Proposition~\ref{prop:representative-embeds-restriction}.
\begin{corollary}\label{cor:poly-risk-fin-rep}
  The following are equivalent for any minimizable loss $L:\R\to\reals^\Y_+$.
  \begin{enumerate}
  \item $\risk{L}$ is polyhedral.
  \item $L$ has a finite representative set.
  \item $L$ embeds a discrete loss.
  \end{enumerate}
\end{corollary}
From Corollary~\ref{cor:poly-risk-fin-rep}, $L$ having a finite representative set is an equivalent condition to $L$ being minimizable and $\risk{L}$ being polyhedral.
(Recall that having a finite representative set already implies minimizability.)
As it is also a more succinct condition, we will use the former in the sequel.
In particular, the implications of Lemma~\ref{lem:X} follow whenever $L$ has a finite representative set.


\subsection{Equivalent condition: matching Bayes risks}\label{subsec:match-BR}


Lemma~\ref{lem:X} leads to another appealing equivalent condition to an embedding: a surrogate embeds a discrete loss if and only if their Bayes risks match.
The proof follows by mapping the two conditions of an embedding onto the geometric structure revealed by Lemma~\ref{lem:X}: (ii) if the properties have the same level sets, the Bayes risks have the same projections onto $\simplex$, and (i) if the loss values match, then the slopes of the Bayes risk must be identical as well.

\begin{proposition}\label{prop:embed-bayes-risks}
  Let discrete loss $\ell$ and minimizable loss $L$ be given.
  Then $L$ embeds $\ell$ if and only if $\risk{L}=\risk{\ell}$.
  % Let discrete loss $\ell:\R\to\reals^Y$ be given.
  % Then $L:\reals^d\to\reals^\Y$ embeds $\ell$ if and only if $\risk{L}=\risk{\ell}$.
  % A loss $L$ embeds a discrete loss $\ell$ if and only if $\risk{L}=\risk{\ell}$.
\end{proposition}
\begin{proof}
  Define $\Gamma = \prop{L}$ and $\gamma = \prop{\ell}$.
  
%  $\implies$
  Suppose $L$ embeds $\ell$, so we have some $\Sc\subseteq \R$ which is representative for $\ell$ and an embedding $\varphi:\Sc\to\reals^d$; take $\U := \varphi(\Sc)$.
  Since $\Sc$ is representative for $\ell$, by embedding condition (ii) we have $\{\gamma_s \mid s\in\Sc\} = \{\Gamma_u \mid u\in\U\}$, so $\U$ is representative for $L$.
  % for all $p\in\simplex$ we have some $r \in \Sc \cap \gamma(p)$ and thus we have $\varphi(r) \in \U \cap \Gamma(p)$.
  % In particular, $\U \cap \Gamma(p)\neq \emptyset$ for all $p\in\simplex$, 
  By Lemma~\ref{lem:loss-restrict}, we have $\risk{\ell} = \risk{\ell|_{\Sc}}$ and $\risk{L} = \risk{L|_{\U}}$.
  As $L(\varphi(\cdot)) = \ell(\cdot)$ by embedding condition (i), for all $p\in\simplex$ we have
  \begin{equation*}
    \risk{\ell}(p) = \risk{\ell|_\Sc}(p) = \min_{r \in \Sc}\inprod{p}{\ell(r)} = \min_{r \in \Sc}\inprod{p}{L(\varphi(r))} = \min_{u \in \U}\inprod{p}{L(u)} = \risk{L|_\U}(p) = \risk{L}(p)~.
  \end{equation*}
  
%  $\impliedby$
	For the reverse implication, assume $\risk{L} = \risk{\ell}$, which are polyhedral functions as $\ell$ is discrete.
  From Lemma~\ref{lem:X}(\ref{item:X-min-V}), we have some set $\V\subseteq\reals^\Y_+$ and minimum representative sets $\R^* \subseteq \R$ and $\U^* \subseteq \U$, for $\ell$ and $L$ respectively, such that $\ell(\R^*) = \V = L(\U^*)$.
  %\jessiet{This is a different $\Sc$ than the first paragraph, right? The first $\Sc$ is representative for $\ell$.}\raft{They are both quantified separately, so not linked; are you saying it's confusing to use $\Sc$ here though because its role is slightly different?}\jessiet{Yes... attempting to clarify.}
  As $\R^*$ and $\U^*$ are miniumum, they cannot repeat loss vectors, and thus $|\R^*|=|\ell(\R^*)|$ and $|L(\U^*)|=|\U^*|$.
  We conclude that $\R^*$ and $\U^*$ are both in bijection with $\V$.
  The map $\varphi :\R^* \to \reals^d$, given by $\varphi(r) = u \in \U^*$ where $\ell(r) = L(u)$, is therefore well-defined.
  Condition (i) of an embedding is immediate.
  From Proposition~\ref{prop:representative-embeds-restriction}, $\ell$ embeds $\ell|_{\R^*}$ and $L$ embeds $L|_{\U^*}$, both via the identity embedding.
  Using condition (ii) from both embeddings, for all $p\in\simplex$ and $r\in\R^*$, we have
  \begin{equation*}
    r \in \gamma(p) \iff r \in \prop{\ell|_{\R^*}}(p) \iff \varphi(r) \in \prop{L|_{\U^*}}(p)
    \iff \varphi(r) \in \prop{L}(p)~,
  \end{equation*}
  giving condition (ii).
\end{proof}


We use this fact in the proof of Theorem~\ref{thm:discrete-loss-poly-embeddable} to show that every discrete loss is embedded by some polyhedral surrogate.
See Figure~\ref{fig:bayes-risks-01} for an illustration.

%\btw{RF: matching risks is not necessary for \emph{consistency}, as evidenced by logistic loss for 0-1 loss.  Maybe we should just make the observation, earlier on, that \emph{embedding} is not necessary for consistency. \jessie{Added a few sentences above, when talking about consistency vs calibration.}}

%\btw{RF: DKR (section 3.1) realized the importance of matching Bayes risks, but they could only give general results for strictly convex (concave I should say) risks, in part because they fixed the link function to be a generalization of $\sgn$.  In contrast, we focus exclusively on non-strictly-convex risks.}


%Previous work from~\citet[Proposition 4]{duchi2018multiclass} realized the significance of matching Bayes risks for calibration with respect to the 0-1 loss.
%Proposition~\ref{prop:embed-bayes-risks} broadens this general insight to any discrete loss.
%Moreover, their result relies the Bayes risk of the surrogate being strictly concave, whereas polyhedral Bayes risks are never strictly concave.

\subsection{Trimming a loss}
\label{sec:trim}

Central to the structural results in Lemma~\ref{lem:X} is the existence of a canonical set of loss vectors $\V$ which match the loss vectors of any minimum representative set.
This fact may seem surprising when one considers that losses may have many mimimum representative sets.
For example, consider hinge loss with a spurious extra dimension, i.e., $L:\reals^2\to\reals^\Y$, $L((r_1, r_2))_y = \max(0,1-r_1y)$ for $\Y = \{-1,+1\}$.
Here the minimum representative sets are exactly the two-element sets of the form $\{(-1,a),(1,b)\}$ for any $a,b\in\reals$. 
Lemma~\ref{lem:X}(\ref{item:X-min-V}) states that, while the minimum representative set is not unique, its loss vectors are.

Motivated by this observation, let us define the ``trim'' of a loss to be this unique set $\V$ of loss vectors induced by any minimum representative set, which again is well-defined by Lemma~\ref{lem:X}(\ref{item:X-min-V}).
\begin{definition}[Trim]\label{def:trim-loss}
  Given a loss $L:\R \to \reals_+^\Y$ with a finite representative set, we define $\trimcover(L) = \{L(r) \mid r \in \R^*\}$ given any minimum representative set $\R^*$ for $L$.
\end{definition}


Using this notion of trimming a loss, we can again recast our embedding condition: a loss embeds another if and only if they induce the same loss vectors, or have the same $\trimcover$.

\begin{proposition}\label{prop:embed-iff-trims-equal}
  Let $L:\reals^d\to\reals^\Y_+$ have a finite representative set, and let $\ell:\R\to\reals^\Y_+$ be a discrete loss.
  Then $L$ embeds $\ell$ if and only if $\trimcover(L) = \trimcover(\ell)$.
  Furthermore, $L$ tightly embeds $\ell$ if and only if $\ell$ is injective and $\trimcover(L) = \ell(\R)$.
\end{proposition}
\begin{proof}
  As $L$ has a finite representative set, it is minimizable.
  Proposition~\ref{prop:embed-bayes-risks} gives $L$ embeds $\ell$ if and only if $\risk L = \risk \ell$.
  If $\risk L = \risk \ell$, Lemma~\ref{lem:X}(\ref{item:X-min-V}) gives $\trim(L) = \trim(\ell)$.
  For the converse, suppose $\trim(L) = \trim(\ell) =: \V$.
  Define the discrete loss $\ell_\trim : \V \to \V, v\mapsto v$.
  Then $\ell_\trim$ is injective and $\ell_\trim(\V) = \V$, so from Lemma~\ref{lem:X}(\ref{item:X-tight-embed}), both $L$ and $\ell$ tightly embed $\ell_\trim$.
  We conclude $\risk L = \risk{\ell_\trim} = \risk \ell$ from Proposition~\ref{prop:embed-bayes-risks}.
  The second statement also follows directly from Lemma~\ref{lem:X}(\ref{item:X-tight-embed}).
\end{proof}

In a strong sonse, the trim operation reduces a loss to its core: the unique minimal set of loss vectors that drive its statistical behavior.
One can therefore think of designing consistent convex surrogates as trying to ``fill out'' this minimal set with additional loss vectors so that one attains convexity while keeping trim the same.
%(At least, the above is true when restricting to minimizable losses with polyhedral Bayes risks.)

\subsection{Minimum representative sets and non-redundancy}
\label{sec:min-rep-sets-trim}

The condition that a representative set be minimum implies that one has identified exactly the ``active'' reports of a loss, in some sense.
We now relate this condition to another natural notion from the property elicitation literature: non-redundancy~\cite{frongillo2014general,lambert2018elicitation}.
Intuitively, a loss is non-redundant if no report is weakly dominated by another report.

\begin{definition}[Non-redundancy]\label{def:nonredundant}
  A loss $L : \R \to \reals^\Y_+$ eliciting $\Gamma:\simplex \toto \R$ is \emph{redundant} if there are reports $r, r' \in \R$ with $r \neq r'$ such that $\Gamma_r \subseteq \Gamma_{r'}$, and \emph{non-redundant} otherwise.
\end{definition}

From the structural result of Lemma~\ref{lem:X}, we can see that in fact these two notions are equivalent when $L$ has a polyhedral Bayes risk.
\begin{proposition}\label{prop:tfae-min-rep-nonredundant}
  Let $L:\R\to\reals^\Y_+$ have a finite representative set $\R'$.
  Then $\R'$ is a minimum representative set for $L$ if and only if $L|_{\R'}$ is non-redundant.
\end{proposition}
\begin{proof}
  Let $\Gamma = \prop{L}$.
  Suppose first that $L|_{\R'}$ is redundant.
  Then there exist $r,r' \in \R'$ such that $\Gamma_r \subseteq \Gamma_{r'}$.
  Thus, for all $p \in \Gamma_r$, we have $\{r, r'\} \subseteq \Gamma(p)$.
  Therefore $\R' \setminus \{r\}$ still a representative set, so $\R'$ is not minimum.

  Now suppose $L|_{\R'}$ is non-redundant.
  As $\R'$ is a representative set, Lemma~\ref{lem:X}(\ref{item:X-rep-contain-min}) gives some minimum representative set $\Sc \subseteq \R'$.
  Suppose we had some $r \in \R' \setminus \Sc$.
  Now Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant}) gives some $s\in\Sc$ such that $\Gamma_r \subseteq \Gamma_s$, which contradicts $L|_{\R'}$ being non-redundant.
  We conclude $L(\Sc)=L(\R')$, meaning $\R'$ is a minimum representative set.
\end{proof}

\begin{corollary}\label{cor:tight-embed-min-rep}
  Let loss $L:\R\to\reals^\Y_+$ with finite representative set $\R'$ be given.
  Then $L$ tightly embeds $L|_{\R'}$ if and only if $L|_{\R'}$ is non-redundant.
\end{corollary}

In fact, we can show something stronger: the reports in minimum representative sets are precisely those which are not strictly redundant.
To formalize this statement, given $\Gamma : \simplex \toto \R$, let $\red(\Gamma) := \{r\in\R \mid \exists r'\in\R,\; \Gamma_r \subsetneq \Gamma_{r'}\}$ be the set of strictly redundant reports.
Similarly, for minimizable $L$, let $\red(L) := \red(\prop L)$.

\begin{proposition}
  Let $L : \R \to \reals^\Y_+$ have a finite representative set.
  Let $\R'$ be the union of all minimum representative sets for $L$.
  Then $\R' = \R \setminus \red(L)$.
  \btw{Quite possibly there is a faster proof, though I do like how the proof shows something stronger.  Old version is in before-reorg-sec-3.tex.}
\end{proposition}

\begin{proof}
  Let $\Gamma = \prop L$.
  Let $\Sc$ be a minimum representative set for $L$, and let $s\in\Sc$.
  Suppose for a contradiction that $s\in\red(\Gamma)$.
  Then we have some $r\in\R$ with $\Gamma_s \subsetneq \Gamma_r$.
  From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant}) we have some $s'\in\Sc$ such that $\Gamma_r \subseteq \Gamma_{s'}$.
  But now $\Gamma_s \subsetneq \Gamma_r \subseteq \Gamma_{s'}$, contradicting $\Sc$ being minimum representative.
  Thus $\Sc \subseteq \R \setminus \red(\Gamma)$.

  For the reverse inclusion, let $r\in\R\setminus\red(\Gamma)$.
  Let $\Sc$ again be a minimum representative set for $L$.
  From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant}), we have some $s\in\Sc$ such that $\Gamma_r \subseteq \Gamma_s$.
  By definition of $\red(L)$, we conclude $\Gamma_r = \Gamma_s$.
  Now take $\Sc' = (\Sc \setminus \{s\}) \cup \{r\}$, that is, the same set of reports with $r$ replacing $s$.
  We have $\{\Gamma_s \mid s\in\Sc\} = \{\Gamma_{s'} \mid s'\in\Sc'\}$, and thus $\Sc'$ is a minimum representative for $L$ by Lemma~\ref{lem:X}(\ref{item:X-min-Theta}).
  As $r\in\Sc'$, we have $r \in \R'$ and we are done.
\end{proof}


As a corollary, we can state another characterization of $\trim$ in terms of redundant reports.
The result follows immediately from the definition of $\trim$.

\begin{corollary}\label{cor:trim-loss-red}
  Let $L : \R \to \reals^\Y_+$ have a finite representative set.
  Then $\trimcover(L) = L(\R \setminus \red(L))$.
  % $\{L(r) \mid r \notin \red(\Gamma)\}$.
  % \in \R,\; \forall r'\in\R,\; \Gamma_r = \Gamma_{r'} \textnormal{ or } \Gamma_r \not\subseteq \Gamma_{r'}\}$
  % $\mid u \in \R \textrm{ s.t. } \neg\exists u'\in\R,u'\neq u,\, \Gamma_u \subsetneq \Gamma_{u'}\}$
\end{corollary}

This result motivates the analogous definition for properties, $\trimred(\Gamma) := \{\Gamma_r \mid r \in \R\setminus\red(\Gamma)\}$.
We leverage this definition next, to study embeddings at the property level.

\subsection{A property elicitation perspective on trimmed losses}
\label{sec:prop-trim}

We conclude this section with a structural result similar to Lemma~\ref{lem:X}, but for properties.
To do so, we must first generalize the definition of embeddeding to properties.
We say a property $\Gamma:\simplex\toto\reals^d$ embeds a finite property $\gamma:\simplex\toto\R$ if condition (ii) of Definition~\ref{def:loss-embed} holds.
In other words, $\Gamma$ embeds $\gamma$ if we have some representative set $\Sc\subseteq\R$ for $\gamma$ and embedding $\varphi:\Sc\to\reals^d$ such that for all $s\in\Sc$ we have $\gamma_s = \Gamma_{\varphi(s)}$.
% The definitions of representative sets and embeddings naturally extend to properties.
% We say a set 
% As we define embeddings from one loss to another, one can define embeddings from one property to another, where essentially condition (i) is relaxed; see \S~\ref{app:embed-props} for a precise definition.

Roughly, our result is as follows.
First, if $\Gamma$ embeds $\gamma$, the level sets of $\Gamma$ must all be redundant relative to $\gamma$.
In other words, $\Gamma$ is exactly the property $\gamma$ up to relabelling reports, but potentially with other reports ``filling in the gaps'' between the embedded reports of $\gamma$.
When working with convex surrogates, extra reports often arise in the convex hull of the embedded reports.
In this sense, we can regard embedding as only a slight departure from direct elicitation: if a loss $L$ directly elicits $\Gamma$ which embeds $\gamma$, we can almost think of $L$ as eliciting $\gamma$ itself.
Finally, we have an important converse: if $\Gamma$ has finitely many full-dimensional level sets, or equivalently, if $\trimred(\Gamma)$ is finite, then $\Gamma$ must embed some finite elicitable property with the same full-dimensional level sets.

The proof relies heavily on Lemma~\ref{lem:X}.
The statements about level sets use the following corollary of Proposition~\ref{prop:embed-iff-trims-equal} for properties.
\begin{corollary}\label{cor:trim-prop-red}
  Let $\Gamma : \simplex \toto \R$ be an elicitable property with a finite representative set.
  Then $\trimred(\Gamma)$ is the set of full-dimensional level sets of $\Gamma$.
\end{corollary}
\begin{proof}
  Let $L$ elicit $\Gamma$.
  From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}), for any finite minumum representative set $\Sc\subseteq\R$, the set $\{\Gamma_s\mid s\in\Sc\}$ is exactly the set of full-dimensional level sets $\Theta$ of $\Gamma$.
  From Proposition~\ref{prop:tfae-min-rep-nonredundant}, we have $r \in \R\setminus \red(\Gamma)$ if and only if $r$ is an element of some minimum representative set.
  As $\Gamma$ has at least one minimum representative set, we conclude $\trimred(\Gamma) = \{\Gamma_r \mid r\in \R\setminus\red(\Gamma)\} = \Theta$.  
\end{proof}

\begin{proposition}\label{prop:embed-trim}
  Let $\Gamma:\simplex\toto\reals^d$ be an elicitable property.
  %\jessiet{$\reals^d$ vs $\R$?}\raft{Can't recall why we needed $\reals^d$.  Probably don't... in any case, can't use $\R$ since that clashes with $\gamma$.}
  The following are equivalent:
  \begin{enumerate}\setlength{\itemsep}{0pt}
  \item $\Gamma$ embeds a elicitable finite property $\gamma:\simplex \toto \R$.
  \item $\trimred(\Gamma)$ is a finite set.%, and $\cup\,\trimred(\Gamma) = \simplex$.
  % \\ $\hat\Gamma := \trimred(\Gamma)$ is a finite property, and $\cup\,\hat\Gamma) = \simplex$.
  \item There is a finite minimum representative set $\U$ for $\Gamma$.
  \item There is a finite set of full-dimensional level sets $\hat\Theta$ of $\Gamma$, and $\cup\,\hat\Theta = \simplex$.
  \end{enumerate}
  % \raft{I see why you wanted this addition here, but (a) it's clear from the def of embedding, and (b) we don't have varphi defined.  So let's leave it out.}
  Moreover, when any of the above hold, $\trimred(\gamma) = \trimred(\Gamma) = \{\Gamma_u \mid u\in\U\} = \hat\Theta$.
\end{proposition}

\begin{proof}
  Let $L$ be a fixed loss eliciting $\Gamma$, so that in particular $\risk L$ is fixed.
  By definition of elicitable properties, $L$ is minimizable.
  In each case, we will show that $\risk L$ is polyhedral (or equivalently, that $L$ has a finite representative set), and thus Lemma~\ref{lem:X} will give us the set $\Theta$ of full-dimensional level sets of $\Gamma$, uniquely determined by $\risk L$.
  We will prove $1 \Rightarrow 2 \Rightarrow 3 \Rightarrow 4 \Rightarrow 1$, and in each case show that the relevant set of level sets is equal to $\Theta$, giving the result.

  $1 \Rightarrow 2$:
  Let $\Sc$ be the representative set for $\gamma$ and $\varphi:\Sc\to\reals^d$ the embedding.
  Since $\Sc$ is finite, $\varphi(\Sc)$ is a finite representative set for $\Gamma$ (and $L$; thus, $\risk L$ is polyhedral).
  Corollary~\ref{cor:trim-prop-red} now gives $\trimred(\Gamma) = \Theta$, which is finite, showing Case 2.

  $2 \Rightarrow 3$:
  If $\trimred(\Gamma)$ is finite, then in particular we have a finite set of reports $\Sc \subseteq \reals^d$ such that $\trimred(\Gamma) = \{\Gamma_s \mid s\in\Sc\}$.
  As $\Gamma$ is elicitable, $\reals^d$ is representative for $\Gamma$.
  By definition of $\trimred$, we have $\simplex = \cup_{r\in\reals^d} \Gamma_r = \cup \trimred(\Gamma) = \cup_{s\in\Sc} \Gamma_s$, and therefore $\Sc$ is representative for $\Gamma$ and for $L$.
  As $\Sc$ is finite, we have $\risk L$ polyhedral.
  %\jessiet{I'm a bit confused by this- how do we get $\cup_{\reals^d}\Gamma_r \subseteq \cup \trimred(\Gamma)$?}\raft{Each level set on the left is contained in one on the right}
  From Lemma~\ref{lem:X}(\ref{item:X-rep-contain-min}), we have some minimum representative set $\U\subseteq\Sc$ for $L$ and $\Gamma$, implying statement 3.
  Moreover, Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}) gives $\{\Gamma_u \mid u\in\U\} = \Theta$.

  $3 \Rightarrow 4$:
  Let $\U$ be a finite minimum representative set for $\Gamma$.
  Then $\risk L = \risk{L|_\U}$ is polyhedral.
  Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}) once again gives $\{\Gamma_u \mid u\in\U\} = \Theta$.
  We simply let $\hat\Theta = \Theta$, giving statement 4 as $\U$ is representative.

  $4 \Rightarrow 1$:
  Let $\Sc\subseteq\R$ such that $\{\Gamma_s \mid s\in\Sc\} = \hat \Theta$.
  Then $\Sc$ is representative for $\Gamma$ and $L$, as $\cup\hat\Theta = \simplex$.
  Again, this yields a finite representative set for $L$.
  Lemma~\ref{lem:loss-restrict} now states that $L$ embeds $L|_\Sc$, so $\Gamma$ embeds $\gamma := \Gamma|_\Sc$, giving Case 1.
  Finally, Corollary~\ref{cor:trim-prop-red} gives $\trimred(\gamma) = \Theta$.
  %
  % Letting $\Theta_\ell$ be the full-dimensional level sets of $\gamma$, Corollary~\ref{cor:trim-prop-red} also gives $\trimred(\gamma) = \Theta'$.
  % From Lemma~\ref{lem:X}(\ref{item:X-rep-contain-min}), we have some minimum representative set $\U\subseteq\U'$ for $L$ and $\Gamma$.
  % We have $\{\gamma_r\mid \varphi(r)\in\U\} = \{\Gamma_u \mid u\in\U\} = \Theta$ by Lemma~\ref{lem:X}(\ref{item:X-min-Theta}) and definition of embedding.
  % As $\Theta$ is a set of full-dimensional level sets of $\gamma$, we must have $\Theta \subseteq \Theta_\ell$.
  % But as $\cup \Theta = \simplex$, we must have $\Theta = \Theta_\ell$, as otherwise there exists $s\in\Sc$ with $\gamma_s \in \Theta_\ell \setminus \Theta$ and thus $\Sc \setminus \{s\}$ would still be representative.
  %
  % 
  % It remains to show $\trimred(\gamma) = \Theta$.
  % Moreover, $\Theta$ is exactly the full-dimensional level sets of $\Gamma$ by Lemma~\ref{lem:X}(\ref{item:X-full-dim}).
  % 
  % From Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-full-dim}) there exists some set of level sets $\Theta'$ which are the full-dimensional level sets of $\gamma$ and there exists a minimum representative set $\Sc\subseteq\R$ with $\{\gamma_s\mid s\in\Sc\} = \Theta'$.
  % Finally, Lemma~\ref{lem:X}(\ref{item:X-redundant}) gives $\trimred(\gamma) = \Theta$.
\end{proof}

As a final observation, recall that a property $\Gamma$ elicited by a polyhedral loss has a finite range, in the sense that there are only finitely many optimal sets $\Gamma(p)$ for $p\in\simplex$ (Lemma~\ref{lem:polyhedral-range-gamma}).
Proposition~\ref{prop:embed-trim} shows a complementary statement: there are only finitely many level sets $\Gamma_u$ for $u\in\reals^d$.
In other words, both $\Gamma$ and $\Gamma^{-1}$ have a finite range as multivalued maps.

\section{Polyhedral Indirect Elicitation Implies Consistency}
\label{sec:poly-ie-consistency}

%While it is well-known that indirect property elicitation is a necessary condition for consistency~\citep{finocchiaro2021unifying}, we proceed to show that indirect elicitation is also a \emph{sufficient} condition for consistency when restricting to polyhedral losses.
%Intuitively, a loss $L$ indirectly elicits a property $\gamma$ if we can compute $\gamma$ from $\prop L$.
%To formalize the condition, we use the notion of a property refining another from~\citet{frongillo2014general}.

%\begin{definition}[Refines]
%  \label{def:refines}
%	Let $\Gamma:\simplex \toto \R$ and $\Gamma':\simplex\toto \R'$.
%	Then $\Gamma$ \emph{refines} $\Gamma'$ if for all $r \in \R$, there exists $r' \in \R'$ such that $\Gamma_{r} \subseteq \Gamma'_{r'}$.
%\end{definition}
%Equivalently, $\Gamma$ refines $\Gamma'$ if there is some ``link'' function $\psi:\R\to\R'$ such that $r\in\Gamma(p) \implies \psi(r) \in \Gamma'(p)$ for all $p\in\simplex$.
%We will use the fact that refinement is transitive: if $\Gamma$ refines $\Gamma'$ and $\Gamma'$ refines $\Gamma''$, then $\Gamma$ refines $\Gamma''$.
%
%\begin{definition}[Indirectly elicits]
%  \label{def:indirectly-elicits}
%  A loss $L$ \emph{indirectly elicits} a property $\gamma$ if $\prop L$ refines $\gamma$.
%\end{definition}




%% context for section: for polyhedral losses, indirect elicitation iff consistent
As we have observed, consistency, and therefore calibration, implies indirect elicitation (\S~\ref{subsec:calibration-links}).
In general, indirect elicitation is simpler and weaker than calibration, since it only depends on the loss through the property it elicits, i.e., its exact minimizers.
Surprisingly, for polyhedral surrogates, we show the converse: indirect elicitation implies calibration, and therefore consistency.

\begin{theorem}\label{thm:poly-ie-implies-consistent}
	Let $L:\reals^d \to \reals^\Y_+$ be a polyhedral loss which indirectly elicits a finite property $\gamma$.
  For any loss $\ell$ eliciting $\gamma$, there exists a link $\psi$ such that $(L, \psi)$ is calibrated with respect to $\ell$.
\end{theorem}

One technical detail is that the link function may have to change.
That is, we will show that if $(L,\psi)$ indirectly elicits $\prop{\ell}$, then there exists some potentially different link $\psi'$ such that $(L,\psi')$ is calibrated with respect to $\ell$.
To see why this change may be necessary, consider again the example from \S~\ref{subsec:calibration-links}:
hinge loss with the link $\psi(u) = -1$ for $u < 1$ and $\psi(u) = 1$ for $u\geq 1$.
Here indirect elicitation is achieved, since we have $\psi((-\infty,-1]) = \{-1\}$ and $\psi([1,\infty)) = \{1\}$, but the link is not $\epsilon$-separated for any $\epsilon>0$.
In general, it is not clear whether one can always adjust the link in this case to achieve separation, and therefore calibration.
Fortunately, for polyhedral surrogates, one can always ``thicken'' a given link to achieve separation.

We give two proofs of Theorem~\ref{thm:poly-ie-implies-consistent}.
The first is direct: we show, as foreshadowed in \S~\ref{sec:calibration}, that our thickened link construction can be generalized for indirect elicitation.
In fact, we will further prove that our general construction recovers every possible calibrated link function.
The second proof highlights the central role that embeddings play when reasoning about polyhedral surrogates.
Specifically, we will show that if a polyhedral surrogate indirectly elicits a finite property, the link function must ``pass through'' an embedding, giving calibration through Construction~\ref{const:eps-thick-link}.

\subsection{Generalizing the thickened link construction}

Given that Construction~\ref{const:eps-thick-link} uses the embedding $\varphi:\Sc\to\reals^d$ in a crucial role, it is not immediately clear how to generalize the construction beyond embeddings.
Specifically, this crucial role is in the definition of $R_U$, the set of target reports which must be optimal whenever a given surrogate report set $U\subseteq\reals^d$ is optimal.
Using the embedding, we can simply define $R_U = \{ r\in\Sc \mid \varphi(r) \in U \}$, since the definition of embedding means that those are exactly the target reports (among the representative set $\Sc$) which are optimal when $U$ is.

Now suppose we merely know that $L$ indirectly elicits some finite property $\gamma:\simplex\toto\R$.
In \S~\ref{app:sep-link-exists}, we give Construction~\ref{const:general-eps-thick-link}, which is the same as Construction~\ref{const:eps-thick-link} but with the following modification of $R_U$ to $\hat R_U$.
Let $\Gamma = \prop L$ and $\U = \{\Gamma(p) \mid p \in \simplex\}$ as before.
Then for all $U\in\U$, we define $\hat R_U := \{r\in\R \mid \Gamma_U \subseteq \gamma_r\}$, where $\Gamma_U := \{p\in\simplex \mid U = \Gamma(p)\}$ is the set of distributions for which $U$ is the surrogate optimal set.
In words, $\hat R_U$ is the set of reports $r$ which may be linked to from points in $U$, in the sense that $U$ being $L$-optimal implies $r$ is $\ell$-optimal.
For the special case where $L$ embeds $\ell$, it is straightforward to verify that $R_U = \hat R_U \cap \Sc$.
As a result, Construction~\ref{const:eps-thick-link} is the special case of Construction~\ref{const:general-eps-thick-link} where one is given an embedding and restricts to the representative set $\Sc$ (Lemma~\ref{lem:general-construction-special-case}).

The main result of \S~\ref{app:sep-link-exists} is that, if a polyhedral surrogate indirectly elicits a finite property, then for small enough $\epsilon>0$, Construction~\ref{const:general-eps-thick-link} always produces a link (Proposition~\ref{prop:general-eps-thick-produce}).
Combined with the fact that, by design, the construction and our choice of $\hat R_U$ enforce separation, we have the following.
\begin{proposition}\label{prop:general-eps-thick-separated}
  Let $L$ be a polyhedral surrogate which indirectly elicits a finite property $\gamma$.
  Then there exists $\epsilon_0>0$ such that for all $0 < \epsilon \leq \epsilon_0$, Construction \ref{const:general-eps-thick-link} for $L,\gamma,\epsilon,\|\cdot\|_{\infty}$ produces a separated link from $\prop{L}$ to $\gamma$.
\end{proposition}
\noindent
Since separation is equivalent to calibration for polyhedral surrogates (Theorem~\ref{thm:calibrated-separated}), we now have Theorem~\ref{thm:poly-ie-implies-consistent}: indirect elicitation implies calibration for polyhedral surrogates.


In fact, we can show something stronger: $\hat R_U$ enforces separation exactly, and therefore every possible calibrated link must arise from Construction~\ref{const:general-eps-thick-link}.

\begin{theorem}\label{thm:link-char}
  A link $\psi$ is calibrated for a given polyhedral surrogate $L$ and discrete target $\ell$ if and only if there exists $\epsilon>0$ such that $\psi$ is produced by Construction \ref{const:general-eps-thick-link} for $L,\prop\ell,\epsilon,\|\cdot\|$.
\end{theorem}


\subsection{Centrality of embeddings}

%% first step to show iff: indirect polyhedral elicitation passes through embedding.
To derive another proof of Theorem~\ref{thm:poly-ie-implies-consistent}, we now show that, for polyhedral surrogates, indirect elicitation must always pass through an embedding.
That is, if $L$ indirectly elicits $\gamma$, then there is some loss $\ell$ which $L$ embeds, such that $\ell$ indirectly elicits $\gamma$.
This result holds more generally whenever $L$ has a finite representative set, as in \S~\ref{sec:min-rep-sets}.
\begin{lemma}\label{lem:ie-iff-embeds-refinement}
  Let $L:\reals^d\to\reals^\Y_+$ be polyhedral.
  Then $L$ indirectly elicits a property $\gamma$ if and only if $L$ tightly embeds a discrete loss $\ell$ that indirectly elicits $\gamma$.
\end{lemma}
\begin{proof}
  Let $\Gamma = \prop L$.
From Lemma~\ref{lem:X}(\ref{item:X-tight-embed}), $L$ tightly embeds a discrete loss.
  Furthermore, Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-redundant},\ref{item:X-tight-embed}) implies that $L$ indirectly elicits $\hat\gamma := \prop \ell$ for any discrete loss $\ell$ that $L$ tightly embeds.
  The link is any function $\hat\psi: u \mapsto r$ such that $\Gamma_u \subseteq \hat\gamma_r$ for all $u \in \reals^d$.

  We will prove the stronger statement that, for any property $\gamma$, and any loss $\ell:\R\to\reals^\Y_+$ that $L$ tightly embeds, $L$ indirectly elicits $\gamma$ if and only if $\ell$ indirectly elicits $\gamma$.
  If $\ell$ indirectly elicits $\gamma$ via the link $\psi$, then $L$ indirectly elicits $\gamma$ by transitivity of subset inclusion, as $\Gamma_u \subseteq \hat\gamma_{\hat\psi(u)} \subseteq \gamma_{\psi \circ \hat\psi(u)}$ for all $u \in \reals^d$.
  Conversely, suppose $L$ indirectly elicits $\gamma$ via the link $\psi$.
  As $L$ tightly embeds $\ell$, from Lemma~\ref{lem:X}(\ref{item:X-min-Theta},\ref{item:X-tight-embed}), the level sets of $\hat \gamma$ are contained in the set $\{\Gamma_u \mid u\in\reals^d\}$.
  Letting the map $\hat\psi:\R\to\reals^d$ exhibit this contaiment, we have $\hat\gamma_r = \Gamma_{\hat\psi(r)} \subseteq \gamma_{\psi \circ \hat\psi (r)}$ for all $r\in\R$.
\end{proof}

\begin{proof}[{Alternate proof of Theorem~\ref{thm:poly-ie-implies-consistent}}]
  Let $\R$ be the range of $\gamma$, so that $\gamma: \simplex \toto \R$, and let $\ell$ elicit $\gamma$.
  By Lemma~\ref{lem:ie-iff-embeds-refinement}, $L$ tightly embeds a discrete loss $\hat\ell:\hat\R\to\reals^\Y_+$ such that $\hat\ell$ indirectly elicits $\gamma$; let $\psi^\R: \hat\R \to \R$ be the corresponding link function.
  Let $\hat\gamma := \prop{\hat\ell}$ be the property that $\hat\ell$ directly elicits.
  Then for all $r\in\hat\R$ and $p\in\simplex$ we have $r\in \hat\gamma(p) \implies \psi^\R(r) \in \gamma(p)$. 
  %From refinement, we can define a function $\psi^\R: \R^\emb \to \R$ such that for all $r\in\R^\emb$ and $p\in\simplex$ we have $r\in\gamma^\emb(p) \implies \psi^\R(r) \in \gamma(p)$. 
  Moreover, Construction~\ref{const:eps-thick-link} gives a link function $\hat \psi : \reals^d \to \hat\R$ such that $(L,\hat\psi)$ is calibrated with respect to $\hat\ell$.

	
  Consider $\psi := \psi^\R \circ \hat\psi$ and fix $p\in\simplex$.
	For any $u\in\reals^d$, if $\hat\psi(u) \in \hat\gamma(p)$, then $\psi(u) = \psi^\R\circ \hat\psi(u) \in \gamma(p)$ by definition of $\hat\psi$ and $\psi^\R$.
  Contrapositively\btw{You heard me},
  $\psi(u) \notin \gamma(p) \implies \hat\psi(u) \notin \hat\gamma(p)$.
  Thus, we have
  \begin{equation*}
    \label{eq:link-set-inclusion}
    \{u\in\reals^d \mid \psi(u) \not \in \gamma(p) \} \subseteq \{u\in\reals^d \mid \hat\psi(u) \not \in \hat\gamma(p) \}~.
  \end{equation*}
  Combined with the fact that $(L,\hat\psi)$ is calibrated with respect to $\hat\ell$, we have
	\begin{align*}
\inf_{u\in\reals^d : \psi(u) \not \in \gamma(p)} \inprod{p}{L(u)} \geq	\inf_{u\in\reals^d : \hat\psi(u) \not \in \hat\gamma(p)} \inprod{p}{L(u)} > \inf_{u\in\reals^d}\inprod{p}{L(u)}~,
	\end{align*}
  showing calibration of $\psi$.
	%Consistency follows as calibration and consistency are equivalent in this setting~\citep{ramaswamy2016convex}.
\end{proof}



\section{Conclusion} \label{sec:conclusion}

In this work, we introduce an embedding framework to design and analyze consistent, convex surrogates for discrete prediction tasks.
Our results are constructive; as we outline in \S~\ref{sec:applications}, they can be fruitfully applied to a range of tasks, from designing new surrogates and link functions to understanding the consistency or inconsistency of existing surrogates.
Beyond these tools, our results shed light on fundamental questions about the design of consistent surrogates.

Perhaps the most pressing open direction is simply to apply our framework to prediction problems of interest.
We hope that the discussion in \S~\ref{sec:apply-embedd-fram}, and the detailed examples in subsequent works, serve as useful guidelines for doing so.
A particularly promising domain to apply our framework is structured prediction, where relatively few consistent surrogates are known.
Indeed, our framework has already been applied to submodular structured problems (\S~\ref{sec:lovasz-hinge}) and to max-margin losses~\cite{nowak2022consistency}.

Beyond applying our fromework, we see several interesting directions for theoretical research.
Below we outline several such directions.



\paragraph{Prediction dimension}

It can be important for applications to understand the minimum prediction dimension $d$ of a consistent convex surrogate $L:\reals^d\to \reals^\Y_+$ for a given target problem, also called its convex elicitation complexity~\cite{frongillo2021elicitation}.
% The prediction dimension of a surrogate $L : \reals^d \to \reals^\Y_+$ is the $d$ in its definition: the dimension of the surrogate report space.
Theorem~\ref{thm:discrete-loss-poly-embeddable} constructs a consistent surrogate for any discrete loss, with prediction dimension $d = n := |\Y|$.
In some settings, such as structured prediction and information retrieval, a prediction dimension of $d=n$ can be prohibitively large.
For example, in \S~\ref{sec:lovasz-hinge} we discuss structured problems which decompose as $k$ simple subproblems, like pixel classification for image segmentation.
The Lov\'asz hinge has prediction dimension $k$ for this problem, whereas our construction would give one with $d = n = 2^k$, an impractical number even for relatively small images.
While one could achieve $d = n-1$ with a simple modification to our construction,\footnote{One can always reduce to $d=n-1$ in Theorem~\ref{thm:discrete-loss-poly-embeddable} via a linear transformation from $\reals^n$ to $\reals^{n-1}$ which is injective on $\simplex$; redefining the surrogate appropriately, the Bayes risks will still match.}
it is unclear when and how the prediction dimension could be further lowered.

Beyond studying convex elicitation complexity directly~\citep{ramaswamy2016convex,finocchiaro2021unifying,frongillo2021elicitation},
one promising approach to this question is to first understand the minimum $d$ for which a polyhedral surrogate $L:\reals^d\to \reals^\Y_+$ embeds $\ell$, called the \emph{embedding dimension} of $\ell$, and then relate this dimension to polyhedral, or general convex, elicitation complexity.
One reason this approach may be fruitful is that embeddings have much more structure than general convex losses, such as the fact that calibrated links arise automatically (\S~\ref{thm:thickened-separated}).
Yet from \S~\ref{sec:poly-ie-consistency} and similar observations, it may well be that the lowest possible prediction dimension is achieved by an embedding.

In previous work, we introduce and present some bounds for embedding dimension
based on optimality conditions \citep{finocchiaro2020embedding}.
We show in particular that a target loss has embedding dimension $1$ if and only if it hase convex elicitation complexity $1$, underscoring the possibility that these quantities may be the same for all discrete losses.
It is unclear if these bounds are tight or if they can be improved by leveraging information about adjacent level sets of an embedded property.
Moreover, beyond the fact that the embedding dimension upper bounds convex elicitation complexity, it remains to understand the relationship between these two quantities in dimensions greater than~$1$.


\paragraph{Indirect elicitation as a condition for consistency}

It is well-known that statistical consistency is equivalent to calibration (Definition~\ref{def:calibrated}) for discrete target problems.
As calibration is a much easier condition to work with, and in particular, only involves the conditional distributions, the bulk of work on consistency of learning algorithms for discrete target problems uses calibration.
It is easy to verify that calibration in turn implies indirect elicitation, meaning that exact minimizers of the surrogate loss are linked to exact minimizers of the target.
In \S~\ref{sec:poly-ie-consistency}, we show that indirect elicitation is actually \emph{equivalent} to calibration, and therefore consistency, when restricting to the class of polyhedral surrogates.
As indirect elicitation is even simpler of a condition than calibration, an important line of future work is to identify other classes of surrogates for which this equivalence holds.


\paragraph{Polyhedral vs.\ smooth surrogates}
The literature on convex surrogates focuses mainly on smooth surrogate losses~\citep{crammer2001algorithmic,bartlett2006convexity,bartlett2008classification, duchi2018multiclass, williamson2016composite, reid2010composite,menon2019multilabel,zhang2020convex,bao2020calibrated}.
In practice, minimizing such surrogates often implicitly fits a model to the full conditional label distributions.
On the other hand, \citet[Section 1.2]{ramaswamy2018consistent} contend that optimizing nonsmooth losses may enable reduction of the prediction dimension while maintaining consistency relative to smooth losses, improving downstream efficiency of the learning algorithm.%
\footnote{Polyhedral losses may be more challenging to optimize in some cases than smooth losses, so the prediction dimension may need to be much smaller, as in the BEP surrogate, until one sees a computational benefit.}
While generalization rates may suffer for nonsmooth losses, polyhedral surrogates achieve linear regret transfer bounds (\S~\ref{subsec:regret-bounds}), so the target generalization rates may remain the same; see also~\citet{frongillo2021surrogate}.
Even further, \citet{lapin2016loss} suggest that optimizing a nonsmooth loss that directly captures the target problem of interest, rather than a smooth one that implicitly fits to the full conditional label distributions, can improve performance in limited data settings.
We would like to verify this intuition, with specific cases or broad results comparing smooth and polyhedral losses.
  % \raf{Show that polyhedral losses are ``more efficient'' in a statistical sense?
  % See references recommended by Peter Bartlett.}
%Nevertheless, nonsmooth losses, such as the polyhedral losses we consider, have been proposed and studied for a variety of classification-like problems~\citep{yang2018consistency,yu2018lovasz,lapin2015top}.



\paragraph{Superprediction sets}

An interesting direction is to understand consistent surrogates by studying their superprediction sets, as has been done for proper losses~\citep{williamson2014geometry}.
The superprediction set of a loss is the set of loss vectors weakly dominated by the range of the loss: $\{v \in \reals^\Y \mid \exists r\; L(r) \leq v\}$, where the inequality holds pointwise.
One appealing aspect of the superprediction set is that it ignores the surrogate reports and focuses directly on the set of loss vectors, in a similar fashion to the trim operation in \S~\ref{sec:trim}.
In particular, it may be that questions about the required prediction dimension (see above) could be more readily answered by trying to find low-dimensional structures in the superprediction set of the target loss.


\paragraph{Convex envelope}

Finally, recall that we motivated the idea of an embedding as a way to ``convexify'' a discrete loss.
It is not clear, however, how embeddings relate to the convex envelope operation, which is perhaps the most direct way to perform this convexification given the map $\varphi$.
For example, suppose $L:\reals^d\to\reals^\Y_+$ embeds $\ell:\R\to\reals^\Y_+$ via the embedding $\varphi:\R\to\reals^d$, and consider the (polyhedral) surrogate $L':\reals^d\to\reals^\Y_+$ given by $L'_y = (L_y + \ones_{\varphi(\R)})^{**}$, where here $\ones$ denotes the convex indicator and $(\cdot)^{**}$ the biconjugate.
(One might also consider similar operations that keep $L'$ finite-valued.)
When is it the case that $L'$ also embeds $\ell$?
Conversely, we would like to know when the construction in Theorem~\ref{thm:discrete-loss-poly-embeddable} can be viewed as a convex envelope.


\subsection*{Acknowledgements}
We thank Arpit Agarwal and Peter Bartlett for many early discussions and insights,
Stephen Becker for a reference to Hoffman constants,
and Nishant Mehta, Enrique Nueve, and Anish Thilagar for other suggestions.\raft{others?}
This material is based upon work supported by the National Science Foundation under Grant Nos.\ CCF-1657598, IIS-2045347, and DGE-1650115.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}
\bibliography{diss,extra}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Power diagrams}\label{app:power-diagrams}
We begin with several definitions from Aurenhammer~\cite{aurenhammer1987power}.
\begin{definition}\label{def:cell-complex}
  A \emph{cell complex} in $\reals^d$ is a set $C$ of faces (of dimension $0,\ldots,d$) which (i) union to $\reals^d$, (ii) have pairwise disjoint relative interiors, and (iii) any nonempty intersection of faces $F,F'$ in $C$ is a face of $F$ and $F'$ and an element of $C$.
\end{definition}

\begin{definition}\label{def:power-diagram}
  Given sites $s_1,\ldots,s_k\in\reals^d$ and weights $w_1,\ldots,w_k \geq 0$, the corresponding \emph{power diagram} is the cell complex given by
  \begin{equation}
    \label{eq:pd}
    \cell(s_i) = \{ x \in\reals^d : \forall j \in \{1,\ldots,k\} \, \|x - s_i\|^2 - w_i \leq \|x - s_j\|^2 - w_j\}~.
  \end{equation}
\end{definition}

\begin{definition}\label{def:affine-equiv}
  A cell complex $C$ in $\reals^d$ is \emph{affinely equivalent} to a (convex) polyhedron $P \subseteq \reals^{d+1}$ if $C$ is a (linear) projection of the faces of $P$.
\end{definition}

Some of the convex polyhedra we study are the (negative) Bayes risks of loss functions, whose projections onto $\simplex$ form the level sets of the property they elicit.
The following result from~\citet{aurenhammer1987power} therefore immediately applies to elicitable properties.
We also make use of the same structure for the loss itself; in particular, one can consider the epigraph of a polyhedral convex function on $\reals^d$ and the projection down to $\reals^d$.
In either case, we refer to the resulting power diagram as being \emph{induced} by the convex function.

\begin{theorem}[Aurenhammer~\cite{aurenhammer1987power}]\label{thm:aurenhammer}
	A cell complex is affinely equivalent to a convex polyhedron if and only if it is a power diagram.
\end{theorem}

%\raf{Here's what I'm thinking for the text from here to Lemma~\ref{lem:poly-loss-poly-risk} (let's not implement just yet though): Let's move Lemma~\ref{lem:polyhedral-pd-same} to the appendix, and state Lemma~\ref{lem:polyhedral-range-gamma} but move the proof to the appendix.  Then we can keep the proof of Lemma~\ref{lem:poly-loss-poly-risk}.}
We extend Theorem~\ref{thm:aurenhammer} to a weighted sum of convex functions, showing that the induced power diagram is the same for any choice of strictly positive weights.

\begin{lemma}\label{lem:polyhedral-pd-same}
	Let $f_1,\ldots,f_m:\reals^d\to\reals$ be polyhedral convex functions.
	The power diagram induced by $\sum_{i=1}^m p_i f_i$ is the same for all $p \in \inter(\simplex)$.
\end{lemma}
\begin{proof}
	For any polyhedral convex function $g$ with epigraph $P$, the proof of~\citet[Theorem 4]{aurenhammer1987power} shows that the power diagram induced by $g$ is determined by the facets of $P$.
	Let $F$ be a facet of $P$, and $F'$ its projection down to $\reals^d$.
	It follows that $g|_{F'}$ is affine, and thus $g$ is differentiable on $\inter(F')$ with constant derivative $d\in\reals^d$.
	Conversely, for any subgradient $d'$ of $g$, the set of points $\{x\in\reals^d : d'\in\partial g(x)\}$ is the projection of a face of $P$; we conclude that $F = \{(x,g(x))\in\reals^{d+1} : d\in\partial g(x)\}$ and $F' = \{x\in\reals^d : d\in\partial g(x)\}$.
	
	Now let $f := \sum_{i=1}^k f_i$ with epigraph $P$, and $f' := \sum_{i=1}^k p_i f_i$ with epigraph $P'$.
	By Rockafellar~\cite{rockafellar1997convex}, $f,f'$ are polyhedral.
	We now show that $f$ is differentiable whenever $f'$ is differentiable:
	\begin{align*}
	\partial f(x) = \{d\}
	&\iff \sum_{i=1}^k \partial f_i(x) = \{d\} \\
	&\iff \forall i\in\{1,\ldots,k\}, \; \partial f_i(x) = \{d_i\} \\
	&\iff \forall i\in\{1,\ldots,k\}, \; \partial p_i f_i(x) = \{p_id_i\} \\
	&\iff \sum_{i=1}^k \partial p_if_i(x) = \left\{\sum_{i=1}^k p_id_i\right\} \\
	&\iff \partial f'(x) = \left\{\sum_{i=1}^k p_id_i\right\}~.
	\end{align*}
	From the above observations, every facet of $P$ is determined by the derivative of $f$ at any point in the interior of its projection, and vice versa.
	Letting $x$ be such a point in the interior, we now see that the facet of $P'$ containing $(x,f'(x))$ has the same projection, namely $\{x'\in\reals^d : \nabla f(x) \in \partial f(x')\} = \{x'\in\reals^d : \nabla f'(x) \in \partial f'(x')\}$.
	Thus, the power diagrams induced by $f$ and $f'$ are the same.
	The conclusion follows from the observation that the above held for any strictly positive weights $p$, and $f$ was fixed.
\end{proof}

We now include the full proof of Lemma~\ref{lem:polyhedral-range-gamma}.

\polyhedralrangegamma*
\begin{proof}
	First, observe that $L: \reals^d \to \reals^\Y_+$ is finite and bounded from below (by $0$), and thus its infimum is finite. 
	Therefore, we can apply \citet[Corollary 19.3.1]{rockafellar1997convex} to conclude that its infimum is attained for all $p \in \simplex$ and is therefore minimizable.
  Thus, $L$ elicits a property.
	
	For all $p$, let $P(p)$ be the epigraph of the convex function $u\mapsto \inprod{p}{L(u)}$.
	From Lemma~\ref{lem:polyhedral-pd-same}, we have that the power diagram $D_\Y$ induced by the projection of $P(p)$ onto $\reals^d$ is the same for any $p\in\inter(\simplex)$.
	Let $\F_\Y$ be the set of faces of $D_\Y$, which by the above are the set of faces of $P(p)$ projected onto $\reals^d$ for any $p\in\inter(\simplex)$.
	
	We claim for all $p\in\inter(\simplex)$, that $\Gamma(p) \in \F_\Y$.
	To see this, let $u \in \Gamma(p)$, and $u' = (u,\inprod{p}{L(u)}) \in P(p)$.
	The optimality of $u$ is equivalent to $u'$ being contained in the face $F$ of $P(p)$ exposed by the normal $(0,\ldots,0,-1)\in\reals^{d+1}$.
	Thus, $\Gamma(p) = \argmin_{u\in\reals^d} \inprod{p}{L(u)}$ is a projection of $F$ onto $\reals^d$, which is an element of $\F_\Y$.
	
	Now for $p \not \in \inter(\simplex)$, consider $\Y'\subsetneq \Y$, $\Y'\neq\emptyset$.
	Applying the above argument, we have a similar guarantee: a finite set $\F_{\Y'}$ such that $\Gamma(p) \in \F_{\Y'}$ for all $p$ with support exactly $\Y'$.
	Taking $\F = \bigcup\{\F_{\Y'} | \Y'\subseteq\Y, \Y'\neq\emptyset\}$, we have for all $p\in\simplex$ that $\Gamma(p) \in \F$, giving $\U \subseteq \F$.
	As $\F$ is finite, so is $\U$, and the elements of $\U$ are closed polyhedra as faces of $D_{\Y'}$ for some $\Y'\subseteq\Y$.
\end{proof}






\section{Equivalence of Separation and Calibration for Polyhedral Surrogates}
\label{sec:equiv-sep-calib}

We recall that Theorem \ref{thm:link-main} states that, if a polyhedral $L$ embeds a discrete $\ell$, then there exists a calibrated link $\psi$.
Theorem \ref{thm:link-main} is directly implied by the combination of Theorem \ref{thm:calibrated-separated}, that calibration is equivalent to separation (Definition \ref{def:sep-link}); and Theorem \ref{thm:thickened-separated}, existence of a separated link.
Theorem \ref{thm:calibrated-separated} is proven in this section and Theorem \ref{thm:thickened-separated} is proven in Appendix \ref{app:sep-link-exists}.

Throughout we will work with the two \emph{regret} functions:
the \emph{surrogate regret} $R_L(u,p) = \inprod{p}{L(u)} - \risk{L}(p)$, and similarly the \emph{target regret} $R_{\ell}(r,p) = \inprod{p}{\ell(r)} - \risk{\ell}(p)$.
We will use these functions again when we prove surrogate regret bounds (\S~\ref{app:regret-bounds}).
% In fact, the results in this section can be extended to surrogate regret bounds; see \citet{frongillo2021surrogate}.

We first show one direction: any calibrated link from a polyhedral surrogate to a discrete target must be $\epsilon$-separated.
The proof follows a similar argument to that of~\citet[Lemma 6]{tewari2007consistency}.
\begin{lemma}\label{lem:calibrated-eps-sep}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is calibrated with respect to $\ell$.
  Then there exists $\epsilon>0$ such that $\psi$ is $\epsilon$-separated with respect to   $\prop{L}$ and $\prop{\ell}$.
\end{lemma}
\begin{proof}
  Let $\Gamma := \prop{L}$ and $\gamma := \prop{\ell}$.
  Suppose that $\psi$ is not $\epsilon$-separated for any $\epsilon>0$.
  Then letting $\epsilon_i := 1/i$ we have sequences $\{p_i\}_i \subset \simplex$ and  $\{u_i\}_i \subset \reals^d$ such that for all $i\in\mathbb N$ we have both $\psi(u_i) \notin \gamma(p_i)$ and $d_\infty(u_i,\Gamma(p_i)) < \epsilon_i$.
  First, observe that there are only finitely many values for $\gamma(p_i)$ and $\Gamma(p_i)$, as $\R$ is finite and $L$ is polyhedral (from Lemma~\ref{lem:polyhedral-range-gamma}).
  Thus, there must be some $p\in\simplex$ and some infinite subsequence indexed by $j\in J \subseteq \mathbb N$ where
  for all $j\in J$, we have $\psi(u_j) \notin \gamma(p)$ and $\Gamma(p_j) = \Gamma(p)$.

  Next, observe that, as $L$ is polyhedral, the expected loss $\inprod{p}{L(u)}$ is $\beta$-Lipschitz in $\|\cdot\|_\infty$ for some $\beta>0$.
  \btw{RF: Could write $\beta$ explicitly; it's in my notes}
  Thus, for all $j\in J$, we have
  \begin{align*}
    d_\infty(u_i,\Gamma(p)) < \epsilon_j
    &\implies \exists u^*\in\Gamma(p)\; \|u_j-u^*\|_\infty < \epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \inprod{p}{L(u^*)} \right| < \beta\epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \risk{L}(p) \right| < \beta\epsilon_j~.
  \end{align*}
  Finally, for this $p$, we have
  \begin{align*}
    \inf_{u:\psi(u)\notin\gamma(p)} \inprod{p}{L(u)}
    \leq
    \inf_{j\in J} \inprod{p}{L(u_j)}
    =
    \risk{L}(p)~,
  \end{align*}
  contradicting the calibration of $\psi$.
\end{proof}

For the other direction, we will make use of Hoffman constants for systems of linear inequalities.
See \citet{zalinescu2003sharp} for a modern treatment.
\begin{theorem}[Hoffman constant \cite{hoffman1952approximate}]
  \label{thm:hoffman}
  Given a matrix $A\in\reals^{m\times n}$, there exists some smallest $H(A)\geq 0$, called the \emph{Hoffman constant} (with respect to $\|\cdot\|_\infty$), such that for all $b\in\reals^m$ and all $x\in\reals^n$,
  \begin{equation}
    \label{eq:hoffman}
    d_\infty(x,S(A,b)) \leq H(A) \|(A x - b)_+\|_\infty~,
  \end{equation}
  where $S(A,b) = \{x\in\reals^n \mid A x \leq b\}$ and $(u)_+ := \max(u,0)$ component-wise.
\end{theorem}

\begin{lemma}\label{lem:hoffman-polyhedral}
  Let $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral loss with $\Gamma = \prop{L}$.
  Then for any fixed $p$, there exists some smallest constant $H_{L,p} \geq 0$ such that $d_{\infty}(u,\Gamma(p)) \leq H_{L,p} R_L(u,p)$ for all $u \in \reals^d$.
\end{lemma}
\begin{proof}
  Since $L$ is polyhedral, there exist $a_1,\ldots,a_m \in \reals^d$ and $c\in\reals^m$ such that we may write $\inprod{p}{L(u)} = \max_{1\leq j\leq m} a_j \cdot u + c_j$.
  Let $A \in \reals^{m\times d}$ be the matrix with rows $a_j$, and let $b = \risk{L}(p)\ones - c$, where $\ones\in\reals^m$ is the all-ones vector.
  Then we have
  \begin{align*}
    S(A,b)
    &:= \{u\in\reals^d \mid A u \leq b\}
    \\
    &= \{u\in\reals^d \mid A u + c \leq \risk{L}(p)\ones\}
    \\
    &= \{u\in\reals^d \mid \forall i\, (A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \max_i \;(A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \inprod{p}{L(u)} \leq \risk{L}(p)\}
    \\
    & = \Gamma(p)~.
  \end{align*}
  Similarly, we have $\max_i\; (A u - b)_i = \inprod{p}{L(u)} - \risk{L}(p) = \regret{L}{u}{p} \geq 0$.
  Thus,
  \begin{align*}
    \|(Au - b)_+\|_\infty
    &= \max_i\; ((Au - b)_+)_i
    \\
    &= \max((Au - b)_1,\ldots,(Au - b)_m, 0)
    \\
    &= \max(\max_i\; (Au - b)_i, \, 0)
    \\
    &= \max_i\; (Au - b)_i
    % \\
    % &= \inprod{p}{L(u)} - \risk{L}(p)
    \\
    &= \regret{L}{u}{p}~.
  \end{align*}
  Now applying Theorem~\ref{thm:hoffman}, we have
  \begin{align*}
    d_\infty(u,\Gamma(p))
    &=    d_\infty(u,S(A,b))
    \\
    &\leq H(A) \|(Au-b)_+\|_\infty
    \\
    &= H(A) \regret{L}{u}{p}~.
  \end{align*}
\end{proof}


Given discrete loss $\ell: \R \to \reals_+^{\Y}$, define the constant $C_{\ell} = \max_{r,r' \in \R, y \in \Y} \ell(r)_y - \ell(r')_y$.
We are now ready to prove Theorem \ref{thm:calibrated-separated}.
\calibratedseparated*
\begin{proof}
  Let $\gamma=\prop{\ell}$ and $\Gamma=\prop{L}$.
  From Lemma~\ref{lem:calibrated-eps-sep}, calibration implies $\epsilon$-separation.
  For the converse, suppose $\psi$ is $\epsilon$-separated with respect to $L$ and $\ell$.
  Fix $p\in\simplex$.
  To show calibration, it suffices to find a positive lower bound for $R_L(u,p)$ that holds for all $u\in\reals^d$ with $\psi(u) \notin \gamma(p)$.
  
  Applying the definition of $\epsilon$-separated and Lemma~\ref{lem:hoffman-polyhedral}, $\psi(u) \notin \gamma(p)$ implies
  \begin{align*}
    \epsilon &\leq    d_{\infty}(u,\Gamma(p)) \leq H_{L,p} R_L(u,p) \implies 1 \leq \frac{H_{L,p}}{\epsilon} R_L(u,p)~.
  \end{align*}
  Let $C_{\ell} = \max_{r,p} R_{\ell}(r,p)$.
  Then $R_{\ell}(\psi(u),p) \leq C_{\ell} \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p)$.

  If $H_{L,p} = 0$, then for all $u\in\reals^d$ we have $R_\ell(\psi(u),p) = 0$, so calibration for this $p$ is trivial.
  \btw{In fact, we must also have $\Gamma(p) = \reals^d$ by Lemma~\ref{lem:hoffman-polyhedral}, since every $u\in\reals^d$ would have $d_\infty(u,\Gamma(p)) = 0$ and $\Gamma(p)$ is closed.}
  Similarly, if $C_\ell = 0$, then $R_\ell(r,p) = 0$ for all $r\in\R$, so again $R_\ell(\psi(u),p) = 0$ for all $u\in\reals^d$.

  Now assume $C_\ell > 0$ and $H_{L,p} > 0$.
  Let $C'_{\ell,p} \doteq \min_{r \notin \gamma(p)} R_\ell(r,p) > 0$.
  (As we assume $C_\ell > 0$, we must have $\gamma(p) \neq \R$, so the minimum is attained.)
  Then for all $u$ such that $\psi(u) \notin \gamma(p)$, we have $R_\ell(\psi(u),p) \geq C'_{\ell,p}$.
  Rearranging, we have
  \[ \psi(u) \notin \gamma(p) \implies R_L(u,p) \geq \frac{C'_{\ell,p} \epsilon}{C_\ell H_{L,p}} > 0~.\]
  Thus, $\inf_{u : \psi(u) \notin \gamma(p)} \inprod{L(u)}{p} > \risk{L}(p)$.
  Since the above holds for all $p\in\simplex$, $\psi$ is calibrated.
\end{proof}

%section{Omitted Proofs}\label{app:omitted-proofs}
% While Definition~\ref{def:loss-embed} gives the notion of one \emph{loss} embedding another, we now generalize the notion of one \emph{property} embedding another. \jessie{Merge with earlier definitions if we keep this section}
% \begin{definition}\label{def:prop-embed}
%   A property $\Gamma : \simplex \toto \reals^d$ embeds a property $\gamma:\simplex \toto \R$ on a set $\Sc$ if there exists some injective embedding $\varphi:\R \to \reals^d$ such that for all $p \in \simplex$ and $r \in \R$, we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
%   If there is a representative set $\Sc$ such that $\Gamma$ embeds $\gamma$ on $\Sc$, then we simply say $\Gamma$ embeds $\gamma$.
%   Moreover, we say a loss embeds a property when it embeds a loss eliciting the property.
% \end{definition}
% 
% 
% By condition (ii.) of Definition~\ref{def:loss-embed}, we then have $L$ embedding $\ell$ implies $\prop{L}$ embeds $\prop{\ell}$.
% However, Definitions~\ref{def:loss-embed} and~\ref{def:prop-embed} are not immediately equivalent because property embedding does not capture the requirement of matching losses on embedded points; i.e., that $L(\varphi(r)) = \ell(r)$ for all $r \in \R$.  
% However, Lemma~\ref{lem:embed-defs-equiv} shows an equivalence follows without loss of generality.
% 
% \begin{lemma}\label{lem:embed-defs-equiv}
%	% \raft{Introduce $L$}
%	Let $L : \reals^d \to \reals^\Y_+$ be a loss function whose infimum is attained in expectation for all $p \in \simplex$.
%	If the property $\Gamma := \prop{L}$ embeds the finite property $\gamma : \simplex \toto \R$, then there is a discrete loss $\ell:\R \to \reals^\Y_+$ such that $\ell$ elicits $\gamma$ and $L$ embeds $\ell$.
% \end{lemma}
% \begin{proof}
%   \raft{All the important pieces are here, but take it slower.  State $\ell$ as a definition (let $\ell$ be given by...) and show why $L$ embeds it.\jessie{Took another pass but changed technique, adding in this detail.  Now very similar to the proof of Prop 1 forward direction.}}
%   Consider the embedding $\varphi$ given by the property embedding and take the discrete loss $\ell:r \mapsto L(\varphi(r))$.
%   Now, we claim that $L$ embeds $\ell$, and by Proposition~\ref{prop:embed-bayes-risks}, we can simply show $\risk{L} = \risk{\ell}$, and the risks being polyhedral follows from $\ell$ being discrete.
%   
%   First, we show that for all $p \in \simplex$, we have $(1) \inf_{u \in \reals^d}\inprod{p}{L(u)} = (2) \inf_{r \in \R}\inprod{p}{L(\varphi(r))}$, and the equality of risks follows.
%   $(1) \leq (2)$ follows from the fact that $\varphi(\R) \subset \reals^d$ and definition of infimum. 
%   Consider that if we had $(1) < (2)$ for some $p \in \simplex$, then there would be no $r$ such that $\varphi(r) \in \Gamma(p)$, and therefore, we would have $\gamma(p) = \emptyset$, yielding a contradiction. 
%   This gives us $(1) = (2)$, so we have $\risk{L} = \risk{L|_{\varphi(\R)}} = \risk{\ell}$.
%	% 
%	% Now consider the property $\Gamma' := p \mapsto \Gamma(p) \cap \varphi(\R)$.
%	% We have $\Gamma'$ nondegenerate since $\Gamma$ embedding $\gamma$ implies that, for all $p \in \simplex$, there is some $r \in \R$ such that $\varphi(r) \in \Gamma(p)$.
%	% Therefore, for all $p \in \simplex$, we have $\inf_{r \in \R}\inprod{p}{L(\varphi(r))} = \inf_{r \in \R} \inprod{p}{\ell(r)}$, which yields $\risk{L|_{\varphi(\R)}} = \risk{\ell}$.
%	% Chaining the two equalities, we now have $\risk{L} = \risk{\ell}$.
%	
%	% To verify $\ell$ elicits $\gamma$, consider $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$ by embedding, $ \iff \varphi(r) \in \argmin_{u \in \reals^d} \inprod{p}{L(u)}$ by $L$ eliciting $\Gamma$, and $ \iff r \in \argmin_{r \in \R} \inprod{p}{\ell(r)}$ by applying the definition of properties to the embedding definition.
%	% \hrule
%	%	If $\Gamma$ embeds a finite $\gamma$, $L$ must embed a discrete loss $\ell$ such that $\ell(r) = L(\varphi(r))$ for all $r \in \R$.  
%	%	Moreover, we can see $\ell$ elicits $\gamma$ since we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p) \iff \varphi(r) \in \argmin_{u \in \reals^d} \inprod{p}{L(u)} \iff r \in \argmin_{r \in \R} \inprod{p}{\ell(r)}$.
%	% 
%	% \raft{Spell this out; I don't think these distributions are enough, for the same reason as above.  \jessie{Took a different approach of using Prop 1 to show the risks are equal.}}
%	%	The final condition of matching loss values is satisfied by considering the equality of the expected loss on each $\delta_y$ distribution.
% \end{proof}


%When working with convex surrogates which are not strictly convex, one quickly encounters redundant properties: if $\inprod{p}{L(\cdot)}$ is minimized by a point where $\inprod{p}{L}$ is differentiable and flat, then there will be an uncountably infinite set of reports which also minimize the expected loss.
%As results in property elicitation typically assume properties are non-redundant (e.g.~\cite{frongillo2014general,frongillo2015elicitation}), we must be careful when considering redundant reports.
% it is useful to consider a transformation which removes redundant level sets, captured by the \emph{trim} operation.

%\begin{definition}\label{def:trim}
%  Given an elicitable property $\Gamma:\simplex \toto\R$, we define $\trim(\Gamma) = \{\Gamma_u : u \in \R \text{ s.t. } \neg\exists u'\in\R,u'\neq u,\, \Gamma_u \subsetneq \Gamma_{u'}\}$ as the set of maximal level sets of $\Gamma$.
%\end{definition}
%\jessie{Do we want to re-define $\trim(\Gamma) = \{\Gamma_r \mid r \in \Sc\}$ for any minimum representative set $\Sc$?}

% \btw{RF: Note for later: should be able to show that the union of trim is the simplex.\jessie{This is part of the proposition statement 2 now.}}
% Take note that the unlabeled property $\trim(\Gamma)$ is non-redundant, meaning that for any $\theta \in \trim(\Gamma)$, there is no level set $\theta' \in \trim(\Gamma)$ such that $\theta \subset \theta'$; this is a corollary of Proposition~\ref{prop:embed-trim}.


%Before we state Proposition~\ref{prop:embed-trim}, we will need to general lemmas about properties and their losses.
%The first follows from standard results relating finite properties to power diagrams (see Theorem~\ref{thm:aurenhammer}), and its proof is omitted.
%The second is closely related to the definition of the trim operator: it states that if some subset of the reports are always represented among the minimizers of a loss, then one may remove all other reports and elicit the same property (with those other reports removed).
%
%\begin{lemma}\label{lem:finite-full-dim}
%  Let $\gamma$ be a finite (non-redundant) property elicited by a loss $L$.
%  Then the negative Bayes risk $G$ of $L$ is polyhedral, and the level sets of $\gamma$ are the projections of the facets of the epigraph of $G$ onto $\simplex$, and thus form a power diagram.
%  In particular, the level sets of $\gamma$ are full-dimensional in $\simplex$ (i.e.,\ of dimension $n-1$).
%\end{lemma}
%\jessie{Commented out 27 Sep 21, duplicate in above section}

%\btw{Commented out result about old trim def = new trim def - Jessie 2 June 21}
%\btw{JESSIE: Figures/an example might help clarify this}
%When a property $\Gamma$ embeds a finite property $\gamma$, we can show that the level sets of $\gamma$ correspond exactly to their embedded level sets of $\Gamma$, and that these embedded level sets are exactly $\trim(\Gamma)$.
%\jessiet{This lemma new to journal version}
%\begin{lemma}\label{lem:embedded-level-sets-trim}
%  Let $\Gamma$ be an elicitable property.
%  If $\Gamma$ embeds a (non-redundant) finite property $\gamma : \simplex \toto \R$ by the injection $\varphi$, then $\{\gamma_r : r \in \R\} = \{\Gamma_u : u \in \varphi(\R)\} = \trim(\Gamma)\proposedadd{=\trim(\gamma)}$.
%  \proposedadd{RETRY: Let $\Gamma$ be an elicitable property.
%    If $\Gamma$ embeds a finite property $\gamma : \simplex \toto \R$ by the injection $\varphi$, then for any minimum representative set $\Sc$ for $\gamma$, we have $\{\gamma_r : r \in \Sc\} = \{\Gamma_u : u \in \varphi(\Sc)\} = \trim(\Gamma)=\trim(\gamma)$.}
%\end{lemma}
%\begin{proof}
%  Let $L$ elicit $\Gamma$.
%  Take $\ell$ to be the loss embedded by $L$ from Lemma~\ref{lem:embed-defs-equiv}.
%  Moreover, by Proposition~\ref{prop:embed-bayes-risks}, we have $\risk{L} = \risk{\ell}$.
%  % \raft{Implicitly? Justify the claim.  \jessie{In definition of finite properties, we say that we assume we are talking about non-redundant.}}
%  As $\gamma$ is finite (and non-redundant by assumption), we know that each level set of $\gamma$ must be full-dimensional in $\affhull(\simplex)$. 
%  For all $r \in \R$, we know $\gamma_r = \Gamma_{\varphi(r)}$, so we must also have $\Gamma_{\varphi(r)}$ full-dimensional.
%  Moreover, we have $\{\gamma_r : r \in \R\} = \{\Gamma_{\varphi(r)} : r \in \R\}$ as a corollary since this is true for each $r \in \R$.
%  \raft{The main action is here.  (1) Epigraph of $-\risk{L}$. (2) There is really only one case: every level set is a face of the power diagram, which must be contained (weakly, so $\subseteq$) in a facet. (3) To show that level sets are faces, you'll want to appeal to some piece of a proof in the prev section.}
%  Since the cell $\Gamma_{\varphi(r)}$ is the projection of a facet of the epigraph of $-\risk{L}$, which is convex, any other level set intersecting $\Gamma_{\varphi(r)}$ can be considered as faces of the epigraph of $-\risk{L}$, and therefore a face of the induced power diagram.
%  Therefore, we remove lower-dimensional faces, as they are proper subsets of the level sets of embedded reports, and conclude such $\Gamma_u \not \in \trim(\Gamma)$.
%  % in one of two cases:
%  % First, if the level set $\Gamma_u$ (for $u \not \in \varphi(\R)$) is a projection of a lower-dimensional face of $\risk{L}$, which is contained in a facet of the epigraph.
%  % Therefore, we must have $\Gamma_u \subset \Gamma_{\varphi(r)}$ for some $r \in \R$, and therefore $\Gamma_u \not \in \trim(\Gamma)$.
%  
%  Second, $\Gamma_u = \Gamma_{\varphi(r)}$ for some $r \in \R$, then the level set is in both $\{\Gamma_{\varphi(r)} : r \in \R\}$ and $\trim(\Gamma)$.
%
%  \proposedadd{First, construct $\Sc := \Sc(L)$ as Definition~\ref{cons:rep-set}\jessiet{Argue there is a bijection from this set to any other minimum representative set}, which is a minimum representative set by Corollary~\ref{cor:poly-risk-fin-rep}. 
%    Moreover, we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$ by embedding, and therefore $\gamma_r = \Gamma_{\varphi(r)}$, yielding $\{\gamma_r \mid r \in \Sc\} = \{\Gamma_{\varphi(r)} \mid r \in \Sc\}$ as this is true for all $r \in \Sc$.}
%
%  \proposedadd{Now let $T := \{\gamma_r \mid r \in \Sc\} = \trim(\gamma)$. 
%    First, if $\gamma_r \in T$, then it is not a subset of any level set as $\gamma_r$ is full-dimensional in the simplex as it corresponds to a facet of the epigraph of the negative risk $E_\ell$, and the supporting hyperplane is therefore unique.
%    Now, if $\theta \in \trim(\gamma)$, then there is some report $r'$ such that $\theta = \gamma_{r'}$.
%    If $r' \not \in \Sc$, we want to claim there is some $r$ such that $\gamma_{r'} = \gamma_r$.
%    As $\theta$ is a maximal level set, it has a nonempty interior.
%    Therefore, for any $p \in \inter{\theta}$ (and one such $p$ must exist), the hyperplane $p \mapsto (p, \ell(r'))$ uniquely supports the epigraph of negative risk.
%    If $r' \not \in \Sc$, then there must be some $r \in \Sc$ such that $p \mapsto (p, \ell(r))$ also supports the same facet of $E_\ell$; if no such $r \in \Sc$ existed, we would contradict our claim that $\Sc$ is representative for $\ell$.
%    Similarly, we have $\{\Gamma_u \mid u \in \varphi(\Sc)\} = \trim(\Gamma)$ by the same logic.}
%\end{proof}

%\jessiet{03.01.21 - Old version of Prop~\ref{prop:embed-trim} moved to comment}
% \jessiet{Can we remove this?}
% \begin{proof}[Original]
%   Let $L$ elicit $\Gamma$.
%   
%   1 $\Rightarrow$ 2:
%   By the embedding condition, taking $\R_1 = \reals^d$ and $\R_2 = \varphi(\R)$ satisfies the conditions of Lemma~\ref{lem:loss-restrict}: for all $p\in\simplex$, as $\gamma(p) \neq \emptyset$ by definition, we have some $r\in\gamma(p)$ and thus some $\varphi(r) \in \Gamma(p)$.
%   Let $G(p) := -\min_{u\in\reals^d} \inprod{p}{L(u)}$ be the negative Bayes risk of $L$, which is convex, and $G_{\R}$ that of $L|_{\varphi(\R)}$.
%   By the Lemma, we also have $G = G_\R$.
%   As $\gamma$ is finite, $G$ is polyhedral.
%   Moreover, the projection of the epigraph of $G$ onto $\simplex$ forms a power diagram, with the facets projecting onto the level sets of $\gamma$, the cells of the power diagram.
%   (See Theorem~\ref{thm:aurenhammer}.)
%   As $L$ elicits $\Gamma$, for all $u\in\reals^d$, the hyperplane $p\mapsto \inprod{p}{L(u)}$ is a supporting hyperplane of the epigraph of $G$ at $(p,G(p))$ if and only if $u\in\Gamma(p)$.
%   This supporting hyperplane exposes some face $F$ of the epigraph of $G$, which must be contained in some facet $F'$.
%   Thus, the projection of $F$, which is $\Gamma_u$, must be contained in the projection of $F'$, which is a level set of $\gamma$.
%   We conclude that $\Gamma_u \subseteq \gamma_r$ for some $r\in\R$.
%   Hence, $\trim(\Gamma) = \{\gamma_r : r\in\R\}$, which is finite, and unions to $\simplex$.
%   
%   2 $\Rightarrow$ 3: let $\R = \{u_1,\ldots,u_k\} \subseteq\reals^d$ be a set of distinct reports such that $\trim(\Gamma) = \{\Gamma_{u_1},\ldots,\Gamma_{u_k}\}$.
%   Now as $\cup\,\trim(\Gamma) = \simplex$, for any $p\in\simplex$, we have $p\in\Gamma_{u_i}$ for some $u_i\in\R$, and thus $\Gamma(p) \cap \R \neq \emptyset$.
%   We now satisfy the conditions of Lemma~\ref{lem:loss-restrict} with $\R_1 = \reals^d$ and $\R_2 = \R$.
%   The property $\gamma:p\mapsto\Gamma(p)\cap\R$ is non-redundant by the definition of $\trim$, finite, and elicitable.
%   Now from Lemma~\ref{lem:finite-full-dim}, the level sets $\Theta = \{\gamma_r:r\in\R\}$ are full-dimensional, and union to $\simplex$.
%   Statement 3 then follows from the fact that $\gamma_r = \Gamma_r$ for all $r\in\R$.
%   
%   3 $\Rightarrow$ 1: let $\Theta = \{\theta_1,\ldots,\theta_k\}$.
%   For all $i\in\{1,\ldots,k\}$ let $u_i\in\reals^d$ such that $\Gamma_{u_i} = \theta_i$.
%   Now define $\gamma:\simplex\toto\{1,\ldots,k\}$ by $\gamma(p) = \{i : p\in\theta_i\}$, which is non-degenerate as $\cup\,\Theta = \simplex$.
%   By construction, we have $\gamma_i = \theta_i = \Gamma_{u_i}$ for all $i$, so letting $\varphi(i) = u_i$ we satisfy the definition of embedding, namely statement 1.
% \end{proof}






% \begin{lemma}\label{lem:fdls-in-trim}
%   If $\theta$ is a full-dimensional level set (in the simplex) of the elicitable property $\Gamma$, then $\theta \in \trim(\Gamma)$.
% \end{lemma}
% \begin{proof}
%   Let $u$ be the report such that $\theta = \Gamma_u$.
%   If there is a $u'$ so that $\Gamma_u = \Gamma_{u'}$, one of the level sets is in $\trim(\Gamma)$, and then we have $\theta$ equal to that level set.
%   If $\Gamma_u \subsetneq \Gamma_{u'}$, then either $\Gamma_u$ is not a full-dimensional level set or $\Gamma_u \cap \Gamma_{u'}$ is full-dimensonal.
%   If $\Gamma_u$ is not full-dimensional, then we contradict our assumption.
%   If $\Gamma_u \cap \Gamma_{u'}$ is full dimensional, then we contradict our elicitability condition (Lemma~\ref{lem:elicitable-level-sets}).
%   Therefore, full-dimensional level sets of an elicitable property must be in the trim operation of the same property.
% \end{proof}



\section{Surrogate Regret Bounds} \label{app:regret-bounds}

\subsection{Proof of Theorem~\ref{thm:linear-regret-bound}}
% The following is a direct result of Jensen's inequality, using the fact that $R_L(h;\D) = \E_X R_L(h(X),p_X)$ where $p_X$ is the conditional distribution on $Y$ given $X$.
% \begin{observation} \label{obs:transfer}
%   If $(L,\psi)$ guarantee a conditional regret transfer of $\zeta$ for $\ell$, and $\zeta$ is concave, then $(L,\psi)$ guarantee a regret transfer of $\zeta$ for $\ell$.
% \end{observation}

\begin{lemma}\label{lem:linear-on-levelset}
  Suppose $(L,\psi)$ indirectly elicits $\ell$ and let $\Gamma = \prop{L}$.
  Then for any fixed $u,u^* \in \reals^d$ and $r \in \R$, the functions $R_L(u,\cdot)$ and $R_{\ell}(r,\cdot)$ are linear in their second arguments on $\Gamma_{u^*}$.
\end{lemma}
\begin{proof}
  Let $u^* \in \reals^d$ and $p \in \Gamma_{u^*}$.
  By definition, for all $p \in \Gamma_{u^*}$, $\risk{L}(p) = \inprod{p}{L(u^*)}$.
  So for fixed $u$,
    \[ R_L(u,p) = \inprod{p}{L(u)} - \inprod{p}{L(u^*)} = \inprod{p}{L(u) - L(u^*)} , \]
  a linear function of $p$ on $\Gamma_{u^*}$.
  Next, by the definition of indirect elicitation, there exists $r^*$ such that $\Gamma_{u^*} \subseteq \gamma_{r^*}$.
  By the same argument, for fixed $r$, $R_{\ell}(r,p) = \inprod{p}{\ell(r) - \ell(r^*)}$, a linear function of $p$ on $\gamma_{r^*}$ and thus on $\Gamma_{u^*}$.
\end{proof}

Recall the definitions of $C_\ell$ and $H_{L,p}$ from \S~\ref{sec:equiv-sep-calib}.

\begin{lemma}\label{lem:separated-constant-p}
  Let $\ell: \R \to \reals_+^{\Y}$ be a discrete target loss, $L: \reals^d \to \reals_+^{\Y}$ be a polyhedral surrogate loss, and $\psi: \reals^d \to \R$ a link function.
  If $(L,\psi)$ indirectly elicit $\ell$ and $\psi$ is $\epsilon$-separated, then for all $u$ and $p$,
    \[ R_{\ell}(\psi(u),p) \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p) . \]
\end{lemma}
\begin{proof}
  If $\psi(u) \in \gamma(p)$, then $R_{\ell}(u,p) = 0$ and we are done.
  Otherwise, applying the definition of $\epsilon$-separated and Lemma~\ref{lem:hoffman-polyhedral},
  \begin{align*}
    \epsilon &<    d_{\infty}(u,\Gamma(p))  \\
             &\leq H_{L,p} R_L(u,p) .
  \end{align*}
  So $R_{\ell}(\psi(u),p) \leq C_{\ell} \leq \frac{C_{\ell} H_{L,p}}{\epsilon} R_L(u,p)$.
\end{proof}

We can now restate and prove Theorem~\ref{thm:linear-regret-bound}.
\begin{theorem}[Theorem~\ref{thm:linear-regret-bound}]
  Let $\ell: \R \to \reals_+^{\Y}$ be discrete, $L: \reals^d \to \reals_+^{\Y}$ polyhedral, and $\psi: \reals^d \to \R$.
  If $(L,\psi)$ are consistent for $\ell$, then there exists constants $\epsilon_\psi, H_L > 0$ such that
    \[ (\forall h,\D) \quad R_{\ell}(\psi \circ h ; \D) \leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(h ; \D) ~. \]
\end{theorem}
\begin{proof}
  Let $\Gamma = \prop{L}$.
  From Lemma~\ref{lem:X}, there is a finite set $U \subset \reals^d$ of predictions such that (a) for each $u \in U$, the level set $\Gamma_u$ is a polytope (see e.g.\ Lemma~\ref{lem:level-set-is-projected-face}), and (b) $\cup_{u \in U} \Gamma_u = \simplex$.
  For each $u\in U$ let $\mathcal{Q}_u \subset \simplex$ be the finite set of vertices of the polytope $\Gamma_u$, and define the finite set $\mathcal{Q} = \cup_{u \in U} \mathcal{Q}_u$.
  Let $H_L := \max_{q\in\mathcal{Q}} H_{L,q}$.
  
  By Lemma~\ref{lem:calibrated-eps-sep}, $\psi$ is $\epsilon$-separated for some $\epsilon>0$; let $\epsilon_{\psi} = \epsilon$.
  By Lemma~\ref{lem:separated-constant-p}, for each $q \in \mathcal{Q}$,
  \[ R_{\ell}(\psi(u),q) \leq \frac{C_{\ell} H_{L,q}}{\epsilon_{\psi}} R_L(u,q) \leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(u,q)\]
  for all $u\in\reals^d$.
  Now consider a general $p \in \simplex$, which is in some full-dimensional polytope level set $\Gamma_u$.
  Write $p = \sum_{q \in \mathcal{Q}_u} \beta(q) q$ for some convex combination $\beta \in \Delta_{\mathcal Q_u}$.
  By Lemma~\ref{lem:linear-on-levelset}, $R_L$ and $R_{\ell}$ are linear in the second argument on $\Gamma_u$, so for any $u'\in\reals^d$,
  \begin{align*}
    R_{\ell}(\psi(u'),p)
    &=    \sum_{q \in \mathcal{Q}_u} \beta(q) R_{\ell}(\psi(u'), q)  \\
    &\leq \sum_{q \in \mathcal{Q}_u} \beta(q) \frac{C_{\ell} H_{L}}{\epsilon_{\psi}} R_L(u', q) \\
    &\leq \frac{C_{\ell} H_L}{\epsilon_{\psi}} \sum_{q \in \mathcal{Q}_u} \beta(q) R_L(u', q)  \\
    &= \frac{C_{\ell} H_L}{\epsilon_{\psi}} R_L(u', p) .
  \end{align*}
  The result for $\D$ now holds by linearity of expectation over $\D$.
  % \raf{Replace this last line (and preamble above) with a direct calculation -- don't need the observation, since we are actually linear!}
  % By Observation~\ref{obs:transfer}, this conditional regret transfer implies a full regret transfer, with the same constant.
\end{proof}


\subsection{Tighter bounds}
\label{sec:regret-tighter-bounds}

Our goal in proving Theorem~\ref{thm:linear-regret-bound} is to show a broad result that consistent polyhedral losses always yield linear regret bounds.
As one may expect given the generality of the result, however, the specific constant we derive may be loose in some cases.
We now discuss some techniques to further tighten the constant.

Let us consider the tightest possible constant $c^*$ for which $R_{\ell}(\psi \circ h;\D) \leq c^* R_L(h;\D)$ for all $h$ and $\D$.
In general, for a fixed $p$, there is some smallest $c_p^*$ such that $R_{\ell}(\psi(u),p) \leq c_p^* R_L(u,p)$ for all $u$.
It therefore follows from our results that $c^* = \max_{p \in \mathcal{Q}} c_p^*$ for the finite set $\mathcal{Q}$ used in the proof, i.e., the vertices of the full-dimensional level sets of $\Gamma = \prop{L}$.

Above, we bounded $c_p^* \leq \frac{C_{\ell} H_{L,p}}{\epsilon_{\psi}}$.
The intuition is that some $u$ at distance $\geq \epsilon_{\psi}$ from $\Gamma(p)$, the optimal set, may link to a ``bad'' report $r = \psi(u) \not\in \gamma(p)$.
The rate at which $L$ grows is at least $H_{L,p}$, so the surrogate loss at $u$ may be as small as $\frac{\epsilon_{\psi}}{H_{L,p}}$, while the target regret may be as high as $C_{\ell} = \max_{r',p'} R_{\ell}(r',p')$.
The ratio of regrets is therefore bounded by $\frac{H_{L,p} C_{\ell}}{\epsilon_{\psi}}$.

The tightest possible bound, on the other hand, is $c_p^* = \sup_{u: \psi(u) \not\in \gamma(p)} \frac{R_{\ell}(\psi(u),p)}{R_L(u,p)}$.
This bound can be smaller if the values of numerator and denominator are correlated across $u$.
For example, $u$ may only be $\epsilon_{\psi}$-close to the optimal set when it links to reports $\psi(u)$ with lower target regret; or $L$ may have a smaller slope in the direction where the link's separation is larger than $\epsilon$.

To illustrate with a concrete example, consider the \emph{binary encoded predictions (BEP) surrogate} of \citet{ramaswamy2018consistent}, which we discuss in \S~\ref{sec:abstain}.
The target loss here is the abstain loss, $\ell(r,y) = \frac{1}{2}$ if $r = \bot$, otherwise $\ell(r,y) = \ones\{r \neq y\}$.
Letting $d = \ceil{\log_2 |\Y|}$, the BEP surrogate $L : \reals^d \to \reals^\Y_+$ is given by
$L(u)_y = \max_{j \in [d]} \left(1 - \varphi(y)_j u_j\right)_+$,
where $\varphi:\Y\to\{-1,1\}^d$ is an injection.
The associated link is $\psi(u) = \bot$ if $\|u\|_\infty \leq \tfrac{1}{2}$, otherwise $\psi(u) = \argmin_{y \in \Y} \|B(y) - u\|_{\infty}$.



One can show for $p = \delta_y$, the distribution with full support on some $y \in \Y$, that $L(u)_y = d_{\infty}(u,\Gamma(p))$ exactly, giving $H_{L,p} = 1$.
It is almost immediate that $\epsilon_{\psi} = \tfrac{1}{2}$.
Meanwhile, $R_{\ell}(r,p) \leq 1$, giving us an upper bound $c^*_p \leq \frac{(1)(1)}{1/2} = 2$.
The exact constant as given by \citeauthor{ramaswamy2018consistent}, however, is $c^* = 1$.
The looseness stems from the fact that for $p = \delta_y$, the closest reports $u$ to the optimal set, i.e., at distance only $\epsilon_{\psi} = \tfrac{1}{2}$ away, do not link to reports maximizing target regret; they link to the abstain report $\bot$, which has regret only $\tfrac{1}{2}$.
With this correction, and an observation that all $u$ linking to reports $y' \neq y$ are at distance at least $\tfrac{3}{2}$ from $\Gamma(p)$, we restore the tight bound $c^*_p \leq 1$.
A similar but slightly more involved calculation can be carried out for the other vertices $p \in \mathcal{Q}$, which turn out to be all vertices of the form $\tfrac{1}{2} \delta_y + \tfrac{1}{2} \delta_{y'}$.

Finally, while we use $\|\cdot\|_{\infty}$ to define the minimum-slope $H_L$ and the separation $\epsilon_\psi$, in principle one could use another norm.
One reason for restricting to $\|\cdot\|_\infty$ is that it is more compatible with Hoffman constants.
However, all definitions hold for other norms and so does the main upper bound, as existence of an $H_L$ and $\epsilon_{\psi}$ in $\|\cdot\|_{\infty}$ imply existence of constants for other norms.
These constants may change for different norms, and in particular, the optimal overall constant may arise from a norm other than $\|\cdot\|_\infty$.




\section{Existence of a Separated Link} \label{app:sep-link-exists}
In this section, we prove Theorem \ref{thm:thickened-separated} from \S~\ref{sec:calibration}, as discussed at the beginning of Appendix \ref{sec:equiv-sep-calib}: embeddings give rise to separated links.
The crux of the proof is showing that embeddings imply eq.~\eqref{eq:intersection-property-embedding}, the intersection condition on optimal sets, and that this condition is sufficient for the construction to produce a link.
Calibration then follows by the fact every link produced by Construction~\ref{const:eps-thick-link} is separated, and therefore calibrated.

In fact, we will show that the approach outlined above also suffices for the more general case where the given polyhedral surrogate indirectly elicits a given finite property.
This more general setting will allow us to prove the results from \S~\ref{sec:poly-ie-consistency}, and in particular, that indirect elicitation implies calibration for polyhedral surrogates.

To relate back to embeddings, we split the first phase into two: (i) an embedding is a special case of indirect elicitation (Lemma~\ref{lem:embedding-implies-indirect-elic}), and (ii) indirect elicitation is equivalent to the intersection condition (Lemma~\ref{lem:intersection-equiv-indirect-elic}).
We will then reason instead about Construction~\ref{const:general-eps-thick-link}, a generalization of Construction~\ref{const:eps-thick-link} for indirect elicitation.

\subsection{A more general construction}

As described in \S~\ref{sec:poly-ie-consistency}, the task of generalizing Construction~\ref{const:eps-thick-link} beyond embeddings reduces to carefully generalizing the definition of $R_U$.
Informally, $R_U$ in Construction~\ref{const:eps-thick-link} is the set of target reports which must be $\ell$-optimal whenever $U$ is $L$-optimal.
There we define $R_U$ simply as $\{r\in\Sc \mid \varphi(r) \in U\}$ where $\Sc$ is the given representative set.
For the more general case, we can define $R_U$ as follows.
\begin{definition}\label{def:general-link-defs}
  For polyhedral loss $L:\reals^d\to\reals^\Y_+$, and finite propert $\gamma:\simplex\toto\R$, we will define
  \begin{itemize}
  \item $\Gamma = \prop L$,
  \item $\U = \{\Gamma(p) \mid p \in \simplex\}$,
  \item $\Gamma_U := \{p\in\simplex \mid U = \Gamma(p)\}$ for all $U \in \U$,
  \item $R_U := \{r\in\R \mid \Gamma_U \subseteq \gamma_r\}$ for all $U \in \U$.
  \end{itemize}
\end{definition}

Construction~\ref{const:general-eps-thick-link} is essentially the same as Construction~\ref{const:eps-thick-link} but with the definition of $R_U$ above.

\begin{construction}[General $\epsilon$-thickened link] \label{const:general-eps-thick-link}
  Let $L:\reals^d\to\reals^\Y_+$, $\gamma:\simplex\toto\R$, $\epsilon > 0$, and a norm $\|\cdot\|$ be given, such that $L$ is polyhedral and indirectly elicits $\gamma$.
  Let $\U$ and $R_U$ be defined as in Definition~\ref{def:general-link-defs}.
  The \emph{$\epsilon$-thickened link} $\psi$ is constructed as follows.
  First, initialize the \emph{link envelope} $\Psi: \reals^d \to 2^{\R}$ by setting $\Psi(u) = \R$ for all $u$.
  Then for each $U \in \U$, for all points $u$ such that $\inf_{u^* \in U} \|u^*-u\| < \epsilon$, update $\Psi(u) = \Psi(u) \cap R_U$.
  If we have $\Psi(u)\neq\emptyset$ for all $u\in\reals^d$, then the construction \emph{produces a link} $\psi \in \Psi$ pointwise, breaking ties arbitrarily.
\end{construction}

It is straightforward to show that Construction~\ref{const:eps-thick-link} is a special case of Construction~\ref{const:general-eps-thick-link}.

\begin{lemma}\label{lem:general-construction-special-case}
  Let $L:\reals^d\to\reals^\Y_+$ be a polyhedral surrogate which embeds $\ell:\R\to\reals^\Y_+$ via the representative set $\Sc\subseteq\R$ and embedding $\varphi:\Sc\to\reals^d$.
  Then for any $\epsilon>0$ and norm $\|\cdot\|$,
  Construction~\ref{const:eps-thick-link} for $L,\ell,\epsilon,\|\cdot\|$ is equivalent to
  Construction~\ref{const:general-eps-thick-link} for $L,\prop{\ell|_\Sc},\epsilon,\|\cdot\|$.
\end{lemma}
\begin{proof}
  Let $\gamma = \prop{\ell|_\Sc}$, which is also given by $\gamma: p \mapsto \prop{\ell}(p) \cap \Sc$.
  Let $U\in\U$.
  Then for $r\in\Sc$, we have $r \in R_U \iff \Gamma_U \subseteq \gamma_r \iff \Gamma_U \subseteq \Gamma_{\varphi(r)} \iff (\Gamma(p) = U \implies \varphi(r) \in \Gamma(p)) \iff \varphi(r) \in U$.
  As we have $\R = \Sc$ in Construction~\ref{const:general-eps-thick-link} for $L,\prop{\ell|_\Sc},\epsilon,\|\cdot\|$, we conclude
  $R_U = \{r\in\Sc \mid \varphi(r) \in U\}$, exactly as in Construction~\ref{const:eps-thick-link} for $L,\ell,\epsilon,\|\cdot\|$.
  The equivalence of the two constructions follows.
\end{proof}

\subsection{Indirect elicitation and optimal set intersection}

We first show (i), that embedding is a special case of indirect elicitation.
\begin{lemma}\label{lem:embedding-implies-indirect-elic}
  If $L$ embeds $\ell$, then $L$ indirectly elicits $\prop\ell$.
\end{lemma}
\begin{proof}
  Let $\Gamma = \prop L$ and $\gamma = \prop\ell$.
  From Proposition \ref{prop:embed-trim}, we have $\trim(\gamma) = \trim(\Gamma) =: \Theta$.
  By definition of trim, for any $u\in\reals^d$, we have some $\theta \in \Theta$ such that $\Gamma_u \subseteq \theta$.
  Since $\trim(\gamma) = \Theta$, we have some $r\in\R$ such that $\theta = \gamma_r$, giving $\Gamma_u \subseteq \gamma_r$.
\end{proof}

We next show (ii), the equivalence of indirect elicitation and the following intersection condition.

\begin{lemma}\label{lem:intersection-equiv-indirect-elic}
  Let $L:\reals^d\to\reals^\Y_+$ be polyhedral and $\gamma:\simplex\toto\R$ be a finite property.
  Let $\U$ and $R_U$ be defined as in Definition~\ref{def:general-link-defs}.
  Then $L$ indirectly elicits $\gamma$
  if and only if the following condition holds
  \begin{align}
    \label{eq:general-intersection-condition}
    \forall \U'\subseteq\U,\; \cap_{U\in\U'} U \neq \emptyset \implies \cap_{U\in\U'} R_U \neq \emptyset~.
  \end{align}
\end{lemma}
\begin{proof}
%  Let $\Gamma$ and $\Gamma_U$ be defined as in Definition~\ref{def:general-link-defs}.
  First assume $L$ indirectly elicits $\gamma$.
  As $\U$ is the range of $\Gamma$, we have for all $u\in\reals^d$ that $\Gamma_u = \cup\{\Gamma_U \mid U\in\U, u\in U\}$.
  Suppose $\cap_{U\in\U'} U \neq \emptyset$; let $u \in \cap_{U\in\U'} U$.
  As $u\in U$ for all $U\in\U'$, we have $\Gamma_U \subseteq \Gamma_u$ for all $U\in\U'$.
  By indirect elicitation, there exists some $r\in\R$ such that $\Gamma_u \subseteq \gamma_r$.
  Thus, for all $U\in\U'$, we have $\Gamma_U \subseteq \gamma_r$ and thus $r\in R_U$.
  We conclude $\cap_{U\in\U'} R_U \neq \emptyset$.

  For the converse, let $u\in\reals^d$.
  If $\Gamma_u = \emptyset$, then $\Gamma_u \subseteq \gamma_r$ for any $r\in\R$.
  Otherwise, $\Gamma_u \neq \emptyset$, and the set $\U'_u = \{U\in\U \mid u\in U\}$ is nonempty.
  Moreover, $\cap \U'_u \neq \emptyset$ as $u\in\cap \U'_u$.
  Eq.~\eqref{eq:general-intersection-condition} now gives some $r\in\cap\{R_U \mid U\in\U'_u\}$.
  By definition of $R_U$, for all $U\in\U'_u$ we have $\Gamma_U \emptyset \gamma_r$.
  Thus $\Gamma_u = \cup\{\Gamma_U \mid U\in\U'_u\} \subseteq \gamma_r$, showing indirect elicitation.
\end{proof}


\subsection{Convex geometry for separation}

Let some norm $\|\cdot\|$ on $\reals^d$ be given.
Given a set $T\subseteq\reals^d$ and a point $u\in\reals^d$, let $d(T,u) = \inf_{t \in T} \|t-u\|$.
Given two sets $T,T'\subseteq\reals^d$, let $d(T,T') = \inf_{t\in T, t' \in T'} \|t-t'\|$.
Finally, for $T\subseteq \reals^d$ and $\epsilon > 0$, let the ``thickening'' $B(T,\epsilon)$ be defined as
\[ B(T,\epsilon) = \{u \in \R' : d(T,u) < \epsilon \} . \]


The goal of this subsection is to prove the first part of step (iii): for small enough $\epsilon>0$, if any set of $\epsilon$-thickened optimal sets intersect, then the optimal sets themselves must intersect.
We will conclude that, for small enough $\epsilon$, the link envelope $\Psi$ is non-empty everywhere, meaning there will be legal choices left over for the link.
\begin{lemma}~\label{lem:thick-intersect}
  Let $\U$ be defined as in Definition~\ref{def:general-link-defs}.
  There exists $\epsilon_0 > 0$ such that, for any $0 < \epsilon \leq \epsilon_0$, for any subset $\{U_j : j \in \mathcal{J}\}$ of $\U$, if $\cap_j U_j = \emptyset$, then $\cap_j B(U_j,\epsilon) = \emptyset$.
\end{lemma}

The next few geometric results build to Lemma~\ref{lem:thick-intersect}.

\begin{lemma} \label{lem:enclose-halfspaces}
  Let $D$ be a closed, convex polyhedron in $\reals^d$.
  For any $\epsilon > 0$, there exists an \emph{open}, convex set $D'$, the intersection of a finite number of open halfspaces, such that
    \[ D \subseteq D' \subseteq B(D,\epsilon) . \]
\end{lemma}
\begin{proof}
  Let $S$ be the standard open $\epsilon$-ball $B(\{\vec{0}\},\epsilon)$.
  Note that $B(D,\epsilon) = D + S$ where $+$ is the Minkowski sum.
%  Now let $S' = \{u : \|u\|_1 < \delta\}$ be the open $\delta$ ball in $L_1$ norm.
%  By equivalence of norms in Euclidean space, \bo{cite} we can take $\delta$ small enough yet positive such that $S' \subseteq S$.
%  By Lemma \ref{lem:open-plus-closed-poly}, the Minkowski sum $D' = D + S'$ is an open polyhedron, i.e. the intersection of a finite number of open halfspaces.
  Now let $S' = \{u : \|u\|_1 \leq \delta\}$ be the closed $\delta$ ball in $L_1$ norm.
  By equivalence of norms in Euclidean space~\cite[Appendix A.1.4]{boyd2004convex}, we can take $\delta$ small enough yet positive such that $S' \subseteq S$.
  By standard results, the Minkowski sum of two closed, convex polyhedra, $D'' = D + S'$ is a closed polyhedron, i.e. the intersection of a finite number of closed halfspaces. (A proof: we can form the higher-dimensional polyhedron $\{(x,y,z) : x \in D, y \in S', z = x+y\}$, then project onto the $z$ coordinates.)

  Now, if $T' \subseteq T$, then the Minkowksi sum satisfies $D + T' \subseteq D + T$.
  In particular, because $\emptyset \subseteq S' \subseteq S$, we have
    \[ D \subseteq D'' \subseteq B(D,\epsilon) . \]
  Now let $D'$ be the interior of $D''$, i.e. if $D'' = \{x : Ax \leq b\}$, then we let $D' = \{x: Ax < b\}$.
  We retain $D' \subseteq B(D,\epsilon)$.
  Further, we retain $D \subseteq D'$, because $D$ is contained in the interior of $D'' = D + S'$.
  (Proof: if $x \in D$, then for some $\gamma$, $x + B(\{\vec{0}\},\gamma) = B(x,\gamma)$ is contained in $D + S'$.)
  This proves the lemma.
\end{proof}

\begin{lemma} \label{lem:thick-nonempty}
  Let $\{U_j : j \in \mathcal{J}\}$ be a finite collection of closed, convex sets with $\cap_{j\in\mathcal{J}} U_j \neq \emptyset$.
  Let $\delta > 0$ be given.
  Then there exists  $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, $\cap_j B(U_j,\epsilon) \subseteq B(\cap_j U_j, \delta)$.
\end{lemma}
\begin{proof}
  We induct on $|\mathcal{J}|$.
  If $|\mathcal{J}|=1$, set $\epsilon = \delta$.
  If $|\mathcal{J}|>1$, let $j\in\mathcal{J}$ be arbitrary, let $U' = \cap_{j'\neq j} U_{j'}$, and let $C(\epsilon) = \cap_{j' \neq j} B(U_{j'},\epsilon)$.
  Let $D = U_j \cap U'$.
  We must show that $B(U_j,\epsilon) \cap C(\epsilon) \subseteq B(D,\delta)$.
  By Lemma \ref{lem:enclose-halfspaces}, we can enclose $D$ strictly within a polyhedron $D'$, the intersection of a finite number of open halfspaces, which is itself strictly enclosed in $B(D,\delta)$.
  (For example, if $D$ is a point, then enclose it in a hypercube, which is enclosed in the ball $B(D,\delta)$.)
  We will prove that, for all small enough $\epsilon$, $B(U_j,\epsilon) \cap C(\epsilon)$ is contained in $D'$.
  This implies that it is contained in $B(D,\delta)$.

  For each halfspace defining $D'$, consider its complement $F$, a closed halfspace.
  We prove that $F \cap B(U_j,\epsilon) \cap C(\epsilon) = \emptyset$.
  Consider the intersections of $F$ with $U$ and $U'$, call them $G$ and $G'$.
  These are closed, convex sets that do not intersect (because $D$ in contained in the complement of $F$).
  So $G$ and $G'$ are separated by a nonzero distance, so $B(G,\gamma) \cap B(G',\gamma) = \emptyset$ for all small enough $\gamma$.
  And $B(G,\gamma) = F \cap B(U_j,\gamma)$ while $B(G',\gamma) = F \cap B(U',\gamma)$.
  This proves that $F \cap B(U_j,\gamma) \cap B(U',\gamma) = \emptyset$.
  By inductive assumption, $C(\epsilon) \subseteq B(U',\gamma)$ for small enough $\epsilon = \epsilon_F$.
  So $F \cap B(U_j,\gamma) \cap C(\epsilon) = \emptyset$.
  We now let $\epsilon_0$ be the minimum over these finitely many $\epsilon_F$ (one per halfspace).
\end{proof}

\begin{figure}
\caption{Illustration of a special case of the proof of Lemma \ref{lem:thick-nonempty} where there are two sets $U_1,U_2$ and their intersection $D$ is a point. We build the polyhedron $D'$ inside $B(D,\delta)$. By considering each halfspace that defines $D'$, we then show that for small enough $\epsilon$, $B(U_1,\epsilon)$ and $B(U_2,\epsilon)$ do not intersect outside $D'$. So the intersection is contained in $D'$, so it is contained in $B(D,\delta)$.}
\includegraphics[width=0.24\textwidth]{figs/separated-proof-2} \hfill
\includegraphics[width=0.24\textwidth]{figs/separated-proof-3} \hfill
\includegraphics[width=0.24\textwidth]{figs/separated-proof-4} \hfill
\includegraphics[width=0.24\textwidth]{figs/separated-proof-5}
\end{figure}

\begin{lemma} \label{lem:thick-empty}
  Let $\{U_j : j \in \mathcal{J}\}$ be a finite collection of nonempty closed, convex sets with $\cap_{j\in\mathcal{J}} U_j = \emptyset$.
  Then there exists  $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, $\cap_{j\in\mathcal{J}} B(U_j,\epsilon) = \emptyset$.
\end{lemma}
\begin{proof}
  By induction on the size of the family.
  Note that the family must have size at least two.
  Let $U_j$ be any set in the family and let $U' = \cap_{j' \neq j} U_{j'}$.
  There are two possibilities.

  The first possibility, which includes the base case where the size of the family is two, is the case $U'$ is nonempty.
  Because $U_j$ and $U'$ are non-intersecting closed convex sets, they are separated by some distance $\delta$.
  So $B(U_j, \delta/3) \cap B(U', \delta/3) = \emptyset$.
  By Lemma \ref{lem:thick-nonempty}, there exists $\epsilon'_0 > 0$ such that $\cap_{j'\neq j} B(U_{j'},\epsilon) \subseteq B(U', \delta/3)$ for all $0 < \epsilon \leq \epsilon'_0$.
  Pick $\epsilon_0 = \min\{\epsilon'_0,\delta/3\}$.
  Then for all $0 < \epsilon \leq \epsilon_0$, the intersection of $\epsilon$-thickenings is contained in the $(\delta/3)$-thickening of the intersection, which is disjoint from the $(\delta/3)$-thickening of $U_j$, which contains the $\epsilon$-thickening of $U_j$.

  The second possibility is that $U'$ is empty.
  This implies we are not in the base case, as the family must have three or more sets.
  By inductive assumption, for all small enough $\epsilon$ we have $\cap_{j' \neq j} B(U_{j'},\epsilon) = \emptyset$, which proves this case.
\end{proof}

The proof of Lemma~\ref{lem:thick-intersect} now follows: for each $\U'\subseteq\U$, Lemma \ref{lem:thick-empty} gives an $\epsilon_0(\U') > 0$; we take the minimum of $\epsilon_0(\U')$ over the finitely many choices of $\U'$.

\subsection{Separation of the general construction}

We now prove the main results for link construction from \S~\ref{sec:calibration} and \S~\ref{sec:poly-ie-consistency}.
Specifically, we show that indirect elicitation implies that Construction~\ref{const:general-eps-thick-link} produces a link, and moreover, a link is produced if and only if it is separated.
As we have established above that Construction~\ref{const:eps-thick-link} is a special case, the results specific to embeddings will follow.

\begin{proposition}\label{prop:general-eps-thick-produce}
  Let $L:\reals^d\to\reals^\Y_+$ be polyhedral and $\gamma:\simplex\toto\R$ be finite.
  If $L$ indirectly elicits $\gamma$, then there exists $\epsilon_0 > 0$ such that, for all $0 < \epsilon \leq \epsilon_0$, Construction~\ref{const:general-eps-thick-link} for $L$, $\gamma$, $\epsilon$, $\|\cdot\|_\infty$, produces a link.
\end{proposition}
\begin{proof}
  Fix a small enough $\epsilon_0$ as promised by Lemma~\ref{lem:thick-intersect}.
  Let $u\in\reals^d$ and $\U'_u = \{U\in\U \mid d_\infty(u,U) < \epsilon\}$.
  From Construction \ref{const:general-eps-thick-link}, we have $\Psi(u) = \cap \{R_U \mid U\in\U'_u\}$.
  Since $u\in\cap\{B(U,\epsilon) \mid U\in\U'_u\} $ by definition, Lemma~\ref{lem:thick-intersect} and our choice of $\epsilon$ give $\cap\U'_u \neq \emptyset$.
  By Lemma~\ref{lem:intersection-equiv-indirect-elic}, we have $\Psi(u) = \cap \{R_U \mid U\in\U'_u\} \neq \emptyset$.
  % Now consider any $p\in\simplex$ and $U=\Gamma(p)$ such that $d_{\infty}(u,U) < \epsilon$.
  % Then $\psi(u) \in \Psi(u) \subseteq R_U = \{r \in \R \mid \Gamma_U \subseteq \gamma_r\}$.
  % By definition, $p\in\Gamma_U$, and thus $p\in\gamma_{\psi(u)}$, or equivalently, $\psi(u) \in \gamma(p)$.
\end{proof}


Perhaps surprisingly, one can also show that \emph{every} calibrated link from a polyhedral surrogate to a discrete loss is produced by Construction \ref{const:eps-thick-link}.
This result follows from the fact, stated now, that Construction~\ref{const:eps-thick-link} with $\|\cdot\|_\infty$ is exactly enforcing $\epsilon$-separation.
From Theorem~\ref{thm:calibrated-separated}, every calibrated link $\psi$ is therefore output by the construction for some sufficiently small $\epsilon$, in the sense that $\psi$ is one of the valid choices within the link envelope.

\begin{proposition}\label{prop:link-converse}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, finite property $\gamma:\simplex\toto\R$, and $\epsilon>0$ be given.
  Then Construction~\ref{const:general-eps-thick-link} for $L,\gamma,\epsilon,\|\cdot\|_\infty$ produces a link $\psi$ if and only if $\psi$ is $\epsilon$-separated.
\end{proposition}
\begin{proof}
  Let $\U'_u = \{U\in\U \mid d_{\infty}(u,U) < \epsilon\}$.
  Then we have the following chain of equivalences.  
  % For any $u\in\reals^d$, we have $\psi(u)\in\Psi(u)$ if and only if $\psi(u) \in R_U$ for all $U\in\U'_u$, which is in turn equivalent to $\Gamma_U \subseteq \gamma_{\psi(u)}$ for all $U\in\U'_u$.
  % Moreover, $\psi$ is $\epsilon$-separated if and only if we have $\psi(u)\in\gamma(p)$ for all $u\in\reals^d$ and $p\in\simplex$ with $d(u,\Gamma(p))<\epsilon$, which is equivalent to $\Gamma_U \subseteq \gamma_{\psi(u)}$ for all $u\in\reals^d$ and $U\in\U'_u$.
  \begin{align*}
    &\phantom{\iff}~\forall u\in\reals^d,\;\; \psi(u)\in\Psi(u)
    \\
    &\iff \forall u\in\reals^d,\, U\in\U'_u,\;\; \psi(u)\in R_U
    \\
    &\iff \forall u\in\reals^d,\, U\in\U'_u,\;\; \Gamma_U \subseteq \gamma_{\psi(u)}
    \\
    &\iff \forall u\in\reals^d,\, p\in\simplex \text{ s.t. } \Gamma(p)\in\U'_u,\;\; p \in \gamma_{\psi(u)}
    \\
    &\iff \forall u\in\reals^d,\, p\in\simplex \text{ s.t. } d_\infty(u,\Gamma(p)) < \epsilon,\;\; \psi(u) \in \gamma(p)
    \\
    &\iff \text{$\psi$ is $\epsilon$-separated}~.
  \end{align*}
\end{proof}

From the equivalence of calibration and separation for polyhedral surrogates (Theorem~\ref{thm:calibrated-separated}), we now have Theorem~\ref{thm:link-char}: the construction produces exactly the set of calibrated links.
(The move from $\|\cdot\|_\infty$ to a general norm follows from norm equivalence in finite-dimensional vector spaces.)
Combined with Proposition~\ref{prop:general-eps-thick-produce}, we also have Proposition~\ref{prop:general-eps-thick-separated}: if $L$ indirectly elicits $\gamma$, the construction produces a calibrated link.

Returning to embeddings, recall that Construction~\ref{const:eps-thick-link} is a special case of Construction~\ref{const:general-eps-thick-link} by Lemma~\ref{lem:general-construction-special-case}.
Moreover, embeddings are a special case of indirect elicitation (Lemma~\ref{lem:embedding-implies-indirect-elic}).
As Proposition~\ref{prop:general-eps-thick-produce} guarantees that Construction~\ref{const:general-eps-thick-link} produces a link, and Proposition~\ref{prop:link-converse} that every link produced is separated, we now have
Theorem \ref{thm:thickened-separated} which we restate.
(Note that the converse need not hold for Construction~\ref{const:eps-thick-link}, and indeed, that construction may miss some separated links which make use of reports outside $\Sc$.)
\thickenedseparated*

\section{Proving Lemma~\ref{lem:X}}
\label{app:polyhedra}

In this section, we give a careful treatment of the results on convex polyhedra needed to prove Lemma~\ref{lem:X}.

\subsection{General definitions for polyhedra}
\label{app:polyhedra:defs}

We begin with general definitions of polyhedra.
See also \citet{ziegler1993lectures} and \citet{gallier2008notes}.


\begin{definition}[Closed halfspace]
  A closed halfspace is a set of the form $H_{(w,b)}^+ := \{ x \in \reals^d \mid \inprod{x}{w} \geq b\}$ for some $(w,b) \in \reals^d \times \reals$.
  % Similarly let $H_{(w,b)}^- := \{ x \in \reals^d \mid \inprod{x}{w} \leq b\}$.
\end{definition}
\begin{definition}[Hyperplane]
  A hyperplane is a set of the form $H_{(w,b)} := \{ x \in \reals^d \mid \inprod{x}{w} = b\}$ for some $(w,b)\in\reals^d \times \reals$.
  % The halfspaces corresponding to a hyperplane $H = H_{(w,b)}$ are the sets $H_{(w,b)}^+$ and $H_{(w,b)}^-$.
\end{definition} 


Observe that $H_{(w,b)} = \partial H^+_{(w,b)}$, meaning the hyperplane $H_{(w,b)}$ is the boundary of $H^+_{(w,b)}$.
Thus, for any halfspace $H^+$, we have that $H^+$ is one of the two closed halfspaces corresponding to the hyperplane $\partial H^+ = H$.


\begin{definition}[Polyhedron halfspace representation~\citep{ziegler1993lectures}]
	A \emph{polyhedron} $P$ is an intersection of a finite set of closed halfspaces $\H$
  % = \{H^+_i\}_{i=1}^k$
  presented in the form $P = \cap \H$.
  Here, we say $\H$ is a halfspace representation for $P$.
\end{definition}

\noindent
Observe that by the halfspace representation, a polyhedron need not be bounded.


\begin{definition}[Supports]
	%A halfspace $H^+$ is \emph{valid} for $P$ if $P \subseteq H^+$.
	A hyperplane $H$ \emph{supports} the polyhedron $P$ if 
	% (i) $P \subseteq H^+$ for a halfspace $H^+$ corresponding to $H$
	(i) $P \subseteq H^+$ for a halfspace $H^+$ with $H = \partial H^+$, and
	(ii) $H \cap \partial P \neq \emptyset$.
	Moreover, $H$ supports $P$ at $x$ if $x \in H \cap \partial P$.
\end{definition}


\begin{definition}[Face, facet]\label{def:face}
  Let $P \subseteq \reals^d$ be a convex polyhedron.
  A \emph{face} $F$ of the polytope $P$ is any set of the form
  \begin{equation*}
    F = P \cap H~,
  \end{equation*}
  for a hyperplane $H$ supporting $P$.
  %valid halfspace $H^+_{(w,b)}$.
  The dimension of a face $F$ is the dimension of its affine hull $\dim(F) := \dim(\affhull(F))$.
  A face $F$ with $\dim(F) = \dim(\affhull(P)) - 1$ is called a facet.
%Let $P \subseteq \reals^d$ be a convex polyhedron.
%A halfspace $H^+_{(w,b)}$ is \emph{valid} for $P$ if $P \subseteq H^+_{(w,b)}$.
%% A linear inequality $\inprod{w}{x} \geq b$ is \emph{valid} for $P$ if it is satisfied for all $x \in P$.
%A \emph{face} $F_{(w,b)}$ of the polytope $P$ is any set of the form
%\begin{equation*}
%F_{(w,b)} = P \cap H_{(w,b)}~,~
%\end{equation*}
%for any valid halfspace $H^+_{(w,b)}$.
%The dimension of a face $F$ is the dimension of its affine hull $\dim(F) := \dim(\affhull(F))$.
%A face $F$ with $\dim(F) = \dim(\affhull(P)) - 1$ is called a facet.
\end{definition}

While one traditionally considers $P$ to be a trivial face of itself, we exclude this case throughout.

%\begin{claim}
%  Let $F$ be a face of the polyhedron $P$, and let $H$ be a hyperplane such that $F = P \cap H$.
%  % such that $F_{(w,b)} = P \cap H_{(w,b)}$
%  Then $F$ is nonempty if and only if $H$ is a supporting hyperplane of $P$.
%  \raft{Did I interpret this statement correctly?  $H$ was not quantified before}
%\end{claim}

It is often useful to understand polyhedra in terms of their halfspace representations and the set of hyperplanes generating facets of $P$.
To find this set, we must first establish when a halfspace representation is irredundant for a given polyhedron, as this irredundant set corresponds to the facets of a polyhedron in a natural way.

\begin{definition}[Irredundant; adapted from {\citet{gallier2008notes}}]\label{def:irredundant}
	Let $P = \cap \H$ for a finite set of closed halfspaces $\H$ be a polyhedron.
	We say that $\cap \H$ is an \emph{irredundant decomposition} for $P$ (and $\H$ is irredundant for $P$) if $P$ cannot be expressed as $P = \cap \H'$ for some set of closed halfspaces $\H'$ such that $|\H'| < |\H|$, and redundant otherwise.
	Moreover, we call $\H$ irredundant for $P$ if $\cap \H$ is an irredundant decomposition of $P$.
\end{definition}

\citet{gallier2008notes} shows that every $d$-dimensional polyhedron $P \subseteq \reals^d$ has a unique and irredundant halfspace representation $\H^*$, and each $H^+ \in \H^*$ generates a facet of $P$.

\begin{theorem}[{\citet{gallier2008notes}}]\label{thm:polyhedron-uniquely-gen-facets}
  Given a $d$-dimensional polyhedron $P \subseteq \reals^d$, 
  (i) there is a unique irredundant and finite set of closed halfspaces $\H^*$ such that $P = \cap \H^*$, 
  (ii) $\{H \cap P \mid H^+ \in \H^*, H = \partial H^+\}$ is the set of facets of $P$, and
  (iii) for all finite sets of closed halfspaces $\H$ such that $P = \cap \H$, we have $\H^* \subseteq \H$.
\end{theorem}
\begin{proof}
	Since $P$ is $d$-dimensional in $\reals^d$, it therefore has nonempty interior.
	As $P$ has a finite halfspace representation, it must have a smallest halfspace representation $\H^*$.
	That is, $|\H^*| = \min \{|\H| : P = \cap \H, \H$ finite$\}$.
	As a smallest halfspace representation, $\H^*$ is irredundant by definition.
%	Since $P$ has a (finite) halfspace representation, if any representation $\cap \H$ was redundant, then its redundant representation would either be irredundant or have a smaller representation.
%	This can only iterate up to $|\H^*|$ times, so there must be some finite irredunant representation for $P$.\jessie{This feels way too spelled out for its lack of detail.}
	\citet[Proposition 4.5(i)]{gallier2008notes} then states that $\H^*$ is unique, giving (i).
	Additionally, (ii) is shown by~\citep[Proposition 4.5(ii)]{gallier2008notes}.
	

	It remains to show (iii).
	Let $\H$ be a finite set of closed halfspaces such that $P = \cap \H$.
  As noted in the last sentence of the proof of \citep[Proposition 4.5]{gallier2008notes},
  the hyperplanes defining facets are unique:
  if $F$ is a facet of $P$ and $H,H'$ are hyperplanes with $F = H \cap P = H' \cap P$, then it must be the case that $H = H'$.
	It therefore suffices to show that, for each facet $F$ of $P$, there is an $H^+ \in \H$ such that $F = H\cap P$.
  \citet[Proposition 3.17]{gallier2008notes} observes that, for all $x \in \partial P$, there exists some hyperplane $\hat H$ such that $\hat H$ supports $P$ at $x$.
	Since $x \in \relint(F)$ is in exactly one face of $P$, namely $F$, there must be a unique $H^+ \in \H$ such that $F = H \cap P$.
%  Let $\H$ be a finite set of closed halfspaces such that $P = \cap \H$.
%  By (ii), each $H^+ \in \H^*$  determines a facet of $P$.
%  If $H^+ \not \in \H$, then $\cap \H \neq P$ as convex polytopes are uniquely determined by their facets (cf.~\citep[pg. 455]{schneider2014convex})\raft{This citation is doing the heavy lifting---what exactly does it say?}, a contradiction.
%  Therefore, we must have $H^+ \in \H$ as well.
\end{proof}

\subsection{Specializing to $\reals^\Y \times \reals$}\label{appsubsec:notation}
Within this appendix, we use some self-contained notation to work in the function graph space $\reals^\Y \times \reals$.
We will later consider losses over a finite set of outcomes $\Y$; to make notation consistent, we use $\reals^\Y_+$ throughout as shorthand for $\reals^{|\Y|}_+$, and let $d := |\Y|+1$.

Given any $v \in \reals^\Y_+$, define $H^+_v := H^+_{(v, -1)} = \{(x, c) \in \reals^{\Y}_+ \times \reals \mid \inprod{v}{x} \geq c\}$.
%, where $(v, -1)\in\reals^\Y_+\times\reals$ defines a halfspace.
Similarly, we denote $H_y^+ := H_{(e_y, 0)}^+$ for any $y \in \Y$; the latter will help us restrict a constructed polyhedron to the nonnegative orthant. 
Extending to hyperplanes, we construct $H_v := H_{(v,-1)}$ and observe that $H_v = \partial H^+_v$ for $v \in \reals^\Y_+$ and define $H_y := H_{(e_y, 0)}$ so that $H_y = \partial H^+_y$.
Given a set $\V \subseteq \reals^d$, we let $\H_{\V} = \{H_v^+ \mid v\in\V\}$ denote the set of halfspaces generated by $\V$.
Similarly, let $\H_\Y = \{H_y^+ \mid y\in\Y\}$.
%If $\V$ and $\Y$ are understood from context, we may denote $\H := \H_\V \cup\H_\Y$.


For any $S \subseteq \reals^k$, let $\delta(\cdot \mid S):\reals^k \to \reals \cup \{\infty\}$ be the convex indicator function, given by $\delta(x \mid S) = 0$ if $x\in S$ and $\infty$ otherwise.
Throughout, we will work with a concave function $g_\V$ generated by a set $\V \subseteq \reals^\Y_+$ of the following form.
%Intuitively, one can think of $g_\V$ as a Bayes risk, where $\V$ is the report set, and the input acts like a label distribution.

\begin{definition}\label{def:g-finite}
  Given a set $\V \subseteq \reals^\Y_+$, define the function $g_\V : \reals^\Y \to \reals_+ \cup \{-\infty\}$ by
  \begin{align*}
    g_\V(x) = \inf_{v\in\V} \inprod{x}{v} - \delta(x \mid \reals_+^\Y)~. % = \min_{v\in\V} \inprod{x}{v} - \delta(x \mid \reals_+^\Y)~.~
  \end{align*}
\end{definition}
\noindent
We denote the hypograph of a function $g:\reals^\Y\to\reals\cup\{-\infty\}$ by $\hyp(g) = \{(x,c) \mid c \leq g(x)\} \subseteq \reals^\Y \times \reals$.

A first observation is that the region generated by the intersection of the $H^+_y$ halfspaces restricts the hypograph $g_\V$ to be finite only on the nonnegative orthant for any $\V \subseteq \reals^\Y_+$.
\begin{lemma}\label{lem:x-nonneg-orthant-iff-intersection-HY}
  $\cap \H_\Y = \reals^\Y_+ \times \reals$.
\end{lemma}
\begin{proof}
  The result follows if we show $x \in \reals^\Y_+ \iff (x,c) \in \cap \H_\Y$ for all $c \in \reals$.

  $\implies$
  Fix any $c \in \reals$.
  $x \in \reals^\Y_+ \iff x_y \geq 0$ for all $y \in \Y$.
  This means that for any $y \in \Y$, $(x,c) \in \{(x,c) \mid x_y \geq 0\} = H^+_y$.
  As $y$ and $c$ were arbitrary, this shows the forward direction.
  
  $\impliedby$
  $(x,c) \in \cap \H_\Y$ implies $x_y \geq 0$ for all $y \in \Y$, and therefore $x \in \reals^d_+$.	
\end{proof}


\subsection{Hypographs of extended Bayes risks}\label{appsubsec:phase2}
\noindent
We now apply this polyhedral perspective to the Bayes risk of a loss function, extended to ``unnormalized distributions'', i.e., all of $\reals^\Y_+$.
Given a minimizable loss function $L : \R \to \reals^\Y_+$, define the $1$-homogeneous extension of its Bayes risk as
$\risk{L}_+:\reals^\Y\to\reals\cup\{\infty\}$, $x \mapsto \inf_{r\in\R} \inprod{x}{L(r)} - \delta(x \mid \reals^\Y_+)$.
In other words, letting $L(\R) := \{L(r) \mid r\in\R\} \subseteq \reals^\Y_+$, we have $\risk L_+ = g_{L(\R)}$.
%
%Using the polyhedral perspective above, we can characterize the hypograph of $\risk L_+ = g_{L(\R)}$ and extract a \emph{finite} set $\V$ of ``active'' loss vectors, so that $\risk L_+ = g_\V$.
%Let $\H_{L(\R)} = \{H_v^+ \mid v\in L(\R)\}$.
Observe that $L(\R)$ and $\H_{L(\R)}$ may be infinite sets.  
%We find the existence of a finite $\V \subseteq L(\R)$ such that $g_\V = g_{L(\R)}$, and proceed to work with such a $\V$.
\begin{claim}\label{claim:gV-equals-riskL}
	Suppose we are given a minimizable $L : \R \to \reals^\Y_+$ with polyhedral extended risk $\risk L_+$.
	Then $\hyp(g_{L(\R)}) = \cap(\H_\Y \cup \H_{L(\R)})$.
	%Moreover, there is a finite $\V \subseteq L(\R)$ such that $g_{L(\R)} = g_{\V}$. %%$\hyp(g_{L(\R)}) = \cap (\H_{\V} \cup \H_\Y) = \hyp(g_{\V})$ and . 
\end{claim}
\begin{proof}
	Observe that $x \in \reals^\Y_+ \iff (x, c) \in \cap \H_\Y$.
	Let $x \in \reals^\Y_+$.
	\begin{align*}
	(x,c) \in \hyp(g_{L(\R)})
	&\iff g_{L(\R)}(x) \geq c & \text{definition of hypograph}\\
	&\iff \risk L_+(x) \geq c & \text{$g_{L(\R)}=\risk L_+$}\\
    &\iff \inprod{v}{x} \geq c \,\, \forall v \in {L(\R)} & \text{definition of $\risk L_+$} \\
    % as the infimum % over $v \in {L(\R)}$ of}\\
    % & & \text{the inner product with $x$ and minimizable;} \\
	&\iff (x,c) \in H^+_{v} \,\, \forall v \in {L(\R)} & \text{definition of $H^+_v$}\\
	&\iff (x,c) \in \cap \H_{L(\R)} & 
	\end{align*}
	Combining the two equalities, we have $\hyp(g_{L(\R)}) = \cap (\H_\Y \cup \H_{L(\R)})$.
	% The existence of a finite $\V \subseteq {L(\R)}$ follows as $\risk L_+$ polyhedral implies $\hyp(\risk L_+)$ is a polyhedron, which has a finite representation by definition. 

%	We now construct a finite $\V \subseteq L(\R)$ such that $g_\V = g_{L(\R)}$.
%  Since $\hyp(g_{L(\R)}) = \hyp(\risk L_+)$ is a polyhedron, we mae iterate over each of the (finitely many) faces of $\hyp(g_{L(\R)})$.
%	For each face $F^{(i)}$ consider $x^{(i)} \in \relint(F^{(i)})$.
%	There is some $v^{(i)} \in L(\R)$ such that $H_{v^{(i)}}$ supports $\hyp(g_{L(\R)})$ at $x^{(i)}$~\citep[Proposition 3.17]{gallier2008notes}.
%	Then $\V = \{v^{(i)}\}_i \subseteq L(\R)$ and is finite by construction.
%	To show that $g_{L(\R)} = g_\V$, we equivalently show their hypographs are equal.
%	% Let $\H_\V := \{H^+_{v}\}_{v \in \V}$.
%	% Observe that we can write $\hyp(g_{L(\R)}) = \cap (\H_{L(\R)} \cup \H_\Y)$, and $\hyp(g_\V) = \cap(\H_\V \cup \H_\Y)$.
%	Since each $H^+_{v^{(i)}}$ is a valid halfspace for $\hyp(g_{L(\R)})$, we immediately have $\hyp(g_{L(\R)}) \subseteq \hyp(g_{\V})$.
%	To observe the reverse inequality, consider that $\V$ was constructed via the faces of $\hyp(g_{L(\R)})$, which includes the facets.
%	As $x^{(i)}$ on the relative interior of a facet must be defined by a unique supporting hyperplane $v^{(i)}$, we have the set $\H_\V$ must be a superset of the set of hyperplanes defining the facets of $\hyp(g_{L(\R)})$.
%	Therefore, $g_{\V} \subseteq g_{L(\R)}$, and thus the two are equal.
%	\raft{TODO: We have to slow down here I think.  How do we know $v^{(i)} \in L(\R)$ and that a supporting hyperplane takes the form $H_{v^{(i)}}$?  Also, recall that the whole point of iterating over faces was that we haven't shown the existence of facets yet.  I'm thinking we may want to refactor after all, to show the existence of facets first.}
%	%Finally, $\hyp(\risk L_+) = \hyp(g_{L(\R)}) = \hyp(g_{\V})$ if and only if $\risk L_+ = g_{L(\R)} = g_{\V}$. 
\end{proof}


\subsection{Finding the unique smallest subset of loss vectors}


%The previous claim allows us to proceed while considering the finite set $\V$ rather than the full range $L(\R)$.
%In particular, the finiteness of $\V$ gives us a finite halfspace representation $\H$ with which to apply Theorem~\ref{thm:polyhedron-uniquely-gen-facets}.
%The unique smallest halfspace representation $\H^*\subseteq\H$ will then correspond to a unique $\V^*\subseteq\V$.

\begin{lemma}\label{lem:G-unique-facets-Hstar}
  %Given a finite set $\V \subset \reals^\Y_+$, define $\H = \H_\V \cup \H_\Y$.
  %\raf{This is the important general polyhedra result to cite/state.  Note: we'll have to work half-spaces of the form $H_{(w,\beta)}^+ = \{ z \in \reals^{n+1} \mid \inprod{z}{w} - \beta \geq 0\}$.}  
  Consider a loss $L : \reals^d \to \reals^\Y_+$ with polyhedral extended risk $\risk L_+$.
  There is a unique irredundant set $\H^*$ of closed halfspaces such that $\hyp(g_{L(\R)}) = \cap \H^*$. 
  Moreover, for each $H^+ \in\H^*$ and $H$ such that $H = \partial H^+$, the face $\hyp(g_{L(\R)}) \cap H$ is a facet. 
  Moreover, $\H_\Y \subseteq \H^*$. 
\end{lemma}
\begin{proof}
	As $g_{L(\R)}$ is nonnegative on $\reals_+^\Y$, the set $\hyp(g_{L(\R)})$ therefore contains $\{(x,c) \mid x\in\reals^\Y_+, c \leq 0\}$, which is $(|\Y| + 1)$-dimensional.
  Therefore $\hyp(g_{L(\R)})$ is full-dimensional.
	Take $\H^*$ to be the unique irredundant and finite set of closed halfspaces such that $\hyp(g_{L(\R)}) = \cap \H^*$ from Theorem~\ref{thm:polyhedron-uniquely-gen-facets}(i).
  Now, $\hyp(g_{L(\R)}) \cap H$ for any $H \in \H^*$ being a facet follows immediately from Theorem~\ref{thm:polyhedron-uniquely-gen-facets}(ii) and (iii).
  
  To show that $\H_\Y \subseteq \H^*$, it suffices from Theorem~\ref{thm:polyhedron-uniquely-gen-facets}(ii) to show that each $F_y := H_y \cap \hyp(g_{L(\R)})$ for $y\in\Y$ is a facet.
  From Claim~\ref{claim:gV-equals-riskL}, since $H_y^+ \in \H_\Y$, we have $\hyp(g_{L(\R)}) \subseteq H_y^+$.
  We have $F_y = H_{y} \cap \hyp(g_{L(\R)}) = \{(x,c) \mid x\in\reals^\Y_+, x_y=0, c \leq g_{L(\R)}(x)\} \supseteq \{(x,c) \mid x\in\reals^\Y_+, x_y=0, c \leq 0\}$ as $g_{L(\R)}(x) \geq 0$.
  Thus, $F_y$ is a nonempty face of $\hyp(g_{L(\R)})$, and contains a $|\Y|$-dimensional set, so must be a facet.
%  To see $\H_\Y \subseteq \H^*$, if there was a $y \in \Y$ such that $H^+_y = \{(x,c) \mid x_y \geq 0\}$ was not in $\H^*$, it is either because it is redundant or $H^+_y \not \subseteq \hyp(g_{L(\R)})$.
%  In the first case, there must be some $\epsilon > 0$ such that $\{(x,c) \mid x_y \geq \epsilon\} \in \H^*$, which cannot happen as $g_{L(\R)}$ is concave and finite (namely, nonnegative) at $x = e_y$.
%  In the second case, we have a point $x \not \in \reals^\Y_+$ with $g_{L(\R)}(x) > -\infty$.
%  This contradicts the fact that $g_{L(\R)}$ is only finite on $\reals^\Y_+$ by construction with the convex indicator. \raft{TODO: Couldn't follow this last line, since $\delta$ can be $\infty$\jessie{does it help that it's minus the convex indicator?} 6/20/2022: still don't get it, sorry :-)... \jessie{Yeah I changed the sentence entirely.  better? \# burn}}. 
\end{proof}

%We now show that the set $\H_\Y$ is contained in $\H^*$ so that we can separate the facets generated by $\H^*$ into a partition of vertical and non-vertical facets of $\hyp(g_\V)$.

%\jessie{Combine with Lemma 18 above?}
%\begin{lemma}\label{lem:HY-subset-Hstar}
%  Given a finite set $\V \subset \reals^\Y_+$, consider the unique finite set $\H^*$ as given by Lemma~\ref{lem:G-unique-facets-Hstar}. % such that $\hyp(g_\V) = \cap \H^*$ and $\H^* \subseteq (\H_\V \cup \H_\Y)$.
%  
%\end{lemma}
%\begin{proof}
%
%\end{proof}

We now use the above result about the unique irredundant halfspace decomposition of $\hyp(g_\V)$ to observe a unique finite set of loss vectors generating these halfspaces.

\begin{corollary}\label{cor:unique-set-loss-vectors-defining-facets}
  %Suppose we are given a finite set $\V \subset \reals^\Y_+$, and 
  Given a minimizable loss $L: \reals^d \to \reals^\Y_+$ with polyhedral extended risk, consider the unique irredundant set $\H^*$ given by Lemma~\ref{lem:G-unique-facets-Hstar}. %such that $\hyp(g_\V) = \cap \H^*$.
  Then
  (i) there is a unique finite set $\V^* \subseteq \reals^\Y_+$ such that $\H^* = \H_\Y \cup \H_{\V^*}$.
  (ii) $F_v := H_v \cap \hyp(g_{L(\R)})$ is a facet of $\hyp(g_{L(\R)})$ for each $v\in\V^*$.
  (iii) $g_{L(\R)}(x) = \min_{v \in \V^*}\inprod{v}{x} - \delta(x \mid \reals_+^\Y) = g_{\V^*}(x)$.
\end{corollary}
\begin{proof}
%  \raf{Argue that all facets of $G$ are an $H_y \cap G$ or some $H_v \cap G$.  Thus, we can define $\V^*$ by $\H_\Y \cup \H_{\V^*} = \H^*$.  (I.e., $\H^* \setminus \H_\Y = \H_{\V^*}$ for some $\V^*$.)}
%
  (i) Since $\hyp(g_{L(\R)})$ is full-dimensional, the facets of $\hyp(g_{L(\R)})$ are uniquely determined by the hyperplanes $H$ such that $H = \partial H^+$ and $H^+ \in \H^*$ by Lemma~\ref{lem:G-unique-facets-Hstar}.
  Any facet must then be some intersection of an $H_y \cap \hyp(g_{L(\R)})$ or $H_v \cap \hyp(g_{L(\R)})$.
  Since $\H_\Y \subseteq \H^*$ by Lemma~\ref{lem:G-unique-facets-Hstar}, take $\H_{\V^*} := \H^* \setminus \H_{\Y}$, and $\V^*$ the unique set generating $\H_{\V^*}$.
  (ii) Moreover, $H_v \in \H_{\V^*} \subseteq \H^*$ generates the facet $F_v = H_v \cap \hyp(g_{L(\R)})$ of $\hyp(g_{L(\R)})$ by Lemma~\ref{lem:G-unique-facets-Hstar}.
%
%  The moreover follows immediately since $\H_{\V^*} \subseteq \H^*$ and every $H^+ \in \H^*$ defines a facet by Lemma~\ref{lem:G-unique-facets-Hstar}.
%  (iii)  $\H_\Y \cup \H_{\V^*} = \H^*$ by (i), and $\H^* \subseteq \H = \H_\Y \cup \H_{\V}$ by Lemma~\ref{lem:G-unique-facets-Hstar}, ergo $\V^* \subseteq \V$.
  (iii)   The result holds if $\hyp(g_{L(\R)}) = \hyp(g_{\V^*})$.
  By construction, $\hyp(g_{L(\R)}) = \cap \H^* = \cap (\H_\Y \cup \H_{\V^*}) = \{(x,c) \in \reals_+^\Y \times \reals \mid \inprod{v^*}{x} \geq c$ for all $v^* \in \V^* \} = \hyp(g_{\V^*})$.
  The result follows from equality of the hypographs.
  %Thus $g_{L(\R)}$ can be written as $g_{L(\R)}(x) = \min_{v \in \V^*}\inprod{v}{x} - \delta(x \mid \reals^\Y_+) = g_{\V^*}(x)$. 
\end{proof}



%\begin{corollary}\label{cor:anything-gen-G-subset-Hstar}
%  Let $\V \subset \reals^\Y_+$ be a finite set, and $\V^*$ the unique finite set from Corollary~\ref{cor:unique-set-loss-vectors-defining-facets} such that $\H^* = \H_\Y \cup \H_{\V^*}$. %such that $\H := \H_\Y \cup \H_{\V}$ satisfies $\hyp(g_{\V}) = \cap \H$, and take $\V^*$ the unique set such that irredundant (wrt. $\hyp(g_\V)$) set $\H^* = \H_\Y \cup \H_{\V^*}$.  
%  Then $\V^* \subseteq \V$.
%\end{corollary}
%\begin{proof}
%  $\H_\Y \cup \H_{\V^*} = \H^*$ by Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}, and $\H^* \subseteq \H = \H_\Y \cup \H_{\V}$ by Lemma~\ref{lem:HY-subset-Hstar}, ergo $\V^* \subseteq \V$.
%\end{proof}

%Thus, we will introduce our first assumption for the existence of $\V^*$ given a finite $\V \subset \reals^\Y_+$.
%\begin{assumption}\label{assum:V-star-exists-finite}
%	Suppose we are given a finite set $\V \subset \reals^\Y_+$, and consider the unique irredundant set $\H^*$ from Corollary~\ref{cor:unique-set-loss-vectors-defining-facets} such that $\hyp(g_\V) = \cap \H^*$.
%	Moreover, $\V^* \subseteq \V$ is the unique finite set from Corollary~\ref{cor:anything-gen-G-subset-Hstar} such that $\H^* = \H_\Y \cup \H_{\V^*}$ is irredundant.
%\end{assumption}
%



The above establishes our primary assumption for the rest of this section.


\begin{assumption}\label{assum:V-star-exists-infinite}
	$L : \R \to \reals^\Y_+$ is a minimizable loss function such that $\risk L_+ = g_{L(\R)}$ is polyhedral.
	%$\V \subseteq L(\R)$ is a finite set such that $g_\V = g_{L(\R)}$.
	$\V^* \subseteq L(\R)$ is the unique finite set such that $g_{L(\R)} = g_{\V^*}$ and $\hyp(g_{L(\R)}) = \hyp(g_{\V^*}) = \cap(\H_{\V^*} \cup \H_\Y)$, the last of which is irredundant.
\end{assumption}

\begin{proposition}\label{prop:poly-risk-assumption-satisfied}
	Given a minimizable loss $L : \R \to \reals^\Y_+$ such that $\risk L_+$ is polyhedral, there exists a finite $\V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
\end{proposition}
\begin{proof}
	%Given $L$ with polyhedral $\risk L_+$, Claim~\ref{claim:gV-equals-riskL} gives a finite set $\V \subseteq L(\R)$ such that $g_{L(\R)} = g_\V$. 
	Consider the unique $\V^* \subseteq L(\R)$ that is given in Corollary~\ref{cor:unique-set-loss-vectors-defining-facets} such that $g_{L(\R)} = g_{\V^*}$.
	Finally, by construction in Lemma~\ref{lem:G-unique-facets-Hstar}, we additionally have $\H^* = \H_\Y \cup \H_{\V^*}$, and $\cap \H^*$ is irredundant.
%	WTS
%	(i) $\V^* \subseteq L(\R)$ -- done
%	(ii) $\V^*$ unique and smallest
%	(iii) $g_{L(\R)} = g_{\V^*}$
%	(iv) $\hyp(g_{L(\R)}) = \hyp(g_{\V^*}) = \cap (\H_{\V^*} \cup \H_\Y)$
\end{proof}



%%\raf{Need to use minimizable}
%If follows that for $\risk L_+$ polyhedral, that $\hyp(g_{L(\R)})$ is a polyhedron, and $(d+1)$-dimensional as $\{(x,c) \mid x \in \reals^\Y_+, c \leq 0\} \subseteq \hyp(g_{L(\R)})$.
%As any polyhedron, by definition, has a finite halfspace representation, we apply Theorem~\ref{thm:polyhedron-uniquely-gen-facets} to conclude that $\hyp(g_\V)$ has a unique irredundant halfspace representation $\cap \H^*$, where $\H^* = \H_\Y \cup \H_{\V^*}$ for the unique finite set $\V^*$.
%Moreover, we have $g_\V = g_{\V^*}$, as $\H^*$ is the unique minimum halfspace representation for $\hyp(g_\V)$.


%With this assumption in hand, we can show a few more statements involving $\V^*$ and how it relates to $\V$.
%For intuition, the construction of $\V^*$ will be helpful to consider as a minimum representative set in the proof of Lemma~\ref{lem:X}.

%\jessie{Can probably get rid of Claim~\ref{claim:finite-rep-set} since it's so short and only used a few times later...}
%\begin{claim}\label{claim:finite-rep-set}
%	Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, there is a finite set $\R^* \subseteq \R$ such that $L(\R^*) = \V^*$ (without duplicates).
%\end{claim}
%\begin{proof}
%	This follows immediately from the Assumption~\ref{assum:V-star-exists-infinite} as $\V^* \subseteq L(\R)$.
%\end{proof}


%\begin{claim}\label{claim:vstar-supporting-G}
%	\jessiet{We don't need $L$ in here for the result, but $\V$ follows from it... how to state?}
%	Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, for all $x\in\reals^\Y_+$, there exists $v^*\in\V^*$ such that $H_{v^*}$ supports $\hyp(g_\V)$ at $(x,g_\V(x))$.
%\end{claim}
%\begin{proof}
%	% \jessie{Notes in meeting:
%	%   For any $x \in \reals^d_+$, consider $v^* \in \argmin_{v \in \V} \inprod{v}{x}$.
%	%   Argue $G \subseteq H^+_{v^*}$ by definition.
%	%   Argue support: $g(x) = \inprod{v^*}{x}$.  WTS $(x,g(x)) \in G$.  Already have $(x,g(x)) \in H_{v^*}$.}
%	%
%	By Assumption~\ref{assum:V-star-exists-infinite}, we have $g_\V(x) = g_{\V^*}(x) = \inf_{v \in \V^*}\inprod{v}{x} = \min_{v \in \V^*}\inprod{v}{x}$ for $x \in \reals^\Y_+$.
%	In particular, consider a normal $v^* \in \argmin_{v \in \V^*}\inprod{v}{x}$; we claim that the hyperplane $H_{v^*}$ such that $H_{v^*} = \partial H^+_{v^*}$ supports $\hyp(g_\V)$ at $(x,g(x))$.
%	First, $\hyp(g_\V) \subseteq H^+_{v^*}$ by definition of $\hyp(g_\V)$ as the intersection of halfspaces including $H^+_{v^*}$.
%	Thus, it is just left to show that $(x, \inprod{v^*}{x}) \in H_{v^*} \cap \hyp(g_\V)$.
%	%Recall from \S~\ref{appsubsec:notation} that $H_{v^*} = \{(x,c) \mid \inprod{v^*}{x} = c\}$.
%	By definition of $g_\V$, we have $g_\V(x)= g_{\V^*}(x) = \inprod{v^*}{x}$, so $(x,g_\V(x)) \in H_{v^*}$.
%	Moreover, $(x,g_\V(x)) \in \hyp(g_\V) = \{(x,c) \mid g_\V(x) \geq c\}$ trivially since $g_\V(x) \geq g_\V(x)$.
%\end{proof}



\subsection{Extended Bayes risk to extended level sets: projecting from $\reals^d_+$ to $\reals^\Y_+$}\label{subsec:project-pi}

When the loss $L$ is clear, we denote the faces of $\hyp(g_{L(\R)})$ by $F_v := H_v \cap \hyp(g_{L(\R)})$.
We now define the projection $\pi:\reals^\Y\times \reals \to \reals^\Y, (x,c) \mapsto x$.


In this subsection, we will establish results about the dimensionality of extended level sets, and conditions under which subsets of these extended level sets cover the nonnegative orthant $\reals^\Y_+$.
As a first step, the extended level sets generated by $\V^*$ cover the nonnegative orthant.

\begin{corollary}\label{cor:projected-Vstar-faces-cover-pos-orthant}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	%For each $v \in \V^*$, let $F_v = F_v^{\hyp(g_{\V^*})}= H_v \cap \hyp(g_{\V^*})$. \jessiet{redundant?}
	Then $\cup_{v\in\V^*} \pi(F_v) = \reals^\Y_+$.
\end{corollary}

Moreover, the projection $\pi$ preserves dimension of faces.

\begin{claim}\label{claim:pi-preserves-dim}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}. %, let $F_v = H_v \cap \hyp(g_\V)$.
%	For all $v \in L(\R)$, define $F_v$ as the face of $g_{L(\R)}=g_{\V^*}$ generated by $H_v$.
	Then $\dim(F_v) = \dim(\pi(F_v))$.
\end{claim}
\begin{proof}
	\btw{See \url{http://homepages.cae.wisc.edu/~linderot/classes/ie418/lecture11.pdf} for def of AI}
	%\jessie{just need to argue that you never needed $c$ (i.e., $c$ was always just $g(x)$).  Maybe: take an affine basis, project, and show it's an affine basis.}
	Recall from Definition~\ref{def:face} that the dimension of a polytope to be the dimension of its affine hull.
	Suppose we are given $|\Y|+1$ affinely independent vectors $z_i$ in $F_v$. 
	We will show that their projections $\{\pi(z_i)\}_i$ are affinely independent, giving the result.

	Let $a_1 + \ldots + a_{|\Y|+1} = 0$, such that $\sum_i a_i \pi(z_i) = 0$.	
	As $z_i \in F_v \subseteq H_v$, we have some $\{x_i\}_i$ such that $z_i = (x_i, \inprod{v}{x_i})$ for all $i$.
  By assumption, then, $0 = \sum_i a_i \pi(z_i) = \sum_i a_i x_i$.
	Thus, $\sum_i a_i z_i = \sum_i a_i (x_i, \inprod{v}{x_i}) = (\sum_i a_i x_i, \inprod{v}{\sum_i a_i x_i}) = (0,0) = 0 \in \reals^{|\Y|+1}$.
  By affine independence of the $z_i$, we conclude $a_i = 0$ for all $i$.
  Thus, the set $\{\pi(z_i)\}_i$ is affinely independent.
\end{proof}


Since we preserve the dimension of these projected spaces, we can now study equivalence of projected faces of the hypograph and regions of support of $g_{\V^*}$ for any $v \in L(\R)$.

\begin{lemma}\label{lem:projected-faces-iff-support-iff-argmin}
Given $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, fix $x \in \reals^\Y_+$.
	For any $v \in L(\R)$, the following are equivalent: 
	
	
	(1) $(x,g_{\V^*}(x)) \in F_v$; % H_v \cap \hyp(g_\V)$
	
	(2) $\inprod{v}{x}= g_{\V^*}(x)$;
	
	(3) $v \in \argmin_{v' \in L(\R)} \inprod{v'}{x}$; and
	
	(4) $x \in \pi(F_v)$~.
\end{lemma}
\begin{proof}
	\begin{align*}
	(1) \quad \quad (x,g_{\V^*}(x)) \in F_v
	&\iff (x,g_{\V^*}(x)) \in \{(x',c) \in \hyp(g_{\V^*}) \mid \inprod{v}{x'} = c\} & \\
	&\iff  \inprod{v}{x} = g_{\V^*}(x) & (2)\\
	&\iff \inprod{v}{x} = \min_{v' \in L(\R)}\inprod{v'}{x} & \\
	&\iff v \in \argmin_{v' \in L(\R)}\inprod{v'}{x}~. & (3)
	\end{align*}
	This covers $1 \iff 2 \iff 3$.
	
	For $1 \iff 4$, the forward implication follows trivially by applying the definition of the projection $\pi$.
	For the reverse implication, consider some $x \in \pi(F_v)$.
	There must be a $c \in \reals$ so that $(x,c) \in F_v$.
	Expanding, this is actually saying $(x,c) \in \{(x',c') \in \hyp(g_{\V^*}) \mid \inprod{v}{x'} = c\}$.
	In particular, this is true when $c = \inprod{v}{x}$, which defines a face of $\hyp(g_{\V^*})$ at $x$ if any only if $\inprod{v}{x} = g_{\V^*}(x)$.
	Therefore, we have $(x, g_{\V^*}(x)) \in F_v$.
\end{proof}


%Taking $L,\V^*$ as in Assumption~\ref{assum:V-star-exists-infinite} and a face of $\hyp(g_{\V^*})$, $F_{v^*} = H_{v^*} \cap \hyp(g_{\V^*})$ for $v^* \in \V^*$, the projection $\pi$ preserves full-dimensionality.
\begin{claim}\label{claim:projected-Vstar-faces-full-dim}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For all $v \in \V^*$, $\pi(F_v)$ is full dimensional in $\reals_+^\Y$.
	\btw{$\Lambda$ is exactly the set of FDLS; Will lead to $\Theta_{\V^*}$ are exactly the full dimensional level sets over simplex (6).}
\end{claim}
\begin{proof}
	By Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}, $F_v$ is a facet of $\hyp(g_{L(\R)})$ in $\reals^d_+$, meaning it is $(d - 1)$-dimensional.
	Moreover, Claim~\ref{claim:pi-preserves-dim} states that the dimension of $F_v$ is preserved for each $v \in \V^*$.
	Thus, $d-1 = |\Y| = \dim(F_v) = \dim(\pi(F_v))$.
\end{proof}


Now we can observe a set of normals $\V'$ generates faces of $\hyp(g_{\V^*})$ whose projections cover $\reals^\Y_+$ if and only if the set contains $\V^*$.
This fact will translate to a set of reports being representative for a loss if and only if it contains a finite minimum representative set.
%\raf{For Lemma X: NTS $\cup_{v\in\V} \pi(F_v) = \reals^\Y_+ \implies \V^* \subseteq \V$.}
%\raf{If $G = \cap( \H_\Y \cup \H_\V)$...}


\begin{claim}\label{claim:Vprime-projected-faces-cover-iff-Vstar-subset-Vprime}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For $\V' \subseteq L(\R)$, we have
	$\cup_{v\in\V'} \pi(F_v) = \reals^\Y_+ \iff \V^* \subseteq \V'$.\btw{Will get us to REP $\iff$ $\V^* \subseteq L(R')$ (3).}
\end{claim}
\begin{proof}
	%For any $v \in \V$, define $F_v := F^{\hyp(g_{\V^*})}_v$.
	%We would like to show $\cup_{v\in\V'} \pi(F_v) = \reals^\Y_+ \iff \cap(\H_\Y \cup \H_{\V'}) = \hyp(g_\V) := G$, then the result follows by a corollary of the assumption. %apply Corollary~\ref{cor:anything-gen-G-subset-Hstar}.
	
	
	%Take any $x\in\reals^\Y_+$.
	%\raf{Argue that we have $\cap(\H_\Y \cup \H_{\V'}) \subseteq G$}
	%\raf{Another approach: assume $v^* \in \V^* \setminus \V'$.  Take $z = (x,g(x)) \in \relint(F_{v^*})$.  Argue that $x \not\in \cup_{v\in\V'} \pi(F_v)$.}
	
	($\implies$) 
	For contraposition, suppose $\V^* \not \subseteq \V'$.
	Then $\exists v \in \V^* \setminus \V'$.
	Observe that $\V^*$ is unique, $\H^*$ is irredundant by assumption, and $\pi(F_v)$ is full-dimensional in $\reals^\Y_+$ by Claim~\ref{claim:projected-Vstar-faces-full-dim}.
	Moreover, $\pi(F_v) \not \in \cup_{v' \in \V'} \pi(F_v)$, which implies $\cup_{v' \in \V'} \pi(F_v) \neq \cup_{v^* \in \V^*} \pi(F_{v^*}) = \reals^\Y_+$.
	
%Commented out March 10 2022 and replaced with above- Jessie
%	Suppose $\cup_{v\in\V'} \pi(F_v) = \reals^\Y_+$.
%	Fix $x \in \reals^\Y_+$.
%	Since $\V' \subseteq \V$, and $G = \cap (\H_\Y \cup \H_\V)$, we immediately have $G \subseteq \cap (\H_\Y \cup \H_{\V'})$.
%	Therefore, we just need to show the other direction of inclusion.
%	%  \raf{Need to add logic: know $G \subseteq \cap(\H_\Y \cup \cap \H_{\V'})$, so only need to show $\cap(\H_\Y \cup \cap \H_{\V'}) \subseteq G$}
%	By Lemma~\ref{lem:x-nonneg-orthant-iff-intersection-HY} we have $(x,c) \in \cap \H_\Y$ for all $c \in \reals$, so it is left to show that $(x,c) \in \cap \H_{\V'}$, which yields the desired result.
%	First, $(x,c) \in \cap \H_{\V'}$ implies $(x,c) \in H^+_{v'}$ for all $v' \in \V'$.
%	This implies $c \leq \inprod{v'}{x}$ for all $v' \in \V'$.
%	By Lemma~\ref{lem:projected-faces-iff-support-iff-argmin}, there exists a $v' \in \V'$ such that $(x,g_\V(x)) \in F_{v'}$, and $\inprod{v'}{x} = g_\V(x)$.
%	Therefore, we have $c \leq \inprod{v'}{x} = g_\V(x)$, and $(x,c) \in \hyp(g_\V)$ follows.
%	
%	%\raf{Want to show $c \leq g(x)$, then done by def $G$}
%	%\raf{Now $(x,c) \in \cap \H_{\V'} \implies (x,c) \in H^+_{v'} \implies c \leq \inprod{v'}{x} = g(x) \implies (x,c) \in G$}
%	
%	% ...$\leq \inprod{v}{x}$ for all $v \in \V$, so $(x,g(x)) \in \cap \H_\V \implies (x,g(x)) \in G = \cap (\H_\Y \cup \H_\V)$.
	
	($\impliedby$)
	Since $\V^* \subseteq \V'$, we immediately have $\cup_{v \in \V^*} \pi(F_v) \subseteq \cup_{v' \in \V'} \pi(F_{v'})$.
	Moreover, $\cup_{v \in \V^*} \pi(F_v) = \reals^\Y_+$ by Corollary~\ref{cor:projected-Vstar-faces-cover-pos-orthant}, so $\reals^\Y_+ \subseteq \cup_{v' \in \V'} \pi(F_{v'})$.
	As $g_{\V^*}$ is only finite on $\reals^\Y_+$ by construction, equality follows.
%commented out Mar 11 - Jessie
%	$\hyp(g_\V) = \cap (\H_\Y \cup \H_{\V'})$ if and only if $\hyp(g_\V) = \cap \H_{\V'}$ when restricting to $x \in \reals_+^\Y$.
%	This implies that for all $x \in \reals_+^\Y$, there exists a $v' \in \V'$ such that $(x,g(x)) \in H_{v'} \cap \hyp(g_\V) = F_{v'} \implies x \in \pi(F_{v'})$.
%	As this is true for all $x \in \reals_+^\Y$, we have $\cup_{v \in \V'} \pi(F_{v}) = \reals_+^\Y$.
	
\end{proof}



We now claim that a set of projected faces $\{F_v\}_{v \in \V'}$ for some $\V'$ will cover $\reals^\Y_+$ if and only if $\V^* \subseteq \V'$.
Given a finite set $\V^* \subset \reals^\Y_+$, we denote $\Lambda_{S} := \{\pi(F_v) \mid v \in S\}$ as the set of projected facets generated by $S$.

\begin{claim}\label{claim:Vprime-projected-faces-cover-iffprojected-faces-subsets}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $\R' \subseteq \R$ with $\V' := L(\R')$. 
	%Let $F_v := F^{\hyp(g_{L(\R)})}_v$ denote the face of $\hyp(g_{\V^*})$ generated by $v \in L(\R)$.
	%For $\V' \subseteq \V$, 
	We have $\cup_{v \in \V'} \pi(F_v) = \reals_+^\Y \iff \Lambda_{\V^*}  \subseteq \Lambda_{\V'}$.
	\btw{$1$-homogeneous version of rep iff $\Theta_{\V^*}$ subset of level sets generated by $\R'$.}
\end{claim}
\begin{proof}
	$\implies$
	The result follows if $\V^* \subseteq \V'$, which follows from the forward implication of Claim~\ref{claim:Vprime-projected-faces-cover-iff-Vstar-subset-Vprime}.
	Explicitly, for all $v \in \V^*$ we also have $v \in \V'$, so, $\pi(F_v) \in \Lambda_{\V^*} \cap \Lambda_{\V'} = \Lambda_{\V^*}$.
	%$\pi(F_v) \in \Lambda \cap \{\pi(F_v) \mid v \in \V'\} = \Lambda$.
	
	$\impliedby$
	If $\Lambda_{\V^*} \subseteq \Lambda_{\V'}$, then $\cup_{v^* \in \V^*} \pi(F_{v^*}) \subseteq \cup_{v \in \V'} \pi(F_v)$.
	By Corollary~\ref{cor:projected-Vstar-faces-cover-pos-orthant}, we have $\cup_{v^* \in \V^*} \pi(F_{v^*}) = \reals_+^\Y$, so $\reals_+^\Y \subseteq \cup_{v \in \V'} \pi(F_v)$.
	The other direction of subset inequality following from $\hyp(g_{\V^*})$ being finite only on $\reals_+^\Y$.	
\end{proof}

%Given $L, \V, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, consider $v = L(r)$ for any $r \in \R$.
%As $H_{v}^+ \in \H$, it supports $\hyp(g_\V)$ or it is redundant in $\H$ (or both).

%\jessie{Not referenced below}
%\begin{claim}\label{claim:faces-contained-in-facets}
%	Given $L, \V$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, if some $H_v \in \H$ supports $\hyp(g_\V)$, then $F_v := H_{v} \cap \hyp(g_\V)$ is a nonempty face of $\hyp(g_\V)$, and thus a subset of a facet. 
%\end{claim}
%\begin{proof}
%	Since $H_v$ supports $\hyp(g_\V)$, we know that $\hyp(g_\V) \subseteq H_v^+$ and $F$ is not empty.
%	Moreover, $H^+_v$ is valid, and we have $F_v = H_v \cap \hyp(g_\V)$ is a face of $\hyp(g_\V)$ by definition.
%	
%	% subset of facet
%	Moreover, the faces of $\hyp(g_\V)$ are all convex polyhedra as $\hyp(g_\V)$ is a polyhedron.
%	Any face of $\hyp(g_\V)$ is must then be a lower-dimensional face of a facet, and therefore a subset.
%	\btw{Gr\"unbaum Page 35, Section 3.1 statement 7}
%\end{proof}


%\btw{Lemmatta for Lemma showing (7) in Lemma \ref{lem:X}}
%\begin{claim}\label{claim:vface-subset-some-vstar-face}
%	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
%	For any $v \in L(\R)$, $F_{v} \subseteq F_{v^*}$ for some $v^* \in \V^*$. %must be a subset of one of the $\V^*$ facets $F_{v^*}$ (as opposed to the vertical ones)
%\end{claim}
%\begin{proof}
%	By Theorem~\ref{thm:polyhedron-uniquely-gen-facets} (ii), every $F_{v^*}$ is a facet of $\hyp(g_{L(\R)})$.
%	Moreover, every face of a convex polyhedron is contained in a facet.
%% Commented out June 27 2022
%%	Since $\hyp(g_{\V^*})$ is a polyhedron, each of its faces are convex polyhedra, and is also a face of some facet of $\hyp(g_{\V^*})$; these facets are defined by $\V^*$ and $\Y$ via Corollary~\ref{cor:unique-set-loss-vectors-defining-facets}.
%%	
%%	It then suffices to show, for $v \in L(\R)$ that if $F_v$ is a face of the facet $F_y$ for some $y\in \Y$, then it must also be a face of $F_{v^*}$ for a $v^* \in \V^*$; it suffices to show $F_v$ is not a facet, and thus $F_v \neq F_y$.
%%	Recall from the definition of a face that $F_v = \hyp(g_{L(\R)}) \cap H_v$ and $F_y = \hyp(g_{L(\R)}) \cap H_y$.
%%	As facets of a polyhedron are (uniquely) determined by hyperplanes and $F_y$ is a facet, then if $F_v$ is a facet we must have $v \in \V^*$, and $\V^*$ is constructed so that $\H_{\V^*} \cap \H_\Y = \emptyset$.
%%	Thus, $F_v \neq F_y$ by unique determination of facets.
%%	If $F_v$ is not a facet of $\hyp(g_{\V^*})$, then $F_v \neq F_y$ is immediate as $F_y$ is a facet of $\hyp(g_{\V^*})$. 
%%	In both cases, we have $F_v \neq F_y$, and the result follows.
%
%	
%	%Commented out 7 Jan 2022
%%	\jessiet{Can use a careful read}
%%	$\{F_{v^*}\}_{v^* \in \V^*}$ is exactly the set of facets of $\hyp(g_{\V^*})$ on $\reals_+^d$ by Theorem~\ref{thm:polyhedron-uniquely-gen-facets}.
%%	Any $k$-face is contained in a facet by Claim~\ref{claim:faces-contained-in-facets}.
%%	Since $\H = \cap (\H_{\V^*} \cup \H_\Y)$, we are done if $F_v$ is contained in a face of $\H_{\V^*}$ and need to consider the case where $F_v \in \H_\Y$.
%%	
%%	If $F_v$ is a subset of a vertical facet $F_y$, we would have $F_v = H_v \cap G \subseteq H_y \cap G$ for some $y \in \Y$.
%%	Expanding, this is $\{(x,g(x)) \mid \inprod{v}{x} = g(x)\} \subseteq \{(x,g(x)) \mid x_y = 0 \}$.
%%	While this can happen, it must be on the boundary of $\reals_+^d$, and must be at the intersection of $H_y$ and some other $H_{v^*}$ for $v^* \in \V^*$, since $g$ is continuous on $\reals^\Y_{+}$, and determined by $\V^*$ on $\reals^\Y_{++}$.
%%	\jessiet{Justification: $g$ is continuous on $\reals_+^\Y$, and there has to be a unique $v^*$ on $\reals^\Y_{++}$. More hand-wavy than I'd like}
%\end{proof}

%\begin{corollary}\label{cor:projected-faces-subset-projected-facet}
%	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}. %, and $F_v := H_v \cap \hyp(g_{\V^*})$ be a face of $\hyp(g_{\V^*})$.
%	For $v \in L(\R),v^* \in \V^*$ such that $F_v \subseteq F_{v^*}$, $\pi(F_{v}) \subseteq \pi(F_{v^*})$. 
%\end{corollary}

Now, we can conclude that projected facets generated by $L(\R)$ contain all other projected faces of $\hyp(g_{\V^*})$.
\begin{corollary}\label{cor:exists-vstar-projected-face-subset}
	\jessiet{BTW- removed previous statements that were separate and just put the proofs in here. burn if okay}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $v \in L(\R)$, there is a $v^* \in \V^*$ such that $\pi(F_v) \subseteq \pi(F_{v^*})$.
	\btw{shows (7) on $\reals^\Y_+$}
\end{corollary}
\begin{proof}
	First, observe that $F_v \subseteq F_{v^*}$ as $F_{v^*}$ is a facet by construction of $\V^*$ and Theorem~\ref{thm:polyhedron-uniquely-gen-facets} (ii).
	As each face of a polyhedron is contained in a facet, we have $F_v \subseteq F_{v^*}$.
	Moreover, $\pi(F_v) \subseteq \pi(F_{v^*})$ follows immediately as a corollary.
\end{proof}





\subsection{Translating to properties: projecting from $\reals^\Y_+$ to $\simplex$}\label{subsec:project-f}

% such that $f_\V(x) = g_\V(x)$ for all $x \in \simplex$.

\begin{definition}\label{def:f-polyhedral}
	%Let $f_\V:\reals^\Y\to\reals_+\cup\{-\infty\}$ be a polyhedral concave function with $\dom(f_\V) = \simplex$.
	For any polyhedral concave function $f_\V$ with domain on $\simplex$, we denote the function $f_\V(p) = \min_{v\in\V} \inprod{p}{v} - \delta(p \mid \simplex)$, where $\V \subset \reals^\Y_+$ is a finite set. 
\end{definition}


\noindent
We now make a few observations about the extensions of $1$-homogeneous polyhedral functions.

\begin{claim}\label{claim:extended-poly-risk-poly}
	For any minimizable function $L$ such that $\risk L$ is polyhedral, its extension $\risk L_+$ is also polyhedral.
\end{claim}
\begin{proof}
  We will think of $\risk L$ as defined $\risk L:\reals^\Y\to\reals_+\cup\{-\infty\}$  with $\dom(\risk L) = \simplex$.
For $p \in \simplex$, we know $\sum_i p_i = 1$, and can write any inner product $\inprod{p}{b} - \beta = \inprod{p}{b} - \inprod{p}{\beta \ones} = \inprod{p}{b - \beta \ones}$.
If $p \not \in \simplex$, then $\risk L(p) = -\infty$.
Moreover, since $\risk L$ is polyhedral, it is finitely generated \citep[Proposition 19.1.2]{rockafellar1997convex} and can be written
\newcommand{\B}{\mathcal{B}} 
\begin{align*}
\risk L(p) 
&= \min(\inprod{p}{b_1} - \beta_1, \ldots, \inprod{p}{b_k} - \beta_k) - \delta(p \mid \simplex)\\
&= \min(\inprod{p}{b_1 - \beta_1 \ones}, \ldots, \inprod{p}{b_k - \beta_k \ones}) - \delta(p \mid \simplex)\\
&= \min_{(b, \beta) \in \B} \inprod{p}{b - \ones \beta} - \delta(p \mid \simplex)~,
\end{align*}
where $\B = \{(b_i, \beta_i)\}_{i = 1}^k$ from the second line.
We claim that $\risk L_+(x) = \min_{(b, \beta) \in \B} \inprod{x}{b - \ones \beta} - \delta(x \mid \reals^\Y_+)$, as this form is clearly 1-homogenous and agrees with $\risk L$ on $\simplex$.
As $\B$ is a finite set, $\risk L_+$ is polyhedral.
% For any $x \in \reals^\Y_+$, consider its normalization $\bar x$ with $(\bar x)_i = \frac{x_i}{\|x\|_1}$ such that $\bar x \in \simplex$.
% As $\risk L_+$ is $1$-homogeneous, we observe $\risk L(x)_+ = \risk L_+(\|x\|_1 \bar x) = \|x\|_1 \risk L_+(\bar x) = \|x\|_1 \risk L(\bar x) \|x\|_1 = \min_{(b, \beta) \in \B} \inprod{\bar x}{b - \ones \beta}$ for all $x \in \reals^\Y_+$, which is polyhedral as the pointwise min over a finite set of linear functions. 
% \raft{Error in the conversion to $\risk L_+$: I think you need to multiply by $\|x\|_1$.  But maybe an easier route is to simply replace the $\delta(p \mid \simplex)$ with $\delta(x \mid \reals^\Y_+)$ and observe that (a) the result is 1-homogenous, and (b) agrees with $\risk L$ on $\simplex$, ergo, must be its 1-homog extension.}
\end{proof}

%\noindent
%The construction of $f_\V$ allows us to project $g_\V$ from $\reals^\Y_+$ to the $\simplex$.
%\begin{lemma}\label{cor:f-matches-g-on-simplex}
%  Consider a finite set $\V \subset \reals^\Y_+$.
%  For all $p \in \simplex$, we have $f_\V(p) = g_\V(p)$.
%\end{lemma}
%%Given the function $f_\V$, we consider $g_\V$ to be its extension and $L$ such that $\risk L_+ = g_\V$ and $\risk L = f_\V$.


\begin{claim}\label{claim:gV-and-fV}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	Then $\risk L$ is polyhedral (on the simplex) and $f_{\V^*} = \risk L$.
	Moreover, $f_{\V^*}(p) = g_{\V^*}(p)$ for all $p \in \simplex$.
\end{claim}


Now, we define the function $\theta(v) = \{p \in \simplex \mid \inprod{v}{p} = f_\V(p)\}$ as the level sets of the loss vector $v \in \V$. %, and the set $\Theta_{\V^*} := \{\theta(v) \mid v \in \V^*\}$ to be the set of level sets uniquely defined by a given set of loss vectors.
\begin{claim}\label{claim:theta-is-pi-cap-simplex}
  Consider $L, \V^*$ as in Assumption~\ref{assum:V-star-exists-infinite}.
  For all $v \in \V^*$, consider the face $F_v$ of $\hyp(g_{L(\R)})$.
  Then $\theta(v) = \pi(F_v) \cap \simplex$.
\end{claim}
\begin{proof}
  % Argue $\theta(v)$ is full-dim in $\simplex$ if and only if $\pi(F_v)$ is $|\Y|$-dim, if and only if $F_v$ is a facet. \raf{(2)}
  \btw{Moved previous proof outline to comment}

  Fix $p \in \simplex$.
  \begin{align*}
    p \in \theta(v)
    &\iff \inprod{v}{p} = f_{\V^*}(p) & \text{Definition of $\theta$}\\
    &\iff \inprod{v}{p} = \min_{v' \in {\V^*}}\inprod{v'}{p} & \text{$f_{\V^*}=g_{\V^*}$ on $\simplex$ (Claim~\ref{claim:gV-and-fV})}\\
    &\iff v \in \argmin_{v' \in {\V^*}}\inprod{v'}{p} &\\
    &\iff p \in \pi(F_v)~. & \text{Lemma~\ref{lem:projected-faces-iff-support-iff-argmin}}
  \end{align*}
\end{proof}


\subsection{Moving back from $\simplex$ to $\reals^\Y_+$}
Now that we have translated from $\reals^d_+$ to $\reals^\Y_+$ in \S~\ref{subsec:project-pi} and from $\reals^\Y_+$ to $\simplex$ in \S~\ref{subsec:project-f}, we take some final steps to prove Lemma~\ref{lem:X} by showing equivalences from $\simplex$ to $\reals^\Y_+$.
\begin{lemma}\label{lem:level-set-is-projected-face}
  Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
  For all $r \in \R$ with $v = L(r)$, $\Gamma_r = \theta(v) = \pi(F_{v}) \cap \simplex$.
\end{lemma}
\begin{proof}
  % Follows immediately from Lemma~\ref{lem:projected-faces-iff-support-iff-argmin}.
  Let us rewrite
  \begin{align*}
    \Gamma_r
    &= \{p \in \simplex \mid r \in \argmin_{r' \in \R} \inprod{L(r')}{p}\}\\
    &= \{p \in \simplex \mid v \in \argmin_{v' \in L(\R)} \inprod{v'}{p}\}\\
    &= \{p \in \simplex \mid \inprod{v}{p} = \min_{v' \in L(\R)}\inprod{v'}{p}\}\\
    &= \{p \in \simplex \mid \inprod{v}{p} = f_{\V^*}(p) \}\\
    &= \theta(v)~.
  \end{align*}
  The rest of the result follows from Claim~\ref{claim:theta-is-pi-cap-simplex}.
%  Now, we have $p \in \theta(v)$ if and only if,
%  \begin{align*}
%    p \in \theta(v)
%    &\iff p \in \{p' \in \simplex \mid \inprod{v}{p'} = f(p')\} & \text{Definition of $\theta$}\\
%    &\iff p \in \{p' \in \simplex \mid \inprod{v}{p'} = g(p')\} &\text {$f = g$ on $\simplex$ (Cor.~\ref{cor:f-matches-g-on-simplex})} \\
%    &\iff p \in \{p' \in \reals_+^\Y \mid \inprod{v}{p'} = g(p')\} \cap \simplex & \\
%    &\iff p \in \pi(F_v) \cap \simplex & \text{Definition of $\pi$, $F_v$}
%  \end{align*}
%  The result follows.
   
\end{proof}


\begin{lemma}\label{lem:g-1-homog}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	Then $g_{\V^*}(x) = \min_{v \in \V^*}\inprod{v}{x}$ is (positively) $1$-homogeneous.
\end{lemma}
\begin{proof}
	If $x \not \in \reals^\Y_+$, then $c g(x) = -\infty = g(cx)$ for any $c > 0$.
	If $x \in \reals^\Y_+$, then we have $g(cx) = \min_{v \in \V^*}\inprod{v}{cx} = c \min_{v \in \V^*}\inprod{v}{x} = c g(x)$ for any $c > 0$ by linearity of the inner product.
\end{proof}


Every minimizable loss $L$ elicits a unique property $\Gamma := \prop{L}$, and we can define the \emph{extended level set} $\bar \Gamma_r := \{x \in \reals^\Y_+ \mid \inprod{L(r)}{x} = \risk L_+(x)\}$.

\begin{lemma}\label{lem:levelset-to-extended-levelset}
	Consider $L: \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $r \in \R$ and $c > 0$, if $p \in \Gamma_r$, then $cp \in \bar \Gamma_r$. 
\end{lemma}
\begin{proof}
	Fix $r \in \R$ and $c > 0$.
	We have
	\begin{align*}
	p \in \Gamma_r
	&= \{p' \in \simplex \mid r \in \argmin_{r' \in \R} \inprod{L(r')}{p'} \} & \text {Definition of level set} \\
	&= \{p' \in \simplex \mid v \in \argmin_{v' \in L(\R)} \inprod{v'}{p'} \} &  \\
	&= \{p' \in \simplex \mid \inprod{v}{p'} = \min_{v' \in L(\R)} \inprod{v'}{p'} \} & \text {$L$ minimizable} \\
	&= \{p' \in \simplex \mid \inprod{v}{p'} = g_{\V^*}(p') \} & \text {Assumption~\ref{assum:V-star-exists-infinite}} \\
	&= \{p' \in \simplex \mid c \inprod{v}{p'} = c g_{\V^*}(p') \} &  \\
	&= \{p' \in \simplex \mid  \inprod{v}{cp'} = g_{\V^*}(cp') \} & \text {Lemma~\ref{lem:g-1-homog}} \\
	\implies cp
	&\in \{x \in \reals^\Y_+ \mid \inprod{v}{x} = g_{\V^*}(x)\} = \bar \Gamma_r~.~
	\end{align*}
\end{proof}

\begin{lemma}\label{lem:extended-levelset-equals-projected-face}
	Consider $L : \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	%For any $v \in L(\R)$, define the face $F_v$ of $\hyp(g_{\V^*})$.
	For any $r\in \R$ with $v = L(r)$, $\bar \Gamma_r = \pi(F_v)$.  
\end{lemma}
\begin{proof}
	\begin{align*}
	\bar \Gamma_r
	&= \{x \in \reals^\Y_+ \mid \inprod{L(r)}{x} = \risk L_+(x)\} & \text{Definition of $\bar \Gamma_r$}\\
	&= \{x \in \reals^\Y_+ \mid \inprod{L(r)}{x} = g_{L(\R)}(x)\} & \text{Assumption~\ref{assum:V-star-exists-infinite}}\\
	&= \{x \in \reals^\Y_+ \mid \inprod{v}{x} = g_{L(\R)}(x)\} & \text{$v = L(r)$}\\
	&= \pi(F_v) & \text{Since $F_v = \{(x,g_{L(\R)}(x)) \mid \inprod{v}{x} = g_{L(\R)}(x)\}$}\\
	\end{align*}
\end{proof}


\begin{claim}\label{claim:projected-faces-cover-RY-iff-representative}
	Consider $L : \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	For any $v \in L(\R)$, denote the face $F_v := H_v \cap \hyp(g_{\V^*})$.
	A set $\R' \subseteq \R$ with $\V' := L(\R')$ is representative for $L$ if and only if $\cup_{v \in \V'} \pi(F_v) = \reals^\Y_+$.  
\end{claim}
\begin{proof}
	%\raf{Need 1-homog here -- suggest separating out statement like cone(simplex cap pi(Fv)) = pi(Fv)}
	$\implies$
	This proof follows from three lemmas: first, we observe that $g_{\V^*}$ is $1$-homogeneous via Lemma~\ref{lem:g-1-homog}.
	Then we extend the notion of a level set $\Gamma_r$ to the nonnegative orthant $\bar \Gamma_r$, and show that any scalar transformation of a distribution in the level set is contained in the same (extended) level set via Lemma~\ref{lem:levelset-to-extended-levelset}.
	Finally, we show the extended level set is exactly the projection $\pi(F_v)$ in Lemma~\ref{lem:extended-levelset-equals-projected-face}.
	As a corollary, we chain the results to observe $\cup_{r \in \R'} \Gamma_r = \simplex \implies \cup_{r \in \R'} \bar \Gamma_r = \reals_+^\Y \implies \cup_{v \in L(\R')}\pi(F_v) = \reals^\Y_+$, yielding the forward implication.
	
	
	$\impliedby$
	Fix $p \in \simplex \subseteq \reals^\Y_+$.
	By the assumption, there is a $v \in \V'$ such that $p \in \pi(F_v)$.
	By Lemma~\ref{lem:level-set-is-projected-face}, we have $p \in \pi(F_v) \cap \simplex = \Gamma_r$ for the $r \in \R'$ such that $v = L(r)$.
	As this is true for all $p \in \simplex$, we have $\R'$ representative.
	
	
\end{proof}

\subsection{Proving Lemma~\ref{lem:X}}\label{subsec:lem-X-proof}
We now proceed with a few final lemmas that ultimately yield the proof of Lemma~\ref{lem:X}.

\begin{lemma}\label{lem:lemX1-rep-iff-subset-vectors}
	\btw{(1)}
	Consider $L : \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	A finite set $\R' \subseteq \R$ with $\V' = L(\R')$ is representative if and only if $\V^* \subseteq \V'$.
\end{lemma}
\begin{proof}
	Chain Claim~\ref{claim:projected-faces-cover-RY-iff-representative} and Claim~\ref{claim:Vprime-projected-faces-cover-iff-Vstar-subset-Vprime} to yield the result.
\end{proof}

Define $\Theta_{S} := \{\theta(v) \mid v \in S\}$; it follows that $\Theta_{\V^*}$ is exactly the set of level sets of the property elicited by $L$.

\begin{lemma}\label{lem:lemX-3-rep-iff-FDLS-subsets}
	\btw{(3)}
	Consider $L : \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	A finite set $\R' \subseteq \R$ with $\V' = L(\R')$ is representative if and only if $\Theta_{\V^*} \subseteq \Theta_{\V'}$.
\end{lemma}
\begin{proof}
	Chain Claim~\ref{claim:projected-faces-cover-RY-iff-representative} and Claim~\ref{claim:Vprime-projected-faces-cover-iffprojected-faces-subsets} to yield the result.
\end{proof}

With $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, we additionally let $\R^*$ be a set such that $\V^* := L(\R^*)$.
Such a set exists as $\V^* \subseteq L(\R)$ in Assumption~\ref{assum:V-star-exists-infinite}, though it is not necessarily unique.

\begin{corollary}
	Consider $L, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $\R^*$ such that $\V^* = L(\R^*)$. 
	Moreover, suppose $L$ elicits $\Gamma$.
	$\Theta_{\V^*} = \{\Gamma_r \mid r \in \R^*\}$.
\end{corollary}

\begin{lemma}\label{lem:fdls-exactly-theta}
	\btw{(6)}
	Consider $L : \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
	Moreover, let $\Gamma := \prop{L}$.
	$\Theta_{\V^*} = \{\Gamma_r \mid r\in\R, \dim(\Gamma_r) = |\Y|-1\}$.
	\btw{The set of full-dimensional level sets of $\Gamma$ is exactly $\Theta$.}
\end{lemma}
\begin{proof}
	From Claim~\ref{claim:projected-Vstar-faces-full-dim}, we know $\Lambda_{\V^*}$ is exactly the set of full-dimensional level sets in $\reals^\Y_+$.
	Each element of $\Lambda_{\V^*}$ is $\pi(F_v)$ for some $v \in \V^*$.
	Take $r \in \R^*$ so that $v = L(r)$.
	By Lemma~\ref{lem:level-set-is-projected-face}, we have $\theta(v) = \Gamma_r = \pi(F_v) \cap \simplex$ is full-dimensional relative to the simplex.
	The result follows.
\end{proof}


\begin{lemma}\label{lem:any-levelset-contained-in-minlevelset}
  Consider $L : \R \to \reals^\Y_+, \V^*$ satisfying Assumption~\ref{assum:V-star-exists-infinite}, and $\R^* \subseteq \R$ the set such that $\V^* = L(\R^*)$.
  Moreover, consider $\Gamma := \prop{L}$.
  For any $r \in \R$, there exists a $v^* \in \V^*$ such that $\Gamma_r \subseteq \theta(v^*)$.\btw{(7)}
\end{lemma}
\begin{proof}
  Take $v = L(r)$.
  %It suffices to show there is a $v^* \in \V$ such that $\theta(v) \subseteq \theta(v^*)$ by their equality in Claim~\ref{claim:level-set-is-projected-face}.
  By Corollary~\ref{cor:exists-vstar-projected-face-subset}, there is a $v^* \in \V^* \subseteq L(\R)$ such that $\pi(F_v) \subseteq \pi(F_{v^*})$.
  Therefore, $\pi(F_v) \cap \simplex \subseteq \pi(F_{v^*})\cap \simplex$.  
  We know $\theta(v) = \pi(F_v) \cap \simplex$ and similarly for $\theta(v^*)$ by Lemma~\ref{lem:level-set-is-projected-face}.
  The result follows.
  % Let $v = L(r) \in \V$.
  % By Claim \ref{claim:vface-subset-some-vstar-face}, the face $F_v$ must be a subset of some $F_{v^*}$ for $v^* \in \V^*$.
  % We must then have $\pi(F_v) \cap \simplex = \Gamma_r \subseteq \pi(F_{v^*}) \cap \simplex = \theta(v^*)$.
\end{proof}



We are now ready to apply the above to prove Lemma~\ref{lem:X}.
In particular, any loss $L$ satisfying the assumptions of Lemma~\ref{lem:X} has some $g_{L(\R)} = \risk{L}_+$ as in this section. 


\lemmaX*
\begin{proof}
	%\jessiet{Get to Assumption~\ref{assum:V-star-exists-infinite} quickly}
%As in this appendix, we consider $\V = L(\R)$ such that $g_\V = \risk L_+$.
%\jessie{$\risk L$ to $\risk L_+$.  Change Claim 6: if you have a minimizable loss and $\risk L$ is polyhedral, then $\risk L_+$ is polyhedral.}
Since $\risk L$ is polyhedral, so is $\risk L_+$ by Claim~\ref{claim:extended-poly-risk-poly}.
Therefore, we have satisfied the requirements of Proposition~\ref{prop:poly-risk-assumption-satisfied}, and can conclude there is a finite set $\V^*\subseteq L(\R)$ such that $\risk L_+ = g_{\V^*}$ satisfying Assumption~\ref{assum:V-star-exists-infinite}.
Hence, there is a finite set $\R^*$ such that $\V^* = L(\R^*)$. %, which is representative as $\risk L = \min_{v \in \V^*}\inprod{p}{v} - \delta\{p \mid \simplex\} = \min_{v \in L(\R^*)}\inprod{p}{v} - \delta\{p \mid \simplex\}$.
For all $x\in\reals^\Y_+$, there exists $v \in \V^*$ such that $\inprod{v}{x} = g_{\V^*}(x)$, and thus some $r\in\R^*$ such that $\inprod{L(r)}{p} = g_{\V^*}(x)$.
Thus, for all $p \in \simplex$, there exists $r \in \R^*$ such that $\risk L(p) = \risk L_+(p) = g_{\V^*}(p) = \inprod{L(r)}{p}$ and therefore $r \in \prop{L}(p)$.
As this is true for all $p \in \simplex$, $\R^*$ is representative for $L$.
%Since $\risk L$ is polyhedral and concave there exists a finite set $\V$ such that $\risk L = f_\V$ by Claim~\ref{claim:f-min-affine}.
%Moreover as $\V$ is finite, $g_{\V} = \risk L_+$ (by $1$-homogeneity following from the form of $g_{\V}$) is also polyhedral and concave, and $f_\V = g_\V$ on $\simplex$.
%Therefore, there is a $\V^* \subseteq \V$ such that $L$ and $\V^*$ satisfy Assumption~\ref{assum:V-star-exists-infinite} by Proposition~\ref{prop:poly-risk-assumption-satisfied}.
%Moreover, any set $\R$ such that $\V^* = L(\R)$ is a finite representative set for $L$, and such a set must exist by the existence and construction of $\V^*$.
%%Given the form of $\risk L$ written as a pointwise max of finitely many affine functions (which are loss vectors) in $\V$, the loss $L$ has a finite representative set. 
%%Moreover, by Claim~\ref{claim:gV-and-fV}, $g_\V = f_\V$ on the simplex. 


Now consider the itemized statements.
Lemma~\ref{lem:lemX1-rep-iff-subset-vectors} is exactly statement \eqref{item:X-rep-V}.
This immediately implies statement~\eqref{item:X-min-V}.
Moreover, Lemma~\ref{lem:lemX-3-rep-iff-FDLS-subsets} is exactly statement~\eqref{item:X-rep-Theta}, and again statement~\eqref{item:X-min-Theta} immediately follows.
Statement~\eqref{item:X-rep-contain-min} is a corollary of the existence of a finite representative set, which follows since $\V^* \subseteq L(\R)$.
Statement~\eqref{item:X-full-dim} is exactly Lemma~\ref{lem:fdls-exactly-theta}.
Statement~\eqref{item:X-redundant} is exactly Lemma~\ref{lem:any-levelset-contained-in-minlevelset}.
Finally, Statement~\eqref{item:X-tight-embed} follows as a corollary of statement~\eqref{item:X-min-V} and Corollary~\ref{cor:tight-embed-min-rep}.
\end{proof}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
