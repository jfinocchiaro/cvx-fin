\documentclass[a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage[draft]{hyperref}
% \usepackage{nohyperref}  % This makes hyperref commands do nothing without errors
\usepackage{url}  % This makes \url work

% WHERE HAS THIS BEEN ALL MY LIFE
\usepackage{xr}
\externaldocument{../poly-embed-journal}

\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below

\newcommand{\Comments}{1}
\definecolor{gray}{gray}{0.5}
\definecolor{lightred}{rgb}{1,0.6,0.6}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\jessie}[1]{\mynote{blue}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{blue!20!white}{JF: #1}}
\newcommand{\raf}[1]{\mynote{darkgreen}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}

%\newcommand{\response}[1]{\textcolor{blue!50!black}{Response: #1}}
\newenvironment{response}{\color{blue!50!black}}{\color{black}}

\begin{document}

\begin{center}
  {\Large Response to Reviews on JMLR-22-0743-1}
  
  (An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates)
\end{center}


We thank the editor and reviewers for their feedback and suggestions.
We believe we have addressed all concerns, and the suggestions have greatly improved the paper.
Below we respond to questions and comments as appropriate.

In addition to these changes, we made a few others to address a surprisingly close connection to Ramaswamy and Agarwal (2016, Theorem 8) which was brought to our attention by Dhamma Kimpara, a PhD student at the University of Colorado Boulder.
After translating their definitions to ours, this result essentially states that, for surrogate losses with a finite representative set (the setting of our \S 6), a version of indirect elicitation implies calibration.
As such, they prove a slight generalization of our Theorem~\ref{thm:poly-ie-implies-consistent}.
To do so, they provide an alternate link construction to ours.
We therefore felt it important to highlight their work, so we added a new appendix section (\S~\ref{app:ccdim-construction}) to describe the connection, and added several references to it in context: footnote 1, just before Theorem~\ref{thm:thickened-separated}, and just before \S 7.1.

\bigskip
Regards,\\
Jessie Finocchiaro, Rafael Frongillo, and Bo Waggoner

\subsection*{Response to Reviewer 1}

We thank the reviewer for their thoughtful review and careful reading of our work. We have incorporated the editing suggestions made, and marked in our revised manuscript where non-typo changes to the text were made. 
Below we respond to some of the clarifying questions:
\begin{enumerate}
	\item \emph{Clarify the point that indirect elicitation is strictly weaker than calibration.}
	
	We have clarified this point with a concrete example; see margin note R1: A in orange.
	
	\item \emph{Proof of Lemma~\ref{lem:polyhedral-range-gamma}; several suggestions for clarity.}
		
	The reviewer is correct on all the suggestions; we have made changes accordingly.
	See margin note R1: B in orange for how we rephrased the sentence with too many parentheticals.

	\item \emph{``Thus, for any $u \in \cup \mathcal{U}$''; potential typo.}
	
	This is actually not a typo, but we expanded the notation to be more explicit: $u \in \bigcup_{U \in \mathcal{U}} U$.
\end{enumerate}



\subsection*{Response to Reviewer 2}

We thank the reviewer for the feedback, further resources, and suggestions for improving the clarity of exposition.
We have addressed them all, as detailed below.

\begin{itemize}
\item \textbf{Related work and discussion:}\\
 We thank the reviewer for the references and questions about the connections to $\mathcal{H}$-consistency and calibration. We have added a discussion about possible extensions to Section~\ref{sec:conclusion}. See margin note R2: A in blue.
  
\item \textbf{Questions and comments:}
  \begin{enumerate}
  \item \emph{More explanation on why a weak inequality in the definition of separated link (Definition~\ref{def:sep-link}) is more natural in applications such as hinge loss for binary classification.}

    Thank you; we clarified footnote 3: ``For example, taking hinge loss for binary classification, the sign link is 1-separated under the weak inequality, but only $(1-\delta)$-separated for $\delta > 0$ under the strict inequality.''
    
  \item \emph{Excess error (regret) bounds vs consistency.}

    From our understanding, consistency will always give rise to an excess error bound, e.g.\ by taking the calibration function which looks at the worst-case relationship between the errors.
    Thus what is special about polyhedral losses is the linearity of this bound; indeed, Frongillo and Waggoner (2021) show that ``smooth enough'' surrogate losses always yield a quadratically worse bound.
    
  \item \emph{Optimization of polyhedral losses.}

    We added footnote 8 to the ``Polyhedral vs.\ smooth'' paragraph in the discussion to make the reviewer's excellent point. 
    See margin note R2: B.
    In general, we lack frameworks which can combine optimization tradeoffs with statistical tradeoffs, making it hard to say whether and when smooth vs polyhedral losses will lead to better computational or statistical performance.
    These questions appear challenging and heavily dependent on the setting (e.g.\ how costly data acquisition vs computation is).
    
  \item \emph{The example in section 5.4 and non-smoothness of polyhedral losses.}

    We are not sure what the reviewer means by ``a Weston--Watkins polyhedral loss'', since we had thought of the Weston--Watkins hinge as a specific surrogate.
    But taking the question to mean ``is there a polyhedral surrogate consistent for 0--1 loss in multiclass settings?'', we added a brief discussion at the end of \S~\ref{sec:top-k} (``Surrogates for top-k classification'') giving such a hinge-like surrogate, a special case of the top-$k$ version with $k=1$.
    See margin note R2: C.
    
  \item \emph{Proposition~\ref{prop:embed-bayes-risks} and upper bounding the original loss.}

    When the target prediction space is finite, one can always (translate and) scale up a consistent convex surrogate until it upper bounds the original loss.
    Thus, while the reviewer may be correct that Bayes risk matching implies that an embedding cannot itself be an upper bound, such as hinge loss embedding \emph{twice} 0--1 loss but not 0--1 loss itself, it can always be corrected to form an upper bound.
    
  \item \emph{Theorem 33 shows that every possible calibrated link must be produced from construction 2. I am wondering if a similar result holds for construction 1 and embedding?}

    Yes, it does.
    We mention this fact informally following Construction 1; see margin note R2: D.

    
  \end{enumerate}

  
\item \textbf{Miscellaneous minor issues and suggestions:}

  We have fixed all minor issues.
  We added the definition of a link function to Definition~\ref{def:calibrated}, and emphasized the definition of a calibrated link in Definition~\ref{def:calibrated} as well.
  We added a formal definition of consistency (Definition~\ref{def:consistency}).
  We thank the reviewer for their suggestions to restate Theorems~\ref{thm:embed-poly-main} and \ref{thm:link-main} where relevant; we have done so.

\end{itemize}


\subsection*{Response to Reviewer 3}

We thank the reviewer the feedback. We will discuss the concerns point by point.

\begin{enumerate}
	\item \emph{Is the current framework can be extended to more general class of loss beyond polyhedral?}
	
	The reviewer is correct that the paper is essentially restricted to the polyhedral setting.  (There are extensions beyond the polyhedral case in \S~\ref{sec:min-rep-sets}, but not far beyond.)  Our goal is not to characterize the full set of consistent surrogates, but to provide a general framework for the design and analysis of polyhedral surrogate losses.
	
	\item \emph{Suggestion for empirical validation on multiclass classification}
	
	We thank the reviewer for the suggestion.  Ultimately, however, we feel that such a toy example would not be very illuminating for our particular results.  For multiclass classification, as in the reviewer’s example, many other papers already cover consistency conditions in theory and practice quite thoroughly, whereas the contribution of our paper is in more complex settings.  Even the comparison among different surrogates for multiclass classification appears in tutorials, blogs, and surveys such as \url{https://faculty.ist.psu.edu/vhonavar/Courses/ds310/lossfunc.pdf}.
	In lieu of a toy example, we feel that the applications in Section~\ref{sec:applications} illustrate the machinery we develop, leaving the evaluation and demonstration of the resulting surrogates (in the proper contexts) to the follow-up works cited there.
	
	\item \emph{When should we adopt the polyhedral loss instead of existing well-known convex surrogates such as logistic or hinge loss for a particular task?}
	
	To clarify, hinge loss is an example of an embedding.  See the previous response and “Polyhedral vs. smooth surrogates” in Section~\ref{sec:conclusion} for discussion on how to select surrogates.
	
	\item \emph{The claim that ``consistency depends crucially on the choice of link function.''}

	The choice of surrogate loss is of course important for consistency, but so is the choice of link function; see the discussion after Definition~\ref{def:indirect-elic}, marked by margin note R3: A in green.  
	(In particular, it seems the reviewer may be fixing the link to be the sign function; e.g. logistic loss is consistent with respect to 0-1 loss when using the sign link, but not with any other link.)	
	
\end{enumerate}



\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
