\documentclass[a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}

% WHERE HAS THIS BEEN ALL MY LIFE
\usepackage{xr}
\externaldocument{../poly-embed-journal}

\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below

\newcommand{\Comments}{1}
\definecolor{gray}{gray}{0.5}
\definecolor{lightred}{rgb}{1,0.6,0.6}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1\todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\ian}[1]{\mynote{blue}{[IK: #1]}}
\newcommand{\iant}[1]{\mytodo{blue!20!white}{IK: #1}}
\newcommand{\raf}[1]{\mynote{darkgreen}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}

%\newcommand{\response}[1]{\textcolor{blue!50!black}{Response: #1}}
\newenvironment{response}{\color{blue!50!black}}{\color{black}}

\begin{document}

\begin{center}
  {\Large Response to Reviews on JMLR-22-0743-1}
  
  (An Embedding Framework for the Design and Analysis of Consistent Polyhedral Surrogates)
\end{center}

\raf{OLD TEXT}
  We thank the editor and reviewers for their feedback.  While the instructions said only a brief response was needed, we have elected to provide a full letter because we discovered we needed a stronger condition than had actually been assumed.  Essentially, the error was that in the proofs of Lemma 8 and Corollary 7 it was mistakenly assumed that the span of a level set of an identifiable property and the kernel of the identification function for that level set should have the same codimension, when actually the codimension of the former could be unboundedly larger than that of the latter.  A way to avoid this, which we have adopted, is to assume that 0 is in the interior of the image of the identification function, an assumption inspired by Assumption V1 of Fissler and Ziegel (2016) and similar in spirit to our existing conditions.

We first discuss the changes needed to implement this correction, and then address the changes to address other comments, all of which were minor.\\
\\
Regards,\\
Jessica Finocchiaro, Rafael Frongillo, and Bo Waggoner

\subsection*{Response to Reviewer 1}


\subsection*{Response to Reviewer 2}

We thank the reviewer for the feedback, further resources, and suggestions for improving the clarity of exposition.
We have addressed them all, as detailed below.

\begin{itemize}
\item \textbf{Related work and discussion:}

 [See Section 8 for added discussion on $\mathcal{H}$-consistency.]
  
\item \textbf{Questions and comments:}
  \begin{enumerate}
  \item \emph{More explanation on why a weak inequality in the definition of separated link (Definition 15) is more natural in applications such as hinge loss for binary classification.}

    Thank you; we clarified the footnote: ``For example, taking hinge loss for binary classification, the sign link is 1-separated under the weak inequality, but only $(1-\delta)$-separated for $\delta > 0$ under the strict inequality.''
    
  \item \emph{Excess error (regret) bounds vs consistency.}

    From our understanding, consistency will always give rise to an excess error bound, e.g.\ by taking the calibration function which looks at the worst-case relationship between the errors.
    Thus what is special about polyhedral losses is the linearity of this bound; indeed, Frongillo and Waggoner (2021) show that ``smooth enough'' surrogate losses always yield a quadratically worse bound.
    
  \item \emph{Optimization of polyhedral losses.}

    We added a footnote to the ``Polyhedral vs.\ smooth'' paragraph in the discussion to make the reviewer's excellent point.
    In general, we lack frameworks which can combine optimization tradeoffs with statistical tradeoffs, making it hard to say whether and when smooth vs polyhedral losses will lead to better computational or statistical performance.
    These questions appear challenging and heavily dependent on the setting (e.g.\ how costly data acquisition vs computation is).
    
  \item \emph{The example in section 5.4 and non-smoothness of polyhedral losses.}

    We are not sure what the reviewer means by ``a Weston--Watkins polyhedral loss'', since we had thought of the Weston--Watkins hinge as a specific surrogate.
    But taking the question to mean ``is there a polyhedral surrogate consistent for 0--1 loss in multiclass settings?'', we added a brief discussion at the end of Section 5.5 (``Surrogates for top-k classification'') giving such a hinge-like surrogate, a special case of the top-$k$ version with $k=1$.
    
  \item \emph{Proposition 21 and upper bounding the original loss.}

    When the target prediction space is finite, one can always (translate and) scale up a consistent convex surrogate until it upper bounds the original loss.
    Thus, while the reviewer may be correct that Bayes risk matching implies that an embedding cannot itself be an upper bound, such as hinge loss embedding \emph{twice} 0--1 loss but not 0--1 loss itself, it can always be corrected to form an upper bound.
    
  \item \emph{Theorem 33 shows that every possible calibrated link must be produced from construction 2. I am wondering if a similar result holds for construction 1 and embedding?}

    
  \end{enumerate}

  
\item \textbf{Miscellaneous minor issues and suggestions:}

  We have fixed all minor issues.
  We added the definition of a link function to Definition 6, and emphasized the definition of a calibrated link in Definition 6 as well.
  We added a formal definition of consistency into the appendix with discussion about this definition and its history.\raft{I'm actually wondering if we should just define it in the paper like they suggest... let's see how the appendix section turns out}
  We thank the reviewer for their suggestions to restate Theorems 1 and 2 where relevant; we have done so.

\end{itemize}


\subsection*{Response to Reviewer 3}


\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
