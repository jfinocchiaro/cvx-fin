\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage[nofoot, left=1.5in, right=1.5in, top=0.5in, bottom=0.8in]{geometry}

\usepackage{float}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{BTW: #1}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi



\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\defeq}{\doteq}%\vcentcolon=} % define equals


\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\regret}[3]{R_{#1}(#2,#3)}

\newcommand{\Ind}[1]{\ones\{#1\}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}


\title{Linear Regret Bounds for Polyhedral Surrogates}

\begin{document}

\maketitle


\section{Setting: Polyhedral Losses and Embeddings}

Let $\Y$ be a finite label space.
The set of probability distributions on $\Y$ is denoted $\simplex\subseteq\reals^{\Y}_+$, represented as vectors of probabilities (requiring $\|p\|_1 = 1$).

We assume that a given discrete prediction problem, such as classification, is given in the form of a \emph{discrete loss} $\ell:\R\to\reals^\Y_+$, which maps a report (prediction) $r$ from a finite set $\R$ to the vector of loss values $\ell(r) = (\ell(r)_y)_{y\in\Y}$ for each possible outcome $y\in\Y$.
We will assume throughout that the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.
Similarly, surrogate losses will be written $L:\reals^d\to\reals^\Y_+$, typically with reports written $u\in\reals^d$.
We write the corresponding expected loss when $Y \sim p$ as $\inprod{p}{\ell(r)}$ and $\inprod{p}{L(u)}$.
The \emph{Bayes risk} of a loss $L:\reals^d\to\reals^\Y_+$ is the function $\risk{L}:\simplex\to\reals_+$ given by $\risk{L}(p) \defeq \inf_{u\in\reals^d} \inprod{p}{L(u)}$; naturally for discrete losses we write $\risk{\ell}$ with the infimum over $\R$.

We say a surrogates $L$ is \emph{polyhedral} if it is piecewise linear and convex.
To introduce this definition more formally, let us recall some definitions from convex geometry.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
% A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\cite{rockafellar1997convex}.

\begin{definition}[Polyhedral loss]
  A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral (convex) function of $u$ for each $y\in\Y$.
\end{definition}

$\Gamma: \simplex \toto \R$ is shorthand for $\Gamma: \simplex \to 2^{\R} \setminus \{\emptyset\}$.

\begin{definition}[Property, level set]\label{def:property}
  A \emph{property} is a function $\Gamma:\simplex\toto\R$.
  The \emph{level set} of $\Gamma$ for report $r$ is the set $\Gamma_r \defeq \{p \in \simplex : r \in \Gamma(p)\}$.
\end{definition}

\begin{definition}[Elicits]
  \label{def:elicits}
  A loss $L:\R\to\reals^\Y_+$, \emph{elicits} a property $\Gamma:\simplex \toto \R$ if
  \begin{equation}
    \forall p\in\simplex,\;\;\;\Gamma(p) = \argmin_{r \in \R} \inprod{p}{L(r)}~.
  \end{equation}
  As $L$ elicits a unique property, we write $\prop{L}$ to refer to the property elicited by a loss $L$.
\end{definition}

\begin{definition}[Embedding a loss]\label{def:loss-embed}
  A loss $L:\reals^d\to\reals^\Y$ \emph{embeds} a loss $\ell:\R\to\reals^\Y$ if there exists some injective embedding $\varphi:\R\to\reals^d$ such that
  (i) for all $r\in\R$ we have $L(\varphi(r)) = \ell(r)$, and (ii) for all $p\in\simplex,r\in\R$ we have
  \begin{equation}\label{eq:embed-loss}
    r \in \prop{\ell}(p) \iff \varphi(r) \in \prop{L}(p)~.
  \end{equation}
\end{definition}
%


\subsection{Surrogate regret bound}

The primary focus of this paper is on surrogate regret bounds, which bound the excess target loss in terms of the excess surrogate loss.

\begin{definition}[Regret]\label{def:regret}
  For loss $L:\R\to\reals^\Y_+$ and distribution $p\in\simplex$, the \emph{regret} of a prediction $r\in\R$ is given by $R_L(r,p) = \inprod{p}{L(r)} - \risk{L}(p)$.
\end{definition}

\begin{definition}[Surrogate Regret Bound]\label{def:regret-bound}
  A \emph{surrogate regret bound} between target $\ell:\R\to\reals^\Y$ and surrogate $L:\reals^d\to\reals^\Y$, together with a link $\psi : \reals^d \to \R$, is the statement
  \begin{equation}
    \label{eq:regret-bound-general}
    \forall p\in\simplex, u\in\reals^d, \quad R_\ell(\psi(u),p) \leq \zeta(\, R_L(u,p) \,)~,
  \end{equation}
  for some function $\zeta : \reals_+ \to \reals_+$ which is continuous at $0$ and satisfies $\zeta(0) = 0$.
\end{definition}



\section{Main result}

\subsection{Statement}

\begin{definition}[Separated Link]\label{def:sep-link}
  Let properties $\Gamma:\simplex\toto\reals^d$ and $\gamma:\simplex\toto\R$ be given.
  We say a link $\psi:\reals^d\to\R$
  % Let $L:\reals^d \to \reals^\Y_+$, $\gamma:\simplex\toto\R$, and link $\psi:\reals^d\to\R$ be given.
  is \emph{$\epsilon$-separated} if for all $u\in\reals^d$ with $\psi(u)\notin\gamma(p)$, we have $d_\infty(u,\Gamma(p)) > \epsilon$, where $d_\infty(u,A) \defeq \inf_{a\in A} \|u-a\|_\infty$.
\end{definition}

The goal of this section is to show that if a polyhedral surrogate is consistent for a discrete loss, then it achieves linear regret bounds (Theorem~\ref{thm:regret-bound-poly-consistent}).

\subsection{Useful results from prior work}

\begin{theorem}[Hoffman constant]
  \label{thm:hoffman}
  Given a matrix $A\in\reals^{m\times n}$, there exists some $H(A)\geq 0$, called the \emph{Hoffman constant} (with respect to $\|\cdot\|_\infty$), such that for all $b\in\reals^m$ and all $x\in\reals^n$,
  \begin{equation}
    \label{eq:hoffman}
    d_\infty(x,S(A,b)) \leq H(A) \|(A x - b)_+\|_\infty~,
  \end{equation}
  where $S(A,b) = \{x\in\reals^n \mid A x \leq b\}$ and $(u)_+ \defeq \max(u,0)$ component-wise.
\end{theorem}

\begin{theorem}[Polyhedral paper]
  \label{thm:poly-embed-refine}
  If a polyhedral loss $L$ indirectly elicits a finite elicitable property $\gamma$, then $L$ embeds a property which refines $\gamma$.
\end{theorem}

\subsection{Part 1: fixed $p$}

\begin{lemma}[Regret bound for fixed $p$]
  \label{lem:regret-bound-fixed-p}
  Let surrogate $L:\reals^d \to \reals^\Y_+$ and discrete loss $\ell:\R\to\reals^\Y_+$ be given.
  Let $\psi:\reals^d\to\R$ be an $\epsilon$-separated link with respect to $\Gamma \defeq \prop{L}$ and $\gamma \defeq \prop{\ell}$ for some $\epsilon>0$.
  For fixed $p\in\simplex$, there exists some $\alpha\geq 0$ such that
  \begin{equation}
    \label{eq:surrogate-regret-bound-fixed}
    \regret{\ell}{\psi(u)}{p}
    \leq
    \alpha \cdot \regret{L}{u}{p}~,
    % \inprod{p}{\ell(\psi(u))} - \risk{\ell}(p)
    % \leq
    % \alpha \cdot \left( \inprod{p}{L(u)} - \risk{L}(p) \right)~,
  \end{equation}
  for all $u\in\reals^d$.
\end{lemma}
\begin{proof}
  Since $L$ is polyhedral, there exist $a_1,\ldots,a_m \in \reals^d$ and $c\in\reals^m$ such that we may write $\inprod{p}{L(u)} = \max_{1\leq j\leq m} a_j \cdot u + c_j$.
  Let $A \in \reals^{m\times d}$ be the matrix with rows $a_j$, and let $b = \risk{L}(p)\ones - c$, where $\ones\in\reals^m$ is the all-ones vector.
  Then we have
  \begin{align*}
    S(A,b)
    &\defeq \{u\in\reals^d \mid A u \leq b\}
    \\
    &= \{u\in\reals^d \mid A u + c \leq \risk{L}(p)\ones\}
    \\
    &= \{u\in\reals^d \mid \forall i\, (A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \max_i \;(A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \inprod{p}{L(u)} \leq \risk{L}(p)\}
    \\
    & = \Gamma(p)~.
  \end{align*}
  Similarly, we have $\max_i\; (A u - b)_i = \inprod{p}{L(u)} - \risk{L}(p) = \regret{L}{u}{p} \geq 0$.
  Thus,
  \begin{align*}
    \|(Au - b)_+\|_\infty
    &= \max_i\; ((Au - b)_+)_i
    \\
    &= \max((Au - b)_1,\ldots,(Au - b)_m, 0)
    \\
    &= \max(\max_i\; (Au - b)_i, \, 0)
    \\
    &= \max_i\; (Au - b)_i
    % \\
    % &= \inprod{p}{L(u)} - \risk{L}(p)
    \\
    &= \regret{L}{u}{p}~.
  \end{align*}

  Now applying Theorem~\ref{thm:hoffman}, we have
  \begin{align*}
    d_\infty(u,\Gamma(p))
    &\leq H(A) \|(Au-b)_+\|_\infty
    \\
    &= H(A) \regret{L}{u}{p}~.
  \end{align*}
  Let $\beta \defeq \max_{r\in\R} \regret{\ell}{r}{p}\geq 0$, which is well-defined as $\R$ is a finite set.
  To complete the proof, define $\alpha = \beta H(A)/\epsilon$.
  Given $u\in\reals^d$, if $\regret{\ell}{\psi(u)}{p} = 0$, we are done.
  Otherwise, we have from $\epsilon$-separation that $d_\infty(u,\Gamma(p)) > \epsilon$.
  The above then gives $H(A) \regret{L}{u}{p} > \epsilon$, which implies
  $(\beta H(A) / \epsilon) \regret{L}{u}{p} > \beta$.
  Finally, as $\regret{\ell}{\psi(u)}{p} \leq \beta$, the result follows.
\end{proof}

\subsection{Part 2: separation WLOG}

\begin{lemma}\label{lem:calibrated-eps-sep}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is calibrated with respect to $\ell$.
  Then there exists $\epsilon>0$ such that $\psi$ is $\epsilon$-separated with respect to   $\Gamma \defeq \prop{L}$ and $\gamma \defeq \prop{\ell}$.
\end{lemma}
\begin{proof}
  Suppose that $\psi$ is not $\epsilon$-separated for any $\epsilon>0$.
  Then letting $\epsilon_i \defeq 1/i$ we have sequences $\{p_i\}_i \subset \simplex$ and  $\{u_i\}_i \subset \reals^d$ such that for all $i\in\mathbb N$ we have both $\psi(u_i) \notin \gamma(p_i)$ and $d_\infty(u_i,\Gamma(p_i)) \leq \epsilon_i$.
  First, observe that there are only finitely many values for $\gamma(p_i)$ and $\Gamma(p_i)$, as $\R$ is finite and $L$ is polyhedral.
  \raft{Using a result from the polyhedral paper here, that $\Gamma$ only takes on finitely many values}
  Thus, there must be some $p\in\simplex$ and some infinite subsequence indexed by $j\in J \subseteq \mathbb N$ where
  for all $j\in J$, we have $\psi(u_j) \notin \gamma(p)$ and $\Gamma(p_j) = \Gamma(p)$.

  Next, observe that, as $L$ is polyhedral, the expected loss $\inprod{p}{L(u)}$ is $\beta$-Lipschitz in $\|\cdot\|_\infty$ for some $\beta>0$.
  \raft{Could write $\beta$ explicitly; it's in my notes}
  Thus, for all $j\in J$, we have
  \begin{align*}
    d_\infty(u_i,\Gamma(p)) \leq \epsilon_j
    &\implies \exists u^*\in\Gamma(p) \|u_j-u^*\|_\infty \leq \epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \inprod{p}{L(u^*)} \right| \leq \beta\epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \risk{L}(p) \right| \leq \beta\epsilon_j~.
  \end{align*}
  Finally, for this $p$, we have
  \begin{align*}
    \inf_{u:\psi(u)\notin\gamma(p)} \inprod{p}{L(u)}
    \leq
    \inf_{j\in J} \inprod{p}{L(u_j)}
    =
    \risk{L}(p)~,
  \end{align*}
  contradicting the calibration of $\psi$.
\end{proof}

\subsection{Part 3: all $p$, all consistent poly surrogates}

% \begin{lemma}\label{lem:regret-linear}
%   Let $L:\reals^d \to \reals^\Y_+$ be a surrogate loss which is consistent with respect to a discrete loss $\ell:\R\to\reals^\Y_+$, and set $\Gamma \defeq \prop{L}$.
%   For all $r'\in\R$, and all $u,u'\in\reals^d$, the functions $\regret{\ell}{r'}{\cdot}$ and $\regret{L}{u'}{\cdot}$ are linear on $\Gamma_u$.
% \end{lemma}
% \begin{proof}
%   From \raf{the implication consistency $\implies$ indirect elicitation}, $\Gamma$ refines $\gamma \defeq \prop{\ell}$.
%   % Theorem~\ref{thm:poly-embed-refine} we have that $L$ embeds some $\gamma'$ which refines $\gamma \defeq \prop{\ell}$.
%   Thus, given any $u\in\reals^d$, there exists some $r\in\R$ such that $\Gamma_u \subseteq \gamma_r$.
%   In particular, for any $p\in\Gamma_u$, we have $\risk{L}(p) = \inprod{p}{L(u)}$ and $\risk{\ell}(p) = \inprod{p}{\ell(r)}$.
%   % When $p\in\gamma_r$, we have $\risk{\ell}(p) = \inprod{p}{\ell(r)}$.
%   % Similarly, by the definition of embedding, we have $\risk{L}(p) = \inprod{p}{L(\varphi(r))}$, where $\varphi:\R\to\reals^d$ is the embedding.
%   We therefore have, for any $p\in\Gamma_u$,
%   \begin{align*}
%     \regret{\ell}{r'}{p} &= \inprod{p}{\ell(r')} - \risk{\ell}(p) = \inprod{p}{\ell(r')-\ell(r)}~,
%     \\
%     \regret{L}{u'}{p} &= \inprod{p}{L(u')} - \risk{L}(p) = \inprod{p}{L(u')-L(u)}~,
%   \end{align*}
%   which are both linear in $p$.  
% \end{proof}
\begin{lemma}\label{lem:regret-linear}
  Let $L:\reals^d \to \reals^\Y_+$ be a surrogate loss which is consistent with respect to a discrete loss $\ell:\R\to\reals^\Y_+$, and set $\Gamma \defeq \prop{L}$.
  For all $r\in\R$, and all $u,u^*\in\reals^d$, the functions $\regret{\ell}{r}{\cdot}$ and $\regret{L}{u}{\cdot}$ are linear on $\Gamma_{u^*}$.
\end{lemma}
\begin{proof}
  From \raf{the implication consistency $\implies$ indirect elicitation}, $\Gamma$ refines $\gamma \defeq \prop{\ell}$.
  Thus, given any $u^*\in\reals^d$, there exists some $r^*\in\R$ such that $\Gamma_{u^*} \subseteq \gamma_{r^*}$.
  In particular, for any $p\in\Gamma_{u^*}$, we have $\risk{L}(p) = \inprod{p}{L(u^*)}$ and $\risk{\ell}(p) = \inprod{p}{\ell(r^*)}$.
  % When $p\in\gamma_r$, we have $\risk{\ell}(p) = \inprod{p}{\ell(r)}$.
  % Similarly, by the definition of embedding, we have $\risk{L}(p) = \inprod{p}{L(\varphi(r))}$, where $\varphi:\R\to\reals^d$ is the embedding.
  We therefore have, for any $p\in\Gamma_{u^*}$,
  \begin{align*}
    \regret{\ell}{r}{p} &= \inprod{p}{\ell(r)} - \risk{\ell}(p) = \inprod{p}{\ell(r)-\ell(r^*)}~,
    \\
    \regret{L}{u}{p} &= \inprod{p}{L(u)} - \risk{L}(p) = \inprod{p}{L(u)-L(u^*)}~,
  \end{align*}
  which are both linear in $p$.  
\end{proof}

\begin{theorem}\label{thm:regret-bound-poly-consistent}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is consistent with respect to $\ell$.
  Then there exists some $\alpha > 0$ such that
  \begin{equation}
    \label{eq:surrogate-regret-bound}
    \regret{\ell}{\psi(u)}{p}
    \leq
    \alpha \cdot \regret{L}{u}{p}~,
    % \inprod{p}{\ell(\psi(u))} - \risk{\ell}(p)
    % \leq
    % \alpha \cdot \left( \inprod{p}{L(u)} - \risk{L}(p) \right)~,
  \end{equation}
  for all $p\in\simplex$ and $u\in\reals^d$.
\end{theorem}

\begin{proof}
  From Lemma~\ref{lem:calibrated-eps-sep}, we have some $\epsilon>0$ such that $\psi$ is $\epsilon$-separated with respect to $\Gamma \defeq \prop{L}$ and $\gamma \defeq \prop{\ell}$.
  From Theorem~\ref{thm:poly-embed-refine}, $L$ embeds some property $\hat\gamma:\simplex\toto\A$ which refines $\gamma$.
  Let $\varphi:\A\to\reals^d$ be the embedding, so that $\hat\gamma_a = \Gamma_{\varphi(a)}$ for all $a\in\A$.
  % In particular, for all $u\in\reals^d$, there is some $a\in\A$ and $r\in\R$ such that $\Gamma_u \subseteq \hat\gamma_a \subseteq \gamma_r$.
  
  Recall that for all $a\in\A$, the set $\hat\gamma_a$ is a compact polyhedral subset of $\simplex$, and hence we may write $\hat\gamma_a = \conv \Q_a$ where $\Q_a \subset \hat\gamma_a$ is the finite set of vertices (extreme points) of $\hat\gamma_a$.
  Define $\Q = \bigcup_{a\in\A} \Q_a$, which is again a finite set.
  From Lemma~\ref{lem:regret-bound-fixed-p}, for each $q \in \Q$ we have some $\alpha_q\geq 0$ such that eq.~\eqref{eq:surrogate-regret-bound} holds for $\alpha = \alpha_q$, $p=q$, and all $u\in\reals^d$.
  % Recall that for all $r\in\R$, the set $\gamma_r$ is a compact polyhedral subset of $\simplex$, and hence we may write $\gamma_r = \conv \Q_r$ where $\Q_r \subset \gamma_r$ is the finite set of vertices (extreme points) of $\gamma_r$.
  % Define $\Q = \bigcup_{r\in\R} \Q_r$, which is again a finite set.
  % From Lemma~\ref{lem:regret-bound-fixed-p}, for each $q \in \Q$ we have some $\alpha_q\geq 0$ such that eq.~\eqref{eq:surrogate-regret-bound} holds for $\alpha = \alpha_q$, $p=q$, and all $u\in\reals^d$.
  Finally, set $\alpha \defeq \max_{q\in\Q} \alpha_q$, which is well-defined as $\Q$ is finite.

  Let $p \in \simplex$ and $u\in\reals^d$ be arbitrary.
  We must have $p\in\hat\gamma_a$ for some $a\in\A$.
  \raft{Using non-emptiness of $\gamma$ here, not that it's a restriction at all}
  By the definition of $\Q_a$, we may write $p = \sum_{q\in\Q_a} \beta_q \, q$ for some $\beta_q \geq 0$, $\sum_{q\in\Q_a} \beta_a = 1$.
  Recall that $\Q_a \subset \hat\gamma_a = \Gamma_{\varphi(a)}$.
  Thus,
  \begin{align*}
    \regret{\ell}{\psi(u)}{p}
    & = \regret{\ell}{\psi(u)}{\textstyle\sum_{q\in\Q_a} \beta_q \, q} &
    \\
    & = \sum_{q\in\Q_a} \beta_q \regret{\ell}{\psi(u)}{q} & \text{Lemma~\ref{lem:regret-linear}}% with $u^*=\varphi(a)$}
    \\
    & = \sum_{q\in\Q_a} \beta_q \alpha_q \regret{L}{u}{q} & \text{Lemma~\ref{lem:regret-bound-fixed-p}}
    \\
    & \leq \left(\max_{q\in\Q_a} \alpha_q\right) \sum_{q\in\Q_a} \beta_q \regret{L}{u}{q} & \text{$\Q_a$ is finite}
    \\
    & \leq \left(\max_{q\in\Q_a} \alpha_q\right) \regret{L}{u}{q} & \text{Lemma~\ref{lem:regret-linear}}% with $u^*=\varphi(a)$}
    \\
    & \leq \alpha \regret{L}{u}{q}~,
  \end{align*}
  completing the proof.
\end{proof}


\section{Lower Bounds for Smooth Surrogates}

\begin{definition}[Strong Convexity and Strong Smoothness]
  Let $f:\reals^d\to\reals$ be a differentiable function.
  We say $f$ is $\alpha$-\emph{strongly convex} if $f$ is convex and
  \begin{equation}
    \label{eq:strong-convexity}
    \|\nabla f(x') - \nabla f(x)\|_2 \geq \alpha \|x'-x\|_2 \quad \forall x,x'\in\reals^d~,
  \end{equation}
  where $\|\cdot\|_2$ denotes the Euclidean norm.
  We say $f$ is $\beta$-\emph{strongly smooth} if
  \begin{equation}
    \label{eq:strong-convexity}
    \|\nabla f(x') - \nabla f(x)\|_2 \leq \beta \|x'-x\|_2 \quad \forall x,x'\in\reals^d~.
  \end{equation}
  
\end{definition}

\begin{theorem}
  Let $L:\reals^d \to \reals^\Y_+$ be a surrogate such that for all $y\in\Y$, the function $L(\cdot)_y$ is differentiable, $\alpha$-strongly convex, and $\beta$-strongly smooth for some $\alpha,\beta>0$.\raft{I think differentiability follows from smoothness...}
  Let $\ell:\R \to \reals^\Y_+$ be a target loss, and $\psi:\reals^d\to\R$ a link.
  If there is a surrogate regret bound between $L$ and $\ell$ with function $\zeta:\reals_+\to\reals_+$, then there exists $c > 0$ such that for sufficiently small $\epsilon>0$ we have
  \begin{equation}
    \label{eq:regret-lower-bound}
    \zeta(\epsilon) \geq c \cdot \sqrt{\epsilon}~.
  \end{equation}
\end{theorem}
\begin{proof}
  Let $\gamma := \prop{\ell}$.
  Fix reports $r,r'$ such that there exists $p_0 \in \gamma_r \cap \gamma_{r'}$.
  Define $u_0$ to be the unique minimizer of $\inprod{p_0}{L(\cdot)}$ on $\reals^d$, the existence of which follows from strong (and hence strict) convexity.
  Assume without loss of generality that $\psi(u_0) = r'$; otherwise, swap the roles of $r$ and $r'$.
  As $\ell$ is non-redundant by assumption, there exists some $p_1 \in \inter{\gamma_r}$.
  We therefore have $R_\ell(r',p_1) = \inprod{p_1}{\ell(r')-\ell(r)} =: c_\ell > 0$, and $R_\ell(r',p_0) = 0$.
  Let $p_\lambda := (1-\lambda) p_0 + \lambda p_1$.
  By convexity of $\gamma_r$, we have $p_\lambda \in \gamma_r$ for all $\lambda \in [0,1]$, which gives $R_\ell(r',p_\lambda) = \lambda c_\ell$.
  We will upper bound $R_L(u_0,p_\lambda)$ by $O(\lambda^2)$, which will imply the result.
  % Our goal is to find some $c>0$ and some path of reports $u_\lambda \in \reals^d$ such that $\psi(u_\lambda)=r'$ and $R_L(u_\lambda,p_\lambda) \geq c\cdot\sqrt{\lambda \epsilon_1}$, for $\lambda$ sufficiently small.

  Let $L_\lambda:\reals^d\to\reals_+$ be given by $L_\lambda(u) = \inprod{p_\lambda}{L(u)} = (1-\lambda) \inprod{p_0}{L(u)} + \lambda \inprod{p_1}{L(u)}$.
  Extending our definition of $u_0$, let $u_\lambda$ be the unique minimizer of $L_\lambda$ on $\reals^d$.
  Then $\risk{L}(p_\lambda) = L_\lambda(u_\lambda)$, and thus $R_L(u_0,p_\lambda) = L_\lambda(u_0) - L_\lambda(u_\lambda)$.

  We first upper bound $\|u_\lambda - u_0\|_2$, and then apply strong smoothness to upper bound $L_\lambda(u_0) - L_\lambda(u_\lambda)$.
  Consider the first-order optimality condition of $L_\lambda$:
  \begin{align*}
    \label{eq:first-order-opt-smooth}
    & 0 = \nabla L_\lambda(u_\lambda) = (1-\lambda) \nabla L_0(u_\lambda) + \lambda \nabla L_1(u_\lambda)
    \\
    & \implies (1-\lambda) \|\nabla L_0(u_\lambda)\|_2 = \lambda \|\nabla L_1(u_\lambda)\|_2~.
  \end{align*}
  By optimality of $u_0$ and $u_1$, strong convexity and smoothness, and the triangle inequality, we have
  \begin{align*}
    \|\nabla L_0(u_\lambda)\|_2 &= \|\nabla L_0(u_\lambda) - \nabla L_0(u_0)\|_2 \geq \alpha \|u_\lambda - u_0\|_2~,
    \\
    \|\nabla L_1(u_\lambda)\|_2 &= \|\nabla L_0(u_\lambda) - \nabla L_1(u_1)\|_2 \leq \beta \|u_\lambda - u_1\|_2
    \\
    &\leq \beta \left( \|u_\lambda - u_0\|_2 + \|u_0 - u_1\|_2 \right)~.
  \end{align*}
  Combining, and taking $\lambda \leq \tfrac 1 2 \tfrac {\alpha}{\alpha+\beta}$, we have
  \begin{align*}
    \|u_\lambda - u_0\|_2 \leq \frac{\lambda\beta}{(1-\lambda)\alpha-\lambda\beta} \|u_0 - u_1\|_2  \leq \lambda \frac{2\beta}{\alpha} \|u_0 - u_1\|_2 ~.
  \end{align*}
  Finally, from strong smoothness and optimality of $u_\lambda$, we have
  \begin{align*}
    L_\lambda(u_0) - L_\lambda(u_\lambda) \leq \frac{\beta}{2} \|u_0 - u_\lambda\|_2^2 \leq \frac{\beta}{2} \left(\lambda \frac{2\beta}{\alpha} \|u_0 - u_1\|_2\right)^2 = c_L \lambda^2~,
  \end{align*}
  where $c_L = \frac{2\beta^3}{\alpha^2} \|u_0 - u_1\|_2^2$.
  Take $c = \frac{c_\ell}{\sqrt{c_L}} = \frac{c_\ell \alpha}{\|u_0 - u_1\|_2 \sqrt{2\beta^3}}$.
  Recall $R_\ell(r',p_\lambda) = c_\ell \lambda$ and $R_L(u_0,p_\lambda) \leq c_L \lambda^2$.
  Then letting $\epsilon = R_L(u_0,p_\lambda)$, we have $\zeta(\epsilon) \geq R_\ell(r',p_\lambda) \geq c \sqrt{\epsilon}$.
\end{proof}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
