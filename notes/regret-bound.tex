\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage[nofoot, left=1.5in, right=1.5in, top=0.5in, bottom=0.8in]{geometry}

\usepackage{float}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\proposedadd}[1]{\mynote{orange}{#1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{BTW: #1}}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}

\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\regret}[3]{R_{#1}(#2,#3)}

\newcommand{\Ind}[1]{\ones\{#1\}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}


\title{Linear Regret Bounds for Polyhedral Surrogates}

\begin{document}

\maketitle


\section{Setting: Polyhedral Losses and Embeddings}

Let $\Y$ be a finite label space.
The set of probability distributions on $\Y$ is denoted $\simplex\subseteq\reals^{\Y}_+$, represented as vectors of probabilities (requiring $\|p\|_1 = 1$).

We assume that a given discrete prediction problem, such as classification, is given in the form of a \emph{discrete loss} $\ell:\R\to\reals^\Y_+$, which maps a report (prediction) $r$ from a finite set $\R$ to the vector of loss values $\ell(r) = (\ell(r)_y)_{y\in\Y}$ for each possible outcome $y\in\Y$.
We will assume throughout that the given discrete loss is \emph{non-redundant}, meaning every report is uniquely optimal (minimizes expected loss) for some distribution $p\in\simplex$.
Similarly, surrogate losses will be written $L:\reals^d\to\reals^\Y_+$, typically with reports written $u\in\reals^d$.
We write the corresponding expected loss when $Y \sim p$ as $\inprod{p}{\ell(r)}$ and $\inprod{p}{L(u)}$.
The \emph{Bayes risk} of a loss $L:\reals^d\to\reals^\Y_+$ is the function $\risk{L}:\simplex\to\reals_+$ given by $\risk{L}(p) := \inf_{u\in\reals^d} \inprod{p}{L(u)}$; naturally for discrete losses we write $\risk{\ell}$ with the infimum over $\R$.

Most of the surrogates $L$ we consider will be \emph{polyhedral}, meaning piecewise linear and convex; we therefore briefly recall the relevant definitions.
In $\reals^d$, a \emph{polyhedral set} or \emph{polyhedron} is the intersection of a finite number of closed halfspaces.
A \emph{polytope} is a bounded polyhedral set.
A convex function $f:\reals^d\to\reals$ is \emph{polyhedral} if its epigraph is polyhedral, or equivalently, if it can be written as a pointwise maximum of a finite set of affine functions~\cite{rockafellar1997convex}.

\begin{definition}[Polyhedral loss]
  A loss $L: \reals^d \to \reals^{\Y}_+$ is \emph{polyhedral} if $L(u)_y$ is a polyhedral (convex) function of $u$ for each $y\in\Y$.
\end{definition}

$\Gamma: \simplex \toto \R$ is shorthand for $\Gamma: \simplex \to 2^{\R} \setminus \{\emptyset\}$.

\begin{definition}[Property, level set]\label{def:property}
  A \emph{property} is a function $\Gamma:\simplex\toto\R$.
  The \emph{level set} of $\Gamma$ for report $r$ is the set $\Gamma_r := \{p \in \simplex : r \in \Gamma(p)\}$.
\end{definition}

\begin{definition}[Elicits]
  \label{def:elicits}
  A loss $L:\R\to\reals^\Y_+$, \emph{elicits} a property $\Gamma:\simplex \toto \R$ if
  \begin{equation}
    \forall p\in\simplex,\;\;\;\Gamma(p) = \argmin_{r \in \R} \inprod{p}{L(r)}~.
  \end{equation}
  As $L$ elicits a unique property, we write $\prop{L}$ to refer to the property elicited by a loss $L$.
\end{definition}

\begin{definition}[Embedding a loss]\label{def:loss-embed}
  A loss $L:\reals^d\to\reals^\Y$ \emph{embeds} a loss $\ell:\R\to\reals^\Y$ if there exists some injective embedding $\varphi:\R\to\reals^d$ such that
  (i) for all $r\in\R$ we have $L(\varphi(r)) = \ell(r)$, and (ii) for all $p\in\simplex,r\in\R$ we have
  \begin{equation}\label{eq:embed-loss}
    r \in \prop{\ell}(p) \iff \varphi(r) \in \prop{L}(p)~.
  \end{equation}
\end{definition}
%

\section{Main result}

\subsection{Statement}

\begin{definition}[Separated Link]\label{def:sep-link}
  Let properties $\Gamma:\simplex\toto\reals^d$ and $\gamma:\simplex\toto\R$ be given.
  We say a link $\psi:\reals^d\to\R$
  % Let $L:\reals^d \to \reals^\Y_+$, $\gamma:\simplex\toto\R$, and link $\psi:\reals^d\to\R$ be given.
  is \emph{$\epsilon$-separated} if for all $u\in\reals^d$ with $\psi(u)\notin\gamma(p)$, we have $d_\infty(u,\Gamma(p)) > \epsilon$, where $d_\infty(u,A) := \inf_{a\in A} \|u-a\|_\infty$.
\end{definition}

\begin{definition}[Regret]\label{def:regret}
  For loss $L:\R\to\reals^\Y_+$ and distribution $p\in\simplex$, the \emph{regret} of a prediction $r\in\R$ is given by $R_L(r,p) = \inprod{p}{L(r)} - \risk{L}(p)$.
\end{definition}

The goal of this section is to show that if a polyhedral surrogate is consistent for a discrete loss, then it achieves linear regret bounds (Theorem~\ref{thm:regret-bound-poly-consistent}).

\subsection{Part 1: fixed $p$}

\begin{theorem}[Hoffman constant]
  \label{thm:hoffman}
  Given a matrix $A\in\reals^{m\times n}$, there exists some $H(A)\geq 0$, called the \emph{Hoffman constant} (with respect to $\|\cdot\|_\infty$), such that for all $b\in\reals^m$ and all $x\in\reals^n$,
  \begin{equation}
    \label{eq:hoffman}
    d_\infty(x,S(A,b)) \leq H(A) \|(A x - b)_+\|_\infty~,
  \end{equation}
  where $S(A,b) = \{x\in\reals^n \mid A x \leq b\}$ and $(u)_+ := \max(u,0)$ component-wise.
\end{theorem}

\begin{lemma}[Hoffman constrant and excess risk]
  
\end{lemma}

\begin{lemma}[Regret bound for fixed $p$]
  \label{lem:regret-bound-fixed-p}
  Let $L:\reals^d \to \reals^\Y_+$ be a surrogate loss which embeds a discrete loss $\ell:\R\to\reals^\Y_+$, and define $\gamma := \prop{\ell}$ and $\Gamma := \prop{L}$.
  Let $\psi:\reals^d\to\R$ be an $\epsilon$-separated link with respect to $\Gamma$ and $\gamma$ for some $\epsilon>0$.
  For fixed $p\in\simplex$, there exists some $\alpha\geq 0$ such that
  \begin{equation}
    \label{eq:surrogate-regret-bound-fixed}
    \regret{\ell}{\psi(u)}{p}
    \leq
    \alpha \cdot \regret{L}{u}{p}~,
    % \inprod{p}{\ell(\psi(u))} - \risk{\ell}(p)
    % \leq
    % \alpha \cdot \left( \inprod{p}{L(u)} - \risk{L}(p) \right)~,
  \end{equation}
  for all $u\in\reals^d$.
\end{lemma}
\begin{proof}
  Since $L$ is polyhedral, there exist $a_1,\ldots,a_m \in \reals^d$ and $c\in\reals^m$ such that we may write $\inprod{p}{L(u)} = \max_{1\leq j\leq m} a_j \cdot u + c_j$.
  Let $A \in \reals^{m\times d}$ be the matrix with rows $a_j$, and let $b = \risk{L}(p)\ones - c$, where $\ones\in\reals^m$ is the all-ones vector.
  Then we have
  \begin{align*}
    S(A,b)
    &:= \{u\in\reals^d \mid A u \leq b\}
    \\
    &= \{u\in\reals^d \mid A u + c \leq \risk{L}(p)\ones\}
    \\
    &= \{u\in\reals^d \mid \forall i\, (A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \max_i \;(A u + c)_i \leq \risk{L}(p)\}
    \\
    &= \{u\in\reals^d \mid \inprod{p}{L(u)} \leq \risk{L}(p)\}
    \\
    & = \Gamma(p)~.
  \end{align*}
  Similarly, we have $\max_i\; (A u - b)_i = \inprod{p}{L(u)} - \risk{L}(p) = \regret{L}{u}{p} \geq 0$.
  Thus,
  \begin{align*}
    \|(Au - b)_+\|_\infty
    &= \max_i\; ((Au - b)_+)_i
    \\
    &= \max((Au - b)_1,\ldots,(Au - b)_m, 0)
    \\
    &= \max(\max_i\; (Au - b)_i, \, 0)
    \\
    &= \max_i\; (Au - b)_i
    % \\
    % &= \inprod{p}{L(u)} - \risk{L}(p)
    \\
    &= \regret{L}{u}{p}~.
  \end{align*}

  Now applying Theorem~\ref{thm:hoffman}, we have
  \begin{align*}
    d_\infty(u,\Gamma(p))
    &\leq H(A) \|(Au-b)_+\|_\infty
    \\
    &= H(A) \regret{L}{u}{p}~.
  \end{align*}
  Let $\beta := \max_{r\in\R} \regret{\ell}{r}{p}\geq 0$, which is well-defined as $\R$ is a finite set.
  To complete the proof, define $\alpha = \beta H(A)/\epsilon$.
  Given $u\in\reals^d$, if $\regret{\ell}{\psi(u)}{p} = 0$, we are done.
  Otherwise, we have from $\epsilon$-separation that $d_\infty(u,\Gamma(p)) > \epsilon$.
  The above then gives $H(A) \regret{L}{u}{p} > \epsilon$, which implies
  $(\beta H(A) / \epsilon) \regret{L}{u}{p} > \beta$.
  Finally, as $\regret{\ell}{\psi(u)}{p} \leq \beta$, the result follows.
\end{proof}

\subsection{Part 2: all $p$}

We now establish surrogate regret bounds for the case where a polyhedral loss embeds a discrete loss.
We will make use of the following lemma.

\begin{lemma}\label{lem:regret-linear}
  Let $L:\reals^d \to \reals^\Y_+$ be a surrogate loss which embeds a discrete loss $\ell:\R\to\reals^\Y_+$, and let $\gamma = \prop{\ell}$.
  For all $r\in\R$, and all $r'\in\R$ and $u\in\reals^d$, the functions $\regret{\ell}{r'}{\cdot}$ and $\regret{L}{u}{\cdot}$ linear on $\gamma_r$.
\end{lemma}
\begin{proof}
  When $p\in\gamma_r$, we have $\risk{\ell}(p) = \inprod{p}{\ell(r)}$.
  Similarly, by the definition of embedding, we have $\risk{L}(p) = \inprod{p}{L(\varphi(r))}$, where $\varphi:\R\to\reals^d$ is the embedding.
  We therefore have
  \begin{align*}
    \regret{\ell}{r'}{p} &= \inprod{p}{\ell(r')} - \risk{\ell}(p) = \inprod{p}{\ell(r')-\ell(r)}~,
    \\
    \regret{L}{u}{p} &= \inprod{p}{L(u)} - \risk{L}(p) = \inprod{p}{L(u)-L(\varphi(r))}~,
  \end{align*}
  which are both linear in $p$.  
\end{proof}

The next subsection will then establish the main result.

\begin{proposition}\label{prop:regret-embedding}
  Let $L:\reals^d \to \reals^\Y_+$ be a surrogate loss which embeds a discrete loss $\ell:\R\to\reals^\Y_+$, and define $\gamma := \prop{\ell}$ and $\Gamma := \prop{L}$.
  Let $\psi:\reals^d\to\R$ be an $\epsilon$-separated link with respect to $\Gamma$ and $\gamma$ for some $\epsilon>0$.
  Then there exists some $\alpha \geq 0$ such that
  \begin{equation}
    \label{eq:surrogate-regret-bound}
    \regret{\ell}{\psi(u)}{p}
    \leq
    \alpha \cdot \regret{L}{u}{p}~,
    % \inprod{p}{\ell(\psi(u))} - \risk{\ell}(p)
    % \leq
    % \alpha \cdot \left( \inprod{p}{L(u)} - \risk{L}(p) \right)~,
  \end{equation}
  for all $p\in\simplex$ and $u\in\reals^d$.
\end{proposition}
% \begin{proof}[Proof of Proposition~\ref{prop:regret-embedding}]
\begin{proof}
  Recall that for all $r\in\R$, the set $\gamma_r$ is a compact polyhedral subset of $\simplex$, and hence we may write $\gamma_r = \conv \Q_r$ where $\Q_r \subset \gamma_r$ is the finite set of vertices (extreme points) of $\gamma_r$.
  Define $\Q = \bigcup_{r\in\R} \Q_r$, which is again a finite set.
  From Lemma~\ref{lem:regret-bound-fixed-p}, for each $q \in \Q$ we have some $\alpha_q\geq 0$ such that eq.~\eqref{eq:surrogate-regret-bound} holds for $\alpha = \alpha_q$, $p=q$, and all $u\in\reals^d$.

  Now set $\alpha := \max_{q\in\Q} \alpha_q$, which is well-defined as $\Q$ is finite.
  Let $p \in \simplex$ and $u\in\reals^d$ be arbitrary.
  We must have $p\in\gamma_r$ for some $r\in\R$.
  \raft{Using non-emptiness of $\gamma$ here, not that it's a restriction at all}
  By the definition of $\Q_r$, we may write $p = \sum_{q\in\Q_r} \beta_q \, q$ for some $\beta_q \geq 0$, $\sum_{q\in\Q_r} \beta_r = 1$.
  Recall that $\Q_r \subset \gamma_r$.
  Thus,
  \begin{align*}
    \regret{\ell}{\psi(u)}{p}
    & = \regret{\ell}{\psi(u)}{\textstyle\sum_{q\in\Q_r} \beta_q \, q} &
    \\
    & = \sum_{q\in\Q_r} \beta_q \regret{\ell}{\psi(u)}{q} & \text{Lemma~\ref{lem:regret-linear}}
    \\
    & = \sum_{q\in\Q_r} \beta_q \alpha_q \regret{L}{u}{q} & \text{Lemma~\ref{lem:regret-bound-fixed-p}}
    \\
    & \leq \left(\max_{q\in\Q_r} \alpha_q\right) \sum_{q\in\Q_r} \beta_q \regret{L}{u}{q} & \text{$\Q_r$ is finite}
    \\
    & \leq \left(\max_{q\in\Q_r} \alpha_q\right) \regret{L}{u}{q} & \text{Lemma~\ref{lem:regret-linear}}
    \\
    & \leq \alpha \regret{L}{u}{q}~,
  \end{align*}
  completing the proof.
\end{proof}





\subsection{Part 3: all consistent surrogates}

\begin{lemma}\label{lem:calibrated-eps-sep}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is calibrated with respect to $\ell$.
  Define $\gamma := \prop{\ell}$ and $\Gamma := \prop{L}$.
  Then there exists $\epsilon>0$ such that $\psi$ is $\epsilon$-separated with respect to $\Gamma$ and $\gamma$.
\end{lemma}
\begin{proof}
  Suppose that $\psi$ is not $\epsilon$-separated for any $\epsilon>0$.
  Then letting $\epsilon_i := 1/i$ we have sequences $\{p_i\}_i \subset \simplex$ and  $\{u_i\}_i \subset \reals^d$ such that for all $i\in\mathbb N$ we have both $\psi(u_i) \notin \gamma(p_i)$ and $d_\infty(u_i,\Gamma(p_i)) \leq \epsilon_i$.
  First, observe that there are only finitely many values for $\gamma(p_i)$ and $\Gamma(p_i)$, as $\R$ is finite and $L$ is polyhedral.
  \raft{Using a result from the polyhedral paper here, that $\Gamma$ only takes on finitely many values}
  Thus, there must be some $p\in\simplex$ and some infinite subsequence indexed by $j\in J \subseteq \mathbb N$ where
  for all $j\in J$, we have $\psi(u_j) \notin \gamma(p)$ and $\Gamma(p_j) = \Gamma(p)$.

  Next, observe that, as $L$ is polyhedral, the expected loss $\inprod{p}{L(u)}$ is $\beta$-Lipschitz in $\|\cdot\|_\infty$ for some $\beta>0$.
  \raft{Could write $\beta$ explicitly; it's in my notes}
  Thus, for all $j\in J$, we have
  \begin{align*}
    d_\infty(u_i,\Gamma(p)) \leq \epsilon_j
    &\implies \exists u^*\in\Gamma(p) \|u_j-u^*\|_\infty \leq \epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \inprod{p}{L(u^*)} \right| \leq \beta\epsilon_j
    \\
    &\implies \left| \inprod{p}{L(u_j)} - \risk{L}(p) \right| \leq \beta\epsilon_j~.
  \end{align*}
  Finally, for this $p$, we have
  \begin{align*}
    \inf_{u:\psi(u)\notin\gamma(p)} \inprod{p}{L(u)}
    \leq
    \inf_{j\in J} \inprod{p}{L(u_j)}
    =
    \risk{L}(p)~,
  \end{align*}
  contradicting the calibration of $\psi$.
\end{proof}

\begin{theorem}\label{thm:regret-bound-poly-consistent}
  Let polyhedral surrogate $L:\reals^d \to \reals^\Y_+$, discrete loss $\ell:\R\to\reals^\Y_+$, and link $\psi:\reals^d\to\R$ be given such that $(L,\psi)$ is consistent with respect to $\ell$.
  Then there exists some $\alpha > 0$ such that
  \begin{equation}
    \label{eq:surrogate-regret-bound}
    \regret{\ell}{\psi(u)}{p}
    \leq
    \alpha \cdot \regret{L}{u}{p}~,
    % \inprod{p}{\ell(\psi(u))} - \risk{\ell}(p)
    % \leq
    % \alpha \cdot \left( \inprod{p}{L(u)} - \risk{L}(p) \right)~,
  \end{equation}
  for all $p\in\simplex$ and $u\in\reals^d$.
\end{theorem}

\raf{Here's my thought going forward: take any $(L,\psi)$ consistent with $\ell$, where $L$ is polyhedral and $\ell$ is discrete.  We know $L$ embeds some $\gamma'$ which refines $\gamma := \prop{\ell}$.  By Lemma~\ref{lem:calibrated-eps-sep}, we also know $\psi$ is $\epsilon$-separated for $\gamma$ (not $\gamma'$).  I think we can then run our link construction with $\epsilon' = \epsilon/2$ (or maybe even the same $\epsilon$, not sure) to find a link $\psi'$ which is $\epsilon'$-separated and calibrated from $L$ to $\gamma'$, but for which we can write $\psi = g \circ \psi'$ for some $g$.  Now letting $\ell'$ elicit $\gamma'$ (probably just $\ell'(r) = L(\varphi(r))$ from the embedding), we have Proposition~\ref{prop:regret-embedding} to bound the regret with respect to $\ell'$.  I think this should give us a regret bound back to $\ell$ but I'm losing the thread...}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
