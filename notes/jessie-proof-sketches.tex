\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage{../neurips-19/neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}

\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}{\mathbf{1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}


\title{Consistent Polyhedral Surrogates via Embedding}
%\title{Convex Surrogates via Polyhedral Losses}
\author{%
 Jessica Finocchiaro\raf{Jessie?} \\
 \texttt{jessica.finocchiaro@colorado.edu}\\
 CU Boulder
 \And
 Rafael Frongillo\\
 \texttt{raf@colorado.edu}\\
 CU Boulder
 \And
 Bo\\
 \texttt{bwag@colorado.edu}\\
 MSFT
}

\begin{document}

\maketitle

\section{Introduction}\label{sec:intro}



%
\begin{definition}[Polyhedral loss]
  A loss $L: \reals^d \to \reals^{\Y}_+$ is a \emph{polyhedral} if $L(u)_y$ is a polyhedral (convex) function of $u$ for each $y\in\Y$.
\end{definition}

\subsection{Links and Embedding}

To ensure consistency, we will seek ``separated'' link functions, which require that the excess loss of some report is bounded by (a constant times) the excess loss of the linked report.

\begin{definition}[Consistency]\label{def:consistency}
We say that a surrogate $L(u)$ is \emph{consistent} with respect to a discrete loss $\ell(r)$ if, for all $p \in \simplex$, \jessie{....?}
\end{definition}

\begin{definition}[Separated Link]\label{def:links}
  Let discrete loss $\ell:\R\to\reals^\Y_+$ and surrogate $L:\reals^d\to\reals^\Y_+$ be given.
  A \emph{link function} is a map $\psi:\reals^d\to\R$.
  We say that a link $\psi$ is \emph{$\delta$-separated} for some $\delta > 0$ if for all $p \in \simplex$ and $u\in\reals^d$, we have
  \begin{align*}
    \inprod{p}{L(u)} - \inf_{u' \in \reals^d} \inprod{p}{L(u')} \geq \delta\left(\inprod{p}{\ell(\psi(u))} - \min_{r \in \R} \inprod{p}{\ell(r)}\right)~.
  \end{align*}
  A link is \emph{separated} if it is $\delta$-separated for some $\delta>0$.
\end{definition}

\begin{definition}\label{def:loss-embed}
  A loss $L:\reals^d\to\reals^\Y$ \emph{embeds} a loss $\ell:\R\to\reals^\Y$ if there exists some injective embedding $\varphi:\R\to\reals^d$ such that
  (i) for all $r\in\R$ we have $L(\varphi(r)) = \ell(r)$, and (ii) for all $p\in\simplex,r\in\R$ we have
  \begin{equation}\label{eq:embed-loss}
    r \in \argmin_{r'\in\R} \inprod{p}{\ell(r')} \iff \varphi(r) \in \argmin_{u\in\reals^d} \inprod{p}{L(u)}~.
  \end{equation}
\end{definition}


\subsection{Property Elicitation}

\raf{Let's leave this here and see how much we really need in the end...}

To make headway, we will appeal to concepts and results from the property elicitation literature, which elevates the \emph{property}, or map from distributions to optimal reports, as a central object to study in its own right.
In our case, this map will often be multivalued, meaning a single distribution could yield multiple optimal reports.
(For example, when $p=(1/2,1/2)$, both $y=1$ and $y=-1$ optimize 0-1 loss.)
To this end, we will use double arrow notation to mean a mapping to all nonempty subsets, so that $\gamma: \simplex \toto \R$ is shorthand for $\gamma: \simplex \to 2^{\R} \setminus \emptyset$.

\begin{definition}[Property, level set]\label{def:property}
  A \emph{property} is a function $\Gamma:\simplex\toto\R$.
  The \emph{level set} of $\Gamma$ for report $r$ is the set $\Gamma_r := \{p : r \in \Gamma(p)\}$.
\end{definition}

\begin{definition}[Finite property, non-redundant]
  A property $\Gamma:\simplex\toto\R$ is \emph{redundant} if for some $r,r'\in\R$ we have $\Gamma_r \subseteq \Gamma_{r'}$, and \emph{non-redundant} otherwise.
  $\Gamma$ is \emph{finite} if it is non-redundant and $\R$ is a finite set.
  We typically write finite properties in lower case, as $\gamma$.
\end{definition}

Thus, $\Gamma(p)$ is the set of reports which should be optimal for a given distribution $p$, and $\Gamma_r$ is the set of distributions for which the report $r$ should be optimal.
(Note that our definitions align such that discrete losses elicit finite properties; both are non-redundant in the correct senses.)
For example, the \emph{mode} is the finite property $\mode(p) = \argmax_{y\in\Y} p_y$, and captures the set of optimal reports for 0-1 loss: for each distribution over the labels, one should report the most likely label.
In this case we say 0-1 loss \emph{elicits} the mode, as we formalize below.
% This terminology comes from the information elicitation \jessiet{information vs property... do we care?}\raft{we could also say economics literature; not ``property'' though since that doesn't offer an explanation of the word ``elicits''} literature~\citep{savage1971elicitation,osband1985information-eliciting,lambert2008eliciting}, in which a report $r$ is elicited from some forecaster by scoring her with a loss on the observed outcome $y$.

\begin{definition}[Elicits]
  A loss $L:\R\to\reals^\Y_+$, \emph{elicits} a property $\Gamma:\simplex \toto \R$ if
  \begin{equation}
    \forall p\in\simplex,\;\;\;\Gamma(p) = \argmin_{r \in \R} \inprod{p}{L(r)}~.
  \end{equation}
  As $\Gamma$ is uniquely defined by $L$, we write $\prop{L}$ to refer to the property elicited by a loss $L$.
\end{definition}

\begin{definition}
  A property $\Gamma : \simplex \toto \reals^d$ \emph{embeds} a property $\gamma : \simplex \toto \R$ if there exists some injective embdedding $\varphi:\R\to\reals^d$ such that for all $p\in\simplex,r\in\R$ we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
  Similarly, we say a loss $L:\reals^d\to\reals^\Y$ embeds $\gamma$ if $\prop{L}$ embeds $\gamma$.
\end{definition}
We can now see that a surrogate $L:\reals^d\to\reals^\Y$ embeds $\ell:\R\to\reals^\Y$ if and only if $\prop{L}$ embeds $\prop{\ell}$ via $\varphi$ and for all $r\in\R$ we have $L(\varphi(r)) = \ell(r)$.



\section{Embeddings and Polyhedral Losses}
\label{sec:poly-loss-embed}

\begin{proposition}\label{prop:embed-bayes-risks}
  A loss $L$ embeds discrete loss $\ell$ if and only if $\risk{L}=\risk{\ell}$.
  % A loss $L:\reals^d\to\reals^\Y$ embeds discrete $\ell:\R\to\reals^Y$ if and only if $\risk{L}=\risk{\ell}$.
\end{proposition}
\begin{proof}[Property version]
  \jessie{Stated in COLT Lemma 11 and Prop 12 (Lemma~\ref{lem:finite-full-dim} and Prop~\ref{prop:embed-trim} here), but I don't see us prove this in the original paper.}
  \jessie{The forward implication is a corollary of COLT Lemma 11}
  First for the forward implication, suppose $L$ embeds the discrete loss $\ell$.
  Then $\Gamma := \prop{L}$ embeds the finite property $\gamma$ elicited by $\ell$.
  Generating $\R := \{u_f\}_{f\in F}$ as is intuitive \jessie{fill in later}, we know that $\gamma$ is $\Gamma \cap \R$ up to a relabeling, and thus has the same Bayes Risk as $L|_\R$.
  Applying \jessie{COLT Lemma 11}, we then observe that $\risk{L} = \risk{L|_\R} = \risk{\ell}$.
  
  For the reverse implication, see proof below.
  \jessie{I don't see a way that COLT results make this easier, since we don't talk about Bayes Risk that much.}
  Consider 
\end{proof}
\hrulefill
\begin{proof}[Non-property version]
  \raf{Use distributions on the interior of the cells; since the risk is differentiable there, the loss is uniquely defined, and thus the same, and the same throughout the cell.}
  %Jessie's first attempt at a proof
  First, suppose $L$ embeds $\ell$, and fix $p \in \simplex$.
  We want to show for all $p \in \simplex$ that $\inf_{u\in\reals^d} L(u,p) = \min_{r \in \R} \ell(r, p)$.
  Consider two cases: first suppose there exists a $u \in \reals^d$ so that $u \in \arginf_{u' \in \reals^d}L(u',p)$ and $u \in \varphi(\R)$.
  Then
  \begin{align*}
  \inf_u L(u,p) = \min_{r \in \R} L(\varphi(r), p) = \min_{r \in \R} \ell(r, p)
  \end{align*}
   first by the case assumption and then by the definition of embedding.
  Now in the second case, if such a $u$ does not exist, then $\inf_{u \in \reals^d}L(u,p) < \min_r L(\varphi(r), p) = \min_r \ell(r,p)$.
  Therefore, $r \in \argmin_r' \ell(r', p) \implies \varphi(r) \not \in \argmin_u L(u,p)$ contradicts the assertion that $L$ embeds $\ell$.

  Now, to see that $\risk{\ell} = \risk{L} \implies L$ embeds $\ell$, consider $F$ to be the set of facets of the surface of $\risk{L}$.
  For each facet $f \in \F$, we know by nonredundancy that there is some $p \in \inter{f}$ so that $\risk{L}$ is differentiable at $p$.
  Moreover, by Frongillo and Kash~\cite[Theorem 2]{frongillo2014general} we know that both $\ell$ and $L$ have a unique minimizer at $p$, and since the subgradients of the risks are the same (by their equality), we know the minimizers of the losses are also the same, up to relabeling the reports via a bijection.
  Therefore, we can see that $L$ embeds $\ell$, as $L(\varphi(r),y) = \ell(r,y)$ for all $r\in\R$ and $y \in \Y$, which is a sufficient condition for embedding.
\end{proof}


\raft{Note at some point: matching risks is not necessary for \emph{consistency}, as evidenced by logistic loss for 0-1 loss.  Maybe we should just make the observation, earlier on, that \emph{embedding} is not necessary for consistency.}
\raft{Note: ``discrete loss'' now connotes some regularity as well, just like we did with finite properties: every report should correspond to a full-dimensional cell.}
\raft{Note: DKR (section 3.1) realized the importance of matching Bayes risks, but they could only give general results for strictly convex (concave I should say) risks, in part because they fixed the link function to be a generalization of $\sgn$.  In contrast, we focus exclusively on non-strictly-convex risks.}

From this more succinct embedding condition, we can in turn simplify the condition that a loss embeds \emph{some} discrete loss: it does if and only if its Bayes risk is polyhedral.
(We say a concave function is polyhedral if its negation is a polyhedral convex function.)
Note that Bayes risk, a function from distributions over $\Y$ to the reals, may be polyhedral even if the loss itself is not.

\begin{proposition}\label{prop:embed-risk-poly}
  A loss $L$ embeds a discrete loss if and only if $\risk{L}$ is polyhedral.
\end{proposition}
\begin{proof}[Property version]
  Suppose $\Gamma := \prop{L}$ embeds the finite property $\gamma$, elicited by the discrete loss $\ell$.
  We know that $\risk{\ell}$ is polyhedral as $\gamma$ is finite.
  Since $L$ embeds $\ell$, we know $\risk{L} = \risk{\ell}$, and therefore $\risk{L}$ is also polyhedral. 
  \jessie{This direction is shorter below without properties.}
  
  Now suppose $\risk{L}$ is polyhedral, and $\Gamma := \prop{L}$.
  For the set of facets $F$ of the surface of $\risk{L}$, construct $\R := \{u_f\}_{f \in F}$. 
  By \jessie{COLT Lemma 11}, we know $\Gamma$ embeds $\gamma := \Gamma \cap \R$ elicited by the discrete loss $\ell := L|_\R$. 
  Thus, $L$ embeds $\ell$.
\end{proof}
\hrulefill
\begin{proof}[Non-property version]
  For the forward direction, suppose $L$ embeds a discrete loss $\ell$.
  By Proposition~\ref{prop:embed-bayes-risks}, we know that $\risk{L} = \risk{\ell}$.
  Since $\ell$ is discrete, we know that $\risk{\ell}$ is polyhedral.
  Therefore, $\risk{L}$ is polyhedral.

  For the reverse implication, suppose $\risk{L}$ is polyhedral.
  For each facet $f \in F$ of $\risk{L}$, consider the report $u_f = \arginf_{u'} L(u', p)$ for some $p \in \inter f$.
  (Note that $u_f$ will be unique and the same regardless of the choice of $p\in \inter f$, since $\risk{L}$ is differentiable here.)
  Since $\risk{L}$ has a finite set of facets, the set $\R = \{u_f\}_{f \in F}$ is finite.
  Now define the discrete loss $\ell:\R \to \Y$ by $\ell(r,y) = L(r,y)$.

  To see that $L$ embeds this discrete $\ell$, consider the embedding function to be the identity for all $r \in \R$.
  By Proposition~\ref{prop:embed-bayes-risks}, it is sufficient to show $\risk{L} = \risk{\ell}$.
  For all $p \in \simplex$, we observe
  \begin{align*}
  	\risk{L}(p) = \inf_{u \in \reals^d} L(u,p) = \min_{r \in \R} L(r,p) = \min_{r \in \R} \ell(r, p) = \risk{\ell}(p)
  \end{align*}
  Note that the second equality holds because for every $p \in \simplex$, we know there is some $r \in \R$ such that $r \in \arginf_u L(u,p)$ by closure of subgradient sets, and thus the loss values are the same, regardless of if we restrict to $\R$ or stay in $\reals^d$.
  Thus, $L$ embeds the discrete $\ell$ as constructed by Proposition~\ref{prop:embed-bayes-risks}.

  \raf{Just take the discrete reports to be the facets, and use the Bayes risk result.}
\end{proof}

Combining Proposition~\ref{prop:embed-risk-poly} with the straightforward result that polyhedral losses have polyhedral Bayes risks, we obtain the first direction of our equivalence: every polyhedral loss embeds some discrete loss.

\begin{theorem}\label{thm:poly-embeds-discrete}
  Every polyhedral loss $L$ embeds a discrete loss.
\end{theorem}
\begin{proof}[Non-property version]
  We merely show $L$ polyhedral $\implies$ $\risk{L}$ polyhedral, and apply Proposition~\ref{prop:embed-risk-poly}.
  \raf{Not sure how trivial this is, but it's definitely true.  I suspect it has a very short proof.}
  \jessie{$L(u,p)$ polyhedral in $\reals^d$ implies $L^*(0,p) = -\risk{L}(p)$ polyhedral in $\simplex$ implies $\risk{L}$ polyhedral.
  Now we can apply Proposition~\ref{prop:embed-risk-poly} to observe the result.}

  \jessie{Can also show that $L$ embeds $L|_\R$ like the original paper and use COLT Lemma 11/Lemma~\ref{lem:finite-full-dim}.}
\end{proof}
\hrulefill
\begin{proof}[Property version]
  We know $L$ polyhedral implies $\risk{L}$ is polyhedral. \jessie{Does the above argument not work?  If it does, the other proof is shorter anyways.}
  As before, construct, $\R = \{u_f\}_{f \in F}$ for the uniquely optimal report associated with the interior of each facet of the loss surface on $\risk{L}$.
  To see our result, we specifically show that $L$ embeds $L|_\R$.
  
  Given the loss $L$, consider $\Gamma := \prop{L}$.
  We know that $\Gamma$ embeds $\gamma := \Gamma \cap \R$ via the identity embedding. \jessie{Cite}
  Moreover, $\ell := L|_\R$ elicits $\gamma$, and thus we can conclude that $L$ embeds the discrete loss $L|_\R$.
\end{proof}


\begin{theorem}\label{thm:discrete-loss-poly-embeddable}
  Every discrete loss $\ell$ is embedded by a polyhedral loss.
\end{theorem}
\begin{proof}[Property version]
  Consider $\gamma := \prop{\ell}$.
  The result is a corollary of \jessie{COLT Theorem 35} Theorem~\ref{thm:general-duality-embedding}, where $L$ is as constructed in the proof.
\end{proof}
\hrulefill
\begin{proof}[Non-property version]
  \raf{Already have the proof from Theorem~\ref{thm:general-duality-embedding}.  Should cite DKR and others...}
  It is sufficient to show the existence of a polyhedral loss $L$ such that $\risk{\ell} = \risk{L}$.
  Consider that $\risk{\ell}$ is closed and concave, since the domain $\simplex$ is closed.
  Now construct $L$ as in \cite[Proposition 3]{duchi2010multiclass}, where $H = \risk{\ell}$.
  Then we know that $\risk{L} = \risk{\ell}$ by their result, and thus $L$ embeds $\ell$.
  Moreover, we can see that $L$ is polyhedral as it is a linear transformation of a polyhedral concave function $\risk{\ell}$.
\end{proof}


\begin{corollary}\label{cor:finite-elicit-embed}
  Let $\gamma$ be a finite property.
  The following are equivalent.
  \begin{enumerate}\setlength{\itemsep}{0pt}
  \item $\gamma$ is elicitable.
  \item $\gamma$ is embeddable.
  \item $\gamma$ is embeddable via a polyhedral loss.
  \end{enumerate}
\end{corollary}
\begin{proof}[Property version]
	We just need to show that $2 \implies 1$.
	Consider the finite property $\gamma$ embedded by \jessie{the discrete loss?} $\ell$.
	Therefore, the property $\gamma' := \prop{\ell}$ .....
	\jessie{I don't see a way this follows from any of the above, since none of the statements are about elicitation.}
\end{proof}
\hrulefill
\begin{proof}[Non-property version]
  We trivially have $3\Rightarrow 2$.
  The direction $1\Rightarrow 3$ follows from Theorem~\ref{thm:discrete-loss-poly-embeddable}, by taking any discrete loss which elicits $\gamma$.
  Finally, to see $2\Rightarrow 1$,
  \raf{This follows from Proposition~\ref{prop:embed-trim}, but we're stating this as a corollary... let's see if it follows more directly from the above.  If it doesn't (one complication: there is no discrete loss starting from $2$ or $3$!) we can rephrase it as a theorem / proposition.}
\end{proof}


\hrule
\bigskip
OLD STUFF
\bigskip
\hrule

\begin{definition}\label{def:trim}
  Given an elicitable property $\Gamma:\simplex \toto\R$, we define $\trim(\Gamma) = \{\Gamma_u : u \in \R \text{ s.t. } \neg\exists u'\in\R,u'\neq u,\, \Gamma_u \subsetneq \Gamma_{u'}\}$ as the set of maximal level sets of $\Gamma$.
\end{definition}
\raft{Note for later: should be able to show that the union of trim is the simplex.}

\begin{lemma}\label{lem:finite-full-dim}
  Let $\gamma$ be a finite (non-redundant) property elicited by a loss $L$.
  Then the negative Bayes risk $G$ of $L$ is polyhedral, and the level sets of $\gamma$ are the projections of the facets of the epigraph of $G$ onto $\simplex$, and thus form a power diagram.
  In particular, the level sets $\gamma$ are full-dimensional in $\simplex$ (i.e.,\ of dimension $n-1$).
\end{lemma}


\begin{proposition}\label{prop:embed-trim}
  Let $\Gamma:\simplex\toto\reals^d$ be an elicitable property.
  The following are equivalent:
  \begin{enumerate}\setlength{\itemsep}{0pt}
  \item $\Gamma$ embeds a finite property $\gamma:\simplex \toto \R$.
  \item $\trim(\Gamma)$ is a finite set, and $\cup\,\trim(\Gamma) = \simplex$.
  \item There is a finite set of full-dimensional level sets $\Theta$ of $\Gamma$, and $\cup\,\Theta = \simplex$.
  \end{enumerate}
  Moreover, when any of the above hold, $\{\gamma_r : r\in\R\} = \trim(\Gamma) = \Theta$, and $\gamma$ is elicitable.
\end{proposition}

\begin{proposition}\label{prop:embed-link}
  Let $L$ elicit $\Gamma:\simplex\toto\reals^d$ which embeds a finite property $\gamma$.
  Then there is a calibrated link from $\Gamma$ to $\gamma$. % which can be taken to be separated if $L$ is polyhedral.
\end{proposition}
\begin{proof}
  Let $\gamma:\simplex\toto\R$.
  Proposition~\ref{prop:embed-trim} gives us that $\trim(\Gamma) = \{\gamma_r : r\in\R\}$.
  Then for any $u\in\reals^d$, there is some $r\in\R$ such that $\Gamma_u \subseteq \gamma_r$.
  The link $\psi : \reals^d \to \R$ which encodes these choices is calibrated.
  \raft{Commented out the separated stuff}
\end{proof}

\bibliographystyle{plainnat}
\bibliography{diss,extra}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix


\newpage
\section{To Sort}
\jessiet{Lol we should probably change the name of this}
\raft{No kidding!  Was this really in the COLT submission??  Hilarious.}
Several definitions from \citet{aurenhammer1987power}.
\begin{definition}\label{def:cell-complex}
  A \emph{cell complex} in $\reals^d$ is a set $C$ of faces (of dimension $0,\ldots,d$) which (i) union to $\reals^d$, (ii) have pairwise disjoint relative interiors, and (iii) any nonempty intersection of faces $F,F'$ in $C$ is a face of $F$ and $F'$ and an element of $C$.
\end{definition}

\begin{definition}\label{def:power-diagram}
  Given sites $s_1,\ldots,s_k\in\reals^d$ and weights $w_1,\ldots,w_k \geq 0$, the corresponding \emph{power diagram} is the cell complex given by
  \begin{equation}
    \label{eq:pd}
    \cell(s_i) = \{ x \in\reals^d : \forall j \in \{1,\ldots,k\} \, \|x - s_i\|^2 - w_i \leq \|x - s_j\| - w_j\}~.
  \end{equation}
\end{definition}

\begin{definition}\label{def:affine-equiv}
  A cell complex $C$ in $\reals^d$ is \emph{affinely equivalent} to a (convex) polyhedron $P \subseteq \reals^{d+1}$ if $C$ is a (linear) projection of the faces of $P$.
\end{definition}

\begin{theorem}[\cite{aurenhammer1987power}]\label{thm:aurenhammer}
  A cell complex is affinely equivalent to a convex polyhedron if and only if it is a power diagram.
\end{theorem}

In particular, one can consider the epigraph of a polyhedral convex function on $\reals^d$ and the projection down to $\reals^d$; in this case we call the resulting power diagram \emph{induced} by the convex function.
We extend Aurenhammer's result to a weighted sum of convex functions, showing that the induced power diagram is the same for any choice of strictly positive weights.

\begin{lemma}\label{lem:polyhedral-pd-same}
  Let $f_1,\ldots,f_k:\reals^d\to\reals$ be polyhedral convex functions.
  \jessiet{Replace $k$ with $m$?}
  The power diagram induced by $\sum_{i=1}^k p_i f_i$ is the same for all $p \in \inter\simplex$.
\end{lemma}
\begin{proof}
  For any convex function $g$ with epigraph $P$, the proof of~\citet[Theorem 4]{aurenhammer1987power} shows that the power diagram induced by $g$ is determined by the facets of $P$.
  Let $F$ be a facet of $P$, and $F'$ its projection down to $\reals^d$.
  It follows that $g|_{F'}$ is affine, and thus $g$ is differentiable on $\inter F'$ with constant derivative $d\in\reals^d$.
  Conversely, for any subgradient $d'$ of $g$, the set of points $\{x\in\reals^d : d'\in\partial g(x)\}$ is the projection of a face of $P$; we conclude that $F = \{(x,g(x))\in\reals^{d+1} : d\in\partial g(x)\}$ and $F' = \{x\in\reals^d : d\in\partial g(x)\}$.

  Now let $f := \sum_{i=1}^k f_i$ with epigraph $P$, and $f' := \sum_{i=1}^k p_i f_i$ with epigraph $P'$.
  By \raf{Rockafellar}\jessiet{\cite{rockafellar1997convex}}, $f,f'$ are polyhedral.
  We now show that $f$ is differentiable whenever $f'$ is differentiable:
  \begin{align*}
    \partial f(x) = \{d\}
    &\iff \sum_{i=1}^k \partial f_i(x) = \{d\} \\
    &\iff \forall i\in\{1,\ldots,k\}, \; \partial f_i(x) = \{d_i\} \\
    &\iff \forall i\in\{1,\ldots,k\}, \; \partial p_i f_i(x) = \{p_id_i\} \\
    &\iff \sum_{i=1}^k \partial p_if_i(x) = \left\{\sum_{i=1}^k p_id_i\right\} \\
    &\iff \partial f'(x) = \left\{\sum_{i=1}^k p_id_i\right\}~.
  \end{align*}
  From the above observations, every facet of $P$ is determined by the derivative of $f$ at any point in the interior of its projection, and vice versa.
  Letting $x$ be such a point in the interior, we now see that the facet of $P'$ containing $(x,f'(x))$ has the same projection, namely $\{x'\in\reals^d : \nabla f(x) \in \partial f(x')\} = \{x'\in\reals^d : \nabla f'(x) \in \partial f'(x')\}$.
  Thus, the power diagrams induced by $f$ and $f'$ are the same.
  The conclusion follows from the observation that the above held for any strictly positive weights $p$, and $f$ was fixed.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:polyhedral-embed}]
  Let $L:\reals^d\to\reals_+^\Y$ be a polyhedral loss.
  For all $p$, let $P(p)$ be the epigraph of the convex function $u\mapsto \inprod{p}{L(u)}$.
  From Lemma~\ref{lem:polyhedral-pd-same}, we have that the power diagram induced by the projection of $P(p)$ onto $\reals^d$ is the same for any $p\in\inter\simplex$.
  Let $q\in\inter\simplex$ be the uniform distribution on $\Y$, and $V_\Y$ be the set of vertices of $P(q)$ projected onto $\reals^d$.
  By the above, this set is the same had we replaced $q$ by any $p\in\inter\simplex$.
  \raf{I just realized that the focus on vertices is ill-fated: the hinge-with-extra-dimenson example shows that there might not be \emph{any} vertices.  We need to stick to faces (not necessarily facets) of the $P(q)$.}

  Now let $\Gamma := \Gamma[L]$.
  We claim for all $p\in\inter\simplex$, that $\Gamma(p) \cap V_\Y \neq \emptyset$.
  To see this, let $u \in \Gamma(p)$, and $u' = (u,\inprod{p}{L(u)}) \in P(p)$.
  The optimality of $u$ is equivalent to $u$ being contained in the face exposed by the normal $(0,\ldots,0,-1)\in\reals^{d+1}$, which is a face of $P(p)$.
  Let $v'\in\reals^{d+1}$ be a vertex on such a face, and $v\in V_\Y$ its projection onto $\reals^d$.
  Then $v$ is also optimal, and therefore $v\in\Gamma(p)$.

  Now consider $\Y'\subset \Y$.
  Applying the above argument on distributions $p$ with support exactly $\Y'$, we have a similar guarantee: a finite set $V_{\Y'}$ such that $\Gamma(p) \cap V_{\Y'} \neq \emptyset$ for all $p$ with support exactly $\Y'$.
  (When $\Y' = \{y\}$ is a singleton, we simply take the projected vertices of $L(\cdot)_y$.)

  Thus, taking $V = \bigcup_{\Y'\subseteq\Y} V_{\Y'}$, we have for all $p\in\simplex$ that $\Gamma(p) \cap V \neq \emptyset$.
  This implies that $\trim(\Gamma) \subseteq \{\Gamma_v : v\in V\}$, which is finite, so Proposition~\ref{prop:embed-trim} now gives the conclusion.
  \jessiet{Do we need to show the preimage of $\Gamma$ is $V$?}

  \raft{I might be delusional, but this second part ended up being much slicker than I'd thought, by essentially chaining definitions and maps.  Please check!}
  For the second part, let $\gamma':\simplex\toto\R'$ be the finite elicitable property embedded by $L$, with embedding $\varphi:\R'\to\reals^d$, and let $\psi$ be a calibrated link to a non-redundant elicitable property $\gamma:\simplex\toto\R$.
  Then letting $\psi' = (\psi \circ \varphi):\R'\to\R$, we see that $\psi'$ is a calibrated link from $\gamma'$ to $\gamma$:
  for all $r'\in\R'$, we have $\gamma'_{r'} = \prop{L}_{\varphi(r')} \subseteq \gamma_{\psi(\varphi(r'))}$.
  In particular, $\gamma'$ refines $\gamma$, and as $\gamma'$ is finite, $\gamma$ must be finite.
\end{proof}

\begin{theorem}\label{thm:general-duality-embedding}
  Let $\gamma:\simplex\toto\R$ be a non-redundant elicitable property, elicited by a loss whose Bayes risk is a closed function.
  Then $\gamma$ is $(n-1)$-embeddable, where $n=|\Y|$.
  \raft{Low priority: what conditions on $L$ or $\gamma$ ensure that the Bayes risk is closed?}
  \jessiet{\url{https://en.wikipedia.org/wiki/Closed_convex_function} so the simplex $\simplex$  being closed should suggest the Bayes Risk and $G$ are closed.}\raft{It's more complicated than that though -- e.g. take a continuous convex function on the simplex and and then increase the values of the function arbitrarily on the poles of the simplex.  Still convex, but not closed (or continuous)}
\end{theorem}
\begin{proof}
  $G:\simplex\to\reals$ be the negative Bayes risk of the loss $L_\gamma$ eliciting $\gamma$, given by $G(p) = -\min_{r\in\R} \inprod{p}{L_\gamma(r)}$.
  Let $C:\reals^\Y\to\reals$ be the convex conjugate of $G$, given by $C(u) = \sup_{p\in\simplex} \inprod{p}{u} - G(p)$, which is finite on all of $\reals^\Y$ by~\citet[Corollary 13.3.1]{rockafellar1997convex} as $\dom\, G = \simplex$ is bounded.

  By~\citet[Theorem 2]{frongillo2014general} we know that for some $\D \subseteq \partial G(\simplex) \subseteq \reals^\Y$ we have a bijection $s : \R \to \D$ such that $\gamma(p) = s^{-1}(\D \cap \partial G(p))$.
  (In other words, $\gamma$ is a relabeling of subgradients of $G$.)
  Thus, we can write
  \begin{equation}
    \label{eq:1}
    r \in \gamma(p) \iff s(r) \in \partial G(p) \iff p \in \partial C(s(r))~,
  \end{equation}
  where the final equivalence follows from~\cite[Corollary 23.5.2]{rockafellar1997convex} as $G$ is a closed convex function.

  Now define $L:\reals^n\to\reals_+^\Y$ by $L(u) = C(u)\ones - u$, where $\ones\in\reals^\Y$ is the all-ones vector.
  \jessiet{Geometrically, what does this look like?  I'm imagining it as our tipping cost function, but I'm not sure on that.}
  We will show that $L$ embeds $\gamma$.
  As $L$ is convex, we may write the optimality condition as
  \begin{equation}
    \label{eq:2}
    u \in \prop{L}(p) \iff u \in \argmin_{u'} \inprod{p}{L(u')} \iff 0 \in \partial \inprod{p}{L(u)}~,
  \end{equation}
  where the subgradient $\partial$ is with respect to $u$.
  Plugging in our definition of $L$, we have
  \begin{align*}
    0 \in \partial \inprod{p}{L(u)}
    & \iff 0 \in \partial \inprod{p}{\left(C(u)\ones - u\right)}
    \\
    % & \iff 0 \in \partial \left(C(u) - p\cdot u\right)
    % \\
    & \iff 0 \in \partial C(u) - \{p\}
    \\
    & \iff p \in \partial C(u)~.
  \end{align*}
  \jessiet{Not sure I see this- REVISIT.}

  Together with eq.~\eqref{eq:1}, we now have for any $u \in \D$ and $p\in\simplex$ that
  \begin{equation}
    \label{eq:3}
    u \in \prop{L}(p) \iff p \in \partial C(u) \iff s^{-1}(u) \in \gamma(p)~.
  \end{equation}
  Recalling that $s$ is a bijection, we conclude that $L$ embeds $\gamma$ with embedding function $\varphi:\R\to\reals^\Y$ given by $\varphi(r) = s(r)$.

  We have now shown that $\gamma$ is $n$-embeddable.
  \jessiet{Notation question: is $C'(u';\ones)$ the directional derivative in the $\ones$ direction?}
  To drop a dimension, first note that $C$ is linear in the $\ones$ direction: for all $u'\in\reals^\Y$, the directional derivative $C'(u';\ones) = \sup_{p\in\partial C(u')} \inprod{p}{\ones} = 1$ as $\partial C(u')\subseteq\simplex$ for all $u'$; thus
  $C(u+\alpha\ones) = C(u) + \int_0^\alpha C'(u+\beta\ones;\ones) d\beta = C(u) + \alpha$.
  This linearity translates to invariance of $L$ in the direction of $\ones$:
  \begin{align*}
    L(u + \alpha\ones) = C(u+\alpha\ones)\ones - (u + \alpha\ones) = C(u)\ones + \alpha\ones - u - \alpha\ones = L(u)~.
  \end{align*}
  We now have for any $u\in\D$,
  \begin{align*}
    s^{-1}(u) \in \gamma(p) \iff u \in \prop{L}(p) \iff u - u_n\ones \in \prop{L}(p)\cap\R_2~.
  \end{align*}
  Letting $L':\reals^{n-1}\to\reals^\Y$ given by $L'(u') = L((u',0))$, \jessiet{specify for $u \in \reals^{n-1}$} and evoking Lemma~\ref{lem:loss-restrict}, we conclude $\prop{L'}(p) = \prop{L}\cap\R_2$ and thus $L'$ embeds $\gamma$ as well, with the embedding $\varphi(r) = (s(r)_1 - s(r)_n, \ldots, s(r)_{n-1} - s(r)_n)$.
  \jessiet{The notation $C'$ being a derivative kind of bothers me since $L'$ is used for another generic function... although I may just be misreading it or not familiar with standard notations.}
\end{proof}

\section{Dimension 1}\label{app:dimension-1}
\begin{proof}[Proof of Proposition~\ref{prop:embed-trim}]
  Let $L$ elicit $\Gamma$.

  1 $\Rightarrow$ 2:
  By the embedding condition, taking $\R_1 = \reals^d$ and $\R_2 = \varphi(\R)$ satisfies the conditions of Lemma~\ref{lem:loss-restrict}: for all $p\in\simplex$, as $\gamma(p) \neq \emptyset$ by definition, we have some $r\in\gamma(p)$ and thus some $\varphi(r) \in \Gamma(p)$.
  Let $G(p) := -\min_{u\in\reals^d} \inprod{p}{L(u)}$ be the negative Bayes risk of $L$, which is convex, and $G_{\R}$ that of $L|_{\varphi(\R)}$.
  By the Lemma, we also have $G = G_\R$.
  As $\gamma$ is finite, $G$ is polyhedral.
  Moreover, the projection of the epigraph of $G$ onto $\simplex$ forms a power diagram, with the facets projecting onto the level sets of $\gamma$, the cells of the power diagram.
  (See Theorem~\ref{thm:aurenhammer}.)
  As $L$ elicits $\Gamma$, for all $u\in\reals^d$, the hyperplane $p\mapsto \inprod{p}{L(u)}$ is a supporting hyperplane of the epigraph of $G$ at $(p,G(p))$ if and only if $u\in\Gamma(p)$.
  This supporting hyperplane exposes some face $F$ of the epigraph of $G$, which must be contained in some facet $F'$.
  Thus, the projection of $F$, which is $\Gamma_u$, must be contained in the projection of $F'$, which is a level set of $\gamma$.
  We conclude that $\Gamma_u \subseteq \gamma_r$ for some $r\in\R$.
  Hence, $\trim(\Gamma) = \{\gamma_r : r\in\R\}$, which is finite, and unions to $\simplex$.


  %%% The below is basically the argument from the Lemma, for this specific case.
  % We will first show $L'$ elicits $\gamma$, where $L':\R\to\reals^\Y_+$ is defined by $L'(r) = L(\varphi(r))$.
  % From the definition of embedding, for all $p\in\simplex$ we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p) = \argmin_{u\in\reals^d} p\cdot L(u)$.
  % Thus, $r\in\gamma(p)$ implies $r \in \argmin_{r'\in\R} p\cdot L(\varphi(r')) = \argmin_{r'\in\R} p\cdot L'(r')$.
  % For the other direction, let $r \in \argmin_{r'\in\R} p\cdot L'(r')$, and let $r'\in\gamma(p)$ (where use $\gamma(p) \neq \emptyset$).
  % By the above, we have $r'\in\argmin_{r'\in\R} p\cdot L'(r')$ as well, so we must have $p\cdot L'(r) = p\cdot L'(r')$, and therefore $p\cdot L(\varphi(r)) = p\cdot L(\varphi(r'))$.
  % But we also have $\varphi(r') \in \argmin_{u\in\reals^d} p\cdot L(u)$, and therefore $\varphi(r) \in \argmin_{u\in\reals^d} p\cdot L(u) = \Gamma(p)$.
  % By the definition of embedding, we now have $r \in \gamma(p)$, as was to be shown.

  % for all the given that $\Gamma$ embeds the finite property $\gamma$, we show that $\trim(\Gamma) = \{\Gamma_u : u \in \varphi(\R) \} = \{\gamma_r : r \in \R\}$.
  % By elicitability, each level set $\Gamma_u$ corresponds to a subgradient of the expected score function, call it $G(p) := -\langle p, L(\Gamma(p))\rangle$ for any choice from $\Gamma$.
  % We claim $G$ is polyhedral. $\gamma$ is finite (and by definition non-redundant), so each level set $\gamma_r = \Gamma_{\phi(r)}$ is full-dimensional\bo{need to prove this separately in a lemma}.
  % So the corresponding subgradient is tangent to $G$ above a full-dimensional level set, and this is true for every $\gamma_r$.
  % Every $p$ is in one of these level sets and so is below some facet.
  % This implies that $G$ is polyhedral with facets exactly corresponding to $\{\gamma_r : r \in \R\} = \{\Gamma_u : u \in \varphi(\R)\}$.
  % Finally, for any $u \not\in \varphi(\R)$, we have\bo{need to cite a lemma statement, per discussion with Raf} that $\Gamma_u$ lies below some face of the epigraph of $G$, hence is contained in the set lying below some facet, which is one of the $\Gamma_{\varphi(r)}$.
  % So the maximal level sets of $\Gamma$ are exactly those lying below facets, which are exactly the level sets of $\gamma$.
  % This gives 2, and the fact that the level sets of $\gamma$ are full-dimensional and union to $\Delta_{\Y}$ gives 3.

  % Clearly, $\trim(\Gamma) \supseteq \{\Gamma_u : u \in \varphi(\R)\}$ as we are simply restricting the number of level sets we are indexing over.
%  Now, consider some $\theta \in \trim(\Gamma)$ and the report $u$ so that $\theta = \Gamma_u$.
%  If $u \in \varphi(\R)$, then clearly we are done.
%  If $u \not\in \varphi(\R)$ but there is some report $w \in \varphi(\R)$ so that $\Gamma_u = \Gamma_w$, then we are again done since $\trim(\Gamma)$ is non-redundant.
%  If there is no such $w \in \R$, then we claim $\Gamma_u \not \in \trim(\Gamma)$.
%  To see this, consider that if $\Gamma_u$ is full-dimensional, it must be a relabeling of another report by Lemma~\ref{lem:unique-opt-on-inter} in the Appendix.
%  If $\Gamma_u$ is not full-dimensional, it must be a proper subset of some full-dimensional level set by Lemma~\ref{lem:trim-subsets-not-full-dimensional}, and therefore not in $\trim(\Gamma)$.
%%\jessiet{skimmed over this, but follows from the convexity of the scoring rule.  My thought for the paper is to put that proof in the appendix,and just have a footnote or side note here with a reference to the proof in the appendix.}
%  Thus, we conclude $\trim(\Gamma) \subseteq \{\Gamma_u : u \in \varphi(\R)\}$.
%  Since the sets are equal and the latter is finite, we conclude that if $\Gamma$ embeds the finite property $\gamma$, then $\trim(\Gamma)$ is finite.

%  For $2 \implies 3$, since each level set $\theta \in \trim(\Gamma)$ is full-dimensional by Lemma~\ref{lem:trim-full-dim}, suppose for now that $\Theta := \trim(\Gamma)$.
%  %As $\Gamma$ is nondegenerate, for every $p \in \simplex$, there must be some report $u$ corresponding to the level set such that $p \in \Gamma_u$.
%  Since $\Gamma$ is nondegenerate, for all $p \in \simplex$, there is a level set $\theta \in \trim(\Gamma)$ such that $p \in \theta$, since the level sets of elicitable properties are closed and convex, and only level sets that are not full-dimensional level sets are removed in taking $\trim(\Gamma)$.
%  If, for some $p \in \simplex$, the level set $\Gamma_u$ corresponding to $p$ was removed, it would be because there is a level set corresponding to $w \in \reals^d$ such that $\Gamma_u \subset \Gamma_w$, but then $p \in \Gamma_w$.

  2 $\Rightarrow$ 3: let $\R = \{u_1,\ldots,u_k\} \subseteq\reals^d$ be a set of distinct reports such that $\trim(\Gamma) = \{\Gamma_{u_1},\ldots,\Gamma_{u_k}\}$.
  Now as $\cup\,\trim(\Gamma) = \simplex$, for any $p\in\simplex$, we have $p\in\Gamma_{u_i}$ for some $u_i\in\R$, and thus $\Gamma(p) \cap \R \neq \emptyset$.
  We now satisfy the conditions of Lemma~\ref{lem:loss-restrict} with $\R_1 = \reals^d$ and $\R_2 = \R$.
  The property $\gamma:p\mapsto\Gamma(p)\cap\R$ is non-redundant by the definition of $\trim$, finite, and elicitable.
  Now from Lemma~\ref{lem:finite-full-dim}, the level sets $\Theta = \{\gamma_r:r\in\R\}$ are full-dimensional, and union to $\simplex$.
  Statement 3 then follows from the fact that $\gamma_r = \Gamma_r$ for all $r\in\R$.

  %%% Again, this is subsumed by the Lemma
  % Let $\R = \{u_1,\ldots,u_k\}$; we show that $\gamma:\simplex\toto\R$, $\gamma(p) = \{u_i : p\in\Gamma_{u_i}\} = \Gamma(p) \cap \R$, is elicitable.
  % Define $L':\R\to\reals^\Y_+$ by $L'(u_i) = L(u_i)$.
  % Clearly, if $u_i \in \gamma(p)$, then $u_i \in \Gamma(p) = \argmin_{u\in\reals^d} p\cdot L(u)$, and thus $u_i \in \argmin_{u\in\R} p\cdot L'(u)$.
  % For the other direction, suppose $u_i \in \argmin_{u\in\R} p\cdot L'(u)$.
  % For any $u' \in \Gamma(p)$, by the definition of $\trim$, we have some $u_j\in\R$ for which $u_j\in\Gamma(p) = \argmin_{u\in\reals^d} p\cdot L(u)$ as well.
  % But now we must have $p\cdot L(u_i) = p\cdot L(u_j)$, and thus $u_i \in \Gamma(p)$ as well.
  % As $u_i\in\R$, we have $u_i \in \Gamma(p)\cap \R = \gamma(p)$, as desired.


  3 $\Rightarrow$ 1: let $\Theta = \{\theta_1,\ldots,\theta_k\}$.
  For all $i\in\{1,\ldots,k\}$ let $u_i\in\reals^d$ such that $\Gamma_{u_i} = \theta_i$.
  Now define $\gamma:\simplex\toto\{1,\ldots,k\}$ by $\gamma(p) = \{i : p\in\theta_i\}$, which is non-degenerate as $\cup\,\Theta = \simplex$.
  By construction, we have $\gamma_i = \theta_i = \Gamma_{u_i}$ for all $i$, so letting $\varphi(i) = u_i$ we satisfy the definition of embedding, namely statement 1.
\end{proof}


\begin{definition}\label{def:monotone-prop}
  A property $\Gamma:\simplex\toto\R$ is \emph{monotone} if there are maps $a:\R\to\reals^\Y$, $b:\R\to\reals^\Y$ and a total ordering $<$ of $\R$ such that the following two conditions hold.
  \begin{enumerate}
  \item For all $r\in\R$, we have $\Gamma_r = \{p\in\simplex : \inprod{a(r)}{p} \leq 0 \leq \inprod{b(r)}{p} \}$.
  \item For all $r < r'$, we have $a(r) \leq b(r) \leq a(r') \leq b(r')$ (component-wise).
  % \item For all $r,r'\in\R$ and $p\in\Gamma_{r'}\setminus\Gamma_r$, we have $b(r) \cdot p < 0 \implies r' > r$ and $a(r) \cdot p > 0 \implies r' < r$.
  \end{enumerate}
\end{definition}

\begin{proof}[Proof of Proposition~\ref{prop:orderable-embed}]
  Let $\R = \{r_1,\ldots,r_k\}$ be the report space for $\gamma$.
  From Lemma~\ref{lem:prop-L-monotone}, stating that finite properties are orderable if and only if they are monotone, we have functions $a:\R\to\reals^\Y, b:\R\to\reals^\Y$ satisfying the following: (i) $\gamma_{r_i} = \{p\in\simplex : \inprod{a(r_i)}{p} \leq 0 \leq \inprod{b(r_i)}{p} \}$ for all $i\in\{1,\ldots,k\}$, and (ii) $a(r_i) \leq b(r_i) = a(r_{i+1}) \leq b(r_{i+1})$ for all $i \in \{1,\ldots,k-1\}$.
  We now define $\varphi(r_i) = i \in \reals$, and define $L$ as follows:
  \begin{equation*}
    L(u)_y =
    \begin{cases}
      a(r_1) u & u < 1 \\
      c_i + b(r_i) u & u \in [i,i+1), i\in\{1,\ldots,k\} \\
      c_k + b(r_k) u & u \geq k+1
    \end{cases}~,
  \end{equation*}
  where the $c_i$ are chosen to make $L(u)_y$ continuous, namely, $c_i = \sum_{j=1}^{i} a(r_j)_y$.
  (Recall that $b(r_i) = a(r_{i+1})$.)
  By the condition (ii) above, $L(\cdot)_y$ is convex for all $y\in\Y$, and moreover, its subgradients at $\varphi(\R) = \{1,\ldots,k\}$ are given by
  $\partial L(i)_y = [a(r_i)_y,b(r_i)_y]$.
  \jessiet{I believe it, but it might help a reader to explain why it's convex a little more.}
  By condition (i), for all $i\in\{1,\ldots,k\}$ and $p\in\simplex$ we now have
  \begin{align*}
    i \in \prop{L}(p)
    &\iff 0 \in \partial \inprod{p}{L(i)} \\
    % &\iff 0 \in \sum_{y\in\Y} p_y \partial L(i)_y \\
    &\iff 0 \in \sum_{y\in\Y} p_y [a(r_i)_y,b(r_i)_y] \\
    &\iff \inprod{a(r_i)}{p} \leq 0 \leq \inprod{b(r_i)}{p} \\
    &\iff r_i \in \gamma(p)~,
  \end{align*}
  where the sum is a Minkowski sum.
\end{proof}

\begin{lemma}\label{lem:orderable-monotone}
  A finite property is orderable if and only if it is monotone.
\end{lemma}
\begin{proof}
  Let $\gamma:\simplex\toto\R$ be finite and monotone.
  Then we can use the total ordering of $\R$ to write $\R = \{r_1,\ldots,r_k\}$ such that $r_i < r_{i+1}$ for all $i \in \{1,\ldots,k-1\}$.
  We now have $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : \inprod{a(r_{i+1})}{p} \leq 0 \leq \inprod{b(r_i)}{p} \}$.
  If this intersection is empty, then there must be some $p$ with $\inprod{b(r_i)}{p} < 0$ and $\inprod{a(r_{i+1})}{p} > 0$; by monotonicity, no earlier or later reports can be in $\gamma(p)$, so we see that $\gamma(p) = \emptyset$, a contradiction.
  Thus the intersection is nonempty, and as we also know $b(r_i) \leq a(r_{i+1})$ we conclude $b(r_i) = a(r_{i+1})$, and the intersection is the hyperplane defined by $b(r_i) = a(r_{i+1})$.

  For the converse, let $\gamma:\simplex\toto\R$ be finite and orderable.
  From~\cite[Theorem 4]{lambert2018elicitation}, we have positively-oriented normals $v_i\in\reals^\Y$ for all $i \in \{1,\ldots,k-1\}$ such that $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : \inprod{v_i}{p} = 0\}$, and moreover, for all $i \in \{2,\ldots,k-1\}$, we have $\gamma_{r_i} = \{p\in\simplex : \inprod{v_{i-1}}{p} \leq 0 \leq \inprod{v_i}{p}\}$, while $\gamma_{r_1} = \{p\in\simplex : 0 \leq \inprod{v_1}{p} \}$ and $\gamma_{r_k} = \{p\in\simplex : \inprod{v_{k-1}}{p} \leq 0\}$.
  \jessiet{Should the sign on $\gamma_{r_k}$ be changed?}
  From the positive orientation of the $v_i$, we have for all $p\in\simplex$ that $\sgn(\inprod{v_i}{p})$ is monotone in $i$.
  In particular, it must be that for all $y$, $\sgn((v_i)_y)$ is monotone in $i$, taking the distribution with all weight on outcome $y$.
  % (Similarly, if $\gamma(\ones_y) = \{r_j,r_{j+1}\}$, then $(v_i)_y = v_i \cdot \ones_y < 0$ for $i < j$, $(v_j)_y = 0$, and $(v_i)_y > 0$ for $i > j$.)
  \raft{This observation could use a better proof.}

  For all $i\in\{2,\ldots,k-1\}$, we wish to find $\alpha_i \geq 0$ such that $v_{i-1} \leq \alpha_i v_i$ (component-wise).
  \jessiet{How should $w$ be interpreted?}
  To this end, fix $i$ and let $v = v_{i-1}, w = v_{i}, \alpha=\alpha_i$.
  By the above, there is no $y\in\Y$ with $v_y > 0 > w_y$, so the condition $v \leq \alpha w$ is equivalent to the following:

  \begin{enumerate}
  \item[(i)] For all $y$ with $v_y,w_y < 0$, $\alpha \leq v_y/w_y$.
  \item[(ii)] For all $y$ with $v_y,w_y > 0$, $\alpha \geq v_y/w_y$.
  \end{enumerate}
  Defining $\Y^- = \{y\in\Y: v_i,w_y < 0\}, \Y^+ = \{y\in\Y: v_i,w_y > 0\}$, these conditions are in turn equivalent to $\min_{y\in\Y^-} v_y/w_y \geq \max_{y\in\Y^+} v_y/w_y$, with the usual conventions $\min \emptyset = \infty,\; \max \emptyset = -\infty$.
  Suppose this inequality were not satisfied.
  Then we would have $y \in \Y^-,y'\in\Y^+$ such that $0 < v_y/w_y < w_{y'}/v_y$, which would in turn imply $|v_y|/v_{y'} < |w_y| / w_{y'}$.
  Letting $c = \tfrac 1 2 \left(|w_y| / w_{y'} + |v_y|/v_{y'}\right)$ and taking $p$ to be the distribution with weight $1/(1+c)$ on $y$ and $c/(1+c)$ on $y'$, we see that
  \begin{align*}
    \inprod{v}{p} &= \frac 1 {1+c} \left(v_y + \tfrac 1 2 (|w_y| / w_{y'} + |v_y|/v_{y'})v_{y'}\right) > \frac 1 {1+c} \left(v_y + (|v_y|/v_{y'})v_{y'}\right) = 0
    \\
    \inprod{w}{p} &= \frac 1 {1+c} \left(w_y + \tfrac 1 2 (|w_y| / w_{y'} + |v_y|/v_{y'})v_{y'}\right) < \frac 1 {1+c} \left(w_y + (|w_y|/w_{y'})w_{y'}\right) = 0~,
  \end{align*}
  thus violating the observation that $\sgn(\inprod{v_i}{p})$ is monotone in $i$ (recall that $v = v_{i-1}, w = v_{i}$).

  We can now construct the $a,b$ in Definition~\ref{def:monotone-prop}.
  Letting
  \[\alpha_i = \tfrac 1 2 \left(\min_{y\in\Y^-(i)} (v_{i-1})_y/(v_{i})_y + \max_{y\in\Y^+(i)} (v_{i-1})_y/(v_{i})_y\right)~,\] the preceding argument shows that $v_{i-1} \leq \alpha_i v_{i}$ for all $i \in \{2,\ldots,k-1\}$.
  We now set $b(r_i) = a(r_{i+1}) = (\prod_{j=2}^i \alpha_j) v_i$ for $i\in\{1,\ldots,k-1\}$, as well as $a(r_1) = -\max_{y\in\Y} |(v_1)_y|\ones$ and $b(r_k) = \max_{y\in\Y} |a(r_k)_y|\ones$, where $\ones\in\reals^\Y$ denotes the all-ones vector.

  The above establishes the second condition of monotone properties in Definition~\ref{def:monotone-prop}.
  To see the first condition, note that we have already established it for $i\in\{2,\ldots,k-1\}$.
  For $i=1,k$, we merely observe that $\inprod{a(r_1)}{p} \leq 0$ and $\inprod{b(r_k)}{p} \geq 0$ for all $p\in\simplex$.
\end{proof}

\raft{The following statement is true I believe, but low priority: ``An elicitable property $\Gamma:\simplex\toto\reals$ is convex elicitable (elicited by a convex $L : \reals \to \reals^\Y$) if and only if it is monotone.''  Start of the proof commented out.  Just need to show that $b$ is the upper limit of $a$ and $a$ the lower of $b$; should follow from elicitability of $\Gamma$.}
\begin{lemma}\label{lem:prop-L-monotone}
  For any convex $L : \reals \to \reals^\Y_+$, the property $\prop{L}$ is monotone.
\end{lemma}
\begin{proof}
  If $L$ is convex and elicits $\Gamma$, let $a,b$ be defined by $a(r)_y = \partial_- L(r)_y$ and $b(r) = \partial_+ L(r)_y$, that is, the left and right derivatives of $L(\cdot)_y$ at $r$, respectively.
  Then $\partial L(r)_y = [a(r)_y,b(r)_y]$.
  We now have $r \in \prop{L}(p) \iff 0 \in \partial \inprod{p}{L(r)} \iff \inprod{a(r)}{p} \leq 0 \leq \inprod{b(r)}{p}$, showing the first condition.
  The second condition follows as the subgradients of $L$ are monotone functions (see e.g.~\citet[Theorem 24.1]{rockafellar1997convex}).
  % Conversely, given such an $a,b$, we appeal to~\citet[Theorem 24.2]{rockafellar1997convex}, which gives us that $L(u)_y := \int_0^u a(u)_y$ is convex, and
\end{proof}

\newcommand{\Pbar}{\overline P}
\begin{lemma}\label{lem:pbar}
  Let $\gamma:\simplex\toto\R$ be a finite elicitable property, and suppose there is a calibrated link $\psi$ from an elicitable $\Gamma$ to $\gamma$.
  For each $r\in\R$, define $P_r = \bigcup_{u\in\psi^{-1}(r)} \Gamma_u \subseteq \simplex$, and let $\Pbar_r$ denote the closure of the convex hull of $P_r$.
  Then $\gamma_r = \Pbar_r$ for all $r\in\R$.
\end{lemma}
\begin{proof}
  As $P_r \subseteq \gamma_r$ by the definition of calibration, and $\gamma_r$ is closed and convex, we must have $\Pbar_r \subseteq \gamma_r$.
  Furthermore, again by calibration of $\psi$, we must have $\bigcup_{r\in\R} P_r = \bigcup_{u\in\reals} \Gamma_u = \simplex$, and thus $\bigcup_{r\in\R} \Pbar_r = \simplex$ as well.
  Suppose for a contradiction that $\gamma_r \neq \Pbar_r$ for some $r\in\R$.
  From Lemma~\ref{lem:finite-full-dim}, $\gamma_r$ has nonempty interior, so we must have some $p\in\inter\gamma_r \setminus \Pbar_r$.
  But as $\bigcup_{r'\in\R} \Pbar_{r'} = \simplex$, we then have some $r'\neq r$ with $p\in\Pbar_{r'} \subseteq \gamma_{r'}$.
  By Theorem~\ref{thm:aurenhammer}, the level sets of $\gamma$ form a power diagram, and in particular a cell complex, so we have contradicted point (ii) of Definition~\ref{def:cell-complex}: the relative interiors of the faces must not be disjoint.
  Hence, for all $r\in\R$ we have $\gamma_r = \Pbar_r$.
\end{proof}

We now prove the remaining results.
%Proposition~\ref{prop:indirect-orderable}, which states the following: if convex $L : \reals \to \reals^\Y$ indirectly elicits a finite elicitable property $\gamma$, then $\gamma$ is orderable.
\begin{proof}[of Proposition~\ref{prop:indirect-orderable}]
  Let $\gamma:\simplex\toto\R$.
  From Lemma~\ref{lem:prop-L-monotone}, $\Gamma := \prop{L}$ is monotone.
  Let $\psi:\reals\to\R$ be the calibrated link from $\Gamma$ to $\gamma$.
  From Lemma~\ref{lem:pbar}, we have $\Pbar_r = \gamma_r$ for all $r\in\R$, where $\Pbar_r$ is the closure of the convex hull of $\bigcup_{u\in\psi^{-1}(r)} \Gamma_u$.

  As $\Gamma$ is monotone, we must have $a,b : \R\to\reals^\Y$ such that $\Pbar_r = \{p\in\simplex : \inprod{a(r)}{p} \leq 0 \leq \inprod{b(r)}{p} \}$.
  (Take $a(r)_y = \inf_{u\in\psi^{-1}(r)} a(u)_y$ and $b(r)_y = \sup_{u\in\psi^{-1}(r)} b(u)_y$.)
  Now taking $p_r\in\inter\gamma_r$ and picking $u_r \in \Gamma(p_r)$, we order $\R = \{r_1,\ldots,r_k\}$ so that $u_{r_i} < u_{r_{i+1}}$ for all $i\in\{1,\ldots,k-1\}$.
  (The $u_{r_i}$ must all be distinct, as we chose $p_r$ so that $\gamma(p_r) = \{r\}$, so $\psi(u_{r_i}) = r_i$ for all $i$.)

  Let $i\in\{1,\ldots,k-1\}$.
  By monotonicity of $\Gamma$, we must have $a(r_i) \leq b(r_i) \leq a(r_{i+1}) \leq b(r_{i+1})$.
  As $\bigcup_{r\in\R} \Pbar_r = \bigcup_{r\in\R} \gamma_r = \simplex$, we must therefore have $b(r_i) = a(r_{i+1})$.
  Finally, we conclude $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : \inprod{b(r_i)}{p} = 0\}$.
  As these statements hold for all $i\in\{1,\ldots,k-1\}$, $\gamma$ is orderable.
\end{proof}

\newcommand{\floor}[1]{\lfloor #1\rfloor}
\begin{proof}[of Theorem~\ref{thm:1d-tfae}]
  As remarked before the theorem statement, it remains only to show $1 \iff 5$.
  \raft{This got a tad hand-wavy toward the end, but I think it's not too bad.}
  For the forward direction, by Lemma~\ref{lem:orderable-monotone}, we have $v(0),\ldots,v(k)\in\reals^\Y$ such that the coefficients $v(i)_y$ are monotone in $i$ for all $y\in\Y$, and the following two conditions hold: (1) \jessiet{Change to (i) and (ii) for consistency?} for all $i\in\{1,\ldots,k\}$ we have $\gamma_{r_i} = \{p\in\simplex : \inprod{v(i-1)}{p} \leq 0 \leq \inprod{v(i)}{p} \}$, and (2) for all $i\in\{1,\ldots,k-1\}$ we have $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : \inprod{v(i)}{p} = 0\}$.
  Letting $\floor{u}$ denote the floor function, and $\mathrm{mod}(u) := u - \floor{u}$, we extend the above definition to $v:[0,k]\to\reals^\Y$ given by
  \begin{align*}
    v(u) = (1-\mathrm{mod}(u))v(\floor{u}) + \mathrm{mod}(u)v(\floor{u}+1)~,
  \end{align*}
  which is continuous by construction.
  Moreover, $v(\cdot)_y$ is monotone for all $y\in\Y$.
  As the level sets $\gamma_r$ are full-dimensional (Lemma~\ref{lem:finite-full-dim}), for all $p\in\simplex$, there is a unique $u\in[0,k]$ such that $\inprod{v(u)}{p} = 0$.
  \jessiet{Why is the above statement true on boundaries of level sets?}
  We conclude that the property $\Gamma:\simplex\to[0,k]$ given by $\Gamma_u = \{p\in\simplex : \inprod{v(u)}{p} = 0\}$ is a single-valued property, which continuous as $v$ is continuous.
  Finally, the link $\psi(u) = \floor{u}+1$ is calibrated from $\Gamma$ to $\gamma$.

  For the converse, let $\Gamma:\simplex\to\reals$ be a continuous, non-locally-constant, elicitable property, with a calibrated link $\psi:\reals\to\R$ to $\gamma$.
  By~\cite{lambert2018elicitation,steinwart2014elicitation}, we have some continuous $v:\Gamma(\simplex)\to\reals^\Y$ such that for all $u\in\Gamma(\simplex)$ we have $\Gamma_u = \{p\in\simplex : \inprod{v(u)}{p} = 0\}$.

  Let $c_r = \inf\psi^{-1}(r)$ and $d_r = \sup \psi^{-1}(r)$ for all $r\in\R$, and let $I_r = (c_r,d_r)$.
  By continuity, $\Gamma^{-1}(I_r)$ is an open set, and the definition of calibration, we have $\Gamma^{-1}(I_r) \subseteq \gamma_r$; we conclude $\Gamma^{-1}(I_r) \subseteq \inter\gamma_r$. \jessiet{Why $\subseteq$ and not $=$?}
  Again by continuity of $\Gamma$, we must have $\Gamma(\gamma_r) = [c_r,d_r] := \overline I_r$.

  As the interiors of the level sets of $\gamma$ are disjoint (Theorem~\ref{thm:aurenhammer}), the intervals $\{I_r:r\in\R\}$ must also be disjoint.
  Define an ordering $\R = \{r_1,\ldots,r_k\}$ so that $I_{r_1} < \cdots < I_{r_k}$.
  Then for all $i\in\{1,\ldots,k-1\}$ we must have $d_{r_i} = c_{r_{i+1}}$; otherwise, the point $u = \tfrac 1 2 (d_{r_i} + c_{r_{i+1}})$ would not belong to any $\overline I_r$, and as $\Gamma(\simplex) = \Gamma(\bigcup_r\gamma_r) = \bigcup_r \overline I_r$, this would violate continuity of $\Gamma$.
  We conclude that $\gamma_{r_i}\cap\gamma_{r_{i+1}} = \Gamma_{d_i} = \{p\in\simplex : \inprod{v(d_i)}{p} = 0\}$ for all $i\in\{1,\ldots,k-1\}$.
\end{proof}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
