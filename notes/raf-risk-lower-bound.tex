\documentclass[11pt]{article}
\usepackage[margin=1.5in]{geometry}
% \newcommand{\paragraph}[1]{\textit{#1}.~}
% \newcommand{\qedhere}{}%\qed}
%\newcommand{\myemph}[1]{\emph{#1}} % redefine to {#1} after scanning
\newcommand{\myemph}[1]{#1} % redefine to {#1} after scanning

\usepackage[OT1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[numbers,sort,compress]{natbib}
% \usepackage[numbers]{natbib}
\usepackage{color}
% \usepackage{pdfsync} FOR SOME REASON SCREWED UP LINENO
\usepackage{amsmath}
\usepackage{amssymb,latexsym,amsthm}
\usepackage[colorlinks=true,breaklinks=true,bookmarks=true,urlcolor=blue,
 citecolor=blue,linkcolor=blue,bookmarksopen=false,draft=false]{hyperref}
\usepackage[font={small}]{caption}
\usepackage{graphicx}
\usepackage{etoolbox,xparse,xspace}
\usepackage{array,multirow,booktabs} % nice tables
\usepackage{bigstrut} % mostly for table spacing
\usepackage{bbm} % for \mathbbm{1}
\usepackage{dsfont} %%%%%% ONLY added for camera-ready, to kill type 3 fonts

\usepackage[multiple]{footmisc} %multiple footnotes

\usepackage{tikz,pgfplots,tikz-3dplot}
\usetikzlibrary{arrows.meta,shapes.misc,positioning,fit,calc,intersections,through,decorations.markings}
\tdplotsetmaincoords{55}{135}        % for simplex diagrams

\tikzstyle{cell}=[dashed,thick]
\tikzstyle{simplex}=[thick]
\newcommand{\tikzfigscale}{2.2}
\newcommand{\placefiglabel}[1]{\node at (1.1,0,1.7) {#1};}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question}
\newtheorem{condition}{Condition}
\newtheorem{assumption}{Assumption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \input{elic-defs.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below

\newcommand{\Comments}{1}
\definecolor{gray}{gray}{0.5}
\definecolor{lightred}{rgb}{1,0.6,0.6}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}

\newcommand{\jessie}[1]{\mynote{teal}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{teal!20!white}{JF: #1}}
\newcommand{\raf}[1]{\mynote{darkgreen}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
%\newcommand{\rafbtw}[1]{\mynote{gray}{[BTW: #1]}}
\newcommand{\btw}[1]{\mytodo{gray!20!white}{#1}}
\newcommand{\future}[1]{\mytodo{red!20!white}{\textcolor{gray!50!black}{#1}}}
\newcommand{\appendixfodder}[1]{\mynote{gray}{[APPENDIX?: #1]}}

\newcommand{\kl}{\textnormal{KL}}
\newcommand{\relint}{\textnormal{relint}}
\newcommand{\interior}{\mathrm{int}\,}
\newcommand{\tr}{\top}
\newcommand{\id}{\mathrm{id}}
\newcommand{\Var}{\mathrm{Var}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\fin}{{\mathrm{fin}}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\gaussians}{\mathcal{G}_\mathrm{mix}}
\newcommand{\Pquant}{\mathcal{P}_\mathrm{q}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\R}{\mathcal{R}}
\let\oldS\S 
\renewcommand{\S}{\mbox{\oldS\hspace{-0.5mm}}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\convhull}{\mathrm{conv}}
\newcommand{\conv}{\convhull}
\newcommand{\ext}{\mathrm{ext}}
\newcommand{\countinf}{\infty}%\boldsymbol{\omega}}

\newcommand{\toto}{\rightrightarrows}
\newcommand{\nondiff}{\mathsf{nondiff}}
\newcommand{\lsc}{l.s.c.}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\defeq}{\doteq}
%\newcommand{\ones}{\mathbbm{1}}
\newcommand{\ones}{\mathds{1}}  % to kill type 3 fonts
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\im}{\mathop{\mathrm{im}}}
\newcommand{\spn}{\mathop{\mathrm{span}}}
\newcommand{\affspn}{\mathop{\mathrm{affinespan}}}

\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}
\def\extreals{\mathbb{\overline{R}}}

\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\arginf}{\mathop{\mathrm{arginf}}}
\newcommand{\argsup}{\mathop{\mathrm{argsup}}}

\newcommand{\inprod}[1]{\left\langle #1 \right\rangle}

\newcommand{\elic}{\mathsf{elic}}
\newcommand{\elici}{\elic_\ID}
\newcommand{\elicifin}{\elic_{\I^\fin}}
\newcommand{\iden}{\mathsf{iden}}
\newcommand{\idprop}{\Gamma_{\id}}
\newcommand{\EL}{\mathcal{E}}
\newcommand{\ID}{\mathcal{I}}
\newcommand{\ES}{\mathrm{ES}}
\newcommand{\lbar}{\underline{L}}
\newcommand{\vspan}{\mathrm{span}}
\newcommand{\affhull}{\mathrm{affhull}}
\newcommand{\affdim}{\mathrm{affdim}}
\newcommand{\codim}{\mathrm{codim}}



\definecolor{myblue}{rgb}{0,0.3,0.8} \definecolor{myorange}{rgb}{0.9,0.5,0}
\renewcommand{\S}{\mbox{\oldS\hspace{-0.5mm}}}
\newcommand{\Clin}{{\C_\mathrm{lin}}}
\newcommand{\Ccvx}{{\C_\mathrm{cvx}}}
\newcommand{\Cstrict}{{\C_\mathrm{strict}}}
\newcommand{\Cstrong}{{\C_\mathrm{strong}}}
\newcommand{\mode}{\mathrm{mode}}
\newcommand{\mi}{\mathrm{mi}}
\renewcommand{\ones}{\mathds{1}} %%%%%%


\newcommand{\range}{\mathrm{range}\,}
\newcommand{\zeros}[1]{\mathrm{ker}_\P\,#1}



\newcounter{foo}
\newcommand{\tempsetcounter}[2]{
  \setcounter{foo}{\value{#1}}
  \setcounter{#1}{#2}
}
\newcommand{\restorecounter}[1]{\setcounter{#1}{\value{foo}}}


\renewcommand{\Comments}{1}
%\renewcommand{\btw}[1]{} % 9/16/2020 shut off btw comments
\ifnum\Comments=1
\setlength{\marginparwidth}{1.35in}
\setlength{\marginparsep}{0.1in}
\fi
% \newcommand{\email}[1]{{\small\nolinkurl{#1}}}%\href{mailto:#1}{\small \nolinkurl{#1}}}

\begin{document}

\title{Notes: Complexity Lower Bound for Bayes Risks}

\author{Raf}

\maketitle

\section{Setup}
\label{sec:elic-complex-setting}

Let $\Y$ be a set of outcomes and $\P$ be a convex set of probability measures on $\Y$.
When $\Y = \reals^d$, we will assume the Borel $\sigma$-algebra, and when $\Y$ is generic, the $\sigma$-algebra will be left implicit, but the relevant functions need to be measurable and $\P$-integrable, i.e., integrable with respect to each $p \in \P$.
Throughout, we will use $Y$ as the random variable representing the outcome itself, i.e.\ $Y:\Y\to\Y$, $y\mapsto y$.

\begin{definition}
  \label{def:property}
  Let $\R$ be a nonempty set.
  A \emph{property} is a functional $\Gamma : \P \to 2^\R$.
%  When $|\R|$ is finite, we call $\Gamma$ a \emph{finite property}.
%  \raf{We only reference this in the discussion and appendix (mode for finite $\Y$); let's remove it from this main def.}
  The \emph{level set} $\Gamma_r \defeq \{p\in \P \,|\, r\in\Gamma(p)\}$ is the set of distributions $p$ corresponding to report value $r\in\R$.
  A property is \emph{single-valued} if $|\Gamma(p)|=1$ for all $p\in\P$, in which case we may write $\Gamma:\P\to\R$ and $\Gamma(p) \in \R$.
  We define the \emph{range} of a property by $\range \Gamma = \bigcup_{p\in\P} \Gamma(p) \subseteq \R$.
  When $\R=\reals^d$ for some $d\in\N$, we write $\dim\Gamma = d$.
\end{definition}

\begin{definition}
  \label{def:loss-elicits}
  A \emph{loss function} is a function $L:\R\times\Y\to\reals$ such that $L(r,\cdot)$ is $\P$-integrable for all $r\in\R$.
  A loss $L$ \emph{elicits} a property $\Gamma:\P\to 2^\R$ if for all $p\in\P$,
  $\Gamma(p) = \argmin_{r\in\R} \E_pL(r,Y)$.
  A property is \emph{elicitable} if some loss elicits it.
\end{definition}

\begin{definition}
  \label{def:refine}
  $\Gamma'$ \emph{refines} $\Gamma$ if for all $r'\in\range\Gamma'$ there exists $r\in\range\Gamma$ with $\Gamma'_{r'} \subseteq \Gamma_r$.
\end{definition}
Equivalently, $\Gamma'$ refines $\Gamma$ if there is a link function $\psi:\range\Gamma'\to\range\Gamma$ such that $\Gamma'_{r'} \subseteq \Gamma_{\psi(r')}$ for all $r'\in\range\Gamma'$.

\begin{definition}
  \label{def:el}
  For $k\in\N\cup\{\countinf\}$, let $\EL_k(\P)$ denote the class of all elicitable properties $\Gamma:\P\to\reals^k$, and $\EL(\P) \defeq \bigcup_{k\in\N\cup\{\countinf\}} \EL_k(\P)$.
  When $\P$ is implicit we simply write $\EL$.
\end{definition}

\begin{definition}
  \label{def:elic-complex}
  Let $\C$ be a class of properties.
  % A property $\Gamma$ is \emph{$k$-elicitable with respect to $\C$}
  % if there exists an intermediate property  and which refines $\Gamma$.
  The \emph{elicitation complexity} of a property $\Gamma$ with respect to $\C$, denoted  $\elic_\C(\Gamma)$, is the minimum value of $k\in\N\cup\{\countinf\}$ such that there exists $\hat \Gamma \in \C\cap\EL_k(\P)$ that refines $\Gamma$.
\end{definition}

\begin{definition}
  \label{def:bayes-risk}
  Given loss function $L:\A\times\Y \to \reals$ for some report set $\A$, the \emph{Bayes risk} of $L$ is defined as $\lbar(p) := \inf_{a \in \A} \E_pL(a,Y)$. \jessiet{Any reason to switch to $\A$?}
\end{definition}

\begin{definition}
  Given $V:\Y\to\reals^d$, we set $\zeros{V} \defeq \{p\in\P : \E_p V = 0\}$.
\end{definition}
Thus, $\zeros{V}$ is the kernel of the linear map $p \mapsto \E_p V$ on $\vspan\, \P$, intersected with $\P$.

\section{A few ingredients}

\begin{proposition}[\citep{osband1985providing}]
  \label{prop:elic-complex-level-sets-convex}
  Let $\Gamma$ be elicitable.
  Then $\Gamma_r$ is convex for all $r\in\range\Gamma$.
\end{proposition}

\begin{lemma}
  \label{lem:refine}
  If $\Gamma'$ refines $\Gamma$ then $\elic_\C(\Gamma') \geq \elic_\C(\Gamma)$.
\end{lemma}

\begin{proof}
  As $\Gamma'$ refines $\Gamma$, we have some $\psi:\range\Gamma'\to\range\Gamma$ such that for all $r'\in \range\Gamma'$ we have $\Gamma'_{r'} \subseteq \Gamma_{\psi(r')}$.
  Suppose we have $\hat\Gamma\in\C$ and $\varphi:\range\hat\Gamma\to\range\Gamma'$ such that for all $u\in \range\hat\Gamma$ we have $\hat\Gamma_u \subseteq \Gamma'_{\varphi(u)}$.
  Then for all $u\in \range\hat\Gamma$ we have $\hat\Gamma_u \subseteq \Gamma'_{\varphi(u)} \subseteq \Gamma_{(\psi \circ \varphi)(u)}$.
  In particular, if $\elic_\C(\Gamma') = m$, then we have such a $\hat\Gamma:\P\to 2^{\reals^m}$, and hence $\elic_\C(\Gamma) \leq m$.
  \raft{The logic for this whole proof could use a very careful check}
\end{proof}

\begin{lemma}%[\citet[Lemma 8]{frongillo2020elicitation}]
  \label{lem:elic-complex-bayes-concave}
  Suppose $L$ elicits $\Gamma:\P\to\R$ and has Bayes risk $\lbar$.
  Then for any $p,p'\in\P$ with $\Gamma(p)\neq\Gamma(p')$, we have $\lbar(\lambda p + (1-\lambda) p') > \lambda \lbar(p) + (1-\lambda) \lbar(p')$ for all $\lambda\in(0,1)$.
\end{lemma}

\begin{theorem}
  \label{thm:bayes-risk-lower-bound}
  If $L$ elicits a single-valued $\Gamma$, and $\hat\Gamma$ refines $\lbar$, then $\hat\Gamma$ refines $\Gamma$.
\end{theorem}
\begin{proof}
  Suppose for a contradiction that $\hat\Gamma$ does not refine $\Gamma$.
  Then we have some $u\in\range\hat\Gamma$ such that for all $r\in\range\Gamma$ we have $\hat\Gamma_u \not\subseteq \Gamma_r$.
  In particular, recalling that $\Gamma$ is single-valued, we must have $p,p'\in\hat\Gamma_u$ such that $\Gamma(p) \neq \Gamma(p')$.
  Moreover, as $\hat\Gamma$ refines $\lbar$, we also have $\lbar(p) = \lbar(p')$.
  From Lemma~\ref{lem:elic-complex-bayes-concave} and $\lambda=1/2$ we have $\lbar(q) > \tfrac 1 2 \lbar(p) + \tfrac 1 2 \lbar(p') = \lbar(p)$, where $q = \tfrac 1 2 p + \tfrac 1 2 p'$.
  As the level set $\hat\Gamma_u$ is convex by Proposition~\ref{prop:elic-complex-level-sets-convex}, we also have $q \in \hat\Gamma_u$, and hence $\lbar(q)=\lbar(p)$, a contradiction.
\end{proof}

\begin{lemma}\label{lem:lin-alg-nested-level-sets}
  Let $\V$ be a real vector space.
  Let $f:\V\to\reals^k$ be linear and $C\subseteq \V$ convex with $\spn C = \V$, and let $m\in\N$.
  Suppose that $0 \in \interior f(C)$, and
  for all $v\in S \defeq C \cap \ker f$, there exists a linear $\hat f_v:\V\to\reals^m$ with $v \in C \cap \ker \hat f_v \subseteq S$.
  Then $m \geq k$.
  If $m=k$, we additionally have $0\in\interior\hat f_v(C)$ for some $v\in S$.
  \raft{I changed the statement to reflect the proof better, but did not change the proof apart from $\hat f \to \hat f_v$.  Could still use a careful check though!}
\end{lemma}
\begin{proof}
  The condition $0 \in \interior f(C)$ is equivalent to the existence of some $v_1,\ldots v_{k+1} \in C$ such that $0\in\interior\conv\{f(v_i) : i\in\{1,\ldots,k+1\}\}$.
  Let $\alpha_1,\ldots,\alpha_{k+1}>0$, $\sum_{i=1}^{k+1} \alpha_i = 1$, such that $\sum_{i=1}^{k+1} \alpha_i f(v_i) = 0$.
  As these are barycentric coordinates,
  \btw{Wikipedia page for affine space, in the section of affine and barycentric coordinates, defines the (unique) version imposing the sum to 1 as ``barycentric coordinates''}
  this choice of $\alpha_i$ is unique, a fact which will be important later.
  We will take $v = \sum_{i=1}^{k+1} \alpha_i v_i$, an element of $C$ by convexity, and thus an element of $S$ as $f(v)=0$.

  Let $\hat f_v:\V\to\reals^m$ be linear with $v \in \hat S := C \cap \ker \hat f_v \subseteq S$.
  Let $\beta_1,\ldots,\beta_{k+1}\in\reals$, $\sum_{i=1}^{k+1} \beta_i = 0$, such that $\sum_{i=1}^{k+1} \beta_i \hat f_v(v_i) = 0$.
  We will show that the $\beta_i$ must be identically zero, i.e. that $\{\hat f_v(v_i):i\in\{1,\ldots,k+1\}\}$ are affinely independent.
%  \raft{We want to show that the $\beta_i$ must be 0 (that's one definition of affine independence)}
  By construction, $v' := \sum_{i=1}^{k+1} \beta_iv_i \in \ker \hat f_v$, and as $v\in\ker\hat f_v$, for all $\lambda > 0$ we have $v_\lambda := v + \lambda v' = \sum_{i=1}^{k+1} (\alpha_i + \lambda \beta_i) v_i \in \ker \hat f_v$.
  Taking $\lambda$ sufficiently small, we have $\gamma_i := \alpha_i + \lambda \beta_i > 0$ for all $i$, and $\sum_{i=1}^{k+1} \gamma_i = \sum_{i=1}^{k+1} \alpha_i + \lambda\sum_{i=1}^{k+1} \beta_i = 1$.
  By convexity of $C$, we have $v_\lambda \in C$.
  Now $v_\lambda \in C \cap \ker \hat f_v \subseteq S = C \cap \ker f$, and in particular $v_\lambda \in \ker f$.
  Thus, $f(v_\lambda) = \sum_{i=1}^{k+1} \gamma_i f(v_i) = 0$.
  By the uniqueness of barycentric coordinates, for all $i\in\{1,\ldots,k+1\}$, we must have $\gamma_i = \alpha_i$ and thus $\beta_i = 0$, as desired.

  As $\hat f_v(C)$ contains $k+1$ affinely independent points, we have $m \geq \dim \im \hat f_v \geq k$.
  When $m=k$,
  by affine independence, the set $\conv\{\hat f_v(v_i) : i\in\{1,\ldots,k+1\}\}$ has dimension $k$ in $\reals^k$.
  As $0 = \hat f_v(v) = \sum_{i=1}^{k+1} \alpha_i \hat f_v(v_i)$, and $\alpha_i > 0$ for all $i$, we conclude $0\in\interior\conv\{\hat f_v(v_i) : i\in\{1,\ldots,k+1\}\} \subseteq \interior \hat f_v(C)$.
\end{proof}

\begin{lemma}\label{lem:lin-alg-span}
  Let $\V$ be a real vector space.
  Let $f:\V\to\reals^k$ be linear, $C\subseteq \V$ convex with $\spn C = \V$, and let $S = C \cap \ker f$.
  If $0 \in \interior f(C)$ then $\spn S = \ker f$.
\end{lemma}

\section{Lower Bound for Bayes Risks}
\begin{condition}\label{cond:v-interior}
  There exists $r\in\range\Gamma$ and $V:\Y\to\reals^d$ such that $\Gamma_r = \zeros{V}$\raft{I think we only need $\Gamma_r \subseteq \zeros{V}$.  I think I'll update the proof at some point to reflect this, since it actually gets a bit easier to follow.} and $0\in\interior\{\E_pV : p\in\P\}$.
\end{condition}

Let $\C^*_d$ be the class of properties $\hat\Gamma$ which are elicited by a convex loss $L:\reals^d\times\Y\to\reals$ for some $d\in\N$, and $\C^*\defeq \bigcup_{d\in\N} \C^*_d$.
\raft{Jessie: Here is where we impose all the restrictions we need on $L$, etc}

\begin{theorem}[Our main result for infinite $\Y$]
  \label{thm:convex-flats-inf-dim}
  Let $\hat\Gamma\in\C^*_d$ for some $d\in\N$.
  For all $u\in\range\hat\Gamma$ and $p\in\hat\Gamma_u$, there is some $\hat V_{u,p}:\Y\to\reals^d$ such that $p \in \zeros{\hat V_{u,p}} \subseteq \hat\Gamma_u$.
  \raft{Jessie: I wrote this from memory/scratch, so I might be off base -- if there are discrepancies we should iron them out!}
\end{theorem}

\begin{theorem}
  Let $\Gamma:\P\to\reals^d$ satisfy Condition~\ref{cond:v-interior} for some $r\in\reals^d$.
  Let $L$ elicit $\Gamma$ such that $\lbar$ is non-constant on $\Gamma_r$.
  Then $\elic_{\C^*}(\lbar) \geq d+1$.
\end{theorem}
\begin{proof}
  Let $V:\Y\to\reals^d$ and $r$ be given by the statement of the theorem and from Condition~\ref{cond:v-interior}.
  Let $m = \elic_{\C^*}(\lbar)$, so that we have $\hat\Gamma\in\C^*_m$ which refines $\lbar$.
  By Theorem~\ref{thm:bayes-risk-lower-bound} we have $\hat\Gamma$ refines $\Gamma$.

  \raft{Notes to self about the proof commented out here}
  % This is how I thought it should go next, but actually, we can't / don't want to make statements about $\elic_{\C^*}(\Gamma)$, since then the theorem will apply even when $L$ itself is not convex.
  % Plus, the proof is actually easier to follow this way!
  % ``Via the identity link function, we also have $\elic_{\C^*}(\hat\Gamma) \leq m$.
  % Moreover, from Lemma~\ref{lem:refine}, $\elic_{\C^*}(\lbar) = m \geq \elic_{\C^*}(\hat\Gamma) \geq \elic_{\C^*}(\Gamma)$.''

  We now establish the conditions of Lemma~\ref{lem:lin-alg-nested-level-sets} for $C=\P$.
  Let $f:\spn \P \to \reals^d$, $p \mapsto \E_pV$.
  From Condition~\ref{cond:v-interior}, we have $0\in\interior f(\P)$ and $\ker f \cap \P = \zeros{V} = \Gamma_r$.
  Now let $p\in\Gamma_r$ be arbitrary, and take any $u\in\hat\Gamma(p)$.
  As $\Gamma$ is single-valued, $r\in\range\Gamma$ is the unique value with $p\in\Gamma_r$.
  As $\hat\Gamma$ refines $\Gamma$, there exists $r'\in\range\Gamma$ with $\hat\Gamma_u \subseteq \Gamma_{r'}$, and since $p\in\hat\Gamma_u$, we conclude $r'=r$ from the above.
  From Theorem~\ref{thm:convex-flats-inf-dim}, we have some $\hat V_{u,p}$ with $p\in\zeros{\hat V_{u,p}} \subseteq \hat\Gamma_u \subseteq \Gamma_r = \zeros{V}$.
  Letting $\hat f_p:\spn \P \to \reals^d$, $p \mapsto \E_p\hat V_{u,p}$, we have now satisfied the conditions of Lemma~\ref{lem:lin-alg-nested-level-sets}.
  We conclude $m \geq d$, and moreover, if $m=d$, then there exists some $q\in\Gamma_r$ such that $0 \in\interior\hat f_q(\P)$.
    
  Now suppose $m = d$ for a contradiction.
  Let $\hat S\defeq \ker f_q\cap \P$.
  Applying Lemma~\ref{lem:lin-alg-span} to the functions $f$ and $\hat f_q$
  we have $\spn \ker f = \spn \Gamma_r$ and $\spn \ker \hat f_q = \spn \hat S$.
  As $\hat S \subseteq \Gamma_r$, we have $\ker \hat f_q = \spn \hat S \subseteq \spn \Gamma_r = \ker f$.
  By the first isomorphism theorem, we also have $\codim \ker \hat f_q = \codim \ker f = d$, as the images of these linear maps span all of $\reals^d$.
  By the third isomorphism theorem we conclude $\Gamma_r = \hat S$.
  Moreover, as $\hat S \subseteq \hat\Gamma_u \subseteq \Gamma_r$, we have $\hat S = \hat\Gamma_u = \Gamma_r$.

  We now see that $\lbar$ is constant on $\Gamma_r$ since there is some link function $\psi:\reals^m\to\reals$ such that $\Gamma_r = \hat\Gamma_u \subseteq \lbar_{\psi(u)}$, meaning $\lbar(p) = \psi(u)$ for all $p\in\Gamma_r$.
  This statement contradicts the assumption that $\lbar$ is non-constant on $\Gamma_r$.
\end{proof}

  % \raf{OLD VERSION}
  % By refinement, if $p\in\Gamma_r$ then there exists $\hat r \in \reals^m$ with $p \in \hat \Gamma_{\hat r} \subseteq \Gamma_r$.
  % The proof of Lemma~\ref{lem:lin-alg-nested-level-sets} shows that there is a particular choice $p^*\in\Gamma_r$ such that if $p^* \in \hat \Gamma_{\hat r} \subseteq \Gamma_r$ then $0\in\interior\{\E_p\hat V : p\in\P\}$.

  % \raf{WANT TO CONCLUDE}
  % Thus, we have some $p^*\in\P$, $u\in\hat\Gamma(p^*)$, and $\hat V:\Y\to\reals^m$, such that, letting $\hat S\defeq \{p\in\P:\E_p\hat V = 0\}$, we have $p^* \in \hat S \subseteq \hat\Gamma_u \subseteq \Gamma_r$ and $0\in\interior\{\E_p\hat V : p\in\P\}$.


\begin{thebibliography}{1}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\bibitem[{Osband(1985)}]{osband1985providing}
\textsc{Osband, K.~H.} (1985).
\newblock \textit{Providing {Incentives} for {Better} {Cost} {Forecasting}}.
\newblock UC Berkeley.
\end{thebibliography}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
