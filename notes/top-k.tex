\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage{../neurips-19/neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}


\title{Consistent Polyhedral Surrogates via Embedding}
%\title{Convex Surrogates via Polyhedral Losses}
\author{%
 Jessica Finocchiaro\raf{Jessie?} \\
 \texttt{jessica.finocchiaro@colorado.edu}\\
 CU Boulder
 \And
 Rafael Frongillo\\
 \texttt{raf@colorado.edu}\\
 CU Boulder
 \And
 Bo\\
 \texttt{bwag@colorado.edu}\\
 MSFT
}

\begin{document}

\section{Top-$k$ surrogate}
Throughout this section, consider the surrogate loss \begin{equation}\label{eq:top-k-surrogate}
L^{top-k}(u)_y~=~\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \right)_+~.~
\end{equation}

We want to write $L^{top-k}$ in a linear form with the $(\cdot)_+$ operator.
Then, using this linear form of the expected loss, we are able to write a finite set of optimal reports over $\simplex$.

\subsection{Notation and Assumptions}
\subsubsection{Notation}
Let $\bar{u} = \frac 1 k \sum_{i = 1}^k u_i$ and similarly, $\bar{u}_{-y} = \frac{1}{k-1} \sum_{i=1, i \neq y}^k u_i$ be the averages of the first $k$ elements of $u$ and the average of the first $k$ elements of $u$ besides $y$, respectively.
Moreover, we say $b = \argmax_i \{u_i : u_i > 1\}$, assuming $u$ is ordered.
See below in Section~\ref{sec:assumptions} for justification.

We use $\odot$ to denote the Hadamard (element-wise) vector product.
$\min(\vec w, \vec v)$ denotes the element-wise minimum of vectors $\vec w, \vec v \in \reals^d$.

\subsubsection{Assumptions}\label{sec:assumptions}
Throughout, we make two assumptions for ease of exposition, but both follow without loss of generality for an optimal report.  %, as well as a third assumption when thinking about minimizing the expected loss.
First, assume that $u$ is ordered, where $u_1 \geq u_2 \geq \ldots \geq u_n$.
This is without loss of generality, as we can reorder $p$ so that each $p_i$ corresponds to the ordered $u_i$ that it originally corresponded to.
This is simply for ease of exposition.

Second, we will say $u_{k+1} = \ldots = u_{n-1} = u_n = 0$.
This partially follows from the invariance of $L^{top-k}$ in the $\ones$ direction, by setting $u_{k+1} = 0$.
For $i > k+1$, observe that $u_i$ can never affect the $\bar{u}$ term that is added, and can only be subtracted in the case that $i = y$.
Therefore, we minimize expected loss by pulling $u_{k+2},\ldots, u_n$ up to $u_{k+1}$, making it as ``tight'' as possible.
This leads to our assumption, which we can make since we are primarily interested in finding \emph{a} minimizer of $L^{top-k}$, rather than \emph{all} minimizers.

\begin{lemma}\label{lem:top-threshold}
	For all $p \in \simplex$, there is a minimizer $u$ of $\inprod{L^{top-k}(\cdot)}{p}$ such that for all $u_i$, we have $u_i \leq \bar u_{-i} +1$. 
\end{lemma}
\begin{proof}
	Reassigning such a $u'_i := \bar u_{-i} +1$ results in $L^{top-k}(u')_i = L^{top-k}(u)_i = 0$, but we observe $L^{top-k}(u')_j \leq L^{top-k}(u)_j$ for all $j \neq i$ by decreasing $\ubar$, since $u_i \geq 0$.

To see this, consider $u_y = \bar u_{-y} + 1 + \epsilon$ and $u'_y = \bar u_{-y} + 1$, with $u_i = u'_i \; \forall i \neq y$. 
\begin{align*}
L^{top-k}(u)_y &= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} + \epsilon\right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} + \epsilon\right) \right)_+\\
&= \left(-\bar u_{-y} - \epsilon + \bar u_{-y} + \frac \epsilon k \right)_+ \\
&= (-\frac{k-1}{k}\epsilon)_+\\
&= 0
\end{align*}
Compare the above to the following:
\begin{align*}
L^{top-k}(u')_y &= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} \right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} \right) \right)_+\\
&= \left(-\bar u_{-y} + \bar u_{-y}\right)_+ \\
&= 0
\end{align*}

Now, for any outcome $i \neq y$, we know that $\bar u_{-i} > \bar u_{-i}$ because of $u_y$.
Since $u_y$ is only added in $L^{top-k}(\cdot)_i$ as part of the $\bar u_{-i}$ term, we can see that $L^{top-k}(u)_i \geq L^{top-k}(u')_i$ for all $i \neq y$.
\end{proof}

\section{Re-writing the loss}


With the assumptions above and considering minimizers satisfying Lemma~\ref{lem:top-threshold}, we can rewrite the loss.
\begin{align*} 
L^{top-k}(u)_y&=\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \right)_+\\
&=\left(1 - u_y + \bar u - \frac{1}{k}(\min(u_y, 1)) \right)_+
%&=\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - \min(u_y, 1)\Ind{y \leq k})_{i} \right)_+ original, but we don't need the indicator with our assumtions that u_{k+1} = ... = u_n = 0, since that is the min of u_y and 1 in that case.
\end{align*}

Consider that the loss value is $1 + \bar u$ when $y > k$, since $u_y = 0$ in this case and the summation resolves to $\bar u$, so the loss is nonnegative even before applying the $(\cdot)_+$ operator.

If $y \leq k$ and $u_y \in [0,1]$, then the loss is nonnegative before applying $(\cdot)_+$ as $1-u_y \geq 0$, as is the term in the summation, $\frac k {k-1} \bar u_{-y}$.

Now if $u_y \in (1, \bar u_{-y} + 1]$, then the term inside the $(\cdot)_+$ operator of $L^{top-k}(u)_y$ is greater than or equal to $0$.
To see this, consider $1 - u_y + \bar u - \frac 1 k = 1 - u_y + \frac k {k-1} \bar u_{-y} + \frac 1 k u_y - \frac 1 k$, and look at the first three elements and the last two.
Since $u_y \geq 1$, the difference between the last two terms $\frac 1 k u_y - \frac 1 k \geq 0$.
Additionally, since $u_y \leq 1 +  \ubar_{-y}$, we can see the sum of these terms is nonnegative.

Therefore, we can drop the $(\cdot)_+$ operator on the assumed report space.

We can now write the expected loss as follows:
\begin{align*}
\inprod{p}{L^{top-k}(u)} &= \sum_{y=1}^n p_y L^{top-k}(u)_y \\
&= \sum_{y=1}^b p_y L^{top-k}(u)_y + \sum_{y=b+1}^k p_y L^{top-k}(u)_y + \sum_{y=k+1}^n p_y L^{top-k}(u)_y\\
&= \sum_{y=1}^b p_y (1 - \frac 1 k - u_y + \ubar) + \sum_{y=b+1}^k p_y (1 - \frac{k+1} k u_y + \ubar) + \sum_{y=k+1}^n p_y(1 - u_y + \ubar)\\
&= 1 + \ubar - \inprod{p}{u} - \inprod{p \odot \Ind{y \leq b}}{\frac 1 k \ones} - \inprod{p \odot \Ind{b+1 \leq y \leq k}}{ \frac u k}\\
&= 1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)}
\end{align*}

%\subsection{Studying the Bayes Risk}
%First, consider the case where $u_y = 0$ and $u'_y = 1$, with $u_j = u'_j \;\forall i \neq j$.
%As the expected loss is linear on $u_y$, and these are the boundary cases, we know that $u$ is optimal on one of these two reports in this case.
%
%So we ask when is $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$?
%\begin{align*}
%- \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq \frac {1}{k} - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%0 &\leq \frac{1}{k} - p_y(\frac{k+1}{k})\\
%p_y &\leq \frac{1}{k+1}
%\end{align*}
%
%Conversely, if $p_y > \frac{1}{k+1}$, it is to better to report $u_y = 1$ than $u_y = 0$.
%However, since we are not restricted to the unit hypercube, we might ask when it is optimal to bring $u_y = 1$ up to $u'_y = 1 + \epsilon$ given $\epsilon > 0$.
%Here, we consider when $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$.
%\begin{align*}
%%1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq 1 + \ubar' - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%\frac{1}{k} - p_y (\frac{k+1}{k}) &\leq \frac{1+\epsilon}{k} - p_y (\frac{k+1}{k}(1 + \epsilon))\\
%p_y &\leq \frac{1}{k}
%\end{align*}
%In this case, $u_y=1$ is preferable to $u+y >1$.
%If $p_y > \frac 1 k$, then we want to push $u_y$ as high as we can, but recall that we restrict to $u_y \leq \ubar_{-y} + 1$.
Because the upper bound on $u_y$ depends on $\ubar$, we can't determine $u$ iteratively.
%In fact, we might have some $p_y \in [\frac 1 {k+1}, \frac 1 k]$ that we want to crank up past one, since this allows us to crank up other high-probability report values even farther.
However, we can show that for each element $y$ of $u$, $L^{top-k}$ is linear on $[0,1]$ and $[1, \bar u_{-y} + 1]$, and therefore an optimal report must be one of these three values.

\begin{lemma}\label{lem:linear-pieces}
	For each element $u_i$ of $u$, the expected loss $\inprod{p}{L^{top-k}(u)}$ is linear on $[0,1]$ and $[1, \bar u_{-i} + 1]$ for all $p \in \simplex$.
	\jessie{Not sure if this is the right statement.}	
\end{lemma}
\begin{proof}
	For all $p \in \simplex$, we know that 
	\begin{equation}
	\inprod{p}{L^{top-k}(u)} = 1 - \bar u + \inprod{p}{u + \frac 1 k \min{u,\ones}}~.~
	\end{equation}
	For any $p$, we then have 
	\begin{align*}
	\inprod{p}{L^{top-k}(u)} &= 1 + \bar u - \inprod{p}{u + \frac 1 k \min{u,\ones}}\\
	 &= 1 + \bar u - \sum_{y \neq i}p_y (u_y + \frac 1 k \min(u_y, 1)) - p_i \frac {k+1} k u_i~,~
	\end{align*}
	which is linear on $u_i$.
	
	Similarly, for $u_i \in [1, \bar u_{-i} + 1]$, we can write $\inprod{p}{L^{top-k}(u)} = 1 + \bar u - \sum_{y \neq i}p_y (u_y + \frac 1 k \min(u_y, 1)) - p_i u_i - p_i$, which is linear in $u_i$.
\end{proof}

\begin{theorem}\label{thm:finite-set}
	For the loss $L^{top-k} : \reals^n \times \Y \to \reals_+$ given in Equation~\ref{eq:top-k-surrogate}, there is an optimal report in the following report set $\R$:
	\begin{align*}
	\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}
	\end{align*}
\end{theorem}
\begin{proof}[Proof Sketch.]
%	The above work tells us that when $p_y > \frac 1 {k+1}$, it is optimal to set $u_y > 0$, and otherwise, it is optimal to set $u_y = 0$.
	Since $L^{top-k}$ is linear on $[0,1]$ and $[1, c]$ for some $c \geq 1$ and all dimensions $i \in [n]$, we know that the optimum is on the boundary of such reports, so when we bump $u_y$ up to some number greater than $0$, it should either be one $1$ or $c$ to be an optimal report.
	$H$ represents the elements of the report that are bumped up to $c$, and $M$ represents the elements of the report that are bumped up to $1$.
	Since we know that no distribution has more than $k$ outcomes with weight greater than $\frac 1 {k+1}$, we know that the size of $H \cup M$ must be less than or equal to $k$, and $H \cap M$ is empty by each element of $u$ being real-valued, so for $c > 1$, $u_i$ cannot simultaneously be equal to $c$ and $1$, hence the restrictions on $\R$.
	(Note: if $c = 1$, then WLOG, we can say $H = \emptyset$.)
	
	To see the reason for $c := \frac{|M| + k -1}{k - |H|}$, consider that, given the loss $L^{top-k}$, when we want to ``crank $u_i$'' above $1$, we want to do so as much as possible, until hitting some threshold discussed in Lemma~\ref{lem:top-threshold}, due to the $(\cdot)_+$ operator surrounding the loss.
	Exceeding the threshold on $u_i > c$ then can only increase expected loss.
	
	Then, for $i \in H$, ``cranking'' $u_i$ as high as possible, we want to have $c = u_i = \bar u _{-i} + 1$, which we can see that 
	\begin{align*}
	c &= \bar u_{-i} + 1 \\
	c &= \frac{(c (|H|-1)) + |M|}{k-1} + 1\\
	ck-c &= c|H| - c + |M| + k - 1\\
	ck - c|H| &= |M| + k - 1\\
	c &= \frac{|M|+k-1}{k-|H|}
	\end{align*}
\end{proof}


By Theorem~\ref{thm:finite-set}, we now have a finite set of optimal reports for $L^{top-k}$.
This yields the finite loss $L|_\R$ embedded by $L^{top-k}$.
This gives the discrete loss $\ell^{top-k} : \R \times\Y \to \reals_+$ by
\begin{align}
\ell^{top-k}(r,y) &= \begin{cases}
0 & u_y = \bar u_{-y} + 1\\
\bar u - \frac 1 k & u_y = 1\\
1 + \bar u & u_y = 0
\end{cases}~.~
\end{align}
\jessie{Not sure if there's more we want to say on this.}

%\begin{conjecture}
%	Fix $p \in \simplex$.
%	Let $g_t = |\{p_i : p_i \geq \frac 1 k\}|$ and $g_s = | \{p_i \in (\frac{1}{k+1}, \frac{1}{k})\}|$.
%	The property \[\gamma(p) = 
%	\left[
%	\begin{cases}
%	0 & p_i \leq \frac {1}{k+1}\\
%	1 & p_i \in (\frac{1}{k+1}, \frac{1}{k})\\
%	\frac{g_s + k-1}{k-g_t} & o/w
%	\end{cases}
%	\right]_{i=1}^n
%	\] refines the property $\prop{L^{top-k}}$.
%\end{conjecture}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
