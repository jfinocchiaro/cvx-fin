\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage{../neurips-19/neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{assumption}{Assumption}


\title{Consistent Polyhedral Surrogates via Embedding}
%\title{Convex Surrogates via Polyhedral Losses}
\author{%
 Jessica Finocchiaro\raf{Jessie?} \\
 \texttt{jessica.finocchiaro@colorado.edu}\\
 CU Boulder
 \And
 Rafael Frongillo\\
 \texttt{raf@colorado.edu}\\
 CU Boulder
 \And
 Bo\\
 \texttt{bwag@colorado.edu}\\
 MSFT
}

\begin{document}

\section{Top-$k$ surrogate}

Throughout this section, consider the surrogate loss \begin{equation}\label{eq:top-k-surrogate}
L^{top-k}(u)_y~=~\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \right)_+~.~
\end{equation}

We write the expected loss $L^{top-k}(u)$ without the $(\cdot)_+$ operator by restricting $u$.
Then, using this form of the expected loss, we are able to find a finite set of optimal reports over all $p \in \simplex$.

\subsection{Notation and Assumptions}
\subsubsection{Notation}
We say $u_{[i]}$ is the $i^{th}$ largest element of the $u$ vector.
Let $\bar{u} = \frac 1 k \sum_{i = 1}^k u_{[i]}$.
Similarly, let $\bar{u}_{-i} = \frac{1}{k-1} (k \bar u - u_{[i]})$ be the averages of the first $k$ sorted elements of $u$ and the average of the first $k-1$ sorted elements of $u$ besides the $i^{th}$ element, respectively.
$u[y]$ refers to the relative order of the $y^{th}$ component of $u$.
For example, if $u = (1,2,4)$, then $u[3] = 1$. 
If $u$ is understood from context, we abuse notation to omit the $u$ and write $u[y] = [y]$.

Let $\min(\vec w, \vec v) \in \reals^d$ denote the element-wise minimum of vectors $\vec w, \vec v \in \reals^d$.

\subsubsection{Closing the search space for a representative set}\label{sec:assumptions}
Throughout, we make two assumptions about our report set, and show the set of reports satisfying both assumptions is representative. 

For our first assumption, we first observe that $L^{top-k}$ is invariant in the $\ones$ direction.
\begin{lemma}\label{lem:invar-ones}
	For all $\alpha \in \reals$ and $y\in\Y$, we have $L^{top-k}(u)_y = L^{top-k}(u + \alpha \ones)_y$.
\end{lemma}
\begin{proof}
	\begin{align*}
	L^{top-k}(u + \alpha \ones)_y &= \left(1 - (u_y + \alpha) + \frac{1}{k} \sum_{i=1}^k ((u + \alpha \ones  - e_y)_{[i]} \right)_+\\
	&= \left(1 - (u_y + \alpha) +\frac{1}{k} \alpha k + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= \left(1 -u_y - \alpha + \alpha + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= \left(1 -u_y + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= L^{top-k}(u)_y
	\end{align*}
\end{proof}

\begin{lemma}\label{lem:extra-elts-zero}
	$R^{\tt low} := \{u \in \reals^n_+ \mid \|u\|_0 \leq k\}$ is a representative set for $L^{top-k}$.
\end{lemma}
\begin{proof}
	By Lemma~\ref{lem:invar-ones}, we can fix $u_{[k+1]} = 0$.
	We then have $u_{[i]} \leq 0$ for all $i \geq k+1$.
	Consider $u'$ such that $u'_{[i]} = u_{[i]}$ for all $i$ so that $u_{[i]} \geq 0$, and $u'_{[i]} = 0$ otherwise.
	Observe that $u$ and $u'$ have the same ordering on their elements and $u' \in R^{\tt low}$.
	We want to show that $L^{top-k}(u)_y \geq L^{top-k}(u')_y$, and representativeness of $R^{\tt low}$ follows.
	
	If $[y] \geq k+1$, then 
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 - u_y + \frac 1 k \sum_{i=1}^k (u - e_y)_{[i]}\right)_+ \\
%	&= 1 - u_y + \frac 1 k \sum_{i=1}^k (u - e_y)_{[i]}\\
	&\geq \left(1 + \frac {1}{k} \sum_{i=1}^k (u' - e_y)_{[i]}\right)_+\\
	&= L^{top-k}(u')_y~.~
	\end{align*}
	The inequality comes from the equality of the first $k+1$ sorted elements of $u$ and $u'$, combined with setting $u_{[y]} \leq u'_{[y]} = 0$ in this case.
	
	Now, if $[y] < k+1$ and $u_y \geq 1$, observing that $\bar u = \bar u'$, we then have 
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 - u_y + \frac 1 k \sum_{i=1}^k (u - e_y)_{[i]}\right)_+ \\
	&= \left(1 - u_y + \bar u - \frac 1 k\right)_+\\
	&= \left(1 - u'_y + \bar u' - \frac 1 k\right)_+\\
	&= L^{top-k}(u')_y~.~
	\end{align*}
	
	Now suppose that $[y] < k+1$ and $u_y \in [0,1)$; we now observe $(u - e_y)_{[k]} = u_{[k+1]} = 0 = (u' - e_y)_{[k]}$. 
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 - u_y + \frac 1 k \sum_{i=1}^{k-1} u_{[i]} + 0 \right)_+\\
	&= \left(1 - u'_y + \frac 1 k \sum_{i=1}^{k-1} u'_{[i]}\right)_+\\
	&= \left(1 - u'_y + \frac 1 k \sum_{i=1}^{k} (u' - e_y)_{[i]}\right)_+\\
	&= L^{top-k}(u')_y~.~
	\end{align*}

	Since $L^{top-k}(u)_y \geq L^{top-k}(u')_y$ for all $y \in \Y$, this also holds for the expected loss for all $p \in \simplex$.
	Thus, $R$ is representative.
\end{proof}

\begin{assumption}\label{assum:nonneg}
	We will say $u_{[k+1]} = \ldots = u_{[n-1]} = u_{[n]} = 0$.
\end{assumption}
It follows from Lemma~\ref{lem:extra-elts-zero} that $R^{\tt low}$ is representative, and every $u$ satisfying Assumption~\ref{assum:nonneg} is in $R^{\tt low}$ by construction.


We now state our final assumption, and show that the set of reports satisfying the two assumptions is representative.
\begin{assumption}\label{assum:top-threshold}
	$u \in R^{\tt high} := \{u \in \reals^n \mid u_i \leq \bar u_{-i} +1 \, \forall i \in [n]\}$.
\end{assumption}

\begin{lemma}\label{lem:top-threshold}
The set $R^{\tt low} \cap R^{\tt high}$ is representative.
%	For all $y \in \Y$, there is a minimizer $u$ of $L^{top-k}(\cdot)_y$ satisfying Assumption~\ref{assum:nonneg} so that for all $i$, we have $u_i \leq \bar u_{-i} +1$. 
\end{lemma}
\begin{proof}
  Since we have already proven $R^{\tt low}$ is representative, suppose $u \in R^{\tt low}$.
  For all $i \in [n]$ such that $u_{[i]} > \bar u_{-i} + 1$, reassigning such a $u'_{[i]} := \bar u_{-i} +1$ results in $L^{top-k}(u')_i = L^{top-k}(u)_i = 0$, but we observe $L^{top-k}(u')_j \leq L^{top-k}(u)_j$ for all $j \neq i$ by decreasing $\ubar$, since $u_i \geq 0$.
  Moreover, $u' in R^{\tt high}$ by construction.

Now, we write $u_y = \bar u_{-y} + 1 + \epsilon$ for some $\epsilon > 0$ and $u'_y = \bar u_{-y} + 1$, with $u_i = u'_i$ for all $i \neq y$. 
\begin{align*}
L^{top-k}(u)_y &= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} + \epsilon\right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} + \epsilon\right) \right)_+\\
&= \left(-\bar u_{-y} - \epsilon + \bar u_{-y} + \frac \epsilon k \right)_+ \\
&= (-\frac{k-1}{k}\epsilon)_+\\
&= 0
\end{align*}
Compare the above to the following:
\begin{align*}
L^{top-k}(u')_y &= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} \right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} \right) \right)_+\\
&= \left(-\bar u_{-y} + \bar u_{-y}\right)_+ \\
&= 0~.~
\end{align*}
Thus, the losses are equal on the outcome $y$.

Now let us consider outcome $y\neq i$.
Since $u$ satisfies Assumption~\ref{assum:nonneg}, we have $\bar u \geq 0$ and $\bar u_{-i} \geq 0$ for any $i$.
Therefore, if $u_y > \bar u_{-y} + 1$, then we have $[y] < k+1$ as $u_y + 1 \geq 1 > 0$.
Now, for outcome $y \neq i$, we have 
\begin{align*}
L^{top-k}(u)_y &= \left( 1 - u_y + \frac 1 k \sum_{j=1}^k (u-e_y)_{[j]} \right)_+ &\text{Plugging in $u$}\\
	&= \left( 1 - u'_y + \frac 1 k \sum_{j=1}^k (u'-e_y)_{[j]} + \frac {\epsilon}{k}\right)_+ & \text{$u'_y$ is in the top $k$ elements of $(u' - e_y)$}\\
	&\geq \left( 1 - u'_y + \frac 1 k \sum_{j=1}^k (u'-e_y)_{[j]}\right)_+ & \text{Since $\epsilon > 0$}\\
	&= L^{top-k}(u')_y
\end{align*}
Therefore, if $u$ minimizes $L^{top-k}(\cdot)_y$, then so does $u'$ for each $y \in \Y$, so we can say the same of the expected loss $\inprod{p}{L^{top-k}(\cdot)}$ for all $p \in \simplex$.
Thus, there is some $u' \in R^{\tt low} \cap R^{\tt high}$ so that $u' \in \argmin_u \inprod{p}{L^{top-k}(u)}$.
\end{proof}



Consider the set $\U := R^{\tt low} \cap R^{\tt high}$.

\bigskip
\hrule
\bigskip

%Consider the following finite set.
%\begin{align*}
%\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}
%\end{align*}
%\jessie{Ultimately, WTS $\R$ is representative for $L^{top-k}$.}
%Throughout, we consider $c(u) := \frac{|M(u)| + k -1}{k - |H(u)|}$, where $M(u) = \{i \in [n] : u_i \in [0,1] \}$ and $H(u) = \{i \in [n] : u_i > 1\}$. 
%\jessie{This has changed so $H$ and $M$ are functions of $p \in \simplex$.

%\begin{lemma}\label{lem:linear-pieces}
%	For each element $u_i$ of $u \in \U$, the expected loss $\inprod{p}{L^{top-k}(u)}$ is linear on $[0,1]$ and $[1, \bar u_{-i} + 1]$ for all $p \in \simplex$.
%	\jessie{Not sure if this is the right statement.}
%    \raf{Not sure either.  One thing to be careful about: you either have to make sure that the intervals in question yield points in the set $\U$, or refer to the original form of the loss.  Because the form of the loss you are using only is shown to hold when $u\in\U$.  Hopefully that made sense.}
%    \raf{One thing that would help: let's visualize $\U$ for some value of $k,n$ to get some intuition for what this set is.  Maybe $k=2$ and $n=3$, but only plot the first 2 coordinates?}
%\end{lemma}
%\begin{proof}
%	For all $p \in \simplex$ and $u \in \U^\circ$, we know that 
%	\begin{equation}
%	\inprod{p}{L^{top-k}(u)} = 1 + \bar u - \inprod{p}{u + \frac 1 k \min{u,\ones}}~.~
%	\end{equation}
%	For any $p$, if $u_i \in [0,1]$, we then have 
%	\begin{align*}
%	\inprod{p}{L^{top-k}(u)} &= 1 + \bar u - \inprod{p}{u + \frac 1 k \min(u,\ones)}\\
%	 &= 1 + \bar u - \sum_{y \neq i}p_y (u_y + \frac 1 k \min(u_y, 1)) - p_i \frac {k+1} k u_i~,~
%	\end{align*}
%	which is linear on $u_i$.
%	
%	Similarly, for $u_i \in [1, \bar u_{-i} + 1]$, we can write $\inprod{p}{L^{top-k}(u)} = 1 + \bar u - \sum_{y \neq i}p_y (u_y + \frac 1 k \min(u_y, 1)) - p_i u_i - p_i/k$, which is linear in $u_i$.
%\end{proof}

%\begin{theorem}\label{thm:finite-set}
%	Consider the loss $L^{top-k} : \reals^n \times \Y \to \reals_+$ given in Equation~\ref{eq:top-k-surrogate}.  
%	The finite set
%	\begin{align*}
%	\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}
%	\end{align*}
%	is representative for $L^{top-k}$.
%\end{theorem}
%\begin{proof}[Sketch]
%%	The above work tells us that when $p_y > \frac 1 {k+1}$, it is optimal to set $u_y > 0$, and otherwise, it is optimal to set $u_y = 0$.
%	Since $L^{top-k}$ is linear in $i$ on $[0,1]$ and $[1, c]$ for some $c \geq 1$ and all dimensions $i \in [n]$, we know that the optimum is on the boundary of such reports, so when we bump $u_y$ up to some number greater than $0$, it should either be one $1$ or $c$ to be an optimal report.
%	$H$ represents the elements of the report that are bumped up to $c$, and $M$ represents the elements of the report that are bumped up to $1$.
%	We know that the size of $H \cup M$ must be less than or equal to $k$ by Assumption~\ref{assum:nonneg}, and $H \cap M$ is empty by each element of $u$ being real-valued, so for $c > 1$, $u_i$ cannot simultaneously be equal to $c$ and $1$, hence the restrictions on $\R$.
%	Note that if $c = 1$, then we can say $H = \emptyset$.
%	
%	To see the reason for $c := \frac{|M| + k -1}{k - |H|}$, consider that, given the loss $L^{top-k}$, when we want to ``crank $u_i$'' above $1$, we want to do so as much as possible, until hitting the threshold discussed in Assumption~\ref{assum:top-threshold}.
%	Exceeding the threshold on $u_i > c$ then can only increase expected loss, as shown in Lemma~\ref{lem:top-threshold}.
%	
%	Then, for $i \in H$, ``cranking'' $u_i$ as high as possible, we want to have $c = u_i = \bar u _{-i} + 1$.
%	However, each $i \in H$ should get ``cranked up'' to the same value of $\bar u_{-i} + 1$ by linearity.
%	We can see that 
%	\begin{align*}
%	c &= \bar u_{-i} + 1 \\
%	c &= \frac{(c (|H|-1)) + |M|}{k-1} + 1\\
%	ck-c &= c|H| - c + |M| + k - 1\\
%	ck - c|H| &= |M| + k - 1\\
%	c &= \frac{|M|+k-1}{k-|H|}~.~
%	\end{align*}
%\end{proof}



\section{Cutting down on search space}
Consider the loss $L^{top-k} : \reals^n \times \Y \to \reals_+$ given in equation~\eqref{eq:top-k-surrogate}.
Since the loss is polyhedral, we know it has a finite set of minimizers.
If we can find a finite representative set, we can easily embed these reports via the identity and more easily study the property elicited by this surrogate, and how it compares to the top-$k$ property originally in mind when the surrogate was designed.

We will start with $\U$ as we show it is representative in Lemma~\ref{lem:top-threshold}, and narrow down to a finite representative set for $L^{top-k}$.

\begin{proposition}
	$\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}$ is representative for $L^{top-k}$.
\end{proposition}


\section{Property elicited by $L^{top-k}$}
There is sometimes a benefit in expected loss for this surrogate by artificially bumping up from the ``low'' to ``middle'' bin when possible because doing so cranks up the constant on the ``high'' reports, yielding better expected loss.

Note that ``bumping down'' does not happen because of how the original bins are set, but we can ''bump up'' from low to medium or medium to high.

Fix $p \in \simplex$.
We construct the bins
\begin{align*}
H_p &:= \left\{i \mid p_i > \frac 1 k\right\}\\
M_p &:= \left\{i \mid p_i \in \left(\frac 1 {k+1}, \frac 1 k\right]\right\} \\
B_p &:= \left\{i \mid p_i \in \left(\frac {k(1-p_H)}{(k+1)(k-H)}, \frac 1 {k+1} \right]\right\}\\
L_p &:= \left\{i \mid p_i \leq \frac {k(1-p_H)}{(k+1)(k-H)} \right\}~,~
\end{align*}
where $p_H := \sum_{i \in H_p} p_i$.
Sometimes we drop $p$ from the notation where it can be understood from context.


The bin $B_p$ allows us to ``bump'' reports up to a higher bin if beneficial, as bumping a report up allows for each element of $H_p$ to be set at a higher constant.
One can verify that it is never beneficial to bump from middle to high, so we need only concern ourselves with bumping from $L_p$ to $M_p$.

To understand when to bump $j$ from $L_p$ to $B_p$, taking $u \to u'$, we can ask when we have
\begin{align*}
\inprod{p}{L(u)} &\geq \inprod{p}{L(u')}\\
\sum_{i \in H} \frac{|M| + k -1}{k-|H|} - p_i \left(\frac{k(|M| + k -1)+ 1}{k(k- |H|)}\right) + \sum_{i \in M} \left(\frac 1 k - p_i \left( \frac {k+1}{k}\right)\right) &\geq \sum_{i \in H} \frac{|M| + k }{k-|H|} - p_i \left(\frac{k(|M| + k)+ 1}{k(k- |H|)}\right) + \sum_{i \in M'} \left(\frac 1 k - p_i \left( \frac {k+1}{k}\right)\right)\\
\frac{|H|(|M| + k - 1)}{k(k- |H|)} + p_H \frac{-k(|M| + k - 1) - 1}{k(k - |H|)} &\geq \frac{|H|(|M| + k)}{k(k - |H)} + p_H \left(\frac{-k(|M| + k)-1}{k (k - |H|)}\right) + \left(\frac 1 k - p_j \left(\frac {k+1}{k}\right)\right) \\
\frac{-|H|}{k (k - |H|)} + \frac{p_H}{k - |H|} &\geq \frac 1 k - p_j \left(\frac{k+1}{k}\right)~.~
\end{align*}

Thus, we can take the set 
\begin{align*}
B_p &:= \left\{i : p_i \leq \frac{1}{k+1} \wedge \frac{p_H k- H}{k(k- H)} > \frac 1 k - p_i\left(\frac{k+1}{k}\right) \right\}\\
&=\left\{i \mid p_i \in \left(\frac {k(1-p_H)}{(k+1)(k-H)}, \frac 1 {k+1} \right]\right\}
\end{align*}


We will construct the report $\hat \Gamma(p) := \frac{M + B + k - 1}{k-H}\ones_{H_p} + \ones_{M_p} + \ones_{B_p}$.


\jessie{Adding 8 July 2021}
%Fix the distribution $p \in \simplex$.
%Denote the sets:
%\begin{align*}
%H_p &= \left\{i \in \Y \mid p_i > \frac 1 k\right\}\\
%M_p &= \left\{i \in \Y \mid p_i \in \left( \frac 1 {k+1}, \frac 1 k\right]\right\}\\
%B_p &= \left\{i \in \Y \mid p_i \leq \frac 1 {k+1} \wedge p_i \frac{k(1-p_H)}{(k+1)(k-H)}\right\}\\
%L_p &= \left\{i \in \Y \mid p_i \leq \frac {1}{k+1}\right\} \setminus B_p
%\end{align*}
%where $H := |H_p|$ and $p_H := \sum_{y \in H_p} p_y$.


\begin{claim}
	Fix $p \in \simplex$.
	$\hat \Gamma(p) \in \U$.
\end{claim}	
\begin{proof}
	Fix $p \in \simplex$, and take $H := |H_p|, M:= |M_p|$, and $B := |B_p|$.
	It suffices to show $H + M + B \leq k$ to show $\hat \Gamma (p) \in \R^{\tt low}$.
	
	\jessie{TODO}
	
	
	\jessie{Thought: Break into cases: the nonzero elements in (a) H (b) M (c) B (d) H,M (e) H,B (f) B,M (g) H,M,B.  Turns out that a,b are pretty trivial, c and f can never happen.  H,M has an untyped proof in my notes, but e and g have yet to be shown.  There's a tension between modifying the sets so this proof is more immediate and the result being true.  I think we can determine the sets algorithmically so this proof is more immediate, but showing it is a correct property value might be harder.}
	
	
	Moreover, $\hat \Gamma(p) \in R^{\tt high}$ by construction.
	Thus, $\hat \Gamma(p) \in \U$.
\end{proof}

\begin{claim}
	$\inprod{L(\hat \Gamma(p))}{p} = (1-p_H) \bar u + p_L - \frac{p_M + p_B}{k}$. 
\end{claim}
\begin{proof}
	\begin{align*}
	\inprod{L(\hat \Gamma(p))}{p} &=
	\sum_{y \in H_p} L(\hat \Gamma(p))_y + \sum_{y \in M_p, B_p} L(\hat \Gamma(p))_y + \sum_{y \in L_p}L(\hat \Gamma(p))_y \\
	&= \sum_{y \in H_p} 0 + \sum_{y \in M_p, B_p} p_y(\bar u - \frac{1}{k}) + \sum_{y \in L_p} p_y(1 + \bar u) \\
	&= (1-p_H) \bar u + p_L - \frac{p_M + p_B}{k}
%	&= \sum_{y \in M_p, B_p, L_p} p_y (\bar u - \frac{1}{k}) + \sum_{y \in L_p} p_y \\
%	&= (1-p_H) (\bar u - \frac{1}{k}) + p_L
	\end{align*}
\end{proof}



\begin{lemma}
	Fix $p \in \simplex$.
	For small enough $\varepsilon$ and any $\hat y \in \Y$, consider $\varepsilon_{\hat y} := \varepsilon e_{\hat y}$.
	For any (small enough) $\varepsilon_{\hat y}$, the loss $\inprod{L(\hat \Gamma(p))}{p} \leq \inprod{L(\hat \Gamma(p) + \varepsilon_{\hat y})}{p}$.
\end{lemma}
\begin{proof}
	We denote $L^* := \inprod{L(\hat \Gamma(p))}{p}$.
	We proceed in cases:
	
	Case 1: $\hat y \in H_p, \varepsilon_{\hat y} > 0$.
	\begin{align*}
	\E_p L(\hat \Gamma(p) + \varepsilon_{\hat y}, Y) &= {\color{blue}\sum_{y \in H_p} p_y (\frac {\varepsilon_{\hat y}}{k})} {\color{teal} - p_{\hat y} \frac{\varepsilon_{\hat y}}{k}} + {\color{blue} \sum_{y \in M_p, B_p} p_y(\frac{\varepsilon_{\hat y}}{k})} + {\color{olive}\sum_{y \in M_p, B_p} p_y(\bar u - \frac 1 k)} + {\color{blue}\sum_{y \in L_p} p_y (\frac{\varepsilon_{\hat y}}{k})}+ {\color{olive}\sum_{y \in L_p}p_y(1+ \bar u)}\\
	&= {\color{blue}\frac{\varepsilon_{\hat y}}{k}} - {\color{teal}p_{\hat y}\frac{\varepsilon_{\hat y}}{k}} + {\color{olive}L^*}
	\end{align*}
	This is greater than or equal to $L^*$ if and only if $\frac{\varepsilon_{\hat y}}{k} \geq p_{\hat y}\frac{\varepsilon_{\hat y}}{k}$.
	This is always true as $p \in \simplex$, so $p_{\hat y} \leq 1$.
	
	Case 2: $\hat y \in M_p \cup  B_p \cup L_p$, and $\varepsilon_{\hat y} > 0$.
	\begin{align*}
	\E_p L(\hat \Gamma(p) + \varepsilon_{\hat y}, Y) &= {\color{blue}\sum_{y \in H_p} p_y (\frac {\varepsilon_{\hat y}}{k}) + \sum_{y \in M_p, B_p} p_y(\frac{\varepsilon_{\hat y}}{k})} + {\color{olive}\sum_{y \in M_p, B_p} p_y(\bar u - \frac 1 k)} - {\color{teal}p_{\hat y} \varepsilon_{\hat y}} + {\color{blue}\sum_{y \in L_p} p_y (\frac{\varepsilon_{\hat y}}{k})}+ {\color{olive}\sum_{y \in L_p}p_y(1+ \bar u)}\\
	&= {\color{blue}\frac{\varepsilon_{\hat y}}{k}} - {\color{teal}p_{\hat y}\frac{\varepsilon_{\hat y}}{k}} + {\color{olive}L^*}
	\end{align*}
	This is greater than or equal to $L^*$ if and only if $\frac{\varepsilon_{\hat y}}{k} \geq p_{\hat y} \varepsilon_{\hat y}$, which is true by the case that $p_{\hat y} \leq \frac 1 k$ since $\hat y \not \in H_p$.
	
	Case 3: $\hat y \in H_p, \varepsilon_{\hat y} < 0$.
	For clarity, we assume $\varepsilon_{\hat y} > 0$, and we consider $\hat \Gamma(p) - \varepsilon_{\hat y}$.
	
	\begin{align*}
	\E_p L(\hat \Gamma(p) - \varepsilon_{\hat y}, Y) &= {\color{teal} p_{\hat y}\varepsilon_{\hat y}} + \sum_{y \in M_p, B_p}p_y({\color{olive}\bar u - \frac 1 k} -{\color{blue} \frac{\varepsilon_{\hat y}}{k}}) + \sum_{y \in L_p} p_y({\color{olive}1 + \bar u} -{\color{blue} \frac{\varepsilon_{\hat y}}{k}})\\
	&= {\color{olive}L^*} + {\color{teal} p_{\hat y}\varepsilon_{\hat y}} - {\color{blue} (1-p_H) \frac{\varepsilon_{\hat y}}{k}}
	\end{align*}
	This is greater than or equal to $L^*$ if and only if 
	\begin{align*}
	p_{\hat y} &\geq \frac{(1 - p_H)}{k}\\
	\iff k p_{\hat y} &\geq (1 - p_H) ~,~
	\end{align*}
	which is true as $p_{\hat y} \geq \frac 1 k$, so the product is at least $1$ and $p_H$ is nonnegative.
	
	Case 4: $\hat y \in M_p \cup B_p, \varepsilon_{\hat y} < 0$.
	Again, we repeat the notation as above.
	\begin{align*}
	\E_p L(\hat \Gamma(p) - \varepsilon_{\hat y}, Y) &= \sum_{y \in M_p, B_p}({\color{olive}\bar u - \frac 1 k} -{\color{blue} \frac{\varepsilon_{\hat y}}{k}}) + {\color{teal} p_{\hat y}\varepsilon_{\hat y}} + \sum_{y \in L_p}({\color{olive}1 + \bar u} -{\color{blue} \frac{\varepsilon_{\hat y}}{k}})\\
	&= {\color{olive}L^*} + {\color{teal} p_{\hat y}\varepsilon_{\hat y}} - {\color{blue} (1-p_H) \frac{\varepsilon_{\hat y}}{k}}
	\end{align*}
	
	Case 4a: $\hat y \in M_p$
	
	It remains to show that $p_{\hat y} \varepsilon_{\hat y} \geq (1 - p_H)\frac{\varepsilon_{\hat y}}{k}$.

	First, suppose $H_p \neq \emptyset$.
	Therefore, $p_H \geq \frac 1 k$, so $1 - p_H \leq \frac{k-1}{k}$.
	
	\begin{align*}
	p_{\hat y} \varepsilon_{\hat y} &\geq (1 - p_H)\frac{\varepsilon_{\hat y}}{k} &\\
	\iff p_{\hat y} &\geq \frac{(1-p_H)}{k} & \text{Since $\varepsilon_{\hat y} > 0$} \\
	\iff p_{\hat y} &\geq \frac{(k-1)/k}{k} \geq \frac{1-p_H}{k} & \text{Since $H \geq 1$}\\
	p_{\hat y} &\geq \frac 1 {k+1} \geq \frac{k-1}{k^2} \geq \frac{(1-p_H)}{k} & \text{Since $\hat y \in M_p$} \\ 
	\end{align*}
	Thus, we conclude $\inprod{L(\hat\Gamma(p) - \varepsilon_{\hat y})}{p} \geq L^*$ for $\hat y \in M_p$.
	
	Now, if $H_p = \emptyset$ then we can write the expected loss
	\begin{align*}
	\E_p L(\hat \Gamma(p) - \varepsilon_{\hat y}, Y) &= \sum_{y \in \Y}({\color{olive}\bar u - \frac 1 k} -{\color{blue} \frac{\varepsilon_{\hat y}}{k}}) + {\color{teal} p_{\hat y}\varepsilon_{\hat y}} + \sum_{y \in L_p}({\color{olive}p_y + \bar u - \frac 1 k} -{\color{blue} \frac{\varepsilon_{\hat y}}{k}})\\
	\end{align*}
	
	Case 4b: $\hat y \in B_p$.
	
	First, observe that $H_p = \emptyset \implies B_p = \emptyset$, since we have $i \in B_p \iff p_i \leq \frac{1}{k+1}$ and $p_i > \frac{k(1-p_H)}{(k+1)(k-H)} = \frac 1 {k+1}$, yielding a contradiction.
	\begin{align*}
	p_{\hat y} \varepsilon_{\hat y} \geq \frac{k(1-p_H)}{(k+1)(k-H)} &\geq \frac{1-p_H}{k} &\text{By assumption}\\
	\iff \frac{k}{(k+1)(k-H)} &\geq \frac{1}{k} & \text{Dividing both sides by $(1-p_H)$} \\
	\iff k^2 &\geq (k+1)(k-H) = k^2 - (H-1) k -1 & \text{Manipulating fractions}
	\end{align*}
	The last statement is true since $H \geq 1$, so the result holds.
	
	Case 5: $\hat y \in L_p, \varepsilon_{\hat y} < 0$.

	\begin{align*}
	\E_p L(\hat \Gamma(p) - \varepsilon_{\hat y}, Y) &= \sum_{y \in M_p, B_p}p_y({\color{olive}\bar u - \frac 1 k}) + \sum_{y \in L_p}p_y({\color{olive}1 + \bar u - \frac 1 k}) + {\color{teal} p_{\hat y}\varepsilon_{\hat y}} \\
	&= {\color{olive}L^*} + {\color{teal} p_{\hat y}\varepsilon_{\hat y}} 
	\end{align*}
	This is greater than or equal to $L^*$ since $\varepsilon_{\hat y} > 0$ (recall instead of adding a negative number, we subtracted a positive one).
\end{proof}

\jessie{I think this is clear, but I'm biased as the writer}
Since $L^{top-k}$ is polyhedral, $\inprod{L^{top-k}(\hat\Gamma(p))}{p}$ is linear in any direction $\varepsilon v$ for small enough $\varepsilon$.
Moreover, $\{\varepsilon_{\hat y}\}_{\hat y \in \Y}$ forms a basis for $\reals^n$, so for all $v \in \reals^n$, there is a unique $\lambda = \{\lambda_i\} \in \reals^n$ such that $v = \lambda_i e_i$.
Therefore, we have 
\begin{align*}
\inprod{L(\hat \Gamma(p) + \sum_i \lambda_{i} \varepsilon_{i})}{p} &= 
\sum_i \lambda_i \inprod{L(\hat \Gamma(p) + \varepsilon_{i})}{p}\\
&\geq \sum_i \lambda_i L^* \\
&= L^*
\end{align*}
This yields the following corollary.

\begin{corollary}
	$\R := \hat \Gamma(\simplex)$ is a finite representative set.
\end{corollary}

The finite loss $L|_{\R}$ embedded by $L^{top-k}$ by
\begin{align}
L|_{\R}(r,y) &= \begin{cases}
0 & r_y = \bar r_{-y} + 1\\
\bar r - \frac 1 k & r_y = 1\\
1 + \bar r & r_y = 0
\end{cases}~.~
\end{align}


\section{Example: $k=2$ and $n=3$}
First, we have $\risk{\ell}(p) = 1 - \min_y p_y$, and so $-\risk{\ell}(p) = \min_y p_y - 1$.
Taking the conjugate $(-\ell^*)(q) = \sup_{p \in \simplex} \{\inprod p q + 1 - \min_y p_y\}$, we see this is maximized by putting weight $1$ on $\argmax_y q_y$ and weight $0$ on every other element for $q \in \simplex$.
Since $n \neq 1$, we then have $\min_y p_y = 0$, so the conjugate of the negative risk reduces to $C(u) = 1 + \max_y u_y$.

Then, plugging this into our given loss yields $L(u)_y = 1 + u_{[1]} - u_y$.


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\jessie{Removed from the document July 14, 2021}
% to be all $u \in \reals^n$ satisfying Assumptions~\ref{assum:nonneg} and ~\ref{assum:top-threshold}.
%\jessie{Can we claim $\U$ is a representative set for $L^{top-k}$?  Would take some more work.}

One can observe that $\U$ is not convex.
However, we can consider permutations $\pi$ of the elements of $\U$ so that $\pi(u)$ is the permutation of $\{1,2,\ldots, n\}$ so that $u$ is ordered in decreasing order according to this permutation.
We then let $\U_\pi$ be all $u \in \reals^d$ ordered by the same $\pi$ of $\{1,2,\ldots,n\}$.
Now regrardless of $\pi$, we can consider $\U \cap \U_\pi$, which is a convex set, and every $u \in \U$ is also in $\U \cap \U_{\pi}$ for some $\pi$.

%Moreover, we relax Assumption~\ref{assum:nonneg} to the following.
%\begin{assumption}\label{assum:nonneg-relax}
%	$u \in \reals^n_+$.
%\end{assumption}
%In a similar vein, we call $\U^\circ$ the set of $u \in \reals^n$ satisfying Assumptions~\ref{assum:top-threshold} and \ref{assum:nonneg-relax}.
%
%\raf{I think it might be handy to introduce the set $\U$ here to be the points satisfying all three assumptions --- maybe explicitly write the constraints and nod to the assumptions.  Then we can say $u\in\U$ and forget the assumptions going forward.}

\begin{lemma}\label{lem:get-rid-max-op}
	For all $u \in \U$, we have $1 -u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \geq 0$.
\end{lemma}
This statement allows us to consider the loss $L^{top-k}$ without the $(\cdot)_+$ operator, since the inside term being nonnegative implies the max of the term and $0$ is always the inside term.

\begin{proof}
	First, we have $\frac 1 k \sum_{i=1}^k (u-e_y)_{[i]} \geq 0$ by nonnegativity of the first $k+1$ elements of $u$ in Assumption~\ref{assum:nonneg}.
	Moreover, we need to show that $u_y \leq 1 + \frac 1 k \sum_{i=1}^k (u-e_y)_{[i]}$.
	
	If $[y] \geq k+1$, then $\frac 1 k \sum_{i=1}^k (u-e_y)_{[i]} = \bar u$, so the inequality becomes $u_y \leq 1 + \bar u$.
	This holds since $u_{[y]} \leq u_{[k]} \leq \bar u \leq 1 + \bar u$.
	
	Now if $[y] < k+1$ and $u_y \in [0,1]$, then we have $(u-e_y)_{[k]} = 0$, and the above term becomes
	\begin{align*}
	&1 -u_y + \frac{1}{k} \sum_{i=1}^{k} u_i - \frac 1 k u_y \geq 0\\
	&\iff 1 + \bar u \geq u_y + \frac{1}{k} u_y\\
	&\iff 1 + \frac k {k-1} \bar u_{-y} \geq u_y\\
	&\implies \frac k {k-1} \bar u_{-y} \geq 0~,~
	\end{align*}
	which is true by Assumption~\ref{assum:nonneg}.
	
	Now if $u_y > 1$, then $\frac 1 k \sum_{i=1}^k (u-e_y)_{[i]} = \bar u - \frac 1 k$.
	Therefore, we have 
	\begin{align*}
	&1 -u_y + \bar u - \frac 1 k \geq 0\\
	&\iff 1 + \frac{k}{k-1}\bar u_{-y} \geq \frac{k-1}{k} u_y + \frac 1 k\\
	&\iff (1 + \bar u_{-y}) + \frac 1 {k-1} \bar u_{-y} \geq \frac{k-1}{k} u_y + \frac 1 k\\
	&\iff \frac{k-1}{k}(1 + \bar u_{-y}) + \frac 1 k + \frac{1}{k}\bar u_{-y} + \frac 1 {k-1} \bar u_{-y} \geq \frac{k-1}{k} u_y + \frac 1 k\\
	&\implies \frac{1}{k}\bar u_{-y}+ \frac{1}{k-1}\bar u_{-y} \geq 0~,~
	\end{align*}
	with the final implication coming from Assumption~\ref{assum:top-threshold}.
	
	Therefore, the term is nonnegative if $u$ satisfies Assumptions~\ref{assum:nonneg} and \ref{assum:top-threshold}.
\end{proof}

With Assumptions~\ref{assum:nonneg} and \ref{assum:top-threshold} and Lemma~\ref{lem:get-rid-max-op}, we can now rewrite the loss for each outcome in a simplified form.
\raf{Note: some inconsistency in the way you use this $\min$ operator and denote $\ones$; see below too}\jessie{$\min$ is element-wise, but in this next instance, is just on scalars.}
\begin{lemma}\label{lem:new-form-surrogate}
	For $u \in \U \cap \U_{\pi(u)}$, the top-$k$ surrogate can be written:
	\begin{align*} 
	L^{top-k}(u)_y  &= 1 - u_y + \bar u - \frac{1}{k}(\min(u_y, 1)) = \begin{cases}
	1 - u_y + \bar u - \frac{u_y}{k} & u_y \leq 1\\
	1 - u_y + \bar u - \frac{1}{k} & u_y > 1
	\end{cases}
	~.~
	\end{align*}
\end{lemma}
\begin{proof}
	First, consider
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 -u_y + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= 1 -u_y + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]}~,
	\end{align*}
	by Lemma~\ref{lem:get-rid-max-op}.
	
	
	If $u_y \in [0,1]$, then $(u-e_y)_{[k]} = u_{k+1} = 0$ by Assumption~\ref{assum:nonneg}.
	This yields
	\begin{align*}
	L^{top-k}(u)_y &= 1 - u_y + \bar u - \frac{u_y}{k}~.~
	\end{align*}
	In this case, $u_y = \min(u_y, 1)$, and $\frac {u_y}{k}$ is subtracted from $1 - u_y + \bar u$.
	
	If $u_y > 1$, then $(u-e_y)_{[k]} \geq 0$, and $(u_y - 1)$ is part of the summation.
	This yields
	\begin{align*}
	L^{top-k}(u)_y &= 1 - u_y + \bar u - \frac{1}{k}~,~
	\end{align*}
	where, as in the above case, $1 = \min(u_y, 1)$, and $\frac{1}{k}$ is the term subtracted from $1 - u_y + \bar u$.
	
	Both cases lead to the lemma statement.
\end{proof}


%Consider that the loss value is $1 + \bar u$ when $y > k$, since $u_y = 0$ in this case and the summation resolves to $\bar u$, so the loss is nonnegative even before applying the $(\cdot)_+$ operator.
%
%If $y \leq k$ and $u_y \in [0,1]$, then the loss is nonnegative before applying $(\cdot)_+$ as $1-u_y \geq 0$, as is the term in the summation, $\frac k {k-1} \bar u_{-y}$.
%
%Now if $u_y \in (1, \bar u_{-y} + 1]$, then the term inside the $(\cdot)_+$ operator of $L^{top-k}(u)_y$ is greater than or equal to $0$.
%To see this, consider $1 - u_y + \bar u - \frac 1 k = 1 - u_y + \frac k {k-1} \bar u_{-y} + \frac 1 k u_y - \frac 1 k$, and look at the first three elements and the last two.
%Since $u_y \geq 1$, the difference between the last two terms $\frac 1 k u_y - \frac 1 k \geq 0$.
%Additionally, since $u_y \leq 1 +  \ubar_{-y}$, we can see the difference of these terms is nonnegative.\jessie{Lemma}

%Therefore, we can drop the $(\cdot)_+$ operator on the assumed report space.

Lemma~\ref{lem:new-form-surrogate} allows us to then write the expected loss as follows, by linearity of the surrogate:
\begin{align*}
\inprod{p}{L^{top-k}(u)} &= \sum_{y=1}^n p_y L^{top-k}(u)_y \\
&= 1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)}
\end{align*}
%\jessie{I re-wrote the form of the loss at the top of the section, and it seems like this final form of the expected loss is now trivial to get to...?  Right?}

%\subsection{Studying the Bayes Risk}
%First, consider the case where $u_y = 0$ and $u'_y = 1$, with $u_j = u'_j \;\forall i \neq j$.
%As the expected loss is linear on $u_y$, and these are the boundary cases, we know that $u$ is optimal on one of these two reports in this case.
%
%So we ask when is $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$?
%\begin{align*}
%- \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq \frac {1}{k} - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%0 &\leq \frac{1}{k} - p_y(\frac{k+1}{k})\\
%p_y &\leq \frac{1}{k+1}
%\end{align*}
%
%Conversely, if $p_y > \frac{1}{k+1}$, it is to better to report $u_y = 1$ than $u_y = 0$.
%However, since we are not restricted to the unit hypercube, we might ask when it is optimal to bring $u_y = 1$ up to $u'_y = 1 + \epsilon$ given $\epsilon > 0$.
%Here, we consider when $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$.
%\begin{align*}
%%1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq 1 + \ubar' - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%\frac{1}{k} - p_y (\frac{k+1}{k}) &\leq \frac{1+\epsilon}{k} - p_y (\frac{k+1}{k}(1 + \epsilon))\\
%p_y &\leq \frac{1}{k}
%\end{align*}
%In this case, $u_y=1$ is preferable to $u+y >1$.
%If $p_y > \frac 1 k$, then we want to push $u_y$ as high as we can, but recall that we restrict to $u_y \leq \ubar_{-y} + 1$.
Because the upper bound on $u_y$ in Assumption~\ref{assum:top-threshold} depends on $\ubar$, we cannot determine $u$ iteratively.
However, we can show that for each element $y$ of $u$, $L^{top-k}$ is linear on $[0,1]$ and $[1, \bar u_{-y} + 1]$, and therefore an optimal report must be one of these three values for each $u_i$.


We claim that there are a finite number of polytopes $P^{a} \subseteq \U$ so that (i.) $\cup_{a \in \I} P^{a} = \U$, (ii.) each $P^{a}$ is the convex hull of points in $\R$, and (iii.) $L^{top-k}(\cdot)_y$ is linear on each $P^{a}$ for each $y \in \Y$.
If these three statements are true, then $L^{top-k}|_\R$ is embedded by $L^{top-k}$.

First, fix a permutation $\pi$ on the elements of $\{1,2,\ldots, n\}$.
Consider $u \in \U \cap \U_\pi$ ordered by $\pi$.
Define the polytopes $P^\pi_a := \{u \in \U\cap \U_\pi : u_i \in [0,1] \text{ or } u_i > 1\}$, for some choice of inequality for each $i \in [n]$.
One can observe there are only a finite number of such polytopes as there are only finitely many permutations $\pi$ of $\{1,2,\ldots, n\}$.
Moreover, there are only finitely many ($2^n$) ways to assign $a = \{u_i \in \{[0,1], (1, \bar u_{-i} + 1]\} \}$ for all $i$, and therefore there are only finitely many polytopes.

For all $u \in P^\pi_a$, we have $c(u)$ constant, as $M(u)$ and $H(u)$ are constant in this polytope by design.


Iterating over each choice of inequality $a \in \mathcal{A}$, where $\mathcal{A}$ is the choice of valuations on $u_i$ for all $i$, obeying the permutation ordering, we union to $\U \cap \U_\pi$.  
Since this holds for every permutation, and all the permutations union to the entire space $\U$, as $\cup_\pi(\U \cap \U_\pi) = \U \cap (\cup_\pi \U_\pi) = \U \cap \reals^n = \U$.  
If this is true for every permutation $\pi$, then it holds for $\U$; hence we satisfy (i.).

\jessie{I'm not sure how to modify so that (ii.) is true... consider $n=3, k=2$ and $u = (2.01, 1.01, 0)$.  This is in $\U$, but is not in the convex hull of points in $\R$, since $\R$ only goes out to $2$ in any given direction here.  We might need to also intersect with $[0,k]^n$?  This seems hard to prove though.  Not sure if this is the exact restriction...}
To see (ii.), fix $a$ for some choice of $u_i \in [0,1]$ or $u_i >1$ for all $i$.
For each $i$, consider that $u_i \in [0,1]$ implies $u_1 \in \conv(\{0,1\})$; both elements are members of $\R$.
Similarly, $u \in \U$ and $u_i \in [1,\bar u_{-i}+1]$ implies  $u_i \in \conv(\{1, \bar u_{-i} + 1\})$, which are both members of $\R$ \jessie{this statement is not true...}.

As $\conv(\times_{j=1}^n a_j) = \times_{j=1}^n \conv(a_j)$ where $a_j$ denotes each component of $a$, we then have the desired result (ii.).

%To see (ii.), we first know this is true for $u \in [0,1]^n$ by construction of the polytope, as this polytope is defined by $u$ obeying the permutation $\pi$, which has a linear boundary defined by $\vec 0$ and an edge of the $\{0,1\}^n$ hypercube.
%
%For $u \not \in [0,1]^n$, consider the boundary of the polytope to be $\{u \in \U \cap \U_\pi : u_i >1 \implies u_i = \bar u_{-i} + 1\}$.
%\jessie{Not sure on this part.}


To see (iii.), we claim that $L^{top-k}(\cdot)_y$ is linear on each polytope $P^\pi_a$.
Consider that on the polytope $P^\pi_a$, we fix $u_y \in [0,1]$ or $u_y >1$ for all $y \in \Y$, so on each piece, $L^{top-k}(\cdot)_y$ is linear when we break the loss down into cases, as we do not switch cases within a polytope since each polytope has $u_i$ fixed in one of the two cases for all $i$.


%For a fixed $u$, take $M$ to be $\{i \in [n] : u_i \in [0,1]\}$ and $H := \{i \in [n] : u_i > 1 \}$.
%
%For each $i$, if $u_i \in [0,1]$, take the set of points $\R'$ such that $u_i \in \R$ such that $u_i \in \{0,1\}$.
%Further, restrict this set $\R'$ to $\R''$ so that for $u_i \in (1, \bar u_{-i} +1]$, $\R'' := \{u \in \R' : u_i \in \{1, c\}\}$, where $c := \frac{|M|+k-1}{k-|H|}$.
%
%We define the polytope $P^u$ to be $\conv(\R'')$, so clearly (ii.) is satisfied, since $\R'' \subseteq \R$.
%
%To see that (i.) is true, \jessie{TODO...}
%
%Lastly, to see that $L^{top-k}$ is linear of each piece $\R''$ of $\U$
%
%\jessie{Thoughts: take $\pi$ to be a permutation.  $\R'(\pi)$ is the set of $u \in \R$ so that $\pi = \pi(u)$, 
%	%and for all $i: u_i > 1$, we have $u \in \R: u_i \in \{1, c\}$ in $\R'(\pi)$, and for $i: u_i \in [0,1]$, we have $u \in \R : u_i \in \{0,1\}\}$ in $\R'(\pi)$
%	$\R'(\pi) = \{r \in \U_\pi \cap \R: u_i >1 \implies r_i \in \{1,c\}, u_i \in [0,1] \implies r_i \in \{0,1\} \}$.
%	
%}


\begin{proposition}
	$\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}$ is representative for $L^{top-k}$.
\end{proposition}
\begin{proof}
	Since $\U''$ is representative, we simply have to show there is a function $\phi : \U'' \to \R$ so that $u \in \U''$ minimizes $L^{top-k}$ implies $\phi(u)$ minimizes $L^{top-k}$.
	In particular, take the sets $H(u) = \{i \mid u_i > 1\}$ and $M(u) = \{i \mid u_i \in (0,1]\}$.
	Observe that for $H(\U'')$ and $M(\U'')$ have finite domains, and for all $u \in \U''$, we have $H(u) \cap M(u) = \emptyset$ and are subsets of $[n]$ by construction, and their sums $|H(u)| + |M(u)| \leq k$ by the restriction of $\U''$ that $\|u\|_0 \leq k$.
	Moreover, we can write $\R = \phi(\U'') = \{\frac{|M(u)| + k -1}{k - |h(u)|} \mathbf{1}_{H(u)} + \mathbf{1}_{M(u)} : u \in \U''\}$.
	It is just left to show $\R$ is representative for $L^{top-k}$.
	
	Suppose $u \in \U''$ minimizes $L^{top-k}$; we want to show $\phi(u)$ also minimizes $L^{top-k}$.
	By virtue of $u \in \U''$, we know we can write the expected loss 
	\begin{align*}
	\inprod{p}{L^{top-k}(u)} &= 1 - \bar u + \inprod{p}{u + \frac 1 k \min(u,\ones)}\\
	&= 1 - \bar{\phi(u)} + \inprod{p}{u + \frac 1 k \min(\phi(u),\ones)}\\
	&\geq 1 - \bar{\phi(u)} + \inprod{p}{u + \frac 1 k \min(u,\ones)}\\
	\end{align*}
	Claims: $\bar u \leq \bar{\phi(u)}$
	$\min(u, \ones) = \min(\phi(u), \ones)$
	If $u$ minimizes $L(u;p)$, then $\inprod{p}{u} \geq \inprod{p}{\phi(u)}$.
\end{proof}