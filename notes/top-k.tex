\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage{../neurips-19/neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{assumption}{Assumption}


\title{Consistent Polyhedral Surrogates via Embedding}
%\title{Convex Surrogates via Polyhedral Losses}
\author{%
 Jessica Finocchiaro\raf{Jessie?} \\
 \texttt{jessica.finocchiaro@colorado.edu}\\
 CU Boulder
 \And
 Rafael Frongillo\\
 \texttt{raf@colorado.edu}\\
 CU Boulder
 \And
 Bo\\
 \texttt{bwag@colorado.edu}\\
 MSFT
}

\begin{document}

\section{Top-$k$ surrogate}

Throughout this section, consider the surrogate loss \begin{equation}\label{eq:top-k-surrogate}
L^{top-k}(u)_y~=~\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \right)_+~.~
\end{equation}

We write the expected loss $L^{top-k}(u)$ without the $(\cdot)_+$ operator by restricting $u$.
Then, using this form of the expected loss, we are able to find a finite set of optimal reports over all $p \in \simplex$.

\subsection{Notation and Assumptions}
\subsubsection{Notation}
We say $u_{[i]}$ is the $i^{th}$ largest element of the $u$ vector.
Let $\bar{u} = \frac 1 k \sum_{i = 1}^k u_{[i]}$ and similarly, $\bar{u}_{-i} = \frac{1}{k-1} (k \bar u - u_{[i]})$ be the averages of the first $k$ sorted elements of $u$ and the average of the first $k-1$ sorted elements of $u$ besides the $i^{th}$ element, respectively.
$u[y]$ refers to the order of the $y^{th}$ component of $u$.
For example, if $u = (1,2,0)$, then $u[2] = 1$. 
If $u$ is understood from context, we abuse notation to omit the $u$ and write $u[y] = [y]$.

Let $\min(\vec w, \vec v) \in \reals^d$ denote the element-wise minimum of vectors $\vec w, \vec v \in \reals^d$.

\subsubsection{Assumptions}\label{sec:assumptions}
Throughout, we make two assumptions about our report set. 
For all $p \in \simplex$, we show there is a minimizer of $\inprod{p}{L^{top-k}(\cdot)}$ satisfying these two assumptions.

%\begin{assumption}\label{assum:ordered-u}
%First, assume that $u$ is ordered, where $u_1 \geq u_2 \geq \ldots \geq u_n$.
%\end{assumption}
%This is without loss of generality, as we can reorder $p$ so that each $p_i$ corresponds to the ordered $u_i$ that it originally corresponded to.
%This is simply for ease of exposition, as it allows us to write $u_i$ instead of $u_{[i]}$ in many situations. 

For our first assumption, we first observe that $L^{top-k}$ is invariant in the $\ones$ direction.
\begin{lemma}
	For all $\alpha \in \reals$ and $y\in\Y$, we have $L^{top-k}(u)_y = L^{top-k}(u + \alpha \ones)_y$.
\end{lemma}
\begin{proof}\label{lem:invar-ones}
	\begin{align*}
	L^{top-k}(u + \alpha \ones)_y &= \left(1 - (u_y + \alpha) + \frac{1}{k} \sum_{i=1}^k ((u + \alpha \ones  - e_y)_{[i]} \right)_+\\
	&= \left(1 - (u_y + \alpha) +\frac{1}{k} \alpha k + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= \left(1 -u_y - \alpha + \alpha + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= \left(1 -u_y + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= L^{top-k}(u)_y
	\end{align*}
\end{proof}

\begin{assumption}\label{assum:nonneg}
	We will say $u_{[k+1]} = \ldots = u_{[n-1]} = u_{[n]} = 0$.
\end{assumption}
This partially follows from the invariance of $L^{top-k}$ in the $\ones$ direction shown in Lemma~\ref{lem:invar-ones}.
We set $u_{[k+1]} = 0$, and then show weak dominance of the nonnegative $u'$ over any $u$ that contains negative components as in Lemma~\ref{lem:extra-elts-zero}.

\begin{lemma}\label{lem:extra-elts-zero}
	For all $u \in \reals^d$, there is a $u'$ satisfying Assumption~\ref{assum:nonneg} so that $L^{top-k}(u)_y \geq L^{top-k}(u')_y$ for all $y \in \Y$.
	\jessie{Claim: $R := \{u \in \reals^n_+ \mid \|u\|_0 \leq k\}$ is a representative set for $L^{top-k}$.}
\end{lemma}
\begin{proof}
	By Lemma~\ref{lem:invar-ones}, we can fix $u_{[k+1]} = 0$.
	We then have $u_{[i]} \leq 0$ for all $i \geq k+1$.
	Consider $u'$ such that $u'_{[i]} = u_{[i]}$ for all $i$ so that $u_{[i]} \geq 0$, and $u'_{[i]} = 0$ otherwise.
	Observe that $u$ and $u'$ have the same ordering on their elements.
	
	If $[y] \geq k+1$, then 
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 - u_y + \frac 1 k \sum_{i=1}^k (u - e_y)_{[i]}\right)_+ \\
%	&= 1 - u_y + \frac 1 k \sum_{i=1}^k (u - e_y)_{[i]}\\
	&\geq \left(1 + \frac {1}{k} \sum_{i=1}^k (u' - e_y)_{[i]}\right)_+\\
	&= L^{top-k}(u')_y~.~
	\end{align*}
	The inequality comes from the equality of the first $k+1$ sorted elements of $u$ and $u'$, combined with setting $u_{[y]} \leq u'_{[y]} = 0$ in this case.
	
	Now, if $[y] < k+1$ and $u_y \geq 1$, then 
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 - u_y + \frac 1 k \sum_{i=1}^k (u - e_y)_{[i]}\right)_+ \\
	&= \left(1 - u_y + \bar u - \frac 1 k\right)_+\\
	&= \left(1 - u'_y + \bar u' - \frac 1 k\right)_+\\
	&= L^{top-k}(u')_y~.~
	\end{align*}
	
	Now suppose that $[y] < k+1$ and $u_y \in [0,1)$; we now observe $(u - e_y)_{[k]} = u_{[k+1]} = 0$. 
	\begin{align*}
	L^{top-k}(u)_y &= \left(1 - u_y + \frac 1 k \sum_{i=1}^{k-1} u_{[i]} + 0 \right)_+\\
	&= \left(1 - u'_y + \frac 1 k \sum_{i=1}^{k-1} u'_{[i]}\right)_+\\
	&= \left(1 - u'_y + \frac 1 k \sum_{i=1}^{k} (u' - e_y)_{[i]}\right)_+\\
	&= L^{top-k}(u')_y~.~
	\end{align*}

	As an aside, since $L^{top-k}(u)_y \geq L^{top-k}(u')_y$ for all $y \in \Y$, this also holds for the expected loss for all $p \in \simplex$.
\end{proof}



%For $i > k+1$, observe that $u_i$ can never affect the $\bar{u}$ term that is added, and can only be subtracted in the case that $i = y$. 
%Therefore, we minimize expected loss by pulling $u_{k+2},\ldots, u_n$ up to $u_{k+1}$, making it as ``tight'' as possible.
%This leads to our assumption, which we can make since we are primarily interested in finding \emph{a} minimizer of $L^{top-k}$, rather than \emph{all} minimizers.
%\jessie{Formalize justification}

The following Lemma leads to our final assumption, which provides an upper bound on the report values we consider.

\begin{lemma}\label{lem:top-threshold}
	For all $y \in \Y$, there is a minimizer $u$ of $L^{top-k}(\cdot)_y$ satisfying Assumption~\ref{assum:nonneg} so that for all $i$, we have $u_i \leq \bar u_{-i} +1$. 
\end{lemma}
\begin{proof}
	If $u_{[i]} > \bar u_{-i} + 1$, reassigning such a $u'_{[i]} := \bar u_{-i} +1$ results in $L^{top-k}(u')_i = L^{top-k}(u)_i = 0$, but we observe $L^{top-k}(u')_j \leq L^{top-k}(u)_j$ for all $j \neq i$ by decreasing $\ubar$, since $u_i \geq 0$.

To see this, consider $u_y = \bar u_{-y} + 1 + \epsilon$ and $u'_y = \bar u_{-y} + 1$, with $u_i = u'_i$ for all $i \neq y$. 
\begin{align*}
L^{top-k}(u)_y &= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} + \epsilon\right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} + \epsilon\right) \right)_+\\
&= \left(-\bar u_{-y} - \epsilon + \bar u_{-y} + \frac \epsilon k \right)_+ \\
&= (-\frac{k-1}{k}\epsilon)_+\\
&= 0
\end{align*}
Compare the above to the following:
\begin{align*}
L^{top-k}(u')_y &= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} \right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} \right) \right)_+\\
&= \left(-\bar u_{-y} + \bar u_{-y}\right)_+ \\
&= 0~.~
\end{align*}
Thus, the losses are equal on the outcome $y$.

Now let us consider outcome $j \neq y$.
By Assumption~\ref{assum:nonneg}, if $u_y > \bar u_{-y} + 1$, then we have $[y] < k+1$ as $u_y > 0$.
Moreover, $u_y \geq 1$ in this case.
Now, for outcome $j \neq y$, we have 
\begin{align*}
L^{top-k}(u)_j &= \left( 1 - u_j + \frac 1 k \sum_{i=1}^k (u-e_j)_{[i]} \right)_+\\
	&= \left( 1 - u'_j + \frac 1 k \sum_{i=1}^k (u'-e_j)_{[i]} + \frac {\epsilon}{k}\right)_+\\
	&> \left( 1 - u'_j + \frac 1 k \sum_{i=1}^k (u'-e_j)_{[i]}\right)_+\\
	&= L^{top-k}(u')_j
\end{align*}
Therefore, if $u$ minimizes $L^{top-k}(\cdot)_y$, then so does $u'$ for each $y \in \Y$, so we can say the same of the expected loss $\inprod{p}{L^{top-k}(\cdot)}$ for all $p \in \simplex$.
\end{proof}

This leads to our final assumption.
\begin{assumption}\label{assum:top-threshold}
	We restrict to $u$ such that for all $i$, we have $u_i \leq \bar u_{-i} +1$.
\end{assumption}



\section{Re-writing the loss}
The following Lemma allows us to consider the loss without the $(\cdot)_+$ operator.
Before we do so, consider the set $\U$ to be all $u \in \reals^n$ satisfying Assumptions~\ref{assum:nonneg} and ~\ref{assum:top-threshold}.
\jessie{Can we claim $\U$ is a representative set for $L^{top-k}$?  Would take some more work.}

One can observe that $\U$ is not convex.
However, we can consider permutations $\pi$ of the elements of $\U$ so that $\pi(u)$ is the permutation of $\{1,2,\ldots, n\}$ so that $u$ is ordered in decreasing order according to this permutation.
We then let $\U_\pi$ be all $u \in \reals^d$ ordered by the same $\pi$ of $\{1,2,\ldots,n\}$.
Now regrardless of $\pi$, we can consider $\U \cap \U_\pi$, which is a convex set, and every $u \in \U$ is also in $\U \cap \U_{\pi}$ for some $\pi$.

%Moreover, we relax Assumption~\ref{assum:nonneg} to the following.
%\begin{assumption}\label{assum:nonneg-relax}
%	$u \in \reals^n_+$.
%\end{assumption}
%In a similar vein, we call $\U^\circ$ the set of $u \in \reals^n$ satisfying Assumptions~\ref{assum:top-threshold} and \ref{assum:nonneg-relax}.
%
%\raf{I think it might be handy to introduce the set $\U$ here to be the points satisfying all three assumptions --- maybe explicitly write the constraints and nod to the assumptions.  Then we can say $u\in\U$ and forget the assumptions going forward.}

\begin{lemma}\label{lem:get-rid-max-op}
	For all $u \in \U$, we have $1 -u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \geq 0$.
\end{lemma}
This statement allows us to consider the loss $L^{top-k}$ without the $(\cdot)_+$ operator, since the inside term being nonnegative implies the max of the term and $0$ is always the inside term.

\begin{proof}
	First, we have $\frac 1 k \sum_{i=1}^k (u-e_y)_{[i]} \geq 0$ by nonnegativity of the first $k+1$ elements of $u$ in Assumption~\ref{assum:nonneg}.
	Moreover, we need to show that $u_y \leq 1 + \frac 1 k \sum_{i=1}^k (u-e_y)_{[i]}$.
	
	If $[y] \geq k+1$, then $\frac 1 k \sum_{i=1}^k (u-e_y)_{[i]} = \bar u$, so the inequality becomes $u_y \leq 1 + \bar u$.
	This holds since $u_{[y]} \leq u_{[k]} \leq \bar u \leq 1 + \bar u$.
	
	Now if $[y] < k+1$ and $u_y \in [0,1]$, then we have $(u-e_y)_{[k]} = 0$, and the above term becomes
	\begin{align*}
	&1 -u_y + \frac{1}{k} \sum_{i=1}^{k} u_i - \frac 1 k u_y \geq 0\\
	&\iff 1 + \bar u \geq u_y + \frac{1}{k} u_y\\
	&\iff 1 + \frac k {k-1} \bar u_{-y} \geq u_y\\
	&\implies \frac k {k-1} \bar u_{-y} \geq 0~,~
	\end{align*}
	which is true by Assumption~\ref{assum:nonneg}.
	
	Now if $u_y > 1$, then $\frac 1 k \sum_{i=1}^k (u-e_y)_{[i]} = \bar u - \frac 1 k$.
	Therefore, we have 
	\begin{align*}
	&1 -u_y + \bar u - \frac 1 k \geq 0\\
	&\iff 1 + \frac{k}{k-1}\bar u_{-y} \geq \frac{k-1}{k} u_y + \frac 1 k\\
	&\iff (1 + \bar u_{-y}) + \frac 1 {k-1} \bar u_{-y} \geq \frac{k-1}{k} u_y + \frac 1 k\\
	&\iff \frac{k-1}{k}(1 + \bar u_{-y}) + \frac 1 k + \frac{1}{k}\bar u_{-y} + \frac 1 {k-1} \bar u_{-y} \geq \frac{k-1}{k} u_y + \frac 1 k\\
	&\implies \frac{1}{k}\bar u_{-y}+ \frac{1}{k-1}\bar u_{-y} \geq 0~,~
	\end{align*}
	with the final implication coming from Assumption~\ref{assum:top-threshold}.
	
	Therefore, the term is nonnegative if $u$ satisfies Assumptions~\ref{assum:nonneg} and \ref{assum:top-threshold}.
\end{proof}

With Assumptions~\ref{assum:nonneg} and \ref{assum:top-threshold} and Lemma~\ref{lem:get-rid-max-op}, we can now rewrite the loss for each outcome in a simplified form.
\raf{Note: some inconsistency in the way you use this $\min$ operator and denote $\ones$; see below too}\jessie{$\min$ is element-wise, but in this next instance, is just on scalars.}
\begin{lemma}\label{lem:new-form-surrogate}
	For $u \in \U \cap \U_{\pi(u)}$, the top-$k$ surrogate can be written:
\begin{align*} 
L^{top-k}(u)_y  &= 1 - u_y + \bar u - \frac{1}{k}(\min(u_y, 1)) = \begin{cases}
1 - u_y + \bar u - \frac{u_y}{k} & u_y \leq 1\\
1 - u_y + \bar u - \frac{1}{k} & u_y > 1
\end{cases}
~.~
\end{align*}
\end{lemma}
\begin{proof}
	First, consider
\begin{align*}
	L^{top-k}(u)_y &= \left(1 -u_y + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]} \right)_+\\
	&= 1 -u_y + \frac{1}{k} \sum_{i=1}^k ((u - e_y)_{[i]}~,
	\end{align*}
	by Lemma~\ref{lem:get-rid-max-op}.

	
	If $u_y \in [0,1]$, then $(u-e_y)_{[k]} = u_{k+1} = 0$ by Assumption~\ref{assum:nonneg}.
	This yields
	\begin{align*}
	L^{top-k}(u)_y &= 1 - u_y + \bar u - \frac{u_y}{k}~.~
	\end{align*}
	In this case, $u_y = \min(u_y, 1)$, and $\frac {u_y}{k}$ is subtracted from $1 - u_y + \bar u$.

	If $u_y > 1$, then $(u-e_y)_{[k]} \geq 0$, and $(u_y - 1)$ is part of the summation.
	This yields
	\begin{align*}
	L^{top-k}(u)_y &= 1 - u_y + \bar u - \frac{1}{k}~,~
	\end{align*}
	where, as in the above case, $1 = \min(u_y, 1)$, and $\frac{1}{k}$ is the term subtracted from $1 - u_y + \bar u$.
	
	Both cases lead to the lemma statement.
\end{proof}


%Consider that the loss value is $1 + \bar u$ when $y > k$, since $u_y = 0$ in this case and the summation resolves to $\bar u$, so the loss is nonnegative even before applying the $(\cdot)_+$ operator.
%
%If $y \leq k$ and $u_y \in [0,1]$, then the loss is nonnegative before applying $(\cdot)_+$ as $1-u_y \geq 0$, as is the term in the summation, $\frac k {k-1} \bar u_{-y}$.
%
%Now if $u_y \in (1, \bar u_{-y} + 1]$, then the term inside the $(\cdot)_+$ operator of $L^{top-k}(u)_y$ is greater than or equal to $0$.
%To see this, consider $1 - u_y + \bar u - \frac 1 k = 1 - u_y + \frac k {k-1} \bar u_{-y} + \frac 1 k u_y - \frac 1 k$, and look at the first three elements and the last two.
%Since $u_y \geq 1$, the difference between the last two terms $\frac 1 k u_y - \frac 1 k \geq 0$.
%Additionally, since $u_y \leq 1 +  \ubar_{-y}$, we can see the difference of these terms is nonnegative.\jessie{Lemma}

%Therefore, we can drop the $(\cdot)_+$ operator on the assumed report space.

Lemma~\ref{lem:new-form-surrogate} allows us to then write the expected loss as follows, by linearity of the surrogate:
\begin{align*}
\inprod{p}{L^{top-k}(u)} &= \sum_{y=1}^n p_y L^{top-k}(u)_y \\
&= 1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)}
\end{align*}
%\jessie{I re-wrote the form of the loss at the top of the section, and it seems like this final form of the expected loss is now trivial to get to...?  Right?}

%\subsection{Studying the Bayes Risk}
%First, consider the case where $u_y = 0$ and $u'_y = 1$, with $u_j = u'_j \;\forall i \neq j$.
%As the expected loss is linear on $u_y$, and these are the boundary cases, we know that $u$ is optimal on one of these two reports in this case.
%
%So we ask when is $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$?
%\begin{align*}
%- \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq \frac {1}{k} - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%0 &\leq \frac{1}{k} - p_y(\frac{k+1}{k})\\
%p_y &\leq \frac{1}{k+1}
%\end{align*}
%
%Conversely, if $p_y > \frac{1}{k+1}$, it is to better to report $u_y = 1$ than $u_y = 0$.
%However, since we are not restricted to the unit hypercube, we might ask when it is optimal to bring $u_y = 1$ up to $u'_y = 1 + \epsilon$ given $\epsilon > 0$.
%Here, we consider when $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$.
%\begin{align*}
%%1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq 1 + \ubar' - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%\frac{1}{k} - p_y (\frac{k+1}{k}) &\leq \frac{1+\epsilon}{k} - p_y (\frac{k+1}{k}(1 + \epsilon))\\
%p_y &\leq \frac{1}{k}
%\end{align*}
%In this case, $u_y=1$ is preferable to $u+y >1$.
%If $p_y > \frac 1 k$, then we want to push $u_y$ as high as we can, but recall that we restrict to $u_y \leq \ubar_{-y} + 1$.
Because the upper bound on $u_y$ in Assumption~\ref{assum:top-threshold} depends on $\ubar$, we cannot determine $u$ iteratively.
However, we can show that for each element $y$ of $u$, $L^{top-k}$ is linear on $[0,1]$ and $[1, \bar u_{-y} + 1]$, and therefore an optimal report must be one of these three values for each $u_i$.

\bigskip
\hrule
\bigskip

Consider the following finite set.
\begin{align*}
\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}
\end{align*}
\jessie{Ultimately, WTS $\R$ is representative for $L^{top-k}$.}
Throughout, we consider $c(u) := \frac{|M(u)| + k -1}{k - |H(u)|}$, where $M(u) = \{i \in [n] : u_i \in [0,1] \}$ and $H(u) = \{i \in [n] : u_i > 1\}$.

We claim that there are a finite number of polytopes $P^{a} \subseteq \U$ so that (i.) $\cup_{a \in \I} P^{a} = \U$, (ii.) each $P^{a}$ is the convex hull of points in $\R$, and (iii.) $L^{top-k}(\cdot)_y$ is linear on each $P^{a}$ for each $y \in \Y$.
If these three statements are true, then $L^{top-k}|_\R$ is embedded by $L^{top-k}$.

First, fix a permutation $\pi$ on the elements of $\{1,2,\ldots, n\}$.
Consider $u \in \U \cap \U_\pi$ ordered by $\pi$.
Define the polytopes $P^\pi_a := \{u \in \U\cap \U_\pi : u_i \in [0,1] \text{ or } u_i > 1\}$, for some choice of inequality for each $i \in [n]$.
One can observe there are only a finite number of such polytopes as there are only finitely many permutations $\pi$ of $\{1,2,\ldots, n\}$.
Moreover, there are only finitely many ($2^n$) ways to assign $a = \{u_i \in \{[0,1], (1, \bar u_{-i} + 1]\} \}$ for all $i$, and therefore there are only finitely many polytopes.

For all $u \in P^\pi_a$, we have $c(u)$ constant, as $M(u)$ and $H(u)$ are constant in this polytope by design.


Iterating over each choice of inequality $a \in \mathcal{A}$, where $\mathcal{A}$ is the choice of valuations on $u_i$ for all $i$, obeying the permutation ordering, we union to $\U \cap \U_\pi$.  
Since this holds for every permutation, and all the permutations union to the entire space $\U$, as $\cup_\pi(\U \cap \U_\pi) = \U \cap (\cup_\pi \U_\pi) = \U \cap \reals^n = \U$.  
If this is true for every permutation $\pi$, then it holds for $\U$; hence we satisfy (i.).

\jessie{I'm not sure how to modify so that (ii.) is true... consider $n=3, k=2$ and $u = (2.01, 1.01, 0)$.  This is in $\U$, but is not in the convex hull of points in $\R$, since $\R$ only goes out to $2$ in any given direction here.  We might need to also intersect with $[0,k]^n$?  This seems hard to prove though.  Not sure if this is the exact restriction...}
To see (ii.), fix $a$ for some choice of $u_i \in [0,1]$ or $u_i >1$ for all $i$.
For each $i$, consider that $u_i \in [0,1]$ implies $u_1 \in \conv(\{0,1\})$; both elements are members of $\R$.
Similarly, $u \in \U$ and $u_i \in [1,\bar u_{-i}+1]$ implies  $u_i \in \conv(\{1, \bar u_{-i} + 1\})$, which are both members of $\R$ \jessie{this statement is not true...}.

As $\conv(\times_{j=1}^n a_j) = \times_{j=1}^n \conv(a_j)$ where $a_j$ denotes each component of $a$, we then have the desired result (ii.).

%To see (ii.), we first know this is true for $u \in [0,1]^n$ by construction of the polytope, as this polytope is defined by $u$ obeying the permutation $\pi$, which has a linear boundary defined by $\vec 0$ and an edge of the $\{0,1\}^n$ hypercube.
%
%For $u \not \in [0,1]^n$, consider the boundary of the polytope to be $\{u \in \U \cap \U_\pi : u_i >1 \implies u_i = \bar u_{-i} + 1\}$.
%\jessie{Not sure on this part.}


To see (iii.), we claim that $L^{top-k}(\cdot)_y$ is linear on each polytope $P^\pi_a$.
Consider that on the polytope $P^\pi_a$, we fix $u_y \in [0,1]$ or $u_y >1$ for all $y \in \Y$, so on each piece, $L^{top-k}(\cdot)_y$ is linear when we break the loss down into cases, as we do not switch cases within a polytope since each polytope has $u_i$ fixed in one of the two cases for all $i$.


%For a fixed $u$, take $M$ to be $\{i \in [n] : u_i \in [0,1]\}$ and $H := \{i \in [n] : u_i > 1 \}$.
%
%For each $i$, if $u_i \in [0,1]$, take the set of points $\R'$ such that $u_i \in \R$ such that $u_i \in \{0,1\}$.
%Further, restrict this set $\R'$ to $\R''$ so that for $u_i \in (1, \bar u_{-i} +1]$, $\R'' := \{u \in \R' : u_i \in \{1, c\}\}$, where $c := \frac{|M|+k-1}{k-|H|}$.
%
%We define the polytope $P^u$ to be $\conv(\R'')$, so clearly (ii.) is satisfied, since $\R'' \subseteq \R$.
%
%To see that (i.) is true, \jessie{TODO...}
%
%Lastly, to see that $L^{top-k}$ is linear of each piece $\R''$ of $\U$
%
%\jessie{Thoughts: take $\pi$ to be a permutation.  $\R'(\pi)$ is the set of $u \in \R$ so that $\pi = \pi(u)$, 
%	%and for all $i: u_i > 1$, we have $u \in \R: u_i \in \{1, c\}$ in $\R'(\pi)$, and for $i: u_i \in [0,1]$, we have $u \in \R : u_i \in \{0,1\}\}$ in $\R'(\pi)$
%	$\R'(\pi) = \{r \in \U_\pi \cap \R: u_i >1 \implies r_i \in \{1,c\}, u_i \in [0,1] \implies r_i \in \{0,1\} \}$.
%	
%}

\bigskip
\hrule
\bigskip

\begin{lemma}\label{lem:linear-pieces}
	For each element $u_i$ of $u \in \U$, the expected loss $\inprod{p}{L^{top-k}(u)}$ is linear on $[0,1]$ and $[1, \bar u_{-i} + 1]$ for all $p \in \simplex$.
	\jessie{Not sure if this is the right statement.}
    \raf{Not sure either.  One thing to be careful about: you either have to make sure that the intervals in question yield points in the set $\U$, or refer to the original form of the loss.  Because the form of the loss you are using only is shown to hold when $u\in\U$.  Hopefully that made sense.}
    \raf{One thing that would help: let's visualize $\U$ for some value of $k,n$ to get some intuition for what this set is.  Maybe $k=2$ and $n=3$, but only plot the first 2 coordinates?}
\end{lemma}
\begin{proof}
	For all $p \in \simplex$ and $u \in \U^\circ$, we know that 
	\begin{equation}
	\inprod{p}{L^{top-k}(u)} = 1 + \bar u - \inprod{p}{u + \frac 1 k \min{u,\ones}}~.~
	\end{equation}
	For any $p$, if $u_i \in [0,1]$, we then have 
	\begin{align*}
	\inprod{p}{L^{top-k}(u)} &= 1 + \bar u - \inprod{p}{u + \frac 1 k \min(u,\ones)}\\
	 &= 1 + \bar u - \sum_{y \neq i}p_y (u_y + \frac 1 k \min(u_y, 1)) - p_i \frac {k+1} k u_i~,~
	\end{align*}
	which is linear on $u_i$.
	
	Similarly, for $u_i \in [1, \bar u_{-i} + 1]$, we can write $\inprod{p}{L^{top-k}(u)} = 1 + \bar u - \sum_{y \neq i}p_y (u_y + \frac 1 k \min(u_y, 1)) - p_i u_i - p_i/k$, which is linear in $u_i$.
\end{proof}

\begin{theorem}\label{thm:finite-set}
	Consider the loss $L^{top-k} : \reals^n \times \Y \to \reals_+$ given in Equation~\ref{eq:top-k-surrogate}.  
	The finite set
	\begin{align*}
	\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}
	\end{align*}
	is representative for $L^{top-k}$.
\end{theorem}
\begin{proof}[Proof Sketch.]
%	The above work tells us that when $p_y > \frac 1 {k+1}$, it is optimal to set $u_y > 0$, and otherwise, it is optimal to set $u_y = 0$.
	Since $L^{top-k}$ is linear in $i$ on $[0,1]$ and $[1, c]$ for some $c \geq 1$ and all dimensions $i \in [n]$, we know that the optimum is on the boundary of such reports, so when we bump $u_y$ up to some number greater than $0$, it should either be one $1$ or $c$ to be an optimal report.
	$H$ represents the elements of the report that are bumped up to $c$, and $M$ represents the elements of the report that are bumped up to $1$.
	We know that the size of $H \cup M$ must be less than or equal to $k$ by Assumption~\ref{assum:nonneg}, and $H \cap M$ is empty by each element of $u$ being real-valued, so for $c > 1$, $u_i$ cannot simultaneously be equal to $c$ and $1$, hence the restrictions on $\R$.
	Note that if $c = 1$, then we can say $H = \emptyset$.
	
	To see the reason for $c := \frac{|M| + k -1}{k - |H|}$, consider that, given the loss $L^{top-k}$, when we want to ``crank $u_i$'' above $1$, we want to do so as much as possible, until hitting the threshold discussed in Assumption~\ref{assum:top-threshold}.
	Exceeding the threshold on $u_i > c$ then can only increase expected loss, as shown in Lemma~\ref{lem:top-threshold}.
	
	Then, for $i \in H$, ``cranking'' $u_i$ as high as possible, we want to have $c = u_i = \bar u _{-i} + 1$.
	However, each $i \in H$ should get ``cranked up'' to the same value of $\bar u_{-i} + 1$ by linearity.
	We can see that 
	\begin{align*}
	c &= \bar u_{-i} + 1 \\
	c &= \frac{(c (|H|-1)) + |M|}{k-1} + 1\\
	ck-c &= c|H| - c + |M| + k - 1\\
	ck - c|H| &= |M| + k - 1\\
	c &= \frac{|M|+k-1}{k-|H|}~.~
	\end{align*}
\end{proof}


By Theorem~\ref{thm:finite-set}, we now have a finite set of optimal reports for $L^{top-k}$.
This yields the finite loss $L|_\R$ embedded by $L^{top-k}$.
This gives the discrete loss $\ell^{top-k} : \R \times\Y \to \reals_+$ by
\begin{align}
\ell^{top-k}(r,y) &= \begin{cases}
0 & r_y = \bar r_{-y} + 1\\
\bar r - \frac 1 k & r_y = 1\\
1 + \bar r & r_y = 0
\end{cases}~.~
\end{align}

\bigskip
\hrule
\bigskip

Story: Here is a region $\U$ containing optimal reports.
Clearly, if $\U \subseteq \U^\circ$, then optimal reports are also in $\U^\circ$.
Look at this convex region.
Since $L^{top-k}$ is polyhedral, we know that one of its minimizers must be on a vertex.
We want to claim that all vertices are in $\hat\U$, which is a finite set of our own construction.
Since all vertices are in $\hat\U$, and all minimizers must be vertices (if not, there is also a vertex that is a minimizer), we then have that $\argmin_u \inprod{p}{L^{top-k}(u)} \cap \hat\U \neq \emptyset$.


\section{Constructing a calibrated surrogate}
Here, we use the surrogate constructed in Proposition 2 of the paper to construct a calibrated surrogate for top $k$.
\subsection{$k=2$ and $n=3$}
First, we have $\risk{\ell}(p) = 1 - \min_y p_y$, and so $-\risk{\ell}(p) = \min_y p_y - 1$.
Taking the conjugate $(-\ell^*)(q) = \sup_{p \in \simplex} \{\inprod p q + 1 - \min_y p_y\}$, we see this is maximized by putting weight $1$ on $\argmax_y q_y$ and weight $0$ on every other element for $q \in \simplex$.
Since $n \neq 1$, we then have $\min_y p_y = 0$, so the conjugate of the negative risk reduces to $C(u) = 1 + \max_y u_y$.

Then, plugging this into our given loss yields $L(u)_y = 1 + u_{[1]} - u_y$.

\subsection{Generalizing $k$ and $n$}
Generalizing the above calibrated surrogate, we observe that $\risk(\ell) = 1 - \sum_{i=k+1}^n p_{[i]}$, and so $(-\risk{\ell})^*(q) = \sup_{p \in \simplex} \{\inprod p q + 1 - \sum_{i=k+1}^n p_{[i]}\}$.
This is maximized in the same way as above, and so we reduce this to $(-\risk{\ell})^*(q) = 1 - q_{[1]}$.

Again, we yield $L(u)_y = 1 + u_{[1]} - u_y$.

\section{Cutting down on search space}
Consider the loss $L^{top-k} : \U \times \Y \to \reals_+$ given in equation~\eqref{eq:top-k-surrogate}.
This surrogate is defined originally on $\U = \reals^n$, but since the loss is polyhedral, we know it has a finite set of minimizers.
If we can find a finite representative set, we can easily embed these reports via the identity and more easily study the property elicited by this surrogate, and how it compares to the top-$k$ property it was originally designed for.

We will start with $\U = \reals^n$, and slowly narrow down to a finite representative set for $L^{top-k}$.

\begin{proposition}
	$\U' := \{u \in \reals^n_+ \mid \|u\|_0 \leq k\}$ is representative for $L^{top-k}$.
\end{proposition}
\begin{proof}
	This is Lemma~\ref{lem:extra-elts-zero}.
\end{proof}

\begin{proposition}
	The set $\U'' := \U' \cap [0,k]^n$ is representative for $L^{top-k}$.
\end{proposition}
\begin{proof}
	Corollary of Lemma~\ref{lem:top-threshold}, as any of the $k$ largest elements cannot be more than $1$ greater than its preceding element.
\end{proof}

\begin{proposition}
	\jessie{Incorporate no more than 1 higher than mean}
\end{proposition}

Now we move from an infinite to finite set via the function $\phi$
\begin{align*}
\phi(u)_i &= 
\begin{cases}
0 & u_i < 1\\
1 & \\
c(u) & \\
\end{cases}~,~
\end{align*}
where $c(u) = \frac{|M(u)| + k - 1}{k - |H(u)|}$ with $M(u)$ is the number of elements $i$ of $u_i$ so that $\phi(u)_i = 1$, and $H(u)$ is the number of elements of $u_i$ so that $\phi(u)_i > 1$.

\begin{proposition}
	$\R = \phi(\U'')$ and is a finite representative set for $L^{top-k}$.
\end{proposition}
\begin{proof}
	
\end{proof}

\begin{proposition}
	$\R := \left\{ \frac{|M| + k -1}{k - |H|} \mathbf{1}_H + \mathbf{1}_M : H, M \subset [n], H\cap M = \emptyset, |H| + |M| \leq k \right\}$ is representative for $L^{top-k}$.
\end{proposition}
\begin{proof}
	Since $\U''$ is representative, we simply have to show there is a function $\phi : \U'' \to \R$ so that $u \in \U''$ minimizes $L^{top-k}$ implies $\phi(u)$ minimizes $L^{top-k}$.
	In particular, take the sets $H(u) = \{i \mid u_i > 1\}$ and $M(u) = \{i \mid u_i \in (0,1]\}$.
	Observe that for $H(\U'')$ and $M(\U'')$ have finite domains, and for all $u \in \U''$, we have $H(u) \cap M(u) = \emptyset$ and are subsets of $[n]$ by construction, and their sums $|H(u)| + |M(u)| \leq k$ by the restriction of $\U''$ that $\|u\|_0 \leq k$.
	Moreover, we can write $\R = \phi(\U'') = \{\frac{|M(u)| + k -1}{k - |h(u)|} \mathbf{1}_{H(u)} + \mathbf{1}_{M(u)} : u \in \U''\}$.
	It is just left to show $\R$ is representative for $L^{top-k}$.
	
	Suppose $u \in \U''$ minimizes $L^{top-k}$; we want to show $\phi(u)$ also minimizes $L^{top-k}$.
	By virtue of $u \in \U''$, we know we can write the expected loss 
	\begin{align*}
	\inprod{p}{L^{top-k}(u)} &= 1 - \bar u + \inprod{p}{u + \frac 1 k \min(u,\ones)}\\
	&= 1 - \bar{\phi(u)} + \inprod{p}{u + \frac 1 k \min(\phi(u),\ones)}\\
	&\geq 1 - \bar{\phi(u)} + \inprod{p}{u + \frac 1 k \min(u,\ones)}\\
	\end{align*}
	Claims: $\bar u \leq \bar{\phi(u)}$
	$\min(u, \ones) = \min(\phi(u), \ones)$
	If $u$ minimizes $L(u;p)$, then $\inprod{p}{u} \geq \inprod{p}{\phi(u)}$.
\end{proof}


\section{Property elicited by $L^{top-k}$}
Consider that we have $\R$ as above, but we are not sure when to ``bump'' reports up or down from one group to another.
There is sometimes a benefit in expected loss to artificially bumping up from the ``low'' to ``middle'' bin when possible because doing so cranks up the constant on the ``high'' reports, yielding better expected loss.

Note that ``bumping down'' does not happen because of how the original bins are set, but we can ''bump up'' from low to medium or medium to high.

Given $p \in \simplex$, we have the original bins
\begin{align*}
\bar H(p) &:= \{i \mid p_i \geq \frac 1 k\}\\
\bar M(p) &:= \{i \mid p_i \in [\frac 1 {k+1}, \frac 1 k]\} \\
\bar L(p) &:= \{i \mid p_i \leq \frac 1 {k+1}\}~,~
\end{align*}
dropping $p$ from the notation where it can be understood from context.

Now we want to modify this bins so that we can ``bump'' reports up to a higher bin if beneficial.
One can verify that it is never beneficial to bump from middle to high, so we need only concern ourselves with bumping from $L$ to $M$.
To understand when to bump $j$ from lower to middle, taking $u \to u'$, we can ask when we have
\begin{align*}
\inprod{p}{L(u)} &\geq \inprod{p}{L(u')}\\
\sum_{i \in \bar H} \frac{|\bar M| + k -1}{k-|\bar H|} - p_i \left(\frac{k(|\bar M| + k -1)+ 1}{k(k- |\bar H|)}\right) + \sum_{i \in \bar M} \left(\frac 1 k - p_i \left( \frac {k+1}{k}\right)\right) &\geq \sum_{i \in \bar H} \frac{|\bar M| + k }{k-|\bar H|} - p_i \left(\frac{k(|\bar M| + k)+ 1}{k(k- |\bar H|)}\right) + \sum_{i \in \bar M'} \left(\frac 1 k - p_i \left( \frac {k+1}{k}\right)\right)\\
\frac{|\bar H|(|\bar M| + k - 1)}{k(k- |\bar H|)} + p_H \frac{-k(|\bar M| + k - 1) - 1}{k(k - |\bar H|)} &\geq \frac{|\bar H|(|\bar M| + k)}{k(k - |\bar H)} + p_H \left(\frac{-k(|\bar M| + k)-1}{k (k - |\bar H|)}\right) + \left(\frac 1 k - p_j \left(\frac {k+1}{k}\right)\right) \\
\frac{-|\bar H|}{k (k - |\bar H|)} + \frac{p_H}{k - |\bar H|} &\geq \frac 1 k - p_j \left(\frac{k+1}{k}\right)~,~
\end{align*}
where $p_H := \sum_{i \in \bar H} p_i$.
Thus, we can take the set 
\begin{align}
B(p) &:= \left\{i : p_i \leq \frac{1}{k+1} \wedge \frac{p_H k- |\bar H|}{k(k- |\bar H|)} \geq \frac 1 k - p_i\left(\frac{k+1}{k}\right) \right\}
\end{align}

Now take the sets
\begin{align*}
H(p) &:= \bar H(p) \\
M(p) &:= \bar H(p) \cup B(p) \\
L(p) &:= \bar L(p) \setminus B(p)\\
\end{align*}
and take the function 
\begin{align}
\phi(p) := \frac{|M(p)| + k - 1}{k-|H(p)|} \ones_{H(p)} + \ones_{M(p)}~.~
\end{align}
\jessie{Fix with algorithm}


\begin{proposition}
	For all $p \in \simplex$, we have $\phi(p) \in \Gamma(p) \cap \R$.
\end{proposition}
\begin{proof}
	To see $\phi(p) \in \R$ by construction. 
\end{proof}


\begin{corollary}
	$\phi(\simplex)$ is a finite representative set.
\end{corollary}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
