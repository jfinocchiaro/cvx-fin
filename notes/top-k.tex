\documentclass[12pt]{article}
\PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage{../neurips-19/neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks  %[implicit=false, bookmarks=false]
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\definecolor{darkblue}{rgb}{0.0,0.0,0.2}
\hypersetup{colorlinks,breaklinks,
            linkcolor=darkblue,urlcolor=darkblue,
            anchorcolor=darkblue,citecolor=darkblue}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\usetikzlibrary{calc}
\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\mathbf{1}\{#1\}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}


\title{Consistent Polyhedral Surrogates via Embedding}
%\title{Convex Surrogates via Polyhedral Losses}
\author{%
 Jessica Finocchiaro\raf{Jessie?} \\
 \texttt{jessica.finocchiaro@colorado.edu}\\
 CU Boulder
 \And
 Rafael Frongillo\\
 \texttt{raf@colorado.edu}\\
 CU Boulder
 \And
 Bo\\
 \texttt{bwag@colorado.edu}\\
 MSFT
}

\begin{document}

\section{Top-$k$ surrogate}
Throughout this section, consider the surrogate and discrete loss $L^{top-k}(u)_y~=~\left(\frac{1}{k} \sum_{i=1}^k (u + \ones - e_y)_{[i]} - u_y \right)_+$.

First, we want to write $L^{top-k}$ in a linear form with the $\max(\cdot, 0)$ operation.
Then, using this linear form of the expected loss, we are able to evaluate when certain report values are optimal for the expected loss as a function of $p$.

\subsection{Notation and Assumptions}
\subsubsection{Notation}
Let $\bar{u} = \frac 1 k \sum_{i = 1}^k u_i$ and similarly, $\bar{u}_{-y} = \frac{1}{k-1} \sum_{i=1, i \neq y}^k u_i$ be the averages of the first $k$ elements of $u$ and the average of the first $k$ elements of $u$ besides $y$, respectively.
Moreover, we say $b = \argmax_i \{u_i : u_i > 1\}$.

We use $\odot$ to denote the Hadamard (element-wise) vector product.
Also, $\min(\vec u, \vec v)$ is the element-wise minimum of vectors $\vec u, \vec v \in \reals^d$.

\subsubsection{Assumptions}
Throughout, we make two assumptions for ease of exposition, but both follow without loss of generality for an optimal report.
First, assume that $u$ is ordered, where $u_1 \geq u_2 \geq \ldots \geq u_n$.
This is without loss of generality, as we can simply reorder $p$ if this is not the case.

Second, we will say $u_{k+1} = \ldots = u_{n-1} = u_n = 0$.
This partially follows from the invariance of $L^{top-k}$ in the $\ones$ direction.
Moreover, for $i > k+1$, this $u_i$ can never affect the $\bar{u}$ term that is added, and can only be subtracted in the case that $i = y$.
Thus, we minimize expected loss by pulling $u_{k+2},\ldots, u_n$ up to $u_{k+1}$.
We then set these terms to $0$ to reach our assumption.

Observe that, for a minimizer of $L^{top-k}$, we can assume there is no $u_i$ such that $u_i > \bar u_{-i} +1$, since reassigning such a $u'_i := \bar u_{-i} +1$ results in $L^{top-k}(u')_i = L^{top-k}(u)_i = 0$, but reduces $L^{top-k}(\cdot)_j$ for all $j \neq i$ by decreasing $\ubar$.

To see this, consider $u_y = \bar u_{-y} + 1 + \epsilon$ and $u'_y = \bar u_{-y} + 1$, with $u_i = u'_i \; \forall i \neq y$. 
\begin{align*}
L^{top-k}(u)_y &= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} + \epsilon\right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + \epsilon + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} + \epsilon\right) \right)_+\\
&= \left(-\bar u_{-y} - \epsilon + \bar u_{-y} + \frac \epsilon k \right)_+ \\
&= (-\frac{k-1}{k}\epsilon)_+\\
&= 0
\end{align*}
\begin{align*}
L^{top-k}(u')_y &= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left(\sum_{i=1, i \neq y}^k u_i + \bar u_{-y} \right) \right)_+\\ 
&= \left(1 - (\bar u_{-y} + 1) + \frac 1 k \left((k-1) \bar u_{-y} + \bar u_{-y} \right) \right)_+\\
&= \left(-\bar u_{-y} + \bar u_{-y}\right)_+ \\
&= 0
\end{align*}

Now, for any outcome $i \neq y$, we know that $\bar u_{-i} > \bar u_{-i}$ because of $u_y$.
Since $u_y$ is only added in $L^{top-k}(\cdot)_i$ as part of the $\bar u_{-i}$ term, we can see that $L^{top-k}(u)_i \geq L^{top-k}(u')_i$ for all $i \neq y$.

\section{Re-writing the loss}

First, observe that we can re-write
\begin{align*} 
L^{top-k}(u)_y&=\left(\frac{1}{k} \sum_{i=1}^k (u + \ones - e_y)_{[i]} - u_y \right)_+\\ 
&=\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - e_y)_{[i]} \right)_+\\
&=\left(1 - u_y + \frac{1}{k} \sum_{i=1}^k (u - \min(u_y, 1)\Ind{y \leq k})_{i} \right)_+
\end{align*}
\jessie{Do we need the indicator given the assumptions...?}

Consider that the loss value is $1 + \bar u$ if $y > k$, since $u_y = 0$ in this case, so the loss is nonnegative even before applying the $(\cdot)_+$ operator.
If $y \leq k$ and $u_y \in [0,1]$, then the loss is nonnegative before applying $(\cdot)_+$ as $1-u_y \geq 0$, as is $\bar u$.
Now if $u_y \in (1, \bar u_{-y} + 1]$, then the term inside $(\cdot)_+$ of $L^{top-k}(u)_y$ is equal to $\frac{1}{k-1} + 1 +  \ubar_{-y} - u_y $
Since $u_y \leq 1 +  \ubar_{-y}$, we can see this term is nonnegative.
Therefore, we can drop the $(\cdot)_+$ operator on this report space.

We can now write the expected loss as follows:
\begin{align*}
\inprod{p}{L^{top-k}(u)} &= \sum_{y=1}^n p_y L^{top-k}(u)_y \\
&= \sum_{y=1}^b p_y L^{top-k}(u)_y + \sum_{y=b+1}^k p_y L^{top-k}(u)_y + \sum_{y=k+1}^n p_y L^{top-k}(u)_y\\
&= \sum_{y=1}^b p_y (1 - \frac 1 k - u_y + \ubar) + \sum_{y=b+1}^k p_y (1 - \frac{k+1} k u_y + \ubar) + \sum_{y=k+1}^n p_y(1 + \ubar)\\
&= 1 + \ubar - \inprod{p}{u} - \inprod{p \odot \Ind{y \leq b}}{\frac 1 k \ones} - \inprod{p \odot \Ind{b+1 \leq y \leq k}}{ \frac u k}\\
&= 1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)}
\end{align*}
When considering the Bayes Risk, we use this final form of the expected loss.

\subsection{Bayes Risk}
Now we consider minimizing the expected loss given above.
Since the expected loss is linear, we can look at each outcome $y$ and when the coefficient of $u_y$ is positive and negative.
In essence, this is the subgradient in the $u_y$ direction, and so we can see when $L^{top-k}$ is increasing as a function if $u_y$ by evaluating when the sign of $u_y$'s coefficient.

Given our final form of $L^{top-k}$, the coefficient of $u_y$ is $\frac 1 k - p_y(\frac {k+1}{k})$ if $u_y \in [0,1]$, and $\frac 1 k - p_y$ if $u_y > 1$.

If $p_y \leq \frac 1 {k+1}$, this coefficient is nonnegative in both cases, so we want to set $u_y = 0$.
If $p_y \in [\frac 1 {k+1}, \frac 1 k]$, the coefficient is nonpositive in the first case, and nonnegative in the second, so we want to set $u_y = 1$.
If $p_y \geq \frac 1 k$, then the coefficient is nonpositive in both cases, so we want to set $u$ as high as permitted.
%First, consider the case where $u_y = 0$ and $u'_y = 1$, with $u_j = u'_j \;\forall i \neq j$.
%As the expected loss is linear on $u_y$, and these are the boundary cases, we know that $u$ is optimal on one of these two reports in this case.
%
%So we ask when is $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$?
%\begin{align*}
%1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq 1 + \ubar' - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%- \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq \frac {1}{k} - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%0 &\leq \frac{1}{k} - p_y(\frac{k+1}{k})\\
%p_y &\leq \frac{1}{k+1}
%\end{align*}
%
%Conversely, if $p_y > \frac{1}{k+1}$, it is to better to report $u_y = 1$ than $u_y = 0$.
%However, since we are not restricted to the unit hypercube, we might ask when it is optimal to bring $u_y = 1$ up to $u'_y = 1 + \epsilon$ given $\epsilon > 0$.
%Here, we consider when $\inprod{p}{L^{top-k}(u)} \leq \inprod{p}{L^{top-k}(u')}$.
%\begin{align*}
%1 + \ubar - \inprod{p}{u + \frac 1 k \min(u, \ones)} &\leq 1 + \ubar' - \inprod{p}{u' + \frac 1 k \min(u', \ones)}\\
%\frac{1}{k} - p_y (\frac{k+1}{k}) &\leq \frac{1+\epsilon}{k} - p_y (\frac{k+1}{k}(1 + \epsilon))\\
%p_y &\leq \frac{1}{k}
%\end{align*}
%In this case, $u_y=1$ is preferable to $u+y >1$.
%If $p_y > \frac 1 k$, then we want to push $u_y$ as high as we can, but recall that we restrict to $u_y \leq \ubar_{-y} + 1$.
%Because the upper bound on $u_y$ depends on $\ubar$, we can't determine $u$ iteratively.


However, we can construct $u$ as given below.
\begin{conjecture}
	Fix $p \in \simplex$.
	Let $g_t = |\{p_i : p_i \geq \frac 1 k\}|$ and $g_s = | \{p_i \in (\frac{1}{k+1}, \frac{1}{k})\}|$.
	The property \[\gamma(p) = 
	\left[
	\begin{cases}
	0 & p_i \leq \frac {1}{k+1}\\
	1 & p_i \in (\frac{1}{k+1}, \frac{1}{k})\\
	\frac{g_s + k-1}{k-g_t} & o/w
	\end{cases}
	\right]_{i=1}^n
	\] refines the property $\prop{L^{top-k}}$.
\end{conjecture}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
