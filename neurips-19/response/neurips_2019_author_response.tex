\documentclass{article}

\usepackage{neurips_2019_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools, amsmath, amssymb, amsthm, graphicx, verbatim}
\usepackage{xcolor}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\newcommand{\btw}[1]{}%TURN OFF FOR NOW \mytodo{gray}{#1}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}
\newcommand{\dom}{\mathrm{dom}}

\newcommand{\prop}[1]{\mathrm{prop}[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\cell}{\mathrm{cell}}

\newcommand{\abstain}[1]{\mathrm{abstain}_{#1}}
\newcommand{\mode}{\mathrm{mode}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\C}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\newcommand{\risk}[1]{\underline{#1}}
\newcommand{\inprod}[2]{\langle #1, #2 \rangle}%\mathrm{int}(#1)}
\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\indopp}{\bar{\mathbbm{1}}}
\newcommand{\ones}{\mathbbm{1}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\newcommand{\Ind}[1]{\ones\{#1\}}

\newcommand{\hinge}{L_{\mathrm{hinge}}}
\newcommand{\ellzo}{\ell_{\text{0-1}}}
\newcommand{\ellabs}[1]{\ell_{#1}}
\newcommand{\elltopk}{\ell^{\text{top-$k$}}}
\newcommand{\elltop}[1]{\ell^{\text{top-$#1$}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

\begin{document}
We thank all reviewers for their constructive feedback. 
Two of the reviewers suggested additional intuition and explanation for definitions and terminology (report, property, elicits, etc) which we will address.
We would also like to mention that we found and fixed an error in the top-k analysis; see response to R3.
Individual responses follow.

\textbf{R1:}
Thank you for your comments.
As we state above, we will clarify terms like \emph{reports} and \emph{properties}.

\textbf{R2:}

Thank you for your comments and questions.
We respond to your individual points below, but to address an overall theme, we would like to emphasize that this is a theory paper.
Our goal is to provide a general theoretical framework which allows practitioners to design new surrogates for new settings without having to do so entirely from scratch.
For example, through our framework there is guaranteed to be a link function which gives consistency, and in most cases the proof is constructive enough to derive it directly, as we illustrate with the abstain surrogate.
We do not, however, seek to create a new learning problem; when practitioners have a reason to study a new problem, they can apply our framework to understand their problem better.

1. We feel that the best motivation for polyhedral losses is to enumerate the many examples which appear already in the literature (hinge, top-k, abstain, Lov\'asz hinge, etc), rather than come up with new settings which may or may not be of practical interest.
Another motivation is the close connection with loss embedding, which is a natural approach to designing convex surrogates (of any kind).

2. We deliberately chose not to focus on any specific setting, to emphasize the generality of our framework.
This choice does make the paper more abstract, so we adopted several running examples (hinge, abstain) to illustrate the results; we will look for more places to add such illustration.
Finally, in some sense, our results do deepen understanding of specific settings.
For example, we give new intuition for a proposed surrogate for the top-k classification problem, and the Lov\'asz hinge: why it is not consistent, and more interestingly, for what problem it \emph{is} consistent.

%Additionally, there is a natural connection between polyhedral losses and classification-like problems since polyhedral losses have a finite set of minimizers over almost all underlying distributions $p(y|x)$.
%For example, in expectation, Hinge Loss on a binary classification problem only has minimizers at either $1$ or $-1$, unless the underlying distribution is $p(y|x) = (0.5,0.5)$ or is a point mass.

3. As mentioned above, we will provide more intuition for these definitions.

4. While we do not invent new learning settings, as justified above, our work does indeed provide new results for specific settings, such as for top-k and the Lov\'asz hinge.  For the latter, it was previously an open question if the Lov\'asz hinge was a consistent surrogate for any of the broad array of settings it encompasses aside from Hamming loss -- we show that in fact it is not consistent for any of these settings.

5. We interpreted your comment to mean Bayesian regret (please correct us in the final review if we are mistaken).
We expect that one can prove a general form for such regret bounds, depending on certain parameters of the polyhedral loss such as the maximum gradient and the minimum distance (in some sense) between embedded points.
Given how complex the analysis is to establish consistency, we have left the challenging question of regret bounds for future work.
%One of the current assumptions of our framework is that our hypothesis class is rich enough to capture the Bayes optimal hypothesis.
%We are currently working on bounding the difference between loss values that link to a given report and the discrete loss at the linked report, but find it to be slightly outside the scope of this paper.
%We suspect that results on regret bounds will stem from future work on the link functions themselves.

\textbf{R3}:

Thank you for your comments and questions.
First, the top-k correction: The form of the discrete loss in eq.~(9) should be slightly different, though the intuition is essentially the same: there is a term for the original top-k discrete loss, plus a cardinality penalty, plus an additional term which allows one to express higher confidence in some labels than others (but still from a discrete set).
We have corrected the proof and exposition.
%The given analysis follows by modifying the surrogate so that the loss increases quickly outside the $[0,1]^n$ hypercube.
%Alternately, fixing our analysis of the surrogate of , we find that there is a more nuanced minimizer of their surrogate loss that is a function of how many outcomes have probability $\geq \frac 1 {k+1}$ and $\geq \frac{1}{k}$.
%bug that we found in our proof around the top-$k$ loss, which makes your summary still accurate, but the new discrete loss is more nuanced than the discrete loss given in the submission.
%This discovered bug does not affect our main results; it was a result of last-minute arithmetic errors \jessie{ehh... ish} while we were pushing for the submission deadline.   % Seems unnecessary -- Bo

Regarding your questions:

1. Excellent question; we will add a discussion in the paper.
The polyhedral loss given in Theorem 2 would likely not be ``computed'' per se, as the discrete loss typically depends on the number of labels $n$, and one would want a mathematical expression for the loss in terms of $n$.
This expression, which is essentially the Fenchel conjugate of a polyhedral function, follows from standard results in convex analysis~\cite[Thm 19.1, Thm 19.2]{rockafellar1997convex}.
Similarly, the link $\psi$ in Theorem 3 would be derived mathematically, which may be challenging in some cases but typically straightforward, such as the new link we give for abstain loss.

2.
The surrogate constructed in Theorem 2 is one consistent surrogate, but takes $2^k$ dimensions.
For which problems this construction is as good as one could hope (i.e., yields the lowest dimensional consistent surrogate), and for which the dimension could be significantly reduced, is a challenging open question, and the subject of our ongoing work.

\bibliographystyle{plainnat}
\bibliography{diss,extra}
\end{document}
