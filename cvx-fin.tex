\documentclass[11pt]{colt2019}
\usepackage[utf8]{inputenc}
\usepackage{mathtools, amsmath, amssymb, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}

\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\ones}{\mathbbm{1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}


\title{Finite Property Convex Elicitation Paper}
\author{Jessie + Raf + Bo}

\begin{document}

\maketitle

\begin{abstract}
  Convex surrogates are sweet.
  Given a loss for a classification-like problem, there are two natural approaches to design convex surrogates.
  First, one may attempt to map each prediction to a low-dimensional vector, and try to find a convex loss in that space with the right calibration.
  Second, one may simply try to find a surrogate within the class of piecewise-linear convex, or polyhedral, losses.
  We show an equivalence between these two approaches, and \raf{more stuff}.
  We show that every loss with a finite number of predictions has a convex surrogate in the above sense using one fewer dimension that the number of outcomes, and give a full characterization of the losses needing only $d$ dimensions for such a surrogate.
  We then apply this characterization to show novel lower bounds for abstain loss, demonstrating the power of our techniques over \raf{check this} alternatives such as feasible subspace dimension.
\end{abstract}

\section{Setting and Background}

\subsection{Notation}
\begin{itemize}
  \item $\Y$ is the finite outcome space, and $n := |\Y|$.
  \item $\simplex\subseteq\reals^\Y$ is the set of distributions over $\Y$, thought of as vectors of probabilities.
  \item For an outcome $y$, we denote $p_y$ as the probability of outcome $y$ being observed in nature.
  \item If $\gamma = \prop{\ell}$, then $\gamma:\simplex \toto \R$ is the property elicited by the loss $\ell: \R \to \reals^\Y$.
  \item $d$ is the dimension of the embedding space
  \item $L:\reals^d \to \reals^\Y_+$ is a loss taking a report $u \in \reals^d$ and mapping the loss over each outcome in $\Y$. \raf{Changed to $\reals^\Y_+$ below.}
  \item $\Gamma: \simplex \toto \reals^d$ is the set-valued property mapping a probability distribution to a report $u \in \reals^d$.
  \item $\Gamma_u = \{ p \in \simplex : u \in \Gamma(p) \}$
  \item $\psi$ is the link function (see below)
  \item $\varphi:\R \to \reals^d$ is the embedding function
  \item $\trim(\Gamma)$ and $\strip(\Gamma)$
  \item $\eliccvx$, $\elicembed$, and $\elicpoly$
  \item $V$ and $B(P)$ as we get ready for later.
\end{itemize}

\subsection{Properties and Losses}
\begin{definition}
  Property $\Gamma:\simplex\toto\R$

  Level set $\Gamma_r$

\raf{Should bake non-degenerate into the definition: insist that $\Gamma(p) \neq \emptyset$ for all $p$.}
  
  Redundant, finite \raf{I think to avoid a TON of extra mentions of ``non-redundant'' we should take finite to mean finite and non-redundant}
\end{definition}

\begin{definition}
  \raf{I realized that (a) it is common to restrict losses to be non-negative, and (b) it solves some annoying issues where we have to say ``if $L$ is bounded below...'' nicely.  So unless any objections, let's say $L \geq 0$.}
  
  A loss $L:\R\to\reals^\Y_+$, elicits a property $\Gamma:\simplex \toto \R$ if, for all $p \in \simplex$ and $r \in \Gamma(p)$, we observe
  \begin{equation}
   = \argmin_r \E_{Y\sim p}[L(r,Y)]
  \end{equation}
  \jessie{Do we want regular elicitation vs essentially elicits...}
\end{definition}

\begin{definition}
  Polyhedral function, loss
\end{definition}

\subsection{Link Functions}

\begin{definition}
  Let properties $\gamma:\simplex\toto\R$ and $\Gamma:\simplex\toto\R'$ be given.
  A \emph{link function} is a map $\psi:\R'\to\R$.
  We say that a link $\psi$ is:
  \begin{itemize}
  \item \emph{Calibrated} if for all $p\in\simplex$, $r'\in \Gamma(p) \implies \psi(r') \in \gamma(p)$; in other words, if for all $r'\in\R'$, $\Gamma_{r'} \subseteq \gamma_{\psi(r')}$;
  \item \emph{Separated}, with respect to a loss $L$ eliciting $\Gamma$, if for all $p \in \simplex$, 
  \begin{align*}
  \inf_{u \in \R'; \psi(u) \not \in \gamma(p)} \E_{Y\sim p}[L(u, Y)] > \inf_{u \in \R'}\E_{Y\sim p}[L(u, Y)]~.
  \end{align*}
  \item \emph{Exhaustive} if for all $p\in\simplex$, $\gamma(p) = \psi(\Gamma(p)) := \{\psi(r') : r'\in\Gamma(p)\}$.
  \end{itemize}
\end{definition}

It is also interesting to contrast the definitions above with the following.
\begin{definition}
  A \emph{strong link} is a set-valued function $\psi:\R'\toto\R$ such that for all $p\in\simplex$ and $r'\in\R'$, we have $r'\in\Gamma(p) \implies \gamma(p) = \psi(r')$.
\end{definition}

\raf{We should note here that a discussion like this has not yet appeared in the property elicitation literature, since most papers address direct elicitation, where no link is needed, or single-valued properties in the case of indirect elicitation.  Cite examples for indirect: Frongillo--Kash (NIPS), Fissler--Ziegel (AoS), mode paper, others?}

\raf{Discussion about these definitions using 0-1 loss, hinge, and logistic.  Namely: sign is calibrated for hinge and logistic.  If sign is interpreted as set-valued (0 maps to $\{1,-1\}$, then it is strong for logistic.  If not, sign is not exhaustive for logistic.  Sign is exhaustive for hinge.  It is also separated, but something like $\psi(u) =
  \begin{cases}
    -1 & u \leq -1\\
    1 & u > -1
  \end{cases}$
  would not be, even though it would be calibrated.}


\subsection{Elicitation Complexity}

\begin{definition}
  A loss \emph{indirectly elicits} a property $\gamma$ if it elicits a property with a calibrated link to $\gamma$.
\end{definition}

\raf{Notation up for discussion}
$\eliccvx$ and $\elicpoly$; forward reference $\elicembed$

\subsection{Important Examples}
\raf{Mode, abstain, others?  Mention what is known, especially $\elicpoly($abstain$_{1/2}) \leq \log_2 n$.}

\section{Polyhedral Losses and Embeddings}

\begin{definition}
  A property $\Gamma : \simplex \toto \reals^d$ \emph{embeds} a property $\gamma : \simplex \toto \R$ if there exists some injective embdedding $\varphi:\R\to\reals^d$ such that for all $p\in\simplex,r\in\R$ we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
  Similarly, we say a loss $L:\reals^d\to\reals^\Y$ embeds $\gamma$ if $\prop{L}$ embeds $\gamma$.
  Finally, we say $\gamma$ is \emph{$d$-embeddable}, or just \emph{embeddable}, if such an $L$ exists and is convex.
\end{definition}

\begin{definition}\label{def:trim}
  Let $\Gamma:\simplex \toto\R'$ be an elicitable property.
  We define $\trim(\Gamma)$ as the set of maximal \botodo{suggest ``maximal'' instead of ``full-dimensional''-- changed} level sets of $\Gamma$.
  Formally, $\trim(\Gamma) = \{\Gamma_u : u \in \R'\} \setminus \{ \Gamma_u : \exists w \in \R' \text{ s.t. } \Gamma_u \subset \Gamma_w \}$.
\end{definition}
\raf{Explain why this is a useful concept in general, and for convex elic}  
Looking at the $\trim$ of a property allows us to relate the finite properties we are interested in and the subtlely different surrogate properties that we actually elicit when we embed the original property into $\reals^d$.
In the surrogate property, there is often an infinite set of ``hidden'' level sets corresponding to the reports in the convex hull of embedded points.
Taking the $\trim$ thus allows us to remove labels so that we can relate the optimal sets of original reports and their embedded points, and then we remove these ``hidden'' level sets that are only optimal when we are indifferent between two of the embedded points.

\begin{proposition}\label{prop:embed-trim}
  Let $\Gamma:\simplex\toto\reals^d$ be an elicitable property.
  The following are equivalent:
  \begin{enumerate}
  \item $\Gamma$ embeds a finite elicitable property $\gamma:\simplex \toto \R$.
  \item $\trim(\Gamma)$ is a finite set.
  \item There is a finite set of maximal level sets $\Theta$ of $\Gamma$ that union to $\simplex$.
  \end{enumerate}
  Moreover, when any of the above hold, $\{\gamma_r : r\in\R\} = \trim(\Gamma) = \Theta$.
\end{proposition}
\begin{proof}
  For $1 \implies 2$, since $\Gamma$ embeds the finite property $\gamma$, we show that $\trim(\Gamma) = \{\Gamma_u : u \in \varphi(\R) \} = \{\gamma_r : r \in \R\}$.
  (Note that the second equality follows from the definition of embedding.)
  Clearly, $\trim(\Gamma) \supseteq \{\gamma_r : r \in \R\}$ as we are simply restricting the number of level sets we are indexing over.
  Now, consider some $\theta \in \trim(\Gamma)$ and the report $u$ so that $\theta = \Gamma_u$.
  If $u \in \varphi(\R)$, then clearly we are done.
  If $u \not\in \varphi(\R)$ but there is some report $w \in \varphi(\R)$ so that $\Gamma_u = \Gamma_w$, then we are again done since the level sets composing $\trim$ are unlabeled.
  If there is no such $w \in \R$, then we claim $\Gamma_u \not \in \trim(\Gamma)$.
  If $\Gamma_u$ is maximal, it must be unique up to relabeling (see Lemma~\ref{lem:trim-subsets-not-maximal} in the Appendix)\jessiet{skimmed over this, but follows from the convexity of the scoring rule.  My thought for the paper is to put that proof in the appendix,and just have a footnote or side note here with a reference to the proof in the appendix.}, giving us one of the first two cases or a contradiction of elicitability.
  If $\Gamma_u$ is not maximal, it must be a subset of some maximal level set\jessiet{also in my notes... going to suggest the same footnote/appendix proof- like the other note.}, and therefore not in $\trim(\Gamma)$.
  Thus, we conclude $\trim(\Gamma) \subseteq \{\gamma_r : r \in \R\}$ and since the sets are equal, can conclude that if $\Gamma$ embeds the finite property $\gamma$, then $\trim(\Gamma)$ is finite.
  
  For $2 \implies 3$, since each level set $\theta \in \trim(\Gamma)$ is maximal, suppose for now that $\Theta := \trim(\Gamma)$.
  %As $\Gamma$ is nondegenerate, for every $p \in \simplex$, there must be some report $u$ corresponding to the level set such that $p \in \Gamma_u$.
  Because $\Gamma$ is nondegenerate, for all $p \in \simplex$, there is a level set $\theta \in \trim(\Gamma)$ such that $p \in \theta$, since the level sets of elicitable properties are closed and convex, and only non-maximal level sets are removed in taking $\trim(\Gamma)$.
  If, for some $p\in \simplex$, the level set $\Gamma_u$ corresponding to $p$ was removed, it would be because there is a level set corresponding to $w \in \reals^d$ such that $\Gamma_u \subset \Gamma_w$, but then $p \in \Gamma_w$.
  
  For $3 \implies 1$, take the set $\R := \{u : \Gamma_u \in\trim(\Gamma)\}$.
  (Take only one such report per level set.)
  The set $\R$ is then finite, and taking $\varphi$ to be the identity, we observe that $\Gamma$ embeds $\gamma: p \mapsto \Gamma(p) \cap \R$.
  We can then see $\gamma$ is elicitable by the loss $L|_{\R}$, where $L$ is the convex surrogate eliciting $\Gamma$.
\end{proof}

% \begin{proof}
% 	We proceed in a cycle to show $1 \implies 2 \implies 3 \implies 1 $.
% 	\begin{enumerate}
% 		\item [$1 \implies 2$]
% 		We know $\strip(\gamma) = \trim(\Gamma)$ (since $\varphi$ is an injection, and there is some intermediate property $\Gamma'$ such that $\Gamma' \preceq \Gamma$ and with a bijection $\phi:\Gamma'(p) \mapsto \gamma(p)$, so $\strip(\Gamma') = \strip(\gamma)$.) 
% 		As $\gamma$ is finite, so is $\strip(\gamma)$ since strip is generated from the (finite) report set.
		
% 		\item [$2 \implies 3$]
% 		Since $\Gamma$ is nondegenerate and every $\theta \in \trim(\Gamma)$ is full dimensional, it is clear that the union of the level sets of $\trim(\Gamma)$ union to $\simplex$ by nondegneracy of $\Gamma$.
		
% 		\item[$3 \implies 1$]
% 		Construct $\R := \{[u] \in \Gamma(p) : p \in \inter{\theta} \}_{\theta \in \Theta}$.
% 		The property $\gamma: p \mapsto \Gamma(p) \cap\R$ is then finite as $\R$ is finite.
% 		Taking $\varphi$ to be the identity, $\Gamma$ embeds this finite $\gamma$.
% 	\end{enumerate} 
% \end{proof}

\begin{proposition}\label{prop:embed-link}
  If $\Gamma$ embeds a finite property $\gamma$, then it has a calibrated link $\psi$ to $\gamma$.
  Moreover, if a polyhedral loss $L$ elicits $\Gamma$, $\psi$ can be taken to be separated with respect to $L$.
\end{proposition}
\begin{proof}
  %\raf{I think you mean $\varphi^{-1}(u)$, but that is only defined on $\varphi(\R)$ (the embedded points).  Use Proposition~\ref{prop:embed-trim}, specifically trim.}
  Since the finite property $\gamma$ is embedded by $\Gamma$, we know that $\trim(\Gamma)$ is finite by Proposition~\ref{prop:embed-trim}, as is $\varphi(\R)$.
  Therefore, for every $u \in \reals^d$, consider two cases.
  First, if $u \in \varphi(\R)$, then $\psi(u) = \varphi^{-1}(u)$ suffices.
  Now, if $u \not\in\varphi(\R)$, then there is an equivalent report $w \in \varphi(\R)$ so that $\Gamma_u \subset \Gamma_w$.
  In such cases, construct $\psi(u) := \varphi^{-1}(w)$.
  If there is more than one equivalent report, choose one arbitrarily.
  Now, for any $p \in \simplex$ such that $u \in \Gamma(p)$, we also have that $w \in \Gamma(p)$, so $\psi(u) = \phi^{-1}(w) \in \gamma(p)$.

  For the second part of the statement, in order for $\psi$ to be separated, for $\epsilon > 0$, for each embedded point $u \in \reals^d$ and every $v \in B(\epsilon, u)$, we must observe $\psi(v) = \psi(u)$.
  If the loss $L$ is polyhedral, we want to show that we can construct a $\psi$ so that the above holds.
  
  First, consider that each of the embedded points in $u \in \varphi(\R)$ must have ball $B(\epsilon, u)$ centered on $u$ so that no other $w \in \varphi(\R)$ is in $B(\epsilon, u)$. 
  If this is not true, then \jessie{...?} \jessiet{I'm not sure how this messses with things, but I would think the level sets corresponding to the two points would differ on a set of zero measure, if at all, or both be in the $\arginf$of the loss...?}

  Now for any $u \in \varphi(\R)$ and $v \in B(\epsilon, u)$, if $\psi(v)$ does not map to $\psi(u)$, then there must be some other $w \in \varphi(\R)$ so that $\psi(v) = \psi(w)$.
  Since $L$ is polyhedral, we know that for $p \in \simplex$ such that $v \in \Gamma(p)$, then since $u$ and $v$ are sufficiently close, $\E_p L(w,Y) = \E_p L(v,Y) \leq \E_p L(u,Y)$.
  However, if the inequality was strict, then we contradict the polyhedral structure of $L$, since $v$ is not an embedded point.
  Therefore, $v \in \Gamma(p) \implies u \in \Gamma(p)$, and we can alter the link function so that $\psi(v) = \psi(u)$.
%If no such $\psi$ existed, then for some $p$ such that $u \in \arginf_{z \in \R} \E_p L(z,Y)$, we know there is a $\delta > 0$ such that $\E_p L(v,Y) - \E_p L(u,Y) < \delta$ by continuity of $L$.
%Additionally, if $\psi$ was not selective, then we know $\psi(v) = \psi(w)$ for some embedded point $w \neq u$.
%Since the losses of $u$ and $v$ are not bounded about away from each other and $\psi(v) = \psi(w)$, then the loss $L$ must be constant on $\conv(\{u,v,w\})$, otherwise we would either contradict elicitability of $\Gamma$ or the construction of the calibrated link $\psi$.
%Therefore, we can reassign $\psi(v) = \psi(u)$ for any $v\in B(\epsilon, u)$, and such a link additionally separated.
\end{proof}

\begin{theorem}
  Let $\gamma$ be a finite property.
  The following are equivalent.
  \begin{enumerate}
  \item $\gamma$ is elicitable.
  \item $\gamma$ is embeddable.
  \item $\gamma$ is embeddable via a polyhedral loss.
  \end{enumerate}
\end{theorem}
\raf{Also mention here that \emph{every} non-redundant elicitable property is $(n-1)$-embeddable.}

\begin{definition}
  Let $\Gamma:\simplex\toto\R$ and $\Gamma':\simplex\toto\R'$.
  Then $\Gamma'$ \emph{refines} $\Gamma$ if for all $r'\in\R'$ we have $\Gamma'_{r'} \subseteq \Gamma_r$ for some $r\in\R$.
  That is, the cells of $\Gamma'$ are all contained in cells of $\Gamma$.
\end{definition}

\begin{theorem}\label{thm:polyhedral-embed}
  Every polyhedral loss embeds a finite elicitable property.
  Moreover, a polyhedral loss indirectly elicits a finite elicitable property $\gamma$ if and only if it embeds a property which refines $\gamma$.
\end{theorem}

\raf{Transition: wait, what about dimension?}
\begin{definition}
  The \emph{embedding dimension} of a finite property $\gamma$, written $\elicembed(\gamma)$, is the minimum $d\geq 0$ such that $\gamma$ is $d$-embeddable.
\end{definition}

\section{Embeddings via Real-Valued Losses}

The following is a minor modification of a similar definition of~\citet{lambert2018elicitation}.
\begin{definition}
  A finite property $\gamma:\simplex\toto\R$ is \emph{orderable} if there is an enumeration $\R = \{r_1,\ldots,r_k\}$ such that for all $i\in\{1,\ldots,k-1\}$, the intersection $\gamma_{r_i} \cap \gamma_{r_{i+1}}$ is a hyperplane intersected with $\simplex$.
\end{definition}

\citet{lambert2018elicitation} shows that a finite property has a \emph{order-sensitive} loss, meaning one assigning higher loss to ``farther'' reports (with respect to a given order), if and only if it is orderable.
We will see that, in fact, a finite property is 1-embeddable if and only if it is orderable.
Both the forward direction and its converse will make use of a related definition: we say a property is \emph{monotone} if, roughly speaking, it can be defined in terms of (possibly infinitely many) hyperplanes whose normal vectors have coefficients which are monotone in the report value.

First, the forward direction.

\begin{proposition}\label{prop:orderable-embed}
  Every orderable property $\gamma$ is 1-embeddable.
\end{proposition}
\begin{proof}
  Let $\R = \{r_1,\ldots,r_k\}$ be the report space for $\gamma$.
  From Lemma~\ref{lem:prop-L-monotone}, stating that finite properties are orderable if and only if they are monotone, we have functions $a:\R\to\reals^\Y, b:\R\to\reals^\Y$ satisfying the following: (i) $\gamma_{r_i} = \{p\in\simplex : a(r_i) \cdot p \leq 0 \leq b(r_i) \cdot p\}$ for all $i\in\{1,\ldots,k\}$, and (ii) $a(r_i) \leq b(r_i) = a(r_{i+1}) \leq b(r_{i+1})$ for all $i \in \{1,\ldots,k-1\}$.
  We now define $\varphi(r_i) = i \in \reals$, and define $L$ as follows:
  \begin{equation*}
    L(u)_y =
    \begin{cases}
      a(r_1) u & u < 1 \\
      c_i + b(r_i) u & u \in [i,i+1), i\in\{1,\ldots,k\} \\
      c_k + b(r_k) u & u \geq k+1
    \end{cases}~,
  \end{equation*}
  where the $c_i$ are chosen to make $L(u)_y$ continuous, namely, $c_i = \sum_{j=1}^{i} a(r_j)_y$.
  (Recall that $b(r_i) = a(r_{i+1})$.)
  By the condition (ii) above, $L(\cdot)_y$ is convex for all $y\in\Y$, and moreover, its subgradients at $\varphi(\R) = \{1,\ldots,k\}$ are given by
  $\partial L(i)_y = [a(r_i)_y,b(r_i)_y]$.
  By condition (i), for all $i\in\{1,\ldots,k\}$ and $p\in\simplex$ we now have
  \begin{align*}
    i \in \prop{L}(p)
    &\iff 0 \in \partial (p \cdot L(i)) \\
    &\iff 0 \in \sum_{y\in\Y} p_y \partial L(i)_y \\
    &\iff 0 \in \sum_{y\in\Y} p_y [a(r_i)_y,b(r_i)_y] \\
    &\iff a(r_i) \cdot p \leq 0 \leq b(r_i) \cdot p \\
    &\iff r_i \in \gamma(p)~,
  \end{align*}
  where the sums are Minkowski sums.
\end{proof}

The converse of Proposition~\ref{prop:orderable-embed}, that every 1-embeddable property is orderable, follows from a much broader statement, given below as Proposition~\ref{prop:indirect-orderable}: finite elicitable properties which are indirectly elicited by 1-dimensional convex losses must be orderable.
Together with Proposition~\ref{prop:embed-link}, which shows that embeddings give rise to calibrated links, we have in particular that 1-embeddable finite properties are orderable.

\begin{proposition}\label{prop:indirect-orderable}
  If convex $L : \reals \to \reals^\Y$ indirectly elicits a finite elicitable property $\gamma$, then $\gamma$ is orderable.
\end{proposition}

The proof of Proposition~\ref{prop:indirect-orderable} follows from Lemma~\ref{lem:prop-L-monotone} in Appendix~\ref{app:dimension-1}, which states that the property elicited by any convex loss must be monotone in the sense above.
(The normal vectors are constructed in terms of the subgradients of the loss, thus guaranteeing monotonicity of their coefficients.)

As a corollary of Proposition~\ref{prop:indirect-orderable} and Theorem~\ref{thm:polyhedral-embed}, we now also see that polyhedral losses embed orderable properties.

\begin{corollary}\label{cor:embed-orderable}
  Every polyhedral $L : \reals \to \reals^\Y$ embeds an orderable property.
\end{corollary}

The central conclusion from this section is the following theorem, which states that several elicitation complexity classes coincide in dimension 1.
The proof is given in Appendix~\ref{app:dimension-1}, which only needs to show the equivalence of statements 1 and 5, as we have already established the equivalence of the first four statements: $1 \implies 2,3,4$ from Proposition~\ref{prop:orderable-embed}, as the proof gives a loss which is polyhedral (and convex), and Proposition~\ref{prop:indirect-orderable} shows $2,3,4 \implies 1$, as each of these cases would give a convex loss indirectly eliciting a finite elicitable property.
\begin{theorem}\label{thm:1d-tfae}
  Let $\gamma$ be a finite elicitable property.
  The following are equivalent.
  \begin{enumerate}
  \item $\gamma$ is orderable.
  \item $\elicembed(\gamma)=1$. ($\gamma$ is 1-embeddable.)
  \item $\elicpoly(\gamma)=1$. ($\gamma$ is indirectly elicitable via a polyhedral loss $L:\reals\to\reals^\Y$.)
  \item $\eliccvx(\gamma)=1$. ($\gamma$ is indirectly elicitable via a convex loss $L:\reals\to\reals^\Y$.)
  \item $\eliccts(\gamma)=1$. ($\gamma$ is indirectly elicitable via a real-valued continuous non-locally-constant property.)
  \end{enumerate}
\end{theorem}

\raf{Note the relationship to Finocchiaro 2018, and that this is nearly a corollary, as that work almost shows $5\implies 4$, but it needs slightly more assumptions than we need.  Instead, we give a direct proof.}

\section{Higher-Dimensional Embeddings}\label{sec:general-dimensions}

Orderability is a useful characterization of 1-embeddable properties, in the sense that the orderability of a property is easily tested.
We seek a similar condition for higher dimensions.
Toward this end, let us decompose orderability into two conditions which will lend themselves to generalization.
A property $\gamma$ is orderable if and only if there exist closed intervals $[a(r)_y,b(r)_y] \subseteq \reals$ for all $r\in\R, y\in\Y$, such that the following two conditions hold:
\begin{enumerate}
\item (Optimality) For all $r\in\R, p\in\simplex$, we have $r\in \gamma(p) \iff a(r)\cdot p \leq 0 \leq b(r)\cdot p$.
\item (Monotonicity) There exists an ordering $\R=\{r_1,\ldots,r_k\}$ such that for all $y\in\Y$, $i\in\{1,\ldots,k-1\}$, we have $b(r_i) = a(r_{i+1})$.
\end{enumerate}
Thinking of the intervals $[a(r)_y,b(r)_y]$ as subgradients of a convex function $L_y$, the first condition states that the report $r$ is optimal exactly when it should be, and the second states that the intervals are indeed subgradients of some convex function.

Generalizing the above to higher dimensions, we obtain the following.
\begin{theorem}
  A finite property $\gamma:\simplex\toto\R$ is $d$-embeddable if and only if there exist polytopes $T(r,y) \subseteq \reals^d$ for all $r\in\R, y\in\Y$, such that the following two conditions hold:
  \begin{enumerate}
  \item (Optimality) For all $r\in\R, p\in\simplex$, we have $r\in \gamma(p) \iff 0 \in \sum_{y\in\Y} p_y T(r,y)$, where summation is the Minkowski sum.
  \item (Monotonicity) There exists an embedding $\varphi: \R \to \reals^d$ and set of convex functions $\{L_y:\reals^d\to\reals\}_{y\in\Y}$ such that for all $y\in\Y, r\in\R$ we have $T(r,y) = \partial L_y(\varphi(r))$.
  \end{enumerate}
\end{theorem}

\bo{Added the below blurb}
\raf{I love it.  This would be a great place to discuss the relationship to FSD, which is ``even more local'' in the sense that it does not even look at a whole cell at once, just one piece of the boundary.  We can then say that the additional power is strong enough to give novel lower bounds where FSD remains silent.  (In particular, abstain looks 1d / orderable when zoomed in as far as FSD, but one can conclude it is not orderable using condition 1 alone.)}
This characterization decomposes embeddability into \emph{local} and \emph{global} necessary and sufficient conditions.
The local conditions at each embedding point turn out to imply very strong restrictions on the structure of possible polyhedral losses, regardless of any other structure in the property or where the embedding points are placed.
Meanwhile, the global condition says that, once each local embedding point's demands are satisfied, we must be able to place them all in $\reals^d$ in a coherent manner that simultaneously respects all of their requirements.

\bo{I think we should write separately the result that the expected loss function only has vertices above the embedding points (um, sort of - WLOG?). I think we've known it for so long we forgot that it's a really interesting and also a necessary and sufficient condition. Does it also imply that each individual loss function's vertices can only be at embedding points?}
\raf{This is one of those statements that is probably true in some sense but we're not sure what sense.  There could be vertices that are just never optimal, and it's hard to know exactly how to rule them out.}

\subsection{Useful necessary conditions}

Each embedding point's optimality condition turns out to imply limitations on \emph{each} of the $n$ loss functions.
This can be enough to rule out embeddability only by considering a single level set, as we develop next.

Recall that a \emph{halfspace-representation (H-rep)} of a closed polyhedral set $S$ in $d$ dimensions is a pair $(V,b)$ with $V \in \reals^{k \times d}$ and $b \in \reals^k$ such that $S = \{ w : Vw \leq b \}$.
We say an H-rep is \emph{tight} if for all $i=1,\dots,k$, there exists $w \in S$ such that $\langle V_i , w \rangle = b_i$.

\begin{definition}
  Given a polytope $\{p \in \Delta_{\Y} : Bp \geq 0\}$ for $B \in \reals^{k \times n}$, a \emph{$d$-construction} of it is a $V \in \reals^{k \times d}$, each row a unit vector, such that, for each column $B_i$ of $B$, the pair $(V,B_i)$ is a tight halfspace representation.
\end{definition}

\begin{corollary}
  If a property is $d$-embeddable, then each of its level sets has a $d$-construction.
  \bo{Is it possible that some uniqueness of the construction is also true?}
\end{corollary}
\begin{proof}
  Suppose the level set is represented as $\Gamma_r = \{p : Bp \geq 0\}$ with $B \in \reals^{k \times n}$ for the minimum $k$ (i.e. the number of facets of the polytope).
  Let $u \in \reals^d$ be its embedding point.
  The convex expected loss is minimized at $u$ if and only if it has the zero vector as a subgradient at $u$, i.e.
    \[ p \in \Gamma_r \qquad \iff \qquad 0 \in \partial \langle L(u), p \rangle . \]
  Subgradient sets satisfy the following two properties: $\partial(f_1 + f_2) = \partial f_1 + \partial f_2$ where the right side is the Minkowski sum of the subgradient sets (i.e. the set of sums of all pairs); and $\partial(\alpha f_1) = \alpha \partial f_1$.
  So
    \[ \partial \langle p , L(u) \rangle = \sum_{y \in \Y} p_y \partial L(u)_y \]
  where the right side is a Minkowski sum of subgradient sets.
  We now consider the halfspaces defining this set.
  Define the $y$th support function as $H_y(v) = \max_{w \in \partial L(u)_y} \langle w, v \rangle$ to obtain that $w \in \partial \langle p, L(u) \rangle$ if and only if, for all unit vectors $v$,
   \[ \langle v, w \rangle \leq \sum_{y \in \Y} p_y H_y(v) . \]
  In particular, the embedding point is optimal for $p$ if and only if this is satisfied for $w=0$.
  So, letting $B_i$ be the $i$th column of $B$, we have
    \[ \Gamma_r = \{p : \sum_y p_y B_y \geq 0\} = \{p : \sum_y p_y H_y(v) \geq 0 ~ (\forall v, \|v\|=1)\} . \]
  \bo{This is the part I'm still not perfectly happy with}
  These sets are equal and full-dimensional (as the property is finite and non-redundant and $B$ is minimal).
  So there exist $k$ unit vectors $v_1,\dots,v_k \in \reals^d$ such that $H_y(v_i) = B_{iy}$.
  Collected as rows of $V$, we obtain that each $(V,B_y)$ is a tight halfspace representation, as it contains $\partial L(u)_y$ and $H_y(v_i) = B_{iy}$, meaning there exists $w \in \partial L(u)_y$ such that $\langle w, v_i \rangle = B_{iy}$.
\end{proof}
This provides the following recipe for both constructing losses and proving lower bounds.
First, take any level set and represent it as $\{p : Bp \geq 0\}$.
Now if $B$ has $k$ columns, search for a collection of $k$ unit vectors in $\reals^d$.
For any candidate $V$ (whose rows are the unit vectors), a computer program can automatically verify whether the representations $\{(V,B_y)\}_{y \in \Y}$ are tight.
In particular, if any $(V,B_y)$ is infeasible (i.e. there does not exist any $w$ satisfying all constraints $\langle v_i, w \rangle \leq B_{iy}$), then $V$ is not a $d$-construction.
If $(V,B_y)$ is feasible but some constraints are redundant, i.e. it is not tight, then again we do not have a $d$-construction.

A lower bound can be proven by showing, for any level set, that no collection of $k$ unit vectors can form a $d$-construction.

If a $d$-construction $V$ is found, one also obtains that the subgradient sets $\partial L(u)_y$ must ``span'' the respective polytopes $\{w : Vw \leq B_y\}$, i.e. must be contained within them and must make each constraint tight.
Repeating this process for each level set and associated embedding point gives a relatively tight picture of what any possible loss must look like.

\begin{corollary}
  \raf{Monotonicity: WMON}
\end{corollary}

\begin{corollary}
  \raf{reduction to 1 dimension}
\end{corollary}

\section{Application: Abstain Loss}

\begin{theorem}
  Abstain with $\alpha=1/2$ and $|\Y|=5$ is not $2$-embeddable.
\end{theorem}

\begin{theorem}
  Abstain with $\alpha > 1/2$ and $|\Y|=7$ is not $2$-embeddable.
\end{theorem}

\begin{conjecture}
  Abstain with $\alpha=1/2$ is $(\log_2 |\Y|)$-embeddable, but no lower.
\end{conjecture}

\section{Conclusion and Future Work}

Let $\mathrm{elic}_{embed}$ denote the elicitation complexity with respect to embeddings, $\mathrm{elic}_{Pcvx}$ the complexity with respect to polyhedral convex losses, and $\mathrm{elic}_{cvx}$ the complexity with respect to arbitrary convex losses.

\begin{conjecture}
  $\mathrm{elic}_{embed}(\Gamma) = \mathrm{elic}_{Pcvx}(\Gamma) = \mathrm{elic}_{cvx}(\Gamma)$ for all finite, elicitable properties $\Gamma$.
\end{conjecture}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Directions}  \label{sec:conclusion}
\paragraph{Summary.}
This work is part of a broader research program to understand convex surrogates through the lens of property elicitation: the relationship between finite losses and convex surrogates, the link functions connecting them, and the properties they elicit.
We seek a general theory that, given a property, can prescribe when and how to construct convex surrogate losses that elicit it, and specifically, determine the minimum dimension required.
Even more broadly, one could replace ``convex'' by any notion of ``nice'' surrogate desired.

This work formalized the \emph{embedding} approach where labels are identified with points in $\reals^d$.
We saw \bo{in theorem ??} that this approach is tightly connected to the use of polyhedral (i.e. piecewise linear convex) loss functions.
We also saw that it is a general technique that can be used for any finite elicitable property.

We then investigated the \emph{dimensionality} of $\reals^d$ required for such embeddings, giving a characterization in terms of the structure of the property in the simplex.
This gave a complete understanding of the one-dimensional case, and a complete characterization albeit weaker understanding in higher dimensions.
%RF got here
The two key conditions are an optimality condition that relates the structure of each level set to existence of polytopes in $\reals^d$ satisfying certain conditions; and a monotonicity condition relating such polytopes for different level sets.
This yields new lower bounds in particular for the abstain loss.

\paragraph{Directions.}
There are several direct open questions involving embedding dimension.
It would be interesting and perhaps practically useful to develop further techniques for upper bounds: automatically constructing embeddings in $\reals^d$ and associated polyhedral losses from a given property, with $d$ as small as possible.
Another direction is additional lower bound techniques, or further development of our necessary conditions.

This paper also suggests an agenda of defining more nuanced classes of surrogate losses and studying their properties.
For example, a tangential topic in this work was the characteristics of a ``good'' link function; formalizing and exploring this question is an exciting direction.
We would also like to move toward a full understanding of the differences between these classes.
For example, how does embedding dimension compare in general to convex elicitation diemsnion (the dimensionality $d$ required of \emph{any} convex surrogate loss)?
These questions have both theoretical interest and potential practical significance.

\subsection*{Acknowledgements}
We thank Peter Bartlett for several discussions early on, which led to a proof of \raf{1-d reduction} among other insights.

\bibliography{diss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Dimension 1}\label{app:dimension-1}

\begin{definition}\label{def:monotone-prop}
  A property $\Gamma:\simplex\to\R$ is \emph{monotone} if there are maps $a:\R\to\reals^\Y$, $b:\R\to\reals^\Y$ and a total ordering $<$ of $\R$ such that the following two conditions hold.
  \begin{enumerate}
  \item For all $r\in\R$, we have $\Gamma_r = \{p\in\simplex : a(r) \cdot p \leq 0 \leq b(r) \cdot p\}$.
  \item For all $r < r'$, we have $a(r) \leq b(r) \leq a(r') \leq b(r')$ (component-wise).
  % \item For all $r,r'\in\R$ and $p\in\Gamma_{r'}\setminus\Gamma_r$, we have $b(r) \cdot p < 0 \implies r' > r$ and $a(r) \cdot p > 0 \implies r' < r$.
  \end{enumerate}
\end{definition}

\begin{lemma}\label{lem:orderable-monotone}
  A finite property is orderable if and only if it is monotone.
\end{lemma}
\begin{proof}
  Let $\gamma:\simplex\toto\R$ be finite and monotone.
  Then we can use the total ordering of $\R$ to write $\R = \{r_1,\ldots,r_k\}$ such that $r_i < r_{i+1}$ for all $i \in \{1,\ldots,k-1\}$.
  We now have $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : a(r_{i+1}) \cdot p \leq 0 \leq b(r_i) \cdot p\}$.
  If this intersection is empty, then there must be some $p$ with $b(r_i) \cdot p < 0$ and $a(r_{i+1}) \cdot p > 0$; by monotonicity, no earlier or later reports can be in $\gamma(p)$, so we see that $\gamma(p) = \emptyset$, a contradiction.
  Thus the intersection is nonempty, and as we also know $b(r_i) \leq a(r_{i+1})$ we conclude $b(r_i) = a(r_{i+1})$, and the intersection is the hyperplane defined by $b(r_i) = a(r_{i+1})$.

  For the converse, let $\gamma:\simplex\toto\R$ be finite and orderable.
  From~\cite[Theorem 4]{lambert2018elicitation}, we have positively-oriented normals $v_i\in\reals^\Y$ for all $i \in \{1,\ldots,k-1\}$ such that $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : v_i\cdot p = 0\}$, and moreover, for all $i \in \{2,\ldots,k-1\}$, we have $\gamma_{r_i} = \{p\in\simplex : v_{i-1} \cdot p \leq 0 \leq v_i \cdot p\}$, while $\gamma_{r_1} = \{p\in\simplex : 0 \leq v_1 \cdot p\}$ and $\gamma_{r_k} = \{p\in\simplex : v_{k-1} \cdot p \leq 0\}$.
  From the positive orientation of the $v_i$, we have for all $p\in\simplex$ that $\sgn(v_i\cdot p)$ is monotone in $i$.
  In particular, it must be that for all $y$, $\sgn((v_i)_y)$ is monotone in $i$, taking the distribution with all weight on outcome $y$.
  % (Similarly, if $\gamma(\ones_y) = \{r_j,r_{j+1}\}$, then $(v_i)_y = v_i \cdot \ones_y < 0$ for $i < j$, $(v_j)_y = 0$, and $(v_i)_y > 0$ for $i > j$.)
  \raf{This observation could use a better proof.}
  
  For all $i\in\{2,\ldots,k-1\}$, we wish to find $\alpha_i \geq 0$ such that $v_{i-1} \leq \alpha_i v_i$ (component-wise).
  To this end, fix $i$ and let $v = v_{i-1}, w = v_{i}, \alpha=\alpha_i$.
  By the above, there is no $y\in\Y$ with $v_y > 0 > w_y$, so the condition $v \leq \alpha w$ is equivalent to the following:
  \begin{enumerate}
  \item[(i)] For all $y$ with $v_y,w_y < 0$, $\alpha \leq v_y/w_y$.
  \item[(ii)] For all $y$ with $v_y,w_y > 0$, $\alpha \geq v_y/w_y$.
  \end{enumerate}
  Defining $\Y^- = \{y\in\Y: v_i,w_y < 0\}, \Y^+ = \{y\in\Y: v_i,w_y > 0\}$, these conditions are in turn equivalent to $\min_{y\in\Y^-} v_y/w_y \geq \max_{y\in\Y^+} v_y/w_y$, with the usual conventions $\min \emptyset = \infty,\; \max \emptyset = -\infty$.
  Suppose this inequality were not satisfied.
  Then we would have $y \in \Y^-,y'\in\Y^+$ such that $0 < v_y/w_y < w_{y'}/v_y$, which would in turn imply $|v_y|/v_{y'} < |w_y| / w_{y'}$.
  Letting $c = \tfrac 1 2 \left(|w_y| / w_{y'} + |v_y|/v_{y'}\right)$ and taking $p$ to be the distribution with weight $1/(1+c)$ on $y$ and $c/(1+c)$ on $y'$, we see that
  \begin{align*}
    v \cdot p &= \frac 1 {1+c} \left(v_y + \tfrac 1 2 (|w_y| / w_{y'} + |v_y|/v_{y'})v_{y'}\right) > \frac 1 {1+c} \left(v_y + (|v_y|/v_{y'})v_{y'}\right) = 0
    \\
    w \cdot p &= \frac 1 {1+c} \left(w_y + \tfrac 1 2 (|w_y| / w_{y'} + |v_y|/v_{y'})v_{y'}\right) < \frac 1 {1+c} \left(w_y + (|w_y|/w_{y'})w_{y'}\right) = 0~,
  \end{align*}
  thus violating the observation that $\sgn(v_i\cdot p)$ is monotone in $i$ (recall that $v = v_{i-1}, w = v_{i}$).

  We can now construct the $a,b$ in Definition~\ref{def:monotone-prop}.
  Letting $\alpha_i = \tfrac 1 2 \left(\min_{y\in\Y^-(i)} (v_{i-1})_y/(v_{i})_y + \max_{y\in\Y^+(i)} (v_{i-1})_y/(v_{i})_y\right)$, the preceding argument shows that $v_{i-1} \leq \alpha_i v_{i}$ for all $i \in \{2,\ldots,k-1\}$.
  We now set $b(r_i) = a(r_{i+1}) = (\prod_{j=2}^i \alpha_j) v_i$ for $i\in\{1,\ldots,k-1\}$, as well as $a(r_1) = -\max_{y\in\Y} |(v_1)_y|\ones$ and $b(r_k) = \max_{y\in\Y} |a(r_k)_y|\ones$, where $\ones\in\reals^\Y$ denotes the all-ones vector.

  The above establishes the second condition of monotone properties in Definition~\ref{def:monotone-prop}.
  To see the first condition, note that we have already established it for $i\in\{2,\ldots,k-1\}$.
  For $i=1,k$, we merely observe that $a(r_1) \cdot p \leq 0$ and $b(r_k) \cdot p \geq 0$ for all $p\in\simplex$.
\end{proof}

\begin{lemma}\label{lem:prop-L-monotone}
  For any convex loss $L : \reals \to \reals^\Y$, $\prop{L}$ is monotone.
\end{lemma}
\begin{proof}
  Let $a,b$ be defined by $a(r)_y = \partial_- L(r)_y$ and $b(r) = \partial_+ L(r)_y$, that is, the left and right derivatives of $L(\cdot)_y$ at $r$, respectively.
  Then $\partial L(r)_y = [a(r)_y,b(r)_y]$.
  We now have $r \in \prop{L}(p) \iff 0 \in \partial p\cdot L(r) \iff a(r)\cdot p \leq 0 \leq b(r) \cdot p$, showing the first condition.
  The second condition follows as the subgradients of $L$ are monotone functions.
  \raf{More detail and some Rockafellar cites}
\end{proof}

\raf{NOTE: for both these results, it should just boil down to a few lines using the convexity and closure of $\gamma_r$ for all $r\in\R$, together with the guarantees of $\psi$.}

We now prove the remaining results.
%Proposition~\ref{prop:indirect-orderable}, which states the following: if convex $L : \reals \to \reals^\Y$ indirectly elicits a finite elicitable property $\gamma$, then $\gamma$ is orderable.
\begin{proof}[of Proposition~\ref{prop:indirect-orderable}]
  Let $\gamma:\simplex\toto\R$.
  From Lemma~\ref{lem:prop-L-monotone}, $\prop{L}$ is monotone.
  Let $\psi:\reals\to\R$ be the calibrated link from $\prop{L}$ to $\gamma$.

  \raf{FROM IPAD NOTES}
\end{proof}

\begin{proof}[of Theorem~\ref{thm:1d-tfae}]
  As remarked before the theorem statement, it remains only to show $1 \iff 5$.
\end{proof}


\section{Considering unlabeled properties}
  Note that $\Gamma$ is elicitable, which means that there is a convex score $G:\simplex \to \reals$ representing the Bayes Risk (i.e. $G(p)$ is the minimal expected loss over all reports at $p$.)
  There is a set $\D \subseteq \partial_p G$, and we note that there is a bijection $\phi:\D \to \R$ so that the subgradients of $G$ correspond to the property value at the distribution $p$.

  The next two proofs take advantage of this duality correspondence between the subgradients of the convex score $G$ of an elicitable property and the property values.

\jessie{Lemma 34 below gets rid of the need for this statement.}
\begin{lemma}\label{lem:trim-full-dim}
  Consider a nondegenerate, elicitable property $\Gamma$ with finitely many level sets.
  Every $\theta \in \Theta := \trim(\Gamma)$ is full-dimensional.  
\end{lemma}
\begin{proof}
  Consider an arbitrary level set $\theta$.
  If $\theta$ is not maximal, we want to show that it must be the intersection of some maximal level sets in $\trim(\Gamma)$.\jessiet{Why?  Add more details about why the level set can't span two maximal level sets}
  If it is a proper subset of the level sets of which it is the intersection, and so it is not in the $\trim$ as it is in the subtracted set.

  Now suppose that a level set $\theta$ was not full-dimensional and not a proper subset of any one level set.
  It must then span multiple level sets, which contradicts elicitability of $\Gamma$ \jessiet{Does it though?}
\end{proof}


\begin{lemma}\label{lem:trim-subsets-not-maximal}
Let $\Gamma:\simplex \toto \R'$ be an elicitable property.
For any $u,w \in \R'$, if $\Gamma_u \subsetneq \Gamma_w$, then $\Gamma_u$ is not maximal.
\end{lemma}
\begin{proof}
  If there is a maximal level set $\Gamma_u$ that is a proper subset of $\Gamma_w$, then the corresponding scoring rule $G$ must not be differentiable (i.e. $|\partial_p G(p)| > 1$) for every distribution $p \in \Gamma_u$.
  However, closed, convex, maximal level sets have positive Lebesgue measure in $\reals^n$, and thus $G$ can not be differentiable on $\Gamma_u$.
  This contradicts the convexity of $G$, as convex functions are differentiable almost everywhere, which in turn contradicts the elicitability of $\Gamma$.
  Therefore, we conclude that $\Gamma_u$ is not maximal.
\end{proof}

\begin{lemma}\label{lem:finite-nonred-fdls}
Let $\Gamma:\simplex \toto \R'$ be a nonredundant, finite elicitable property.
Each level set $\Gamma_u \in \{\Gamma_u : u \in \R' \}$ is full-dimensional.
\end{lemma}
\begin{proof}
Let us consider 3 cases: first, we cannot observe a level set is a subset of or equal to another by nonredundancy of the property.
Now, if one level set spans the interiors of two different level sets, we will see that this property is not elicitable.
Since we assume $\Gamma$ is elicitable, we know the score $G$ is convex over $\simplex$.

Now, suppose there are two distributions $q,q'$ such that $q \in \inter{\Gamma_u}$ and $q' \in \inter{\Gamma_{u'}}$ for $u \neq u'$. 
Additionally, if there is a report $w$ such that $\conv(\{q,q'\}) \subseteq \Gamma_w$, we say that the level set $\Gamma_w$ spans the interior of two level sets.

First, consider that if $q \in \inter{\Gamma_u}$ and $q' \in \inter{\Gamma_{u'}}$ are two nonredundant points in the set $S$ such that $\Gamma_w = \conv(S)$, then there are distributions $p := q + \epsilon(q-q')$ and $p' = q' + \epsilon(q'-q)$ that are both in the interior of their respective level sets and $\Gamma(p) \cap \Gamma(p') = \emptyset$.
By \jessie{Raf and Ian 2018} Lemma 10, then for $\lambda \in (0,1)$, the score $G(\lambda p + (1-\lambda) p') < \lambda G(p) + (1-\lambda) G(p')$.
However, if $q$ and $q'$ share a subgradient, then the score $G$ is flat on $\conv(\{q,q'\})$, which is a subset of $\conv(\{p,p'\})$.
However, this contradicts \jessie{Raf and Ian 2018} Lemma 10, as the score $G$ is flat (and therefore the strict inequality is actually an equality) on $(a,b) \subseteq (0,1)$.

Now, if the level set $\Gamma_w$ is described as the convex hull of distributions $q \not \in \inter{\Gamma_u}$ or $q' \not \in \inter{\Gamma_{u'}}$, such distributions $p$ and $p'$ do not exist in the respective level sets.
\jessie{.... other argument} 

Therefore, for the elicitable property $\Gamma$, there cannot be a level set $\Gamma_w$ that spans the interiors of two other level sets.

\end{proof}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
