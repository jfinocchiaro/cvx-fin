\documentclass[11pt]{colt2019}
\usepackage[utf8]{inputenc}
\usepackage{mathtools, amsmath, amssymb, graphicx, verbatim}
%\usepackage[thmmarks, thref, amsthm]{ntheorem}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[colorinlistoftodos,textsize=tiny]{todonotes} % need xargs for below
%\usepackage{accents}
\usepackage{bbm}
\usepackage{xspace}

\newcommand{\Comments}{1}
\newcommand{\mynote}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\mytodo}[2]{\ifnum\Comments=1%
  \todo[linecolor=#1!80!black,backgroundcolor=#1,bordercolor=#1!80!black]{#2}\fi}
\newcommand{\raf}[1]{\mynote{green}{[RF: #1]}}
\newcommand{\raft}[1]{\mytodo{green!20!white}{RF: #1}}
\newcommand{\jessie}[1]{\mynote{purple}{[JF: #1]}}
\newcommand{\jessiet}[1]{\mytodo{purple!20!white}{JF: #1}}
\newcommand{\bo}[1]{\mynote{blue}{[Bo: #1]}}
\newcommand{\botodo}[1]{\mytodo{blue!20!white}{[Bo: #1]}}
\ifnum\Comments=1               % fix margins for todonotes
  \setlength{\marginparwidth}{1in}
\fi


\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}%{\reals_{++}}

\newcommand{\prop}[1]{\Gamma[#1]}
\newcommand{\eliccts}{\mathrm{elic}_\mathrm{cts}}
\newcommand{\eliccvx}{\mathrm{elic}_\mathrm{cvx}}
\newcommand{\elicpoly}{\mathrm{elic}_\mathrm{pcvx}}
\newcommand{\elicembed}{\mathrm{elic}_\mathrm{embed}}

\newcommand{\simplex}{\Delta_\Y}

% alphabetical order, by convention
\newcommand{\D}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}


\newcommand{\inter}[1]{\mathring{#1}}%\mathrm{int}(#1)}
%\newcommand{\expectedv}[3]{\overline{#1}(#2,#3)}
\newcommand{\expectedv}[3]{\E_{Y\sim{#3}} {#1}(#2,Y)}
\newcommand{\toto}{\rightrightarrows}
\newcommand{\strip}{\mathrm{strip}}
\newcommand{\trim}{\mathrm{trim}}
\newcommand{\fplc}{finite-piecewise-linear and convex\xspace} %xspace for use in text
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\ones}{\mathbbm{1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\sgn}{sgn}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{corollary}{Corollary}
%\newtheorem{conjecture}{Conjecture}


\title{Finite Property Convex Elicitation Paper}
\author{Jessie + Raf + Bo}

\begin{document}

\maketitle

\begin{abstract}
  Convex surrogates are sweet.
  Given a loss for a classification-like problem, there are two natural approaches to design convex surrogates.
  First, one may attempt to map each prediction to a low-dimensional vector, and try to find a convex loss in that space with the right calibration.
  Second, one may simply try to find a surrogate within the class of piecewise-linear convex, or polyhedral, losses.
  We show an equivalence between these two approaches, and \raf{more stuff}.
  We show that every loss with a finite number of predictions has a convex surrogate in the above sense using one fewer dimension that the number of outcomes, and give a full characterization of the losses needing only $d$ dimensions for such a surrogate.
  We then apply this characterization to show novel lower bounds for abstain loss, demonstrating the power of our techniques over \raf{check this} alternatives such as feasible subspace dimension.
\end{abstract}

\section{Setting and Background}

\subsection{Notation}
\begin{itemize}
	\item $\Y$ is the finite outcome space, and $n := |\Y|$.
	\item $\simplex\subseteq\reals^\Y$ is the set of distributions over $\Y$, thought of as vectors of probabilities.
	\item For an outcome $y$, we denote $p_y$ as the probability of outcome $y$ being observed in nature.
	\item If $\gamma = \prop{\ell}$, then $\gamma:\simplex \toto \R$ is the property elicited by the loss $\ell: \R \to \reals^\Y$.
	\item $d$ is the dimension of the embedding space
	\item $L:\reals^d \to \reals^\Y_+$ is a loss taking a report $u \in \reals^d$ and mapping the loss over each outcome in $\Y$. \raf{Changed to $\reals^\Y_+$ below.}
	\item $\Gamma: \simplex \toto \reals^d$ is the set-valued property mapping a probability distribution to a report $u \in \reals^d$.
	\item $\Gamma_u = \{ p \in \simplex : u \in \Gamma(p) \}$
	\item $\psi$ is the link function (see below)
	\item $\varphi:\R \to \reals^d$ is the embedding function
	\item $\trim(\Gamma)$ and $\strip(\Gamma)$
	\item $\eliccvx$, $\elicembed$, and $\elicpoly$
	\item $V$ and $B(P)$ as we get ready for later.
\end{itemize}

\subsection{Properties and Losses}
\begin{definition}
  Property $\Gamma:\simplex\toto\R$

  Level set $\Gamma_r$

\raf{Should bake non-degenerate into the definition: insist that $\Gamma(p) \neq \emptyset$ for all $p$.}
  
  Redundant, finite \raf{I think to avoid a TON of extra mentions of ``non-redundant'' we should take finite to mean finite and non-redundant}
\end{definition}

\begin{definition}
  \raf{I realized that (a) it is common to restrict losses to be non-negative, and (b) it solves some annoying issues where we have to say ``if $L$ is bounded below...'' nicely.  So unless any objections, let's say $L \geq 0$.}
  
  A loss $L:\R\to\reals^\Y_+$, elicits a property $\Gamma:\simplex \toto \R$ if, for all $p \in \simplex$ and $r \in \Gamma(p)$, we observe
  \begin{equation}
   = \argmin_r \E_{Y\sim p}[L(r,Y)]
  \end{equation}
  \jessie{Do we want regular elicitation vs essentially elicits...}
\end{definition}

\begin{definition}
  Polyhedral function, loss
\end{definition}

\subsection{Link Functions}

\begin{definition}
  Let properties $\gamma:\simplex\toto\R$ and $\Gamma:\simplex\toto\R'$ be given.
  A \emph{link function} is a map $\psi:\R'\to\R$.
  We say that a link $\psi$ is:
  \begin{itemize}
  \item \emph{Calibrated} if for all $p\in\simplex$, $r'\in \Gamma(p) \implies \psi(r') \in \gamma(p)$; in other words, if for all $r'\in\R'$, $\Gamma_{r'} \subseteq \gamma_{\psi(r')}$;
  \item \emph{Separated}, with respect to a loss $L$ eliciting $\Gamma$, if for all $p \in \simplex$, 
  \begin{align*}
  \inf_{u \in \R'; \psi(u) \not \in \gamma(p)} \E_{Y\sim p}[L(u, Y)] > \inf_{u \in \R'}\E_{Y\sim p}[L(u, Y)]~.
  \end{align*}
  \item \emph{Exhaustive} if for all $p\in\simplex$, $\gamma(p) = \psi(\Gamma(p)) := \{\psi(r') : r'\in\Gamma(p)\}$.
  \end{itemize}
\end{definition}

It is also interesting to contrast the definitions above with the following.
\begin{definition}
  A \emph{strong link} is a set-valued function $\psi:\R'\toto\R$ such that for all $p\in\simplex$ and $r'\in\R'$, we have $r'\in\Gamma(p) \implies \gamma(p) = \psi(r')$.
\end{definition}

\raf{We should note here that a discussion like this has not yet appeared in the property elicitation literature, since most papers address direct elicitation, where no link is needed, or single-valued properties in the case of indirect elicitation.  Cite examples for indirect: Frongillo--Kash (NIPS), Fissler--Ziegel (AoS), mode paper, others?}

\raf{Discussion about these definitions using 0-1 loss, hinge, and logistic.  Namely: sign is calibrated for hinge and logistic.  If sign is interpreted as set-valued (0 maps to $\{1,-1\}$, then it is strong for logistic.  If not, sign is not exhaustive for logistic.  Sign is exhaustive for hinge.  It is also separated, but something like $\psi(u) =
  \begin{cases}
    -1 & u \leq -1\\
    1 & u > -1
  \end{cases}$
  would not be, even though it would be calibrated.}


\subsection{Elicitation Complexity}

\begin{definition}
  A loss \emph{indirectly elicits} a property $\gamma$ if it elicits a property with a calibrated link to $\gamma$.
\end{definition}

\raf{Notation up for discussion}
$\eliccvx$ and $\elicpoly$; forward reference $\elicembed$

\subsection{Important Examples}
\raf{Mode, abstain, others?  Mention what is known, especially $\elicpoly($abstain$_{1/2}) \leq \log_2 n$.}

\section{Polyhedral Losses and Embeddings}

\begin{definition}
  A property $\Gamma : \simplex \toto \reals^d$ \emph{embeds} a property $\gamma : \simplex \toto \R$ if there exists some injective embdedding $\varphi:\R\to\reals^d$ such that for all $p\in\simplex,r\in\R$ we have $r \in \gamma(p) \iff \varphi(r) \in \Gamma(p)$.
  Similarly, we say a loss $L:\reals^d\to\reals^\Y$ embeds $\gamma$ if $\prop{L}$ embeds $\gamma$.
  Finally, we say $\gamma$ is \emph{$d$-embeddable}, or just \emph{embeddable}, if such an $L$ exists and is convex.
\end{definition}

\begin{definition}\label{def:trim}
  Let $\Gamma:\simplex \toto\R'$ be an elicitable property.
  We define $\trim(\Gamma)$ as the set of maximal\botodo{suggest ``maximal'' instead of ``full-dimensional''-- changed} level sets of $\Gamma$.
  Formally, $\trim(\Gamma) = \{\Gamma_u : u \in \R'\} \setminus \{ \Gamma_u : \exists w \in \R' \text{ s.t. } \Gamma_u \subset \Gamma_w \}$.
\end{definition}
\raf{Explain why this is a useful concept in general, and for convex elic}  
Looking at the $\trim$ of a property allows us to relate the finite properties we are interested in and the subtlely different surrogate properties that we actually elicit when we embed the original property into $\reals^d$.
In the surrogate property, there is often an infinite set of ``hidden'' level sets corresponding to the reports in the convex hull of embedded points.
Taking the $\trim$ thus allows us to remove labels so that we can relate the optimal sets of original reports and their embedded points, and then we remove these ``hidden'' level sets that are only optimal when we are indifferent between two of the embedded points.

\begin{proposition}\label{prop:embed-trim}
  Let $\Gamma:\simplex\toto\reals^d$ be an elicitable property.
  The following are equivalent:
  \begin{enumerate}
  \item $\Gamma$ embeds a finite elicitable property $\gamma:\simplex \toto \R$.
  \item $\trim(\Gamma)$ is a finite set.
  \item There is a finite set of maximal level sets $\Theta$ of $\Gamma$ that union to $\simplex$.
  \end{enumerate}
  Moreover, when any of the above hold, $\{\gamma_r : r\in\R\} = \trim(\Gamma) = \Theta$.
\end{proposition}
\begin{proof}
  For $1 \implies 2$, since $\Gamma$ embeds the finite property $\gamma$, we show that $\trim(\Gamma) = \{\Gamma_u : u \in \varphi(\R) \} = \{\gamma_r : r \in \R\}$.
  (Note that the second equality follows from the definition of embedding.)
  Clearly, $\trim(\Gamma) \supseteq \{\gamma_r : r \in \R\}$ as we are simply restricting the number of level sets we are indexing over.
  Now, consider some $\theta \in \trim(\Gamma)$ and the report $u$ so that $\theta = \Gamma_u$.
  If $u \in \varphi(\R)$, then clearly we are done.
  If $u \not\in \varphi(\R)$ but there is some report $w \in \varphi(\R)$ so that $\Gamma_u = \Gamma_w$, then we are again done since the level sets composing $\trim$ are unlabeled.
  If there is no such $w \in \R$, then we claim $\Gamma_u \not \in \trim(\Gamma)$.
  If $\Gamma_u$ is maximal, it must be unique up to relabeling\jessiet{skimmed over this, but follows from the convexity of the scoring rule... it's in detail in my writeup.  My thought for the paper is to put that proof in the appendix,and just have a footnote or side note here with a reference to the proof in the appendix.}, giving us one of the first two cases or a contradiction of elicitability.
  If $\Gamma_u$ is not maximal, it must be a subset of some maximal level set\jessiet{also in my notes... going to suggest the same footnote/appendix proof- like the other note.}, and therefore not in $\trim(\Gamma)$.
  Thus, we conclude $\trim(\Gamma) \subseteq \{\gamma_r : r \in \R\}$ and since the sets are equal, can conclude that if $\Gamma$ embeds the finite property $\gamma$, then $\trim(\Gamma)$ is finite.
  
  For $2 \implies 3$, since each level set $\theta \in \trim(\Gamma)$ is maximal, suppose for now that $\Theta := \trim(\Gamma)$.
  %As $\Gamma$ is nondegenerate, for every $p \in \simplex$, there must be some report $u$ corresponding to the level set such that $p \in \Gamma_u$.
  Because $\Gamma$ is nondegenerate, for all $p \in \simplex$, there is a level set $\theta \in \trim(\Gamma)$ such that $p \in \theta$, since the level sets of elicitable properties are closed and convex, and only non-maximal level sets are removed in taking $\trim(\Gamma)$.
  If, for some $p\in \simplex$, the level set $\Gamma_u$ corresponding to $p$ was removed, it would be because there is a level set corresponding to $w \in \reals^d$ such that $\Gamma_u \subset \Gamma_w$, but then $p \in \Gamma_w$.
  
  For $3 \implies 1$, take the set $\R := \{u : \Gamma_u \in\trim(\Gamma)\}$.
  (Take only one such report per level set.)
  The set $\R$ is then finite, and taking $\varphi$ to be the identity, we observe that $\Gamma$ embeds $\gamma: p \mapsto \Gamma(p) \cap \R$.
  We can then see $\gamma$ is elicitable by the loss $L|_{\R}$, where $L$ is the convex surrogate eliciting $\Gamma$.
\end{proof}

% \begin{proof}
% 	We proceed in a cycle to show $1 \implies 2 \implies 3 \implies 1 $.
% 	\begin{enumerate}
% 		\item [$1 \implies 2$]
% 		We know $\strip(\gamma) = \trim(\Gamma)$ (since $\varphi$ is an injection, and there is some intermediate property $\Gamma'$ such that $\Gamma' \preceq \Gamma$ and with a bijection $\phi:\Gamma'(p) \mapsto \gamma(p)$, so $\strip(\Gamma') = \strip(\gamma)$.) 
% 		As $\gamma$ is finite, so is $\strip(\gamma)$ since strip is generated from the (finite) report set.
		
% 		\item [$2 \implies 3$]
% 		Since $\Gamma$ is nondegenerate and every $\theta \in \trim(\Gamma)$ is full dimensional, it is clear that the union of the level sets of $\trim(\Gamma)$ union to $\simplex$ by nondegneracy of $\Gamma$.
		
% 		\item[$3 \implies 1$]
% 		Construct $\R := \{[u] \in \Gamma(p) : p \in \inter{\theta} \}_{\theta \in \Theta}$.
% 		The property $\gamma: p \mapsto \Gamma(p) \cap\R$ is then finite as $\R$ is finite.
% 		Taking $\varphi$ to be the identity, $\Gamma$ embeds this finite $\gamma$.
% 	\end{enumerate} 
% \end{proof}

\begin{proposition}\label{prop:embed-link}
  If $\Gamma$ embeds a finite property $\gamma$, then it has a calibrated link $\psi$ to $\gamma$.
  Moreover, if a polyhedral loss $L$ elicits $\Gamma$, $\psi$ can be taken to be separated with respect to $L$.
\end{proposition}
\begin{proof}
  %\raf{I think you mean $\varphi^{-1}(u)$, but that is only defined on $\varphi(\R)$ (the embedded points).  Use Proposition~\ref{prop:embed-trim}, specifically trim.}
  Since the finite property $\gamma$ is embedded by $\Gamma$, we know that $\trim(\Gamma)$ is finite by Proposition~\ref{prop:embed-trim}, as is $\varphi(\R)$.
  Therefore, for every $u \in \reals^d$, consider two cases.
  First, if $u \in \varphi(\R)$, then $\psi(u) = \varphi^{-1}(u)$ suffices.
  Now, if $u \not\in\varphi(\R)$, then there is an equivalent report $w \in \varphi(\R)$ so that $\Gamma_u \subset \Gamma_w$.
  In such cases, construct $\psi(u) := \varphi^{-1}(w)$.
  If there is more than one equivalent report, choose one arbitrarily.
  Now, for any $p \in \simplex$ such that $u \in \Gamma(p)$, we also have that $w \in \Gamma(p)$, so $\psi(u) = \phi^{-1}(w) \in \gamma(p)$.

  For the second part of the statement, in order for $\psi$ to be separated, for $\epsilon > 0$, for each embedded point $u \in \reals^d$ and every $v \in B(\epsilon, u)$, we must observe $\psi(v) = \psi(u)$.
  If the loss $L$ is polyhedral, we want to show that we can construct a $\psi$ so that the above holds.
  
  First, consider that each of the embedded points in $u \in \varphi(\R)$ must have ball $B(\epsilon, u)$ centered on $u$ so that no other $w \in \varphi(\R)$ is in $B(\epsilon, u)$. 
  If this is not true, then \jessie{...?} \jessiet{I'm not sure how this messses with things, but I would think the level sets corresponding to the two points would differ on a set of zero measure, if at all, or both be in the $\arginf$of the loss...?}

  Now for any $u \in \varphi(\R)$ and $v \in B(\epsilon, u)$, if $\psi(v)$ does not map to $\psi(u)$, then there must be some other $w \in \varphi(\R)$ so that $\psi(v) = \psi(w)$.
  Since $L$ is polyhedral, we know that for $p \in \simplex$ such that $v \in \Gamma(p)$, then since $u$ and $v$ are sufficiently close, $\E_p L(w,Y) = \E_p L(v,Y) \leq \E_p L(u,Y)$.
  However, if the inequality was strict, then we contradict the polyhedral structure of $L$, since $v$ is not an embedded point.
  Therefore, $v \in \Gamma(p) \implies u \in \Gamma(p)$, and we can alter the link function so that $\psi(v) = \psi(u)$.
%If no such $\psi$ existed, then for some $p$ such that $u \in \arginf_{z \in \R} \E_p L(z,Y)$, we know there is a $\delta > 0$ such that $\E_p L(v,Y) - \E_p L(u,Y) < \delta$ by continuity of $L$.
%Additionally, if $\psi$ was not selective, then we know $\psi(v) = \psi(w)$ for some embedded point $w \neq u$.
%Since the losses of $u$ and $v$ are not bounded about away from each other and $\psi(v) = \psi(w)$, then the loss $L$ must be constant on $\conv(\{u,v,w\})$, otherwise we would either contradict elicitability of $\Gamma$ or the construction of the calibrated link $\psi$.
%Therefore, we can reassign $\psi(v) = \psi(u)$ for any $v\in B(\epsilon, u)$, and such a link additionally separated.
\end{proof}

\begin{theorem}
  Let $\gamma$ be a finite property.
  The following are equivalent.
  \begin{enumerate}
  \item $\gamma$ is elicitable.
  \item $\gamma$ is embeddable.
  \item $\gamma$ is embeddable via a polyhedral loss.
  \end{enumerate}
\end{theorem}
\raf{Also mention here that \emph{every} non-redundant elicitable property is $(n-1)$-embeddable.}

\begin{definition}
  Let $\Gamma:\simplex\toto\R$ and $\Gamma':\simplex\toto\R'$.
  Then $\Gamma'$ \emph{refines} $\Gamma$ if for all $r'\in\R'$ we have $\Gamma'_{r'} \subseteq \Gamma_r$ for some $r\in\R$.
  That is, the cells of $\Gamma'$ are all contained in cells of $\Gamma$.
\end{definition}

\begin{theorem}\label{thm:polyhedral-embed}
  Every polyhedral loss embeds a finite elicitable property.
  Moreover, a polyhedral loss indirectly elicits a finite elicitable property $\gamma$ if and only if it embeds a property which refines $\gamma$.
\end{theorem}

\raf{Transition: wait, what about dimension?}

\section{Embedding Dimension}

\begin{definition}
  The \emph{embedding dimension} of a finite property $\gamma$, written $\elicembed(\gamma)$, is the minimum $d\geq 0$ \jessiet{$d \geq 1?$}\raft{Technically constant properties $\gamma(p) = \{c\} \forall p$ would have complexity 0.  Not terribly interesting... but it would also feel weird to call that complexity $1$.  Hmm.} such that $\gamma$ is $d$-embeddable.
\end{definition}

\subsection{Dimension 1: real-valued losses}

\raf{From Lambert et al. (2009) -- or maybe this is only in his unpublished paper; need to check}
\begin{definition}
  A finite property $\gamma:\simplex\toto\R$ is \emph{orderable} if there is an enumeration $\R = \{r_1,\ldots,r_k\}$ such that for all $i\in\{1,\ldots,k-1\}$, the intersection $\gamma_{r_i} \cap \gamma_{r_{i+1}}$ is a hyperplane intersected with $\simplex$.
\end{definition}

Lambert et al. shows that a finite property has a \emph{order-sensitive} loss, meaning one assigning higher loss to ``farther'' reports (with respect to a given order), if and only if it is orderable.
We will see that, in fact, a finite property is 1-embeddable if and only if it is orderable.
First, the forward direction.

\begin{proposition}\label{prop:orderable-embed}
  Every orderable property $\gamma$ is 1-embeddable.
\end{proposition}
\begin{proof}
  Let $\R = \{r_1,\ldots,r_k\}$ be the report space for $\gamma$.
  From Lemma~\ref{lem:prop-L-monotone}, and its proof, we have $a:\R\to\reals^\Y, b\to\reals^\Y$ such that $\gamma_r = \{p\in\simplex : a(r) \cdot p \leq 0 \leq b(r) \cdot p\}$ for all $r\in\R$, and for all $i \in \{1,\ldots,k-1\}$, we have $a(r_i) \leq b(r_i) = a(r_{i+1}) \leq b(r_{i+1})$.
  We now define $\varphi(r_i) = i \in \reals$, and $L$... \raf{integrate?  this should be simple... want $\partial L(i)_y = [a(r_i)_y,b(r_i)_y]$.}
\end{proof}

The converse of Proposition~\ref{prop:orderable-embed} is a corollary of the following result, together with Theorem~\ref{thm:polyhedral-embed} (polyhedral losses embed a finite elicitable property) and Proposition~\ref{prop:embed-link} (embeddings give rise to calibrated links).
For a proof, see Appendix~\ref{app:dimension-1}.
\begin{proposition}\label{prop:indirect-orderable}
  If convex $L : \reals \to \reals^\Y$ indirectly elicits a finite elicitable property $\gamma$, then $\gamma$ is orderable.
\end{proposition}

\begin{corollary}\label{cor:embed-orderable}
  Every polyhedral $L : \reals \to \reals^\Y$ embeds an orderable property.
\end{corollary}


Combining these above observations with a few others, we arrive at the following result, proven in Appendix~\ref{app:dimension-1}, which states that several elicitation complexity classes collapse in dimension 1.
\begin{theorem}
  Let $\gamma$ be a finite elicitable property.
  The following are equivalent.
  \begin{enumerate}
  \item $\gamma$ is orderable.
  \item $\elicembed(\gamma)=1$. ($\gamma$ is 1-embeddable.)
  \item $\elicpoly(\gamma)=1$. ($\gamma$ is indirectly elicitable via a polyhedral loss $L:\reals\to\reals^\Y$.)
  \item $\eliccvx(\gamma)=1$. ($\gamma$ is indirectly elicitable via a convex loss $L:\reals\to\reals^\Y$.)
  \item $\eliccts(\gamma)=1$. ($\gamma$ is indirectly elicitable via a real-valued continuous non-locally-constant property.)
  \end{enumerate}
\end{theorem}

\subsection{General dimensions}

\raf{Use orderable to motivate}

\raf{The full characterization; optimality and monotonicity:}
\begin{theorem}
  A finite property $\Gamma:\simplex\toto\R$ is $d$-embeddable if and only if there exists an embedding $\varphi: \R \to \reals^d$ and polytopes $T(r,y) \subseteq \reals^d$ for all $r\in\R, y\in\Y$, such that the following two conditions hold:
  \begin{enumerate}
  \item For all $r\in\R, p\in\simplex$, we have $r\in \Gamma(p) \iff 0 \in \sum_{y\in\Y} p_y T(r,y)$, where summation is the Minkowski sum.
  \item For all $y\in\Y$ there exists some convex function $L_y$ such that for all $r\in\R$ we have $T(r,y) = \partial L_y(\varphi(r))$.
  \end{enumerate}
\end{theorem}

\subsection{Useful necessary conditions}

\begin{corollary}
  \raf{Optimality: Bo's polytope stuff}
\end{corollary}

\begin{corollary}
  \raf{Monotonicity: WMON}
\end{corollary}

\begin{corollary}
  \raf{reduction to 1 dimension}
\end{corollary}

\section{Application: Abstain Loss}

\begin{theorem}
  Abstain with $\alpha=1/2$ and $|\Y|=5$ is not $2$-embeddable.
\end{theorem}

\begin{theorem}
  Abstain with $\alpha > 1/2$ and $|\Y|=7$ is not $2$-embeddable.
\end{theorem}

\begin{conjecture}
  Abstain with $\alpha=1/2$ is $(\log_2 |\Y|)$-embeddable, but no lower.
\end{conjecture}

\section{Conclusion and Future Work}

Let $\mathrm{elic}_{embed}$ denote the elicitation complexity with respect to embeddings, $\mathrm{elic}_{Pcvx}$ the complexity with respect to polyhedral convex losses, and $\mathrm{elic}_{cvx}$ the complexity with respect to arbitrary convex losses.

\begin{conjecture}
  $\mathrm{elic}_{embed}(\Gamma) = \mathrm{elic}_{Pcvx}(\Gamma) = \mathrm{elic}_{cvx}(\Gamma)$ for all finite, elicitable properties $\Gamma$.
\end{conjecture}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Directions}  \label{sec:conclusion}
\paragraph{Summary.}
This work forms part of a broader research program to understand the behaviors of loss functions, surrogate losses, link functions connecting the two, and the properties of distributions that are elicited by these losses.
In particular, the goal is a general theory that, given a property, can understand when and how to construct ``nice'' surrogate losses that elicit it and what notions of ``nice'' should be used.

This work formalized the \emph{embedding} approach where labels are identified with points in $\reals^d$.
We saw \bo{in theorem ??} that this approach is tightly connected to the use of polyhedral (i.e. piecewise linear convex) loss functions.
We also saw that it is a general technique that can be used for any finite elicitable property.

We then investigated the \emph{dimensionality} of $\reals^d$ required for such embeddings, giving a characterization in terms of the structure of the property in the simplex.
This gave a complete understanding of the one-dimensional case and a set of necessary and sufficient conditions in higher dimensions.
The two key conditions are an optimality condition that relates the structure of each level set to existence of polytopes in $\reals^d$ satisfying certain conditions; and a monotonicity condition relating such polytopes for different level sets.
This yields new lower bounds in particular for the abstain loss.

\paragraph{Directions.}
There are several direct open questions involving embedding dimension.
It would be interesting and perhaps practically useful to develop further techniques for upper bounds: automatically constructing embeddings in $\reals^d$ and associated polyhedral losses from a given property, with $d$ as small as possible.
Another direction is additional lower bound techniques, or further development of our necessary conditions.

This paper also suggests an agenda of defining more nuanced classes of surrogate losses and studying their properties.
For example, a tangential topic in this work was the characteristics of a ``good'' link function; formalizing and exploring this question is an exciting direction.
We would also like to move toward a full understanding of the differences between these classes.
For example, how does embedding dimension compare in general to convex elicitation diemsnion (the dimensionality $d$ required of \emph{any} convex surrogate loss)?
These questions have both theoretical interest and potential practical significance.

\subsection*{Acknowledgements}
We thank Peter Bartlett for several discussions early on, which led to a proof of \raf{1-d reduction} among other insights.

\bibliography{diss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{Dimension 1}\label{app:dimension-1}

\begin{definition}
  A property $\Gamma:\simplex\to\R$ is \emph{monotone} if there are maps $a:\R\to\reals^\Y$, $b:\R\to\reals^\Y$ and a total ordering $<$ of $\R$ such that the following two conditions hold.
  \begin{enumerate}
  \item For all $r\in\R$, we have $\Gamma_r = \{p\in\simplex : a(r) \cdot p \leq 0 \leq b(r) \cdot p\}$.
  \item For all $r < r'$, we have $a(r) \leq b(r) \leq a(r') \leq b(r')$ (component-wise).
  % \item For all $r,r'\in\R$ and $p\in\Gamma_{r'}\setminus\Gamma_r$, we have $b(r) \cdot p < 0 \implies r' > r$ and $a(r) \cdot p > 0 \implies r' < r$.
  \end{enumerate}
\end{definition}

\begin{lemma}\label{lem:orderable-monotone}
  A finite property is orderable if and only if it is monotone.
\end{lemma}
\begin{proof}
  Let $\gamma:\simplex\toto\R$ be finite and orderable.
  For $1 \leq i < k$, let $v_i\in\reals^\Y$ such that $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : v_i\cdot p = 0\}$.
  As we assume $\gamma$ is non-redundant, we have $p^1 \in \inter\gamma_{r_1}$.
  Without loss of generality, we take the $v_i$ above so that $v_i \cdot p^1 > 0$ for all $i$.
  Now let $b(1) = v_1$ and $a(1) = -\max_y |b(1)_y|\ones$.
  \raf{Really not sure if that is the right start.}
  By our assumptions, $p \in \gamma_1 \iff b(1) \cdot p \geq 0$.
  \raf{Show how to make them monotone... I remember doing this for the continuous convex paper...}

  Conversely, suppose $\gamma:\simplex\toto\R$ be finite and monotone.
  Then we can use the total ordering of $\R$ to write $\R = \{r_1,\ldots,r_k\}$ such that $r_i < r_{i+1}$ for all $i \in \{1,\ldots,k-1\}$.
  We now have $\gamma_{r_i} \cap \gamma_{r_{i+1}} = \{p\in\simplex : a(r_{i+1}) \cdot p \leq 0 \leq b(r_i) \cdot p\}$.
  If this intersection is empty, then there must be some $p$ with $b(r_i) \cdot p < 0$ and $a(r_{i+1}) \cdot p > 0$; by monotonicity, no earlier or later reports can be in $\gamma(p)$, so we see that $\gamma(p) = \emptyset$, a contradiction.
  Thus the intersection is nonempty, and as we also know $b(r_i) \leq a(r_{i+1})$ we conclude $b(r_i) = a(r_{i+1})$, and the intersection is the hyperplane defined by $b(r_i) = a(r_{i+1})$.
\end{proof}

\begin{lemma}\label{lem:prop-L-monotone}
  For any convex loss $L : \reals \to \reals^\Y$, $\prop{L}$ is monotone.
\end{lemma}
\begin{proof}
  Let $a,b$ be defined by $a(r)_y = \partial_- L(r)_y$ and $b(r) = \partial_+ L(r)_y$, that is, the left and right derivatives of $L(\cdot)_y$ at $r$, respectively.
  Then $\partial L(r)_y = [a(r)_y,b(r)_y]$.
  We now have $r \in \prop{L}(p) \iff 0 \in \partial p\cdot L(r) \iff a(r)\cdot p \leq 0 \leq b(r) \cdot p$, showing the first condition.
  The second condition follows as the subgradients of $L$ are monotone functions.
  \raf{More detail and some Rockafellar cites}
\end{proof}

We now prove Proposition~\ref{prop:indirect-orderable}, which states the following: if convex $L : \reals \to \reals^\Y$ indirectly elicits a finite elicitable property $\gamma$, then $\gamma$ is orderable.
\begin{proof}[of Proposition~\ref{prop:indirect-orderable}]
  Let $\gamma:\simplex\toto\R$.
  From Lemma~\ref{lem:prop-L-monotone}, $\prop{L}$ is monotone.
  Let $\psi:\reals\to\R$ be the calibrated link from $\prop{L}$ to $\gamma$.

  \raf{FROM IPAD NOTES}
\end{proof}

\subsection*{For Proposition 9}
\begin{lemma}\label{lem:trim-full-dim}
  Consider a nondegenerate, elicitable property $\Gamma$ with finitely many level sets.
  Every $\theta \in \Theta := \trim(\Gamma)$ is full-dimensional  
\end{lemma}
\begin{proof}
  \jessiet{TODO}
\end{proof}


\begin{lemma}
Let $\Gamma:\simplex \toto \R'$ be an elicitable property.
For any $u,w \in \R'$, if $\Gamma_u \subsetneq \Gamma_w$, then $\Gamma_u$ is not maximal.
\end{lemma}
\begin{proof}
  \jessiet{TODO}
\end{proof}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
